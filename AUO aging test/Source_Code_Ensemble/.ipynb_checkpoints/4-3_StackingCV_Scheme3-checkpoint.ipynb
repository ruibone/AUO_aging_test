{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:12:24.287373Z",
     "start_time": "2022-05-25T04:12:20.586972Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import random\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor,\\\n",
    "    AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio, train_col\n",
    "from library.Imbalance_Sampling import label_divide, resampling_dataset\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, cf_matrix, AUC, PR_curve, \\\n",
    "     multiple_curve, PR_matrix, best_threshold, all_optuna, optuna_history, AdaBoost_creator \n",
    "from library.XGBoost import XGBoost_creator\n",
    "from library.LightGBM import LightGBM_creator\n",
    "from library.CatBoost import CatBoost_creator\n",
    "from library.RandomForest import RandomForest_creator\n",
    "from library.ExtraTrees import ExtraTrees_creator\n",
    "from library.NeuralNetwork import RunhistSet, NeuralNetworkC, trainingC, NeuralNetwork_creator\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:12:24.443032Z",
     "start_time": "2022-05-25T04:12:24.429034Z"
    }
   },
   "outputs": [],
   "source": [
    "# load hyperparameters of base learners finished by scheme 2 (for training data transformation)\n",
    "def month_param(date, month_list, model_list, iter_dict, filename, mode, TPE_multi):\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    month_dict = {}\n",
    "    for month in month_list:\n",
    "        \n",
    "        model_dict = {}\n",
    "        for model in model_list:\n",
    "                \n",
    "            with open(f'hyperparameter/{date}/{filename}_m{month}_{model}{mode}_{sampler}_{iter_dict[model]}.data', 'rb') as f:\n",
    "                model_param = pickle.load(f)\n",
    "            model_dict[model] = model_param\n",
    "        \n",
    "        month_dict[f'm{month}'] = model_dict\n",
    "\n",
    "    return month_dict\n",
    "\n",
    "\n",
    "# (optional) load hyperparameters of base learners finished by scheme 1 (for testing data transformation)\n",
    "def all_param(date, model_list, iter_dict, filename, mode, TPE_multi):\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    model_dict = {}\n",
    "    for model in model_list:\n",
    "\n",
    "        with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_dict[model]}.data', 'rb') as f:\n",
    "                set_dict = pickle.load(f)           \n",
    "        model_dict[model] = set_dict\n",
    "    \n",
    "    done_dict = dict(all = model_dict)\n",
    "        \n",
    "    return done_dict\n",
    "\n",
    "\n",
    "# search for the best hyperparameters of base learners\n",
    "def optimize_base(train_data, mode, TPE_multi, base_list, iter_dict, filename):\n",
    "    \n",
    "    best_param = {}\n",
    "    month_list = list(train_data.keys())\n",
    "    available_model = ['AdaBoost', 'XGBoost', 'LightGBM', 'CatBoost', 'RandomForest', 'ExtraTrees', 'NeuralNetwork']\n",
    "    \n",
    "    for i in tqdm(month_list):\n",
    "        \n",
    "        best_param[f'{i}'] = {}\n",
    "        for model in base_list:\n",
    "            \n",
    "            if model not in available_model:\n",
    "                raise('Invalid Model !')\n",
    "            elif model == available_model[0]:\n",
    "                creator = AdaBoost_creator\n",
    "            elif model == available_model[1]:\n",
    "                creator = XGBoost_creator\n",
    "            elif model == available_model[2]:\n",
    "                creator = LightGBM_creator\n",
    "            elif model == available_model[3]:\n",
    "                creator = CatBoost_creator\n",
    "            elif model == available_model[4]:\n",
    "                creator = RandomForest_creator\n",
    "            elif model == available_model[5]:\n",
    "                creator = ExtraTrees_creator\n",
    "            elif model == available_model[6]:\n",
    "                creator = NeuralNetwork_creator\n",
    "            print(f'\\nStarting for {model}:\\n')\n",
    "        \n",
    "            best_param[f'{i}'][model], _ = all_optuna(all_data = train_data[f'{i}'], \n",
    "                                                      mode = mode, \n",
    "                                                      TPE_multi = TPE_multi, \n",
    "                                                      n_iter = iter_dict[model],\n",
    "                                                      filename = f'{filename}_{i}_{model}',\n",
    "                                                      creator = creator)\n",
    "            \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data by Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:12:24.819814Z",
     "start_time": "2022-05-25T04:12:24.663731Z"
    }
   },
   "outputs": [],
   "source": [
    "# concept of strtified cross-validation\n",
    "def stratified_data(train_data, cv):\n",
    "    \n",
    "    good = train_data[train_data.GB == 0]\n",
    "    bad = train_data[train_data.GB == 1]\n",
    "    good_index = random.sample(good.index.to_list(), k = len(good))\n",
    "    bad_index = random.sample(bad.index.to_list(), k = len(bad))\n",
    "    \n",
    "    train_x_dict = {}\n",
    "    train_y_dict = {}\n",
    "    valid_x_dict = {}\n",
    "    valid_y_dict = {}\n",
    "    for i in range(cv):\n",
    "        \n",
    "        if (i+1) == cv:\n",
    "            good_valid_index = good_index[int(np.floor((i/cv)*len(good))): ]\n",
    "            bad_valid_index = bad_index[int(np.floor((i/cv)*len(bad))): ]\n",
    "        else:\n",
    "            good_valid_index = good_index[int(np.floor((i/cv)*len(good))) : int(np.floor(((i+1)/cv)*len(good)))]\n",
    "            bad_valid_index = bad_index[int(np.floor((i/cv)*len(bad))) : int(np.floor(((i+1)/cv)*len(bad)))]\n",
    "        good_train_index = [x for x in good_index if x not in good_valid_index]\n",
    "        bad_train_index = [x for x in bad_index if x not in bad_valid_index]\n",
    "        \n",
    "        good_train = good.loc[good_train_index]\n",
    "        good_valid = good.loc[good_valid_index]\n",
    "        bad_train = bad.loc[bad_train_index]\n",
    "        bad_valid = bad.loc[bad_valid_index]\n",
    "        train = pd.concat([good_train, bad_train], axis = 0)\n",
    "        valid = pd.concat([good_valid, bad_valid], axis = 0)\n",
    "        train_x_dict[i], train_y_dict[i], valid_x_dict[i], valid_y_dict[i] = label_divide(train, valid, train_only = False)\n",
    "\n",
    "    return train_x_dict, train_y_dict, valid_x_dict, valid_y_dict\n",
    "\n",
    "\n",
    "# input training data to the base learners and output the outcome\n",
    "def transform_train(train_data, mode, base_param, cv, add_origin = False):\n",
    "    \n",
    "    month_list = list(base_param.keys())\n",
    "    model_list = list(base_param[month_list[0]].keys())\n",
    "    set_list = list(base_param[month_list[0]][model_list[0]].keys())\n",
    "    set_dict = {}\n",
    "    for x in set_list:\n",
    "        set_dict[x] = pd.DataFrame()\n",
    "        \n",
    "    for month in tqdm(month_list):\n",
    "        for i in tqdm(set_list):\n",
    "            \n",
    "            train_x_dict, train_y_dict, valid_x_dict, valid_y_dict = stratified_data(train_data[month][i], cv = cv)\n",
    "            all_cv = pd.DataFrame()\n",
    "            for j in range(cv):\n",
    "            \n",
    "                model_predict = pd.DataFrame()            \n",
    "                if mode == 'C':\n",
    "                    \n",
    "                    if 'NeuralNetwork' in model_list:\n",
    "                        temp_train = RunhistSet(train_x_dict[j], train_y_dict[j])\n",
    "                        temp_valid = RunhistSet(valid_x_dict[j], valid_y_dict[j])\n",
    "                        train_loader = DataLoader(temp_train, \n",
    "                                                  batch_size = base_param[month]['NeuralNetwork'][i]['batch_size'], \n",
    "                                                  shuffle = True)\n",
    "                        valid_loader = DataLoader(temp_valid, batch_size = len(valid_x_dict[j]), shuffle = False)\n",
    "                        nn_model = NeuralNetworkC(dim = train_x_dict[j].shape[1])\n",
    "                        optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                                     lr = base_param[month]['NeuralNetwork'][i]['learning_rate'], \n",
    "                                                     weight_decay = base_param[month]['NeuralNetwork'][i]['weight_decay'])\n",
    "                        criterion = nn.CrossEntropyLoss(\n",
    "                            weight = torch.tensor([1-base_param[month]['NeuralNetwork'][i]['bad_weight'], \n",
    "                                                   base_param[month]['NeuralNetwork'][i]['bad_weight']])).to('cpu')\n",
    "                        network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 100, \n",
    "                                                  early_stop = 10)\n",
    "                        \n",
    "                        for x, y in valid_loader:\n",
    "                            output = network(x)\n",
    "                            predict_y = output.data[:, 1]\n",
    "                        predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'XGBoost' in model_list:                     \n",
    "                        clf = XGBClassifier(**base_param[month]['XGBoost'][i], use_label_encoder = False, n_jobs = -1)\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'LightGBM' in model_list:                        \n",
    "                        clf = LGBMClassifier(**base_param[month]['LightGBM'][i])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'CatBoost' in model_list:\n",
    "                        clf = CatBoostClassifier(**base_param[month]['CatBoost'][i])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'AdaBoost' in model_list:\n",
    "                        tree_param = {\n",
    "                            'base_estimator': DecisionTreeClassifier(\n",
    "                                max_depth = base_param[month]['AdaBoost'][i]['max_depth']\n",
    "                            )}\n",
    "                        boost_param = dict(\n",
    "                            (key, base_param[month]['AdaBoost'][i][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                        )\n",
    "                        boost_param.update(tree_param)\n",
    "                        clf = AdaBoostClassifier(**boost_param)\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'RandomForest' in model_list:\n",
    "                        clf = RandomForestClassifier(**base_param[month]['RandomForest'][i])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'ExtraTrees' in model_list:\n",
    "                        clf = ExtraTreesClassifier(**base_param[month]['ExtraTrees'][i])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                elif mode == 'R':\n",
    "                    \n",
    "                    if 'XGBoost' in model_list:\n",
    "                        reg = XGBRegressor(**base_param[month]['XGBoost'][i], n_jobs = -1)\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'X': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'LightGBM' in model_list:\n",
    "                        reg = LGBMRegressor(**base_param[month]['LightGBM'][i])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'L': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'CatBoost' in model_list:\n",
    "                        reg = CatBoostRegressor(**base_param[month]['CatBoost'][i])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'C': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'AdaBoost' in model_list:\n",
    "                        tree_param = {\n",
    "                            'base_estimator': DecisionTreeRegressor(\n",
    "                                max_depth = base_param[month]['AdaBoost'][i]['max_depth']\n",
    "                            )}\n",
    "                        boost_param = dict(\n",
    "                            (key, base_param[month]['AdaBoost'][i][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                        )\n",
    "                        boost_param.update(tree_param)\n",
    "                        reg = AdaBoostRegressor(**boost_param)\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'A': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'RandomForest' in model_list:\n",
    "                        reg = RandomForestRegressor(**base_param[month]['RandomForest'][i])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'R': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                    \n",
    "                    if 'ExtraTrees' in model_list:\n",
    "                        reg = ExtraTreesRegressor(**base_param[month]['ExtraTrees'][i])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'E': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                test_label = valid_y_dict[j].reset_index(drop = True)\n",
    "                origin_feature = valid_x_dict[j].reset_index(drop = True)\n",
    "                if add_origin:\n",
    "                    done_cv = pd.concat([model_predict, origin_feature, test_label], axis = 1)\n",
    "                else:\n",
    "                    done_cv = pd.concat([model_predict, test_label], axis = 1)\n",
    "                \n",
    "                all_cv = pd.concat([all_cv, done_cv], axis = 0)\n",
    "            set_dict[i] = pd.concat([set_dict[i], all_cv], axis = 0).fillna(0)\n",
    "            all_column = set_dict[i].columns.to_list()\n",
    "            GB_index = all_column.index('GB')\n",
    "            all_column = all_column[: GB_index] + all_column[GB_index+1: ] + all_column[GB_index: GB_index+1]\n",
    "            set_dict[i] = set_dict[i][all_column]\n",
    "    \n",
    "    return set_dict\n",
    "\n",
    "\n",
    "# input testing data to the base learners and output the outcome\n",
    "def transform_test(train_data, test_data, mode, base_param, add_origin = False):\n",
    "    \n",
    "    month_list = list(base_param.keys())\n",
    "    model_list = list(base_param[month_list[0]].keys())\n",
    "    set_list = list(base_param[month_list[0]][model_list[0]].keys())\n",
    "    test_dict = {}\n",
    "    for i in tqdm(set_list):\n",
    "        \n",
    "        month_test = pd.DataFrame()\n",
    "        for month in month_list:\n",
    "\n",
    "            select_test = train_col(train_data[month][i], test_data)\n",
    "            train_x, train_y, test_x, test_y = label_divide(train_data[month][i], select_test, train_only = False)\n",
    "            model_predict = pd.DataFrame()\n",
    "            if mode == 'C':\n",
    "\n",
    "                if 'NeuralNetwork' in model_list:\n",
    "                    temp_train = RunhistSet(train_x, train_y)\n",
    "                    temp_test = RunhistSet(test_x, test_y)\n",
    "                    train_loader = DataLoader(temp_train, \n",
    "                                              batch_size = base_param[month]['NeuralNetwork'][i]['batch_size'], \n",
    "                                              shuffle = True)\n",
    "                    test_loader = DataLoader(temp_test, batch_size = len(test_x), shuffle = False)\n",
    "                    nn_model = NeuralNetworkC(dim = train_x.shape[1])\n",
    "                    optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                                 lr = base_param[month]['NeuralNetwork'][i]['learning_rate'], \n",
    "                                                 weight_decay = base_param[month]['NeuralNetwork'][i]['weight_decay'])\n",
    "                    criterion = nn.CrossEntropyLoss(\n",
    "                        weight = torch.tensor([1-base_param[month]['NeuralNetwork'][i]['bad_weight'], \n",
    "                                               base_param[month]['NeuralNetwork'][i]['bad_weight']])).to('cpu')\n",
    "                    network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 100, \n",
    "                                              early_stop = 10)\n",
    "                    for X, Y in test_loader:\n",
    "                        X, Y = X.float(), Y.long()\n",
    "                        output = network(X)\n",
    "                        predict_y = output.data[:, 1]\n",
    "                    predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                \n",
    "                if 'XGBoost' in model_list:\n",
    "                    clf = XGBClassifier(**base_param[month]['XGBoost'][i], use_label_encoder = False, n_jobs = -1)\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:\n",
    "                    clf = LGBMClassifier(**base_param[month]['LightGBM'][i])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    clf = CatBoostClassifier(**base_param[month]['CatBoost'][i])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeClassifier(\n",
    "                            max_depth = base_param[month]['AdaBoost'][i]['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[month]['AdaBoost'][i][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    clf = AdaBoostClassifier(**boost_param)\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    clf = RandomForestClassifier(**base_param[month]['RandomForest'][i])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    clf = ExtraTreesClassifier(**base_param[month]['ExtraTrees'][i])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            elif mode == 'R':\n",
    "\n",
    "                if 'XGBoost' in model_list:\n",
    "                    reg = XGBRegressor(**base_param[month]['XGBoost'][i], n_jobs = -1)\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'X': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:\n",
    "                    reg = LGBMRegressor(**base_param[month]['LightGBM'][i])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'L': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    reg = CatBoostRegressor(**base_param[month]['CatBoost'][i])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'C': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeRegressor(\n",
    "                            max_depth = base_param[month]['AdaBoost'][i]['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[month]['AdaBoost'][i][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    reg = AdaBoostRegressor(**boost_param)\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'A': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    reg = RandomForestRegressor(**base_param[month]['RandomForest'][i])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'R': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    reg = ExtraTreesRegressor(**base_param[month]['ExtraTrees'][i])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'E': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            month_test = pd.concat([month_test, model_predict], axis = 1)\n",
    "        if add_origin:\n",
    "            test_dict[i] = pd.concat([month_test, test_x, test_y], axis = 1)\n",
    "        else:\n",
    "            test_dict[i] = pd.concat([month_test, test_y], axis = 1)\n",
    "        \n",
    "    return test_dict\n",
    "\n",
    "\n",
    "# the testing data will be trnasformed by base learners of several months, so taking average of these probability is necessary\n",
    "def test_average(trans_test):\n",
    "    \n",
    "    set_list = list(trans_test.keys())\n",
    "    model_list = ['N', 'X', 'L', 'C', 'R', 'E']\n",
    "    new_test = dict()\n",
    "    for i in set_list:\n",
    "        label = trans_test[i]['GB']\n",
    "        new_test[i] = trans_test[i].iloc[:, :-1].copy()\n",
    "        for j in model_list:\n",
    "            if j in trans_test[i].columns:\n",
    "                temp_avg = trans_test[i][j].apply(np.mean, axis = 1)\n",
    "                del new_test[i][j]\n",
    "                new_test[i][j] = temp_avg\n",
    "            else:\n",
    "                continue\n",
    "        new_test[i]['GB'] = label\n",
    "    \n",
    "    return new_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:12:25.021786Z",
     "start_time": "2022-05-25T04:12:25.005701Z"
    }
   },
   "outputs": [],
   "source": [
    "# input training data transformed by base classifiers and output the final prediction \n",
    "def LR(train_x, test_x, train_y, test_y, config, return_prob = False):\n",
    "    \n",
    "    subconfig = config.copy()\n",
    "    del subconfig['meta_learner']\n",
    "    if config['meta_learner'] == 'LogisticRegression':\n",
    "        clf = LogisticRegression(**subconfig)\n",
    "        clf.fit(train_x, train_y)\n",
    "        coef = clf.coef_\n",
    "    elif config['meta_learner'] == 'ExtraTrees':\n",
    "        clf = ExtraTreesClassifier(**subconfig)\n",
    "        clf.fit(train_x, train_y)\n",
    "        coef = clf.feature_importances_\n",
    "    predict_y = clf.predict(test_x)\n",
    "    define_predict = (predict_y > 0.5).astype(int)\n",
    "    if return_prob:\n",
    "        result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    else:\n",
    "        result = pd.DataFrame({'truth': test_y, 'predict': define_predict})\n",
    "    \n",
    "    return result, coef\n",
    "\n",
    "\n",
    "# input training data transformed by base regressors and output the final prediction (optional)\n",
    "def RidgeR(train_x, test_x, train_y, test_y, config):\n",
    "    \n",
    "    reg = Ridge(**config)\n",
    "    reg.fit(train_x, train_y)\n",
    "    coef = reg.coef_\n",
    "    predict_y = reg.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result, coef\n",
    "\n",
    "\n",
    "# run all resampling datasets to the meta classifer\n",
    "def runall_LR(trainset_x, testset_x, trainset_y, testset_y, config):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    coef_set = pd.DataFrame()\n",
    "    set_index = list(config.keys())\n",
    "    judge = set_index[0]\n",
    "\n",
    "    for i, j in tqdm(enumerate(set_index)):\n",
    "        print('\\n', f'Data{j}:')\n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[j]\n",
    "        else :\n",
    "            best_config = config\n",
    "        model_name = trainset_x[j].columns.to_list()\n",
    "\n",
    "        result, coef = LR(trainset_x[j], testset_x[j], trainset_y[j], testset_y[j], \n",
    "                          best_config)\n",
    "        table = cf_matrix(result, trainset_y[j])\n",
    "        coef_df = pd.DataFrame({str(j): coef.flatten()})\n",
    "        table_set = pd.concat([table_set, table]).rename(index = {0: f'data{j}'})\n",
    "        coef_set = pd.concat([coef_set, coef_df], axis = 1)\n",
    "    coef_set.index = model_name\n",
    "    \n",
    "    return table_set, coef_set\n",
    "\n",
    "\n",
    "# run all resampling datasets to the meta regressor (optional)\n",
    "def runall_RidgeR(num_set, trainset_x, testset_x, trainset_y, testset_y, config, thres_target = 'Recall', \n",
    "                    threshold = False):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    coef_set = pd.DataFrame()\n",
    "    pr_dict = {}\n",
    "    judge = list(config.keys())[0]\n",
    "\n",
    "    for i in range(num_set):\n",
    "        print('\\n', f'Dataset {i}:')\n",
    "        \n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[f'set{i}']\n",
    "        else :\n",
    "            best_config = config\n",
    "        model_name = trainset_x[f'set{i}'].columns.to_list()\n",
    "\n",
    "        predict, coef = RidgeR(trainset_x[f'set{i}'], testset_x[f'set{i}'], trainset_y[f'set{i}'], testset_y[f'set{i}'], \n",
    "                           best_config)\n",
    "        pr_matrix = PR_matrix(predict, trainset_y[f'set{i}'])\n",
    "        pr_dict[f'set{i}'] = pr_matrix\n",
    "        \n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = thres_target, threshold = threshold)\n",
    "        table_set = pd.concat([table_set, best_data]).rename(index = {best_data.index.values[0]: f'dataset {i}'})\n",
    "        coef_df = pd.DataFrame({f'set{i}': coef.flatten()})\n",
    "        coef_set = pd.concat([coef_set, coef_df], axis = 1)\n",
    "    coef_set.index = model_name\n",
    "        \n",
    "    return pr_dict, table_set, coef_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T05:39:46.236974Z",
     "start_time": "2022-04-06T05:39:46.219022Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_plot(target_data):\n",
    "    \n",
    "    correlation = target_data.iloc[:, :-1].corr()\n",
    "    plot = sn.heatmap(correlation, annot = True, cmap = 'magma')\n",
    "    plot.set_title('Correlation Coefficient of Base Learner Outputs')\n",
    "    \n",
    "    return correlation \n",
    "    \n",
    "    \n",
    "def vif(target_data):\n",
    "    \n",
    "    corr = target_data.corr()\n",
    "    vif = round(corr / (1 - corr), 2)   \n",
    "    return vif\n",
    "\n",
    "\n",
    "def forest_importance(target_data, mode = 'C'):\n",
    "    \n",
    "    colname = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    if mode == 'C':\n",
    "        clf = RandomForestClassifier(n_estimators = 500)\n",
    "    elif mode == 'R':\n",
    "        clf = RandomForestRegressor(n_estimators = 500)\n",
    "    clf.fit(X, Y)\n",
    "    importance = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis = 0)\n",
    "    importances = pd.DataFrame(dict(importance = importance, std = std), index = colname)\n",
    "    importances = importances.sort_values('importance', ascending = True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.barh(importances.index, importances['importance'].values, color = 'darkgreen', \n",
    "             xerr = importances['std'].values, ecolor = 'limegreen')\n",
    "    plt.title('Feature Importance by RandomForest Node Split')\n",
    "    plt.xlabel('importance')\n",
    "    plt.ylabel('model')\n",
    "    \n",
    "    return importances\n",
    "    \n",
    "    \n",
    "def xgb_permutation(target_data, mode = 'C'):\n",
    "    \n",
    "    colnames = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "    if mode == 'C':\n",
    "        clf = XGBClassifier(n_estimators = 500)\n",
    "    elif mode == 'R':\n",
    "        clf = XGBRegressor(n_estimators = 500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    importance = permutation_importance(clf, X_test, y_test, n_repeats = 10, n_jobs = 2)\n",
    "    importances = pd.DataFrame(dict(importance = importance.importances_mean, std = importance.importances_std), \n",
    "                               index = colnames)\n",
    "    importances = importances.sort_values('importance', ascending = True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.barh(importances.index, importances['importance'].values, color = 'firebrick', \n",
    "             xerr = importances['std'].values, ecolor = 'coral')\n",
    "    plt.title('Feature Importance by LightGBM Permutation')\n",
    "    plt.xlabel('importance')\n",
    "    plt.ylabel('model')\n",
    "    \n",
    "    return importances\n",
    "\n",
    "\n",
    "#####!!!!! forest_shap can only run for regressor !!!!!#####\n",
    "def lgbm_shap(target_data, mode = 'C'):\n",
    "    \n",
    "    colnames = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    if mode == 'C':\n",
    "        d_train = lightgbm.Dataset(X, label = Y)\n",
    "        clf = lightgbm.train(dict(n_estimators = 300, objective = 'binary'), d_train, 500, verbose_eval = -1)\n",
    "    elif mode == 'R':\n",
    "        clf = LGBMRegressor(n_estimators = 500)\n",
    "        clf.fit(X, Y)\n",
    "    \n",
    "    explainer = shap.Explainer(clf)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    values = abs(shap_values[1]).mean(axis = 0)\n",
    "    shap_df = pd.DataFrame(dict(value = values), index = colnames).sort_values('value', ascending = True)\n",
    "    \n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X)\n",
    "    \n",
    "    return shap_df\n",
    "\n",
    "\n",
    "def GLM_coefficient(target_data, mode = 'C'):\n",
    "    \n",
    "    colnames = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    if mode == 'C':\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X, Y)\n",
    "        coefficient = abs(clf.coef_[0,:])\n",
    "    elif mode == 'R':\n",
    "        clf = Ridge()\n",
    "        clf.fit(X, Y)\n",
    "        coefficient = abs(clf.coef_)\n",
    "    coef_df = pd.DataFrame(dict(GLM = coefficient), index = colnames).sort_values('GLM', ascending = True)\n",
    "    \n",
    "    return coef_df\n",
    "\n",
    "\n",
    "#####!!!!! forest_shap can only run for regressor !!!!!#####\n",
    "def rank_importance(target_data, mode = 'C'):\n",
    "    \n",
    "    print(vif(target_data))\n",
    "    correlation = correlation_plot(target_data)\n",
    "    coefficient = GLM_coefficient(target_data, mode = mode).GLM\n",
    "    forest = forest_importance(target_data, mode = mode).importance\n",
    "    permutation = xgb_permutation(target_data, mode = mode).importance\n",
    "    shapvalue = lgbm_shap(target_data).value\n",
    "    rank_df = pd.DataFrame()\n",
    "    rank_df['GLM'] = coefficient.rank(ascending = False)\n",
    "    rank_df['forest'] = forest.rank(ascending = False)\n",
    "    rank_df['permutation'] = permutation.rank(ascending = False)\n",
    "    rank_df['SHAP'] = shapvalue.rank(ascending = False)\n",
    "    rank_df['total_rank'] = rank_df.apply(sum, axis = 1).rank()\n",
    "    rank_df = rank_df.sort_values('total_rank', ascending = True)\n",
    "    \n",
    "    return rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:12:26.802964Z",
     "start_time": "2022-05-25T04:12:26.788355Z"
    }
   },
   "outputs": [],
   "source": [
    "# creator of optuna study for all 3 schemes of stackingCV\n",
    "def stackingCV_creator(train_data, mode, learner = 'ExtraTrees', num_valid = 5, label = 'GB'):\n",
    "    \n",
    "    if learner not in ['LogisticRegression', 'ExtraTrees']:\n",
    "        raise(f'{learner} is not implemented !')\n",
    "    \n",
    "    def objective(trial):\n",
    "        \n",
    "        if mode == 'C':\n",
    "            meta_learner = {\n",
    "                'meta_learner': trial.suggest_categorical('meta_learner', [learner])\n",
    "            }\n",
    "            \n",
    "            if meta_learner['meta_learner'] == 'LogisticRegression':      \n",
    "                param = {\n",
    "                    'solver': 'lbfgs',\n",
    "                    'C': trial.suggest_categorical('C', [100, 10 ,1 ,0.1, 0.01]),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['none', 'l2']),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "\n",
    "            elif meta_learner['meta_learner'] == 'ExtraTrees':\n",
    "                param = {\n",
    "                    'n_estimators': trial.suggest_categorical('n_estimators', [100, 500, 1000]),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 24, step = 2),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 2, 4, step = 1),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "            \n",
    "            param.update(meta_learner)\n",
    "\n",
    "        elif mode == 'R':\n",
    "            meta_learner = 'RidgeCV'\n",
    "            \n",
    "            if meta_learner == 'RidgeCV':\n",
    "                param = {\n",
    "                    'alpha': trial.suggest_float('alpha', 0, 1, step = 0.1)\n",
    "                }\n",
    "            \n",
    "            elif meta_learner == 'Extra Trees':\n",
    "                param = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "        \n",
    "        # objective function\n",
    "        result_list = []\n",
    "        for i in range(num_valid):\n",
    "\n",
    "            train_good = train_data[train_data.GB == 0]\n",
    "            train_bad = train_data[train_data.GB == 1]\n",
    "            train_good_x, train_good_y = label_divide(train_good, None, label, train_only = True)\n",
    "            train_bad_x, train_bad_y = label_divide(train_bad, None, label, train_only = True)\n",
    "            train_g_x, valid_g_x, train_g_y, valid_g_y = train_test_split(train_good_x, train_good_y, test_size = 0.25)\n",
    "            train_b_x, valid_b_x, train_b_y, valid_b_y = train_test_split(train_bad_x, train_bad_y, test_size = 0.25)\n",
    "            train_x = pd.concat([train_g_x, train_b_x], axis = 0)\n",
    "            train_y = pd.concat([train_g_y, train_b_y], axis = 0)\n",
    "            valid_x = pd.concat([valid_g_x, valid_b_x], axis = 0)\n",
    "            valid_y = pd.concat([valid_g_y, valid_b_y], axis = 0)\n",
    "\n",
    "            if mode == 'C':\n",
    "                result, _ = LR(train_x, valid_x, train_y, valid_y, param)\n",
    "                table = cf_matrix(result, valid_y)\n",
    "                recall = table['Recall']\n",
    "                precision = table['Precision']\n",
    "                beta = 1\n",
    "                if recall.values > 0:\n",
    "                    fscore = ((1+beta**2)*recall*precision) / (recall+(beta**2)*precision)\n",
    "                else:\n",
    "                    fscore = 0\n",
    "                result_list.append(fscore)\n",
    "\n",
    "            elif mode == 'R':\n",
    "                result, _ = RidgeR(train_x, valid_x, train_y, valid_y, param)\n",
    "                pr_matrix = PR_matrix(result, valid_y)\n",
    "                auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "                result_list.append((-1)*auc)\n",
    "\n",
    "        return np.mean(result_list)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:12:50.007759Z",
     "start_time": "2022-05-25T04:12:49.976977Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_stackingcv3(train_month, times):\n",
    "    prob_dict = dict()\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # load relabel datasets\n",
    "    runhist = {}\n",
    "    kinds = {}\n",
    "    for i in train_month:\n",
    "        runhist[f'm{i}'] = pd.read_csv(f'relabel_runhist_m{i}.csv', index_col = 'id').iloc[:, 1:]\n",
    "        kinds[f'm{i}'] = pd.read_csv(f'kind_m{i}.csv').iloc[:, 2:-3]\n",
    "\n",
    "    # do several times to average the random effect of resampling\n",
    "    for i in tqdm(range(times)):\n",
    "        # generate resampled datasets\n",
    "        resampling_dataset(runhist = runhist, kinds = kinds, train_month = train_month, final_br = 1, num_os = 10)\n",
    "\n",
    "        # load & prepare the resampled datasets \n",
    "        data_dict, trainset_x, trainset_y = multiple_month(train_month, num_set = 10, filename = 'dataset')\n",
    "        all_train = multiple_set(num_set = 10)\n",
    "        all_train_x, all_train_y = train_set(all_train)\n",
    "        all_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "        all_test_x, all_test_y = label_divide(all_test, None, 'GB', train_only = True)\n",
    "\n",
    "        # optimization for each month of data\n",
    "        base_param = optimize_base(train_data = data_dict, \n",
    "                                   mode = 'C', \n",
    "                                   TPE_multi = False, \n",
    "                                   base_list = ['XGBoost', 'LightGBM', 'NeuralNetwork'],\n",
    "                                   iter_dict = {'LightGBM': 25, 'NeuralNetwork': 10, 'XGBoost': 25, 'CatBoost':25, \n",
    "                                                'RandomForest': 20, 'ExtraTrees': 20},\n",
    "                                   filename = f'runhist_array_m2m4_m5_3criteria_scheme3-{i}')\n",
    "        \n",
    "        # data transformation\n",
    "        trans_train = transform_train(data_dict, mode = 'C', base_param = base_param, cv = 5, add_origin = True)\n",
    "        temp_test = transform_test(data_dict, all_test, mode = 'C', base_param = base_param, add_origin = True)\n",
    "        trans_test = test_average(temp_test)\n",
    "        for k in trans_train.keys():\n",
    "            trans_train[k] = train_col(trans_test[k], trans_train[k])\n",
    "        trans_train_x, trans_train_y = train_set(trans_train)\n",
    "        trans_test_x, trans_test_y = train_set(trans_test) \n",
    "        trans_train['set0'] = {}      \n",
    "        \n",
    "        # searching for hyperparameters\n",
    "        best_param, _ = all_optuna(all_data = trans_train, \n",
    "                                   mode = 'C', \n",
    "                                   TPE_multi = False, \n",
    "                                   n_iter = 10,\n",
    "                                   filename = f'runhist_array_m2m4_m5_3criteria_StackingCV3-{i}',\n",
    "                                   creator = stackingCV_creator)\n",
    "        \n",
    "        # store the probability predicted by the classifier \n",
    "        for j in best_param.keys():\n",
    "            if i == 0:\n",
    "                prob_dict[j] = pd.DataFrame()\n",
    "            table, _ = LR(trans_train_x[j], trans_test_x[j], trans_train_y[j], trans_test_y[j], best_param[j], \n",
    "                          return_prob = True)\n",
    "            prob_dict[j] = pd.concat([prob_dict[j], table[['predict']]], axis = 1)\n",
    "            \n",
    "    # average to get final prediction\n",
    "    for j in best_param.keys():\n",
    "        prediction = (prob_dict[j].apply(np.sum, axis = 1) >= 0.5).astype(int)\n",
    "        result = pd.DataFrame(dict(truth = all_test_y, predict = prediction))\n",
    "        table = cf_matrix(result, all_train_y[j])\n",
    "        result_df = pd.concat([result_df, table]).rename(index = {0: f'data{j}'})\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:09:33.891286Z",
     "start_time": "2022-05-25T04:12:57.796461Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_month = range(2, 5)\n",
    "table_setC = full_stackingcv3(training_month, times = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:09:34.234978Z",
     "start_time": "2022-05-25T17:09:34.031921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_chart(table_setC, title = 'StackingCV Scheme 3 Classifier')\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:09:34.937959Z",
     "start_time": "2022-05-25T17:09:34.375588Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20220601'\n",
    "TPE_multi = False\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'StackingCV3'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'StackingCV3')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
