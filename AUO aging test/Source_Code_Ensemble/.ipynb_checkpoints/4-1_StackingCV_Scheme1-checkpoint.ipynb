{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:13:40.083378Z",
     "start_time": "2022-05-25T04:13:35.490491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor,\\\n",
    "    AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio, train_col\n",
    "from library.Imbalance_Sampling import label_divide, resampling_dataset\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, cf_matrix, AUC, PR_curve, \\\n",
    "     multiple_curve, PR_matrix, best_threshold, all_optuna, optuna_history, AdaBoost_creator \n",
    "from library.XGBoost import XGBoost_creator\n",
    "from library.LightGBM import LightGBM_creator\n",
    "from library.CatBoost import CatBoost_creator\n",
    "from library.RandomForest import RandomForest_creator\n",
    "from library.ExtraTrees import ExtraTrees_creator\n",
    "from library.NeuralNetwork import RunhistSet, NeuralNetworkC, trainingC\n",
    "from library.StackingCV_Scheme3 import optimize_base, stratified_data, runall_LR, runall_RidgeR, stackingCV_creator, \\\n",
    "     vif, correlation_plot, rank_importance, LR\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:13:40.162762Z",
     "start_time": "2022-05-25T04:13:40.147138Z"
    }
   },
   "outputs": [],
   "source": [
    "# load hyperparameters of all base learners in a dictionary\n",
    "def load_hyper(num_set, date, model_list, iter_list, filename, mode, TPE_multi) :\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    allset_dict = {}\n",
    "    for i, model in enumerate(model_list):\n",
    "        allset_dict[model] = dict()\n",
    "        with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[i]}.data', 'rb') as f:\n",
    "            allset_dict[model] = pickle.load(f)\n",
    "        \n",
    "    return allset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data by Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:13:40.301908Z",
     "start_time": "2022-05-25T04:13:40.240767Z"
    }
   },
   "outputs": [],
   "source": [
    "# input training data to the base learners and output the outcome\n",
    "def transform_train(train_data, mode, base_param, cv, add_origin = False):\n",
    "    \n",
    "    model_list = list(base_param.keys())\n",
    "    set_list = list(base_param[model_list[0]].keys())\n",
    "    set_dict = {}\n",
    "    for i in tqdm(set_list):\n",
    "\n",
    "        train_x_dict, train_y_dict, valid_x_dict, valid_y_dict = stratified_data(train_data[i], cv = cv)\n",
    "        all_cv = pd.DataFrame()\n",
    "        for j in tqdm(range(cv)):\n",
    "\n",
    "            model_predict = valid_x_dict[j].copy().reset_index(drop = True) if add_origin else pd.DataFrame()\n",
    "            if mode == 'C':          \n",
    "                if 'NeuralNetwork' in model_list:\n",
    "                    temp_train = RunhistSet(train_x_dict[j], train_y_dict[j])\n",
    "                    temp_valid = RunhistSet(valid_x_dict[j], valid_y_dict[j])\n",
    "                    train_loader = DataLoader(temp_train, \n",
    "                                              batch_size = base_param['NeuralNetwork'][i]['batch_size'], \n",
    "                                              shuffle = True)\n",
    "                    valid_loader = DataLoader(temp_valid, batch_size = len(valid_x_dict[j]), shuffle = False)\n",
    "                    nn_model = NeuralNetworkC(dim = train_x_dict[j].shape[1])\n",
    "                    optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                                 lr = base_param['NeuralNetwork'][i]['learning_rate'], \n",
    "                                                 weight_decay = base_param['NeuralNetwork'][i]['weight_decay'])\n",
    "                    criterion = nn.CrossEntropyLoss(\n",
    "                        weight = torch.tensor([1-base_param['NeuralNetwork'][i]['bad_weight'], \n",
    "                                               base_param['NeuralNetwork'][i]['bad_weight']])).to('cpu')\n",
    "                    network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 100, \n",
    "                                              early_stop = 10)\n",
    "                    for x, y in valid_loader:\n",
    "                        output = network(x)\n",
    "                        predict_y = output.data[:, 1]\n",
    "                    predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                \n",
    "                if 'XGBoost' in model_list:                     \n",
    "                    clf = XGBClassifier(**base_param['XGBoost'][i], use_label_encoder = False, n_jobs = -1)\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:                        \n",
    "                    clf = LGBMClassifier(**base_param['LightGBM'][i])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    clf = CatBoostClassifier(**base_param['CatBoost'][i])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeClassifier(\n",
    "                            max_depth = base_param['AdaBoost'][i]['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param['AdaBoost'][i][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    clf = AdaBoostClassifier(**boost_param)\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    clf = RandomForestClassifier(**base_param['RandomForest'][i])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    clf = ExtraTreesClassifier(**base_param['ExtraTrees'][i])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            elif mode == 'R':\n",
    "\n",
    "                if 'XGBoost' in model_list:\n",
    "                    reg = XGBRegressor(**base_param[i]['XGBoost'], use_label_encoder = False, n_jobs = -1)\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'X': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:\n",
    "                    reg = LGBMRegressor(**base_param[i]['LightGBM'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'L': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    reg = CatBoostRegressor(**base_param[i]['CatBoost'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'C': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeRegressor(\n",
    "                            max_depth = base_param[i]['AdaBoost']['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[i]['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    reg = AdaBoostRegressor(**boost_param)\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'A': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    reg = RandomForestRegressor(**base_param[i]['RandomForest'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'R': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    reg = ExtraTreesRegressor(**base_param[i]['ExtraTrees'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'E': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            test_label = valid_y_dict[j].reset_index(drop = True)\n",
    "            done_cv = pd.concat([model_predict, test_label], axis = 1)\n",
    "            all_cv = pd.concat([all_cv, done_cv], axis = 0)\n",
    "\n",
    "        set_dict[i] = all_cv\n",
    "    \n",
    "    return set_dict\n",
    "\n",
    "\n",
    "# input testing data to the base learners and output the outcome\n",
    "def transform_test(train_data, test_data, mode, base_param, add_origin = False):\n",
    "    \n",
    "    model_list = list(base_param.keys())\n",
    "    set_list = list(base_param[model_list[0]].keys())\n",
    "    test_dict = {}\n",
    "    for i in tqdm(set_list):\n",
    "        \n",
    "        train_x, train_y, test_x, test_y = label_divide(train_data[i], test_data, train_only = False)\n",
    "        model_predict = test_x.copy().reset_index(drop = True) if add_origin else pd.DataFrame()\n",
    "        if mode == 'C':\n",
    "\n",
    "            if 'NeuralNetwork' in model_list:\n",
    "                temp_train = RunhistSet(train_x, train_y)\n",
    "                temp_test = RunhistSet(test_x, test_y)\n",
    "                train_loader = DataLoader(temp_train, \n",
    "                                          batch_size = base_param['NeuralNetwork'][i]['batch_size'], \n",
    "                                          shuffle = True)\n",
    "                test_loader = DataLoader(temp_test, batch_size = len(test_x), shuffle = False)\n",
    "                nn_model = NeuralNetworkC(dim = train_x.shape[1])\n",
    "                optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                             lr = base_param['NeuralNetwork'][i]['learning_rate'], \n",
    "                                             weight_decay = base_param['NeuralNetwork'][i]['weight_decay'])\n",
    "                criterion = nn.CrossEntropyLoss(\n",
    "                    weight = torch.tensor([1-base_param['NeuralNetwork'][i]['bad_weight'], \n",
    "                                           base_param['NeuralNetwork'][i]['bad_weight']])).to('cpu')\n",
    "                network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 100, \n",
    "                                          early_stop = 10)\n",
    "                for X, Y in test_loader:\n",
    "                    X, Y = X.float(), Y.long()\n",
    "                    output = network(X)\n",
    "                    predict_y = output.data[:, 1]\n",
    "                predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "            \n",
    "            if 'XGBoost' in model_list:\n",
    "                clf = XGBClassifier(**base_param['XGBoost'][i], use_label_encoder = False, n_jobs = -1)\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'LightGBM' in model_list:\n",
    "                clf = LGBMClassifier(**base_param['LightGBM'][i])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'CatBoost' in model_list:\n",
    "                clf = CatBoostClassifier(**base_param['CatBoost'][i])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'AdaBoost' in model_list:\n",
    "                tree_param = {\n",
    "                    'base_estimator': DecisionTreeClassifier(\n",
    "                        max_depth = base_param['AdaBoost'][i]['max_depth']\n",
    "                    )}\n",
    "                boost_param = dict(\n",
    "                    (key, base_param['AdaBoost'][i][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                )\n",
    "                boost_param.update(tree_param)\n",
    "                clf = AdaBoostClassifier(**boost_param)\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'RandomForest' in model_list:\n",
    "                clf = RandomForestClassifier(**base_param['RandomForest'][i])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'ExtraTrees' in model_list:\n",
    "                clf = ExtraTreesClassifier(**base_param['ExtraTrees'][i])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "        elif mode == 'R':\n",
    "\n",
    "            if 'XGBoost' in model_list:\n",
    "                reg = XGBRegressor(**base_param[i]['XGBoost'], use_label_encoder = False, n_jobs = -1)\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'X': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'LightGBM' in model_list:\n",
    "                reg = LGBMRegressor(**base_param[i]['LightGBM'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'L': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'CatBoost' in model_list:\n",
    "                reg = CatBoostRegressor(**base_param[i]['CatBoost'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'C': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'AdaBoost' in model_list:\n",
    "                tree_param = {\n",
    "                    'base_estimator': DecisionTreeRegressor(\n",
    "                        max_depth = base_param[i]['AdaBoost']['max_depth']\n",
    "                    )}\n",
    "                boost_param = dict(\n",
    "                    (key, base_param[i]['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                )\n",
    "                boost_param.update(tree_param)\n",
    "                reg = AdaBoostRegressor(**boost_param)\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'A': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'RandomForest' in model_list:\n",
    "                reg = RandomForestRegressor(**base_param[i]['RandomForest'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'R': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'ExtraTrees' in model_list:\n",
    "                reg = ExtraTreesRegressor(**base_param[i]['ExtraTrees'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'E': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "        model_done = pd.concat([model_predict, test_y], axis = 1)\n",
    "        test_dict[i] = model_done\n",
    "        \n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:13:42.262250Z",
     "start_time": "2022-05-25T04:13:42.214439Z"
    }
   },
   "outputs": [],
   "source": [
    "def stackingcv1_creator(train_data, mode, learner = 'LogisticRegression', num_valid = 5):\n",
    "    return stackingCV_creator(train_data = train_data, mode = mode, learner = learner, num_valid = num_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T04:13:59.895918Z",
     "start_time": "2022-05-25T04:13:59.883950Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_stackingcv1(train_month, times):\n",
    "    prob_dict = dict()\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # load relabel datasets\n",
    "    runhist = {}\n",
    "    kinds = {}\n",
    "    for i in train_month:\n",
    "        runhist[f'm{i}'] = pd.read_csv(f'relabel_runhist_m{i}.csv', index_col = 'id').iloc[:, 1:]\n",
    "        kinds[f'm{i}'] = pd.read_csv(f'kind_m{i}.csv').iloc[:, 2:-3]\n",
    "\n",
    "    # do several times to average the random effect of resampling\n",
    "    for i in tqdm(range(times)):\n",
    "        # generate resampled datasets\n",
    "        resampling_dataset(runhist = runhist, kinds = kinds, train_month = train_month, final_br = 1, num_os = 10)\n",
    "\n",
    "        # load & prepare the resampled datasets \n",
    "        all_train = multiple_set(num_set = 10)\n",
    "        all_train_x, all_train_y = train_set(all_train)\n",
    "        all_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "        all_test_x, all_test_y = label_divide(all_test, None, 'GB', train_only = True)\n",
    "\n",
    "        # optimization for each month of data\n",
    "        base_param = optimize_base(train_data = {'all': all_train}, \n",
    "                                   mode = 'C', \n",
    "                                   TPE_multi = False, \n",
    "                                   base_list = ['NeuralNetwork', 'LightGBM', 'XGBoost'],\n",
    "                                   iter_dict = {'LightGBM': 25, 'NeuralNetwork': 10, 'XGBoost': 25, 'CatBoost': 25, \n",
    "                                               'RandomForest': 20, 'ExtraTrees': 20},\n",
    "                                   filename = f'runhist_array_m2m4_m5_3criteria_scheme1-{i}')\n",
    "        \n",
    "        # data transformation\n",
    "        trans_train = transform_train(all_train, mode = 'C', base_param = base_param['all'], cv = 5, add_origin = False)\n",
    "        trans_test = transform_test(all_train, all_test, mode = 'C', base_param = base_param['all'], add_origin = False)\n",
    "        trans_train_x, trans_train_y = train_set(trans_train)\n",
    "        trans_test_x, trans_test_y = train_set(trans_test) \n",
    "        trans_train['set0'] = {}\n",
    "        \n",
    "        # searching for hyperparameters\n",
    "        best_param, _ = all_optuna(all_data = trans_train, \n",
    "                                   mode = 'C', \n",
    "                                   TPE_multi = False, \n",
    "                                   n_iter = 10,\n",
    "                                   filename = f'runhist_array_m2m4_m5_3criteria_StackingCV1-{i}',\n",
    "                                   creator = stackingcv1_creator)\n",
    "        \n",
    "        # store the probability predicted by the classifier \n",
    "        for j in best_param.keys():\n",
    "            if i == 0:\n",
    "                prob_dict[j] = pd.DataFrame()\n",
    "            table, _ = LR(trans_train_x[j], trans_test_x[j], trans_train_y[j], trans_test_y[j], best_param[j], \n",
    "                          return_prob = True)\n",
    "            prob_dict[j] = pd.concat([prob_dict[j], table[['predict']]], axis = 1)\n",
    "            \n",
    "    # average to get final prediction\n",
    "    for j in best_param.keys():\n",
    "        prediction = (prob_dict[j].apply(np.sum, axis = 1) >= 0.5).astype(int)\n",
    "        result = pd.DataFrame(dict(truth = all_test_y, predict = prediction))\n",
    "        table = cf_matrix(result, all_train_y[j])\n",
    "        result_df = pd.concat([result_df, table]).rename(index = {0: f'data{j}'})\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-26T06:26:06.651Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f0118471c04b5da98c451c3a56dd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2c2db3afa34f279067f282f0cf4fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2:\n",
      "# bad: 69\n",
      "Size before Border: 39009\n",
      "Distance = 1 ...\n",
      "Distance = 2 ...\n",
      "Distance = 3 ...\n",
      "Distance = 4 ...\n",
      "Distance = 5 ...\n",
      "Distance = 6 ...\n",
      "Distance = 7 ...\n",
      "Distance = 8 ...\n",
      "# over: 10\n",
      "Distance = 9 ...\n",
      "# over: 10\n",
      "Distance = 10 ...\n",
      "# over: 10\n",
      "Distance = 11 ...\n",
      "# over: 50\n",
      "Distance = 12 ...\n",
      "# over: 30\n",
      "Distance = 13 ...\n",
      "# over: 70\n",
      "Distance = 14 ...\n",
      "# over: 70\n",
      "Distance = 15 ...\n",
      "# over: 240\n",
      "Distance = 16 ...\n",
      "# over: 150\n",
      "# over: 150\n",
      "Size after Border: 39624\n",
      "\n",
      " Generating Dataset 2\n",
      "Size after Undersampling: 1368\n",
      "Size after Undersampling: (1368, 88) , Balance Ratio: 1.0\n",
      "Size before Undersampling: 39009\n",
      "\n",
      " Generating Dataset 6\n",
      "Size after Undersampling: 759\n",
      "Size after Undersampling: 759\n",
      "Distance = 1 ...\n",
      "Distance = 2 ...\n",
      "Distance = 3 ...\n",
      "Distance = 4 ...\n",
      "Distance = 5 ...\n",
      "Distance = 6 ...\n",
      "Distance = 7 ...\n",
      "Distance = 8 ...\n",
      "# over: 10\n",
      "Distance = 9 ...\n",
      "# over: 10\n",
      "Distance = 10 ...\n",
      "# over: 10\n",
      "Distance = 11 ...\n",
      "# over: 50\n",
      "Distance = 12 ...\n",
      "# over: 30\n",
      "Distance = 13 ...\n",
      "# over: 70\n",
      "Distance = 14 ...\n",
      "# over: 70\n",
      "Distance = 15 ...\n",
      "# over: 240\n",
      "Distance = 16 ...\n",
      "# over: 160\n",
      "# over: 160\n",
      "Size after Border: (1372, 88) , Balance Ratio: 1.01\n",
      "\n",
      " Generating Dataset 1\n",
      "Size before Oversampling: 39009\n",
      "Size after Oversampling: 39642\n",
      "Size after Undersampling: 1404\n",
      "\n",
      " Generating Dataset 3\n",
      "Size before Oversampling: 39009\n",
      "Size after Oversampling: 39629\n",
      "Size after Undersampling: 1378\n",
      "\n",
      " Generating Dataset 4\n",
      "Size before Oversampling: 39009\n",
      "Size after Oversampling: 39629\n",
      "Size after Undersampling: 1378\n",
      "\n",
      " Generating Dataset 5\n",
      "Size before Undersampling: 39009\n",
      "Size after Undersampling: 759\n",
      "Size before Oversampling: 759\n",
      "Size after Oversampling: 1370\n",
      "\n",
      " Generating Dataset 7\n",
      "Size before Undersampling: 39009\n",
      "Size after Undersampling: 759\n",
      "Size before Oversampling: 759\n",
      "Size after Oversampling: 1380\n",
      "\n",
      " Generating Dataset 8\n",
      "Size before Undersampling: 39009\n",
      "Size after Undersampling: 759\n",
      "Size before Oversampling: 759\n",
      "Size after Oversampling: 1380\n",
      "\n",
      " Generating Dataset 9\n",
      "Size after Undersampling: 759\n",
      "Month 3:\n",
      "# bad: 113\n",
      "Size before Border: 60396\n",
      "Distance = 1 ...\n",
      "Distance = 2 ...\n",
      "Distance = 3 ...\n",
      "Distance = 4 ...\n",
      "Distance = 5 ...\n",
      "# over: 20\n",
      "Distance = 6 ...\n",
      "Distance = 7 ...\n",
      "Distance = 8 ...\n",
      "Distance = 9 ...\n",
      "# over: 20\n",
      "Distance = 10 ...\n",
      "# over: 10\n",
      "Distance = 11 ...\n",
      "# over: 10\n",
      "Distance = 12 ...\n",
      "# over: 50\n",
      "Distance = 13 ...\n",
      "# over: 110\n",
      "Distance = 14 ...\n",
      "# over: 100\n",
      "Distance = 15 ...\n",
      "# over: 170\n",
      "Distance = 16 ...\n",
      "# over: 310\n",
      "Distance = 17 ...\n",
      "# over: 265\n",
      "# over: 265\n",
      "Size after Border: 61402\n",
      "\n",
      " Generating Dataset 2\n",
      "Size after Undersampling: 2238\n",
      "Size after Undersampling: (2238, 97) , Balance Ratio: 1.0\n",
      "Size before Undersampling: 60396\n",
      "\n",
      " Generating Dataset 6\n",
      "Size after Undersampling: 1243\n",
      "Size after Undersampling: 1243\n",
      "Distance = 1 ...\n",
      "Distance = 2 ...\n",
      "Distance = 3 ...\n",
      "Distance = 4 ...\n",
      "Distance = 5 ...\n",
      "# over: 20\n",
      "Distance = 6 ...\n",
      "Distance = 7 ...\n",
      "Distance = 8 ...\n",
      "Distance = 9 ...\n",
      "# over: 20\n",
      "Distance = 10 ...\n",
      "# over: 10\n",
      "Distance = 11 ...\n",
      "# over: 10\n",
      "Distance = 12 ...\n",
      "# over: 50\n",
      "Distance = 13 ...\n",
      "# over: 110\n",
      "Distance = 14 ...\n",
      "# over: 100\n",
      "Distance = 15 ...\n",
      "# over: 170\n",
      "Distance = 16 ...\n",
      "# over: 310\n",
      "Distance = 17 ...\n",
      "# over: 250\n",
      "# over: 250\n",
      "Size after Border: (2243, 97) , Balance Ratio: 1.02\n",
      "\n",
      " Generating Dataset 1\n",
      "Size before Oversampling: 60396\n",
      "Size after Oversampling: 61435\n",
      "Size after Undersampling: 2304\n",
      "\n",
      " Generating Dataset 3\n",
      "Size before Oversampling: 60396\n",
      "Size after Oversampling: 61412\n",
      "Size after Undersampling: 2258\n",
      "\n",
      " Generating Dataset 4\n",
      "Size before Oversampling: 60396\n",
      "Size after Oversampling: 61412\n",
      "Size after Undersampling: 2258\n",
      "\n",
      " Generating Dataset 5\n",
      "Size before Undersampling: 60396\n",
      "Size after Undersampling: 1243\n",
      "Size before Oversampling: 1243\n",
      "Size after Oversampling: 2271\n",
      "\n",
      " Generating Dataset 7\n",
      "Size before Undersampling: 60396\n",
      "Size after Undersampling: 1243\n",
      "Size before Oversampling: 1243\n",
      "Size after Oversampling: 2260\n",
      "\n",
      " Generating Dataset 8\n",
      "Size before Undersampling: 60396\n",
      "Size after Undersampling: 1243\n",
      "Size before Oversampling: 1243\n",
      "Size after Oversampling: 2260\n",
      "\n",
      " Generating Dataset 9\n",
      "Size after Undersampling: 1243\n",
      "Month 4:\n",
      "# bad: 122\n",
      "Size before Border: 57743\n",
      "Distance = 1 ...\n",
      "Distance = 2 ...\n",
      "Distance = 3 ...\n",
      "Distance = 4 ...\n",
      "Distance = 5 ...\n",
      "Distance = 6 ...\n",
      "Distance = 7 ...\n",
      "Distance = 8 ...\n",
      "# over: 20\n",
      "Distance = 9 ...\n",
      "Distance = 10 ...\n",
      "Distance = 11 ...\n",
      "# over: 80\n",
      "Distance = 12 ...\n",
      "# over: 100\n",
      "Distance = 13 ...\n",
      "# over: 130\n",
      "Distance = 14 ...\n",
      "# over: 220\n",
      "Distance = 15 ...\n",
      "# over: 310\n",
      "Distance = 16 ...\n",
      "# over: 285\n",
      "# over: 285\n",
      "Size after Border: 58813\n",
      "\n",
      " Generating Dataset 2\n",
      "Size after Undersampling: 2384\n",
      "Size after Undersampling: (2384, 100) , Balance Ratio: 1.0\n",
      "Size before Undersampling: 57743\n",
      "\n",
      " Generating Dataset 6\n",
      "Size after Undersampling: 1342\n",
      "Size after Undersampling: 1342\n",
      "Distance = 1 ...\n",
      "Distance = 2 ...\n",
      "Distance = 3 ...\n",
      "Distance = 4 ...\n",
      "Distance = 5 ...\n",
      "Distance = 6 ...\n",
      "Distance = 7 ...\n",
      "Distance = 8 ...\n",
      "# over: 20\n",
      "Distance = 9 ...\n",
      "Distance = 10 ...\n",
      "Distance = 11 ...\n",
      "# over: 80\n",
      "Distance = 12 ...\n",
      "# over: 100\n",
      "Distance = 13 ...\n",
      "# over: 130\n",
      "Distance = 14 ...\n",
      "# over: 220\n",
      "Distance = 15 ...\n",
      "# over: 310\n",
      "Distance = 16 ...\n",
      "# over: 290\n",
      "# over: 290\n",
      "Size after Border: (2413, 100) , Balance Ratio: 1.02\n",
      "\n",
      " Generating Dataset 1\n",
      "Size before Oversampling: 57743\n",
      "Size after Oversampling: 58834\n",
      "Size after Undersampling: 2426\n",
      "\n",
      " Generating Dataset 3\n",
      "Size before Oversampling: 57743\n",
      "Size after Oversampling: 58841\n",
      "Size after Undersampling: 2440\n",
      "\n",
      " Generating Dataset 4\n",
      "Size before Oversampling: 57743\n",
      "Size after Oversampling: 58841\n",
      "Size after Undersampling: 2440\n",
      "\n",
      " Generating Dataset 5\n",
      "Size before Undersampling: 57743\n",
      "Size after Undersampling: 1342\n",
      "Size before Oversampling: 1342\n",
      "Size after Oversampling: 2419\n",
      "\n",
      " Generating Dataset 7\n",
      "Size before Undersampling: 57743\n",
      "Size after Undersampling: 1342\n",
      "Size before Oversampling: 1342\n",
      "Size after Oversampling: 2440\n",
      "\n",
      " Generating Dataset 8\n",
      "Size before Undersampling: 57743\n",
      "Size after Undersampling: 1342\n",
      "Size before Oversampling: 1342\n",
      "Size after Oversampling: 2440\n",
      "\n",
      " Generating Dataset 9\n",
      "Size after Undersampling: 1342\n",
      "Dimension of dataset 0 : (157148, 128)  balance ratio: 515.93\n",
      "Dimension of dataset 1 : (6134, 128)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (5990, 128)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (6076, 128)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (6076, 128)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (6060, 128)  balance ratio: 1.01\n",
      "Dimension of dataset 6 : (6028, 128)  balance ratio: 1.02\n",
      "Dimension of dataset 7 : (6080, 128)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (6080, 128)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (3344, 128)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bff8df7d8744dabb79e9c5544ffbf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting for CatBoost:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4579d44899c5447d8ee538881c2c8f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 14:27:29,529]\u001b[0m A new study created in memory with name: no-name-e16bae1c-75d0-4dd1-b2c2-6d06a7736b3c\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fd5410bcae44478993a7876b816383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9870633893919794 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5039113428943938\n",
      "Precision: 0.9896238651102465 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5026075619295959\n",
      "Precision: 0.9883419689119171 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5032594524119948\n",
      "\u001b[32m[I 2022-05-26 14:27:30,586]\u001b[0m Trial 0 finished with value: 0.9915532355573724 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 1.100432520394748, 'min_data_in_leaf': 40}. Best is trial 0 with value: 0.9915532355573724.\u001b[0m\n",
      "Precision: 0.9921875 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.500651890482399\n",
      "Precision: 0.9934895833333334 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.500651890482399\n",
      "Precision: 0.9857881136950905 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5045632333767927\n",
      "\u001b[32m[I 2022-05-26 14:27:34,796]\u001b[0m Trial 1 finished with value: 0.9924122483461463 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.018844575695757124, 'min_data_in_leaf': 25, 'max_leaves': 15}. Best is trial 1 with value: 0.9924122483461463.\u001b[0m\n",
      "Precision: 0.9870466321243523 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5032594524119948\n",
      "Precision: 0.993421052631579 \n",
      "Recall: 0.984354628422425 \n",
      "Aging Rate: 0.4954367666232073\n",
      "Precision: 0.9921773142112125 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2022-05-26 14:27:36,078]\u001b[0m Trial 2 finished with value: 0.9904325950370767 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 1.9682205616773538, 'min_data_in_leaf': 45}. Best is trial 1 with value: 0.9924122483461463.\u001b[0m\n",
      "Precision: 0.9947848761408083 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9921568627450981 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.49869621903520206\n",
      "Precision: 0.9986945169712794 \n",
      "Recall: 0.9973924380704041 \n",
      "Aging Rate: 0.49934810951760106\n",
      "\u001b[32m[I 2022-05-26 14:27:39,462]\u001b[0m Trial 3 finished with value: 0.9945631825924458 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 0.871910669943196, 'min_data_in_leaf': 10}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9986893840104849 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.4973924380704042\n",
      "Precision: 0.9895697522816167 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9934810951760105 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2022-05-26 14:27:43,738]\u001b[0m Trial 4 finished with value: 0.9930430929433921 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 0.7820826470392966, 'min_data_in_leaf': 30}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9934296977660972 \n",
      "Recall: 0.9856584093872229 \n",
      "Aging Rate: 0.49608865710560623\n",
      "Precision: 0.9857697283311773 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5039113428943938\n",
      "Precision: 0.9896103896103896 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5019556714471969\n",
      "\u001b[32m[I 2022-05-26 14:27:44,524]\u001b[0m Trial 5 finished with value: 0.9902270500961771 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 0.1513527816327558, 'min_data_in_leaf': 5}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.974293059125964 \n",
      "Recall: 0.9882659713168188 \n",
      "Aging Rate: 0.5071707953063885\n",
      "Precision: 0.9714285714285714 \n",
      "Recall: 0.9752281616688396 \n",
      "Aging Rate: 0.5019556714471969\n",
      "Precision: 0.9765319426336375 \n",
      "Recall: 0.9765319426336375 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2022-05-26 14:27:46,830]\u001b[0m Trial 6 finished with value: 0.9770287915073084 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 2, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.10502212455185135, 'min_data_in_leaf': 5}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9934810951760105 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9831824062095731 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.5039113428943938\n",
      "Precision: 0.9908972691807543 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5013037809647979\n",
      "\u001b[32m[I 2022-05-26 14:27:50,398]\u001b[0m Trial 7 finished with value: 0.9908938607296659 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 10, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.08882855407697901}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9908854166666666 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.500651890482399\n",
      "Precision: 0.9947916666666666 \n",
      "Recall: 0.9960886571056062 \n",
      "Aging Rate: 0.500651890482399\n",
      "Precision: 0.9908972691807543 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5013037809647979\n",
      "\u001b[32m[I 2022-05-26 14:27:52,717]\u001b[0m Trial 8 finished with value: 0.9930527280130294 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 5.313810608287571, 'min_data_in_leaf': 15}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9908616187989556 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.49934810951760106\n",
      "Precision: 0.9947643979057592 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.4980443285528031\n",
      "Precision: 0.9934640522875817 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.49869621903520206\n",
      "\u001b[32m[I 2022-05-26 14:28:02,897]\u001b[0m Trial 9 finished with value: 0.9917325065033306 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 0.1403198005359364}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9934383202099738 \n",
      "Recall: 0.9869621903520208 \n",
      "Aging Rate: 0.49674054758800523\n",
      "Precision: 0.9844760672703752 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5039113428943938\n",
      "Precision: 0.9921671018276762 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.49934810951760106\n",
      "\u001b[32m[I 2022-05-26 14:28:07,441]\u001b[0m Trial 10 finished with value: 0.9900070834632774 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.3, 'l2_leaf_reg': 8.05383532186586, 'min_data_in_leaf': 60, 'max_leaves': 50}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9947848761408083 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9921875 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.500651890482399\n",
      "Precision: 0.9922077922077922 \n",
      "Recall: 0.9960886571056062 \n",
      "Aging Rate: 0.5019556714471969\n",
      "\u001b[32m[I 2022-05-26 14:28:09,744]\u001b[0m Trial 11 finished with value: 0.9939210631925538 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.3, 'l2_leaf_reg': 9.179128770941526, 'min_data_in_leaf': 15}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9908496732026144 \n",
      "Recall: 0.9882659713168188 \n",
      "Aging Rate: 0.49869621903520206\n",
      "Precision: 0.9857512953367875 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5032594524119948\n",
      "Precision: 0.9973821989528796 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.4980443285528031\n",
      "\u001b[32m[I 2022-05-26 14:28:12,903]\u001b[0m Trial 12 finished with value: 0.991312608956032 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.3, 'l2_leaf_reg': 3.440186232651044, 'min_data_in_leaf': 15}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9947575360419397 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.4973924380704042\n",
      "Precision: 0.9921259842519685 \n",
      "Recall: 0.9856584093872229 \n",
      "Aging Rate: 0.49674054758800523\n",
      "Precision: 0.9909208819714657 \n",
      "Recall: 0.9960886571056062 \n",
      "Aging Rate: 0.5026075619295959\n",
      "\u001b[32m[I 2022-05-26 14:28:14,793]\u001b[0m Trial 13 finished with value: 0.9915121780450232 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.41260660845234864, 'min_data_in_leaf': 15}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9947916666666666 \n",
      "Recall: 0.9960886571056062 \n",
      "Aging Rate: 0.500651890482399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9896103896103896 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5019556714471969\n",
      "Precision: 0.9895697522816167 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2022-05-26 14:28:17,683]\u001b[0m Trial 14 finished with value: 0.9921838188539737 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 9.25871228505751, 'min_data_in_leaf': 5}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9960526315789474 \n",
      "Recall: 0.9869621903520208 \n",
      "Aging Rate: 0.4954367666232073\n",
      "Precision: 0.9883720930232558 \n",
      "Recall: 0.9973924380704041 \n",
      "Aging Rate: 0.5045632333767927\n",
      "Precision: 0.9870298313878081 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5026075619295959\n",
      "\u001b[32m[I 2022-05-26 14:28:20,100]\u001b[0m Trial 15 finished with value: 0.9913150773711793 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 4, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 0.018178555233032908}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9947712418300654 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.49869621903520206\n",
      "Precision: 0.9921671018276762 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.49934810951760106\n",
      "Precision: 0.9921875 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.500651890482399\n",
      "\u001b[32m[I 2022-05-26 14:28:23,110]\u001b[0m Trial 16 finished with value: 0.9926087855691267 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 2.4544521433699966, 'min_data_in_leaf': 20, 'max_leaves': 50}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9922077922077922 \n",
      "Recall: 0.9960886571056062 \n",
      "Aging Rate: 0.5019556714471969\n",
      "Precision: 0.9895697522816167 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9934895833333334 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.500651890482399\n",
      "\u001b[32m[I 2022-05-26 14:28:25,992]\u001b[0m Trial 17 finished with value: 0.9926169991048536 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 0.4213048669877551, 'min_data_in_leaf': 10}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9646133682830931 \n",
      "Recall: 0.9595827900912647 \n",
      "Aging Rate: 0.4973924380704042\n",
      "Precision: 0.9547803617571059 \n",
      "Recall: 0.9634941329856584 \n",
      "Aging Rate: 0.5045632333767927\n",
      "Precision: 0.9489144316730523 \n",
      "Recall: 0.9687092568448501 \n",
      "Aging Rate: 0.5104302477183833\n",
      "\u001b[32m[I 2022-05-26 14:28:27,432]\u001b[0m Trial 18 finished with value: 0.9599728789615344 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 2, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 0.0440743814774147, 'min_data_in_leaf': 35}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9921363040629095 \n",
      "Recall: 0.9869621903520208 \n",
      "Aging Rate: 0.4973924380704042\n",
      "Precision: 0.9934640522875817 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.49869621903520206\n",
      "Precision: 0.987012987012987 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.5019556714471969\n",
      "\u001b[32m[I 2022-05-26 14:29:00,932]\u001b[0m Trial 19 finished with value: 0.9902163593352329 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 1.146121492373014}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9921976592977894 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5013037809647979\n",
      "Precision: 0.9908616187989556 \n",
      "Recall: 0.9895697522816167 \n",
      "Aging Rate: 0.49934810951760106\n",
      "Precision: 0.9857328145265889 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.5026075619295959\n",
      "\u001b[32m[I 2022-05-26 14:29:05,832]\u001b[0m Trial 20 finished with value: 0.9906671121559615 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.3, 'l2_leaf_reg': 4.1920237744050075, 'min_data_in_leaf': 25, 'max_leaves': 25}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9947643979057592 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.4980443285528031\n",
      "Precision: 0.988296488946684 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.5013037809647979\n",
      "Precision: 0.9921671018276762 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.49934810951760106\n",
      "\u001b[32m[I 2022-05-26 14:29:07,988]\u001b[0m Trial 21 finished with value: 0.9913061274857554 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 7.3505030190673875, 'min_data_in_leaf': 15}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9960732984293194 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.4980443285528031\n",
      "Precision: 0.9882506527415144 \n",
      "Recall: 0.9869621903520208 \n",
      "Aging Rate: 0.49934810951760106\n",
      "Precision: 0.9921671018276762 \n",
      "Recall: 0.9908735332464146 \n",
      "Aging Rate: 0.49934810951760106\n",
      "\u001b[32m[I 2022-05-26 14:29:10,349]\u001b[0m Trial 22 finished with value: 0.9910824620522821 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 6.187746943561602, 'min_data_in_leaf': 10}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9844760672703752 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5039113428943938\n",
      "Precision: 0.9883116883116884 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5019556714471969\n",
      "Precision: 0.9870298313878081 \n",
      "Recall: 0.9921773142112125 \n",
      "Aging Rate: 0.5026075619295959\n",
      "\u001b[32m[I 2022-05-26 14:29:12,197]\u001b[0m Trial 23 finished with value: 0.9893830986892217 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 1.7689777449743171, 'min_data_in_leaf': 20}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Precision: 0.9896103896103896 \n",
      "Recall: 0.9934810951760105 \n",
      "Aging Rate: 0.5019556714471969\n",
      "Precision: 0.9921976592977894 \n",
      "Recall: 0.9947848761408083 \n",
      "Aging Rate: 0.5013037809647979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 14:29:14,337]\u001b[0m A new study created in memory with name: no-name-1354e046-b664-40c5-b7dc-4254def098e9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9896373056994818 \n",
      "Recall: 0.9960886571056062 \n",
      "Aging Rate: 0.5032594524119948\n",
      "\u001b[32m[I 2022-05-26 14:29:14,274]\u001b[0m Trial 24 finished with value: 0.9926280166081294 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 3.8955438839822443, 'min_data_in_leaf': 10}. Best is trial 3 with value: 0.9945631825924458.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcbc028e9584881ad207ef40262138a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9271523178807947 \n",
      "Recall: 0.9345794392523364 \n",
      "Aging Rate: 0.5040053404539386\n",
      "Precision: 0.9314516129032258 \n",
      "Recall: 0.9252336448598131 \n",
      "Aging Rate: 0.49666221628838453\n",
      "Precision: 0.9130998702983139 \n",
      "Recall: 0.9399198931909212 \n",
      "Aging Rate: 0.5146862483311081\n",
      "\u001b[32m[I 2022-05-26 14:29:15,725]\u001b[0m Trial 0 finished with value: 0.9284996901053991 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 2, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 1.269523802156775, 'min_data_in_leaf': 50, 'max_leaves': 30}. Best is trial 0 with value: 0.9284996901053991.\u001b[0m\n",
      "Precision: 0.9798927613941019 \n",
      "Recall: 0.9759679572763685 \n",
      "Aging Rate: 0.4979973297730307\n",
      "Precision: 0.9840425531914894 \n",
      "Recall: 0.9879839786381842 \n",
      "Aging Rate: 0.5020026702269693\n",
      "Precision: 0.9736495388669302 \n",
      "Recall: 0.986648865153538 \n",
      "Aging Rate: 0.5066755674232309\n",
      "\u001b[32m[I 2022-05-26 14:29:17,071]\u001b[0m Trial 1 finished with value: 0.9813472831052316 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 0.9080262857239608, 'min_data_in_leaf': 20}. Best is trial 1 with value: 0.9813472831052316.\u001b[0m\n",
      "Precision: 0.9802371541501976 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5066755674232309\n",
      "Precision: 0.9853723404255319 \n",
      "Recall: 0.9893190921228304 \n",
      "Aging Rate: 0.5020026702269693\n",
      "Precision: 0.9853917662682603 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5026702269692924\n",
      "\u001b[32m[I 2022-05-26 14:29:36,672]\u001b[0m Trial 2 finished with value: 0.9873650504591587 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 0.0635305460037983}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.96684350132626 \n",
      "Recall: 0.9732977303070761 \n",
      "Aging Rate: 0.5033377837116155\n",
      "Precision: 0.9682119205298013 \n",
      "Recall: 0.9759679572763685 \n",
      "Aging Rate: 0.5040053404539386\n",
      "Precision: 0.9645203679369251 \n",
      "Recall: 0.9799732977303071 \n",
      "Aging Rate: 0.5080106809078772\n",
      "\u001b[32m[I 2022-05-26 14:29:39,870]\u001b[0m Trial 3 finished with value: 0.9714399262627346 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 4.667312902940042}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9814569536423841 \n",
      "Recall: 0.9893190921228304 \n",
      "Aging Rate: 0.5040053404539386\n",
      "Precision: 0.9866844207723036 \n",
      "Recall: 0.9893190921228304 \n",
      "Aging Rate: 0.5013351134846462\n",
      "Precision: 0.9788359788359788 \n",
      "Recall: 0.9879839786381842 \n",
      "Aging Rate: 0.5046728971962616\n",
      "\u001b[32m[I 2022-05-26 14:29:44,222]\u001b[0m Trial 4 finished with value: 0.9855870149148228 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 12, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 0.20888858677916133, 'min_data_in_leaf': 35, 'max_leaves': 45}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9267015706806283 \n",
      "Recall: 0.945260347129506 \n",
      "Aging Rate: 0.5100133511348465\n",
      "Precision: 0.8978007761966365 \n",
      "Recall: 0.9265687583444593 \n",
      "Aging Rate: 0.5160213618157543\n",
      "Precision: 0.9202614379084967 \n",
      "Recall: 0.9399198931909212 \n",
      "Aging Rate: 0.5106809078771696\n",
      "\u001b[32m[I 2022-05-26 14:29:45,642]\u001b[0m Trial 5 finished with value: 0.9259445674508587 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 2, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 0.7838243314063013, 'min_data_in_leaf': 30, 'max_leaves': 40}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9390243902439024 \n",
      "Recall: 0.9252336448598131 \n",
      "Aging Rate: 0.49265687583444595\n",
      "Precision: 0.928476821192053 \n",
      "Recall: 0.9359145527369827 \n",
      "Aging Rate: 0.5040053404539386\n",
      "Precision: 0.9283783783783783 \n",
      "Recall: 0.9172229639519359 \n",
      "Aging Rate: 0.4939919893190921\n",
      "\u001b[32m[I 2022-05-26 14:29:47,833]\u001b[0m Trial 6 finished with value: 0.9290086060561613 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 4, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 7.918142848240358, 'min_data_in_leaf': 25, 'max_leaves': 30}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9720744680851063 \n",
      "Recall: 0.9759679572763685 \n",
      "Aging Rate: 0.5020026702269693\n",
      "Precision: 0.9707835325365206 \n",
      "Recall: 0.9759679572763685 \n",
      "Aging Rate: 0.5026702269692924\n",
      "Precision: 0.9681697612732095 \n",
      "Recall: 0.9746328437917223 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:29:51,027]\u001b[0m Trial 7 finished with value: 0.9729255718529863 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 2.125047619317934}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9839786381842457 \n",
      "Recall: 0.9839786381842457 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9814077025232404 \n",
      "Recall: 0.986648865153538 \n",
      "Aging Rate: 0.5026702269692924\n",
      "Precision: 0.9866131191432396 \n",
      "Recall: 0.9839786381842457 \n",
      "Aging Rate: 0.4986648865153538\n",
      "\u001b[32m[I 2022-05-26 14:29:53,903]\u001b[0m Trial 8 finished with value: 0.984431353586023 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.03049217348034851, 'min_data_in_leaf': 5, 'max_leaves': 20}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.972 \n",
      "Recall: 0.9732977303070761 \n",
      "Aging Rate: 0.5006675567423231\n",
      "Precision: 0.9772727272727273 \n",
      "Recall: 0.9759679572763685 \n",
      "Aging Rate: 0.4993324432576769\n",
      "Precision: 0.9694960212201591 \n",
      "Recall: 0.9759679572763685 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:29:55,022]\u001b[0m Trial 9 finished with value: 0.9739965209953495 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 6, 'learning_rate': 0.325, 'subsample': 0.3, 'l2_leaf_reg': 0.011101351877862573, 'min_data_in_leaf': 5, 'max_leaves': 20}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9867197875166003 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5026702269692924\n",
      "Precision: 0.9724047306176085 \n",
      "Recall: 0.9879839786381842 \n",
      "Aging Rate: 0.5080106809078772\n",
      "Precision: 0.9880478087649402 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5026702269692924\n",
      "\u001b[32m[I 2022-05-26 14:31:03,526]\u001b[0m Trial 10 finished with value: 0.9867196938298605 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.5, 'l2_leaf_reg': 0.12658891910120945}. Best is trial 2 with value: 0.9873650504591587.\u001b[0m\n",
      "Precision: 0.9841688654353562 \n",
      "Recall: 0.9959946595460614 \n",
      "Aging Rate: 0.5060080106809078\n",
      "Precision: 0.9841059602649007 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5040053404539386\n",
      "Precision: 0.9880636604774535 \n",
      "Recall: 0.9946595460614153 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:32:11,689]\u001b[0m Trial 11 finished with value: 0.9898096656210922 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.5, 'l2_leaf_reg': 0.09233193811937178}. Best is trial 11 with value: 0.9898096656210922.\u001b[0m\n",
      "Precision: 0.9893190921228304 \n",
      "Recall: 0.9893190921228304 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9854111405835544 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5033377837116155\n",
      "Precision: 0.9854304635761589 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5040053404539386\n",
      "\u001b[32m[I 2022-05-26 14:32:31,993]\u001b[0m Trial 12 finished with value: 0.9891233607803253 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.06279614402537187}. Best is trial 11 with value: 0.9898096656210922.\u001b[0m\n",
      "Precision: 0.9880159786950732 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5013351134846462\n",
      "Precision: 0.978891820580475 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5060080106809078\n",
      "Precision: 0.986737400530504 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:32:52,523]\u001b[0m Trial 13 finished with value: 0.9880303944201843 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.023079122375373122}. Best is trial 11 with value: 0.9898096656210922.\u001b[0m\n",
      "Precision: 0.9867021276595744 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5020026702269693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9893333333333333 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5006675567423231\n",
      "Precision: 0.9815059445178336 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5053404539385847\n",
      "\u001b[32m[I 2022-05-26 14:32:58,358]\u001b[0m Trial 14 finished with value: 0.988462444530355 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.07611989428884437, 'min_data_in_leaf': 55}. Best is trial 11 with value: 0.9898096656210922.\u001b[0m\n",
      "Precision: 0.9906790945406125 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5013351134846462\n",
      "Precision: 0.9906291834002677 \n",
      "Recall: 0.9879839786381842 \n",
      "Aging Rate: 0.4986648865153538\n",
      "Precision: 0.986737400530504 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:33:18,569]\u001b[0m Trial 15 finished with value: 0.990441590971355 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.2929459136948026}. Best is trial 15 with value: 0.990441590971355.\u001b[0m\n",
      "Precision: 0.9854111405835544 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5033377837116155\n",
      "Precision: 0.9867724867724867 \n",
      "Recall: 0.9959946595460614 \n",
      "Aging Rate: 0.5046728971962616\n",
      "Precision: 0.9880952380952381 \n",
      "Recall: 0.9973297730307076 \n",
      "Aging Rate: 0.5046728971962616\n",
      "\u001b[32m[I 2022-05-26 14:33:39,594]\u001b[0m Trial 16 finished with value: 0.9909141480788883 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.380929049822925}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9879194630872483 \n",
      "Recall: 0.9826435246995995 \n",
      "Aging Rate: 0.49732977303070763\n",
      "Precision: 0.9789750328515112 \n",
      "Recall: 0.9946595460614153 \n",
      "Aging Rate: 0.5080106809078772\n",
      "Precision: 0.9893758300132802 \n",
      "Recall: 0.9946595460614153 \n",
      "Aging Rate: 0.5026702269692924\n",
      "\u001b[32m[I 2022-05-26 14:34:00,443]\u001b[0m Trial 17 finished with value: 0.988013350136121 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.4062109223984209}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9826897470039947 \n",
      "Recall: 0.9853137516688919 \n",
      "Aging Rate: 0.5013351134846462\n",
      "Precision: 0.9854111405835544 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5033377837116155\n",
      "Precision: 0.9892761394101877 \n",
      "Recall: 0.9853137516688919 \n",
      "Aging Rate: 0.4979973297730307\n",
      "\u001b[32m[I 2022-05-26 14:34:07,107]\u001b[0m Trial 18 finished with value: 0.9866600859967171 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.402220991978786, 'min_data_in_leaf': 45}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9867021276595744 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5020026702269693\n",
      "Precision: 0.9759358288770054 \n",
      "Recall: 0.9746328437917223 \n",
      "Aging Rate: 0.4993324432576769\n",
      "Precision: 0.9919354838709677 \n",
      "Recall: 0.9853137516688919 \n",
      "Aging Rate: 0.49666221628838453\n",
      "\u001b[32m[I 2022-05-26 14:34:22,012]\u001b[0m Trial 19 finished with value: 0.9841905493766352 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 0.21855578677444798}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9773635153129161 \n",
      "Recall: 0.9799732977303071 \n",
      "Aging Rate: 0.5013351134846462\n",
      "Precision: 0.9773936170212766 \n",
      "Recall: 0.9813084112149533 \n",
      "Aging Rate: 0.5020026702269693\n",
      "Precision: 0.9853333333333333 \n",
      "Recall: 0.986648865153538 \n",
      "Aging Rate: 0.5006675567423231\n",
      "\u001b[32m[I 2022-05-26 14:34:35,325]\u001b[0m Trial 20 finished with value: 0.9813348096796686 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 2.978972382752246}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9789473684210527 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.507343124165554\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5046728971962616\n",
      "Precision: 0.9815059445178336 \n",
      "Recall: 0.9919893190921228 \n",
      "Aging Rate: 0.5053404539385847\n",
      "\u001b[32m[I 2022-05-26 14:35:44,230]\u001b[0m Trial 21 finished with value: 0.9871692018198139 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.14133695297725335}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9840848806366048 \n",
      "Recall: 0.9906542056074766 \n",
      "Aging Rate: 0.5033377837116155\n",
      "Precision: 0.9853723404255319 \n",
      "Recall: 0.9893190921228304 \n",
      "Aging Rate: 0.5020026702269693\n",
      "Precision: 0.9920318725099602 \n",
      "Recall: 0.9973297730307076 \n",
      "Aging Rate: 0.5026702269692924\n",
      "\u001b[32m[I 2022-05-26 14:36:53,322]\u001b[0m Trial 22 finished with value: 0.9897913855206504 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.5283761607231975}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9815546772068511 \n",
      "Recall: 0.9946595460614153 \n",
      "Aging Rate: 0.5066755674232309\n",
      "Precision: 0.9802631578947368 \n",
      "Recall: 0.9946595460614153 \n",
      "Aging Rate: 0.507343124165554\n",
      "Precision: 0.986737400530504 \n",
      "Recall: 0.9933244325767691 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:37:13,695]\u001b[0m Trial 23 finished with value: 0.9884975002034365 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 0.2596375838452136}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Precision: 0.9854497354497355 \n",
      "Recall: 0.9946595460614153 \n",
      "Aging Rate: 0.5046728971962616\n",
      "Precision: 0.9816031537450722 \n",
      "Recall: 0.9973297730307076 \n",
      "Aging Rate: 0.5080106809078772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 14:38:23,142]\u001b[0m A new study created in memory with name: no-name-5433bad0-5e32-4232-980d-fdc80c200ed6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9893899204244032 \n",
      "Recall: 0.9959946595460614 \n",
      "Aging Rate: 0.5033377837116155\n",
      "\u001b[32m[I 2022-05-26 14:38:23,064]\u001b[0m Trial 24 finished with value: 0.9907061667199485 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.5, 'l2_leaf_reg': 0.12284307980997534}. Best is trial 16 with value: 0.9909141480788883.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 3 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d82bd648444f96aebe6c9c0f52f9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9959677419354839 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.998661311914324 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9986449864498645 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.4855263157894737\n",
      "\u001b[32m[I 2022-05-26 14:38:31,587]\u001b[0m Trial 0 finished with value: 0.9864658095034141 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 2.5384816180424425, 'min_data_in_leaf': 35, 'max_leaves': 35}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9927849927849928 \n",
      "Recall: 0.9052631578947369 \n",
      "Aging Rate: 0.45592105263157895\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9157894736842105 \n",
      "Aging Rate: 0.45921052631578946\n",
      "Precision: 0.9957081545064378 \n",
      "Recall: 0.9157894736842105 \n",
      "Aging Rate: 0.4598684210526316\n",
      "\u001b[32m[I 2022-05-26 14:38:33,459]\u001b[0m Trial 1 finished with value: 0.951938946692889 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 2, 'learning_rate': 0.025, 'subsample': 0.3, 'l2_leaf_reg': 0.44045686210710805, 'min_data_in_leaf': 60}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9929775280898876 \n",
      "Recall: 0.9302631578947368 \n",
      "Aging Rate: 0.46842105263157896\n",
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9342105263157895 \n",
      "Aging Rate: 0.47039473684210525\n",
      "Precision: 0.9958275382475661 \n",
      "Recall: 0.9421052631578948 \n",
      "Aging Rate: 0.4730263157894737\n",
      "\u001b[32m[I 2022-05-26 14:38:36,569]\u001b[0m Trial 2 finished with value: 0.9638438206536479 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 2, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 0.03888249756758852, 'min_data_in_leaf': 50, 'max_leaves': 40}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9972183588317107 \n",
      "Recall: 0.9434210526315789 \n",
      "Aging Rate: 0.4730263157894737\n",
      "Precision: 0.997179125528914 \n",
      "Recall: 0.9302631578947368 \n",
      "Aging Rate: 0.4664473684210526\n",
      "Precision: 0.9985795454545454 \n",
      "Recall: 0.925 \n",
      "Aging Rate: 0.4631578947368421\n",
      "\u001b[32m[I 2022-05-26 14:38:40,370]\u001b[0m Trial 3 finished with value: 0.9641720381672779 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.7244832767757048, 'min_data_in_leaf': 5}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.995945945945946 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.4868421052631579\n",
      "Precision: 1.0 \n",
      "Recall: 0.9618421052631579 \n",
      "Aging Rate: 0.48092105263157897\n",
      "Precision: 0.995945945945946 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.4868421052631579\n",
      "\u001b[32m[I 2022-05-26 14:38:42,196]\u001b[0m Trial 4 finished with value: 0.9819610999329309 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 2, 'learning_rate': 0.325, 'subsample': 0.5, 'l2_leaf_reg': 0.6045741851545887, 'min_data_in_leaf': 35}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 1.0 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.48486842105263156\n",
      "Precision: 1.0 \n",
      "Recall: 0.9657894736842105 \n",
      "Aging Rate: 0.48289473684210527\n",
      "\u001b[32m[I 2022-05-26 14:38:47,723]\u001b[0m Trial 5 finished with value: 0.9859871567503858 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 0.038978073203875946}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9906417112299465 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9933333333333333 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 14:38:52,985]\u001b[0m Trial 6 finished with value: 0.985859082684842 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 0.01659204274879708, 'min_data_in_leaf': 55, 'max_leaves': 10}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9657894736842105 \n",
      "Aging Rate: 0.48289473684210527\n",
      "Precision: 1.0 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4881578947368421\n",
      "Precision: 0.9973118279569892 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.48947368421052634\n",
      "\u001b[32m[I 2022-05-26 14:39:08,944]\u001b[0m Trial 7 finished with value: 0.9857717204136197 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.9, 'l2_leaf_reg': 0.12457954105428501}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9888579387186629 \n",
      "Recall: 0.9342105263157895 \n",
      "Aging Rate: 0.4723684210526316\n",
      "Precision: 0.9986013986013986 \n",
      "Recall: 0.9394736842105263 \n",
      "Aging Rate: 0.47039473684210525\n",
      "Precision: 0.9971870604781997 \n",
      "Recall: 0.9328947368421052 \n",
      "Aging Rate: 0.4677631578947368\n",
      "\u001b[32m[I 2022-05-26 14:39:10,066]\u001b[0m Trial 8 finished with value: 0.9642878207934795 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 5.951169838375549}. Best is trial 0 with value: 0.9864658095034141.\u001b[0m\n",
      "Precision: 0.9946949602122016 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9973474801061007 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4953947368421053\n",
      "\u001b[32m[I 2022-05-26 14:39:12,812]\u001b[0m Trial 9 finished with value: 0.9909706657958925 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 2, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 0.15121016664350911}. Best is trial 9 with value: 0.9909706657958925.\u001b[0m\n",
      "Precision: 0.9986577181208054 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9959731543624161 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9973509933774835 \n",
      "Recall: 0.9907894736842106 \n",
      "Aging Rate: 0.4967105263157895\n",
      "\u001b[32m[I 2022-05-26 14:49:47,928]\u001b[0m Trial 10 finished with value: 0.9896034121684595 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.325, 'subsample': 0.3, 'l2_leaf_reg': 0.1491602202326065}. Best is trial 9 with value: 0.9909706657958925.\u001b[0m\n",
      "Precision: 0.9959731543624161 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9973404255319149 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9986684420772304 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49407894736842106\n",
      "\u001b[32m[I 2022-05-26 15:00:45,582]\u001b[0m Trial 11 finished with value: 0.9902766855454894 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 0.12855155584882486}. Best is trial 9 with value: 0.9909706657958925.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 1.0 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9986666666666667 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.4934210526315789\n",
      "\u001b[32m[I 2022-05-26 15:11:36,306]\u001b[0m Trial 12 finished with value: 0.9922680178299649 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 0.11547469673910576}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 1.0 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4888157894736842\n",
      "Precision: 1.0 \n",
      "Recall: 0.9736842105263158 \n",
      "Aging Rate: 0.4868421052631579\n",
      "\u001b[32m[I 2022-05-26 15:14:17,016]\u001b[0m Trial 13 finished with value: 0.9886948105261147 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.045960886533715294}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9986666666666667 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9959677419354839 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 1.0 \n",
      "Recall: 0.9671052631578947 \n",
      "Aging Rate: 0.48355263157894735\n",
      "\u001b[32m[I 2022-05-26 15:16:59,442]\u001b[0m Trial 14 finished with value: 0.9869009708437421 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 0.2489488801307226}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9973118279569892 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.9986737400530504 \n",
      "Recall: 0.9907894736842106 \n",
      "Aging Rate: 0.49605263157894736\n",
      "\u001b[32m[I 2022-05-26 15:17:07,217]\u001b[0m Trial 15 finished with value: 0.9904837781329631 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 1.6924258114720987}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9946308724832215 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.998661311914324 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 15:17:39,944]\u001b[0m Trial 16 finished with value: 0.9876092394504467 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 10, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.017535987177569212}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9986559139784946 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9933862433862434 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.49736842105263157\n",
      "\u001b[32m[I 2022-05-26 15:28:23,372]\u001b[0m Trial 17 finished with value: 0.9898418544062556 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 0.06617409479688267}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9973190348525469 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 0.996 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9973297730307076 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49276315789473685\n",
      "\u001b[32m[I 2022-05-26 15:28:51,505]\u001b[0m Trial 18 finished with value: 0.9891704748073304 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 0.2714582665524926}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9933774834437086 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4967105263157895\n",
      "Precision: 0.9986648865153538 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49276315789473685\n",
      "\u001b[32m[I 2022-05-26 15:28:59,255]\u001b[0m Trial 19 finished with value: 0.9898492311908708 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 1.1165987815674312, 'min_data_in_leaf': 5}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9736842105263158 \n",
      "Aging Rate: 0.4868421052631579\n",
      "Precision: 1.0 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.9945872801082544 \n",
      "Recall: 0.9671052631578947 \n",
      "Aging Rate: 0.4861842105263158\n",
      "\u001b[32m[I 2022-05-26 15:29:00,829]\u001b[0m Trial 20 finished with value: 0.9855607126579263 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 4, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.07079182338460112, 'min_data_in_leaf': 20, 'max_leaves': 15}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 1.0 \n",
      "Recall: 0.9710526315789474 \n",
      "Aging Rate: 0.4855263157894737\n",
      "Precision: 0.9986595174262735 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49078947368421055\n",
      "\u001b[32m[I 2022-05-26 15:29:09,098]\u001b[0m Trial 21 finished with value: 0.9897995254637223 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 1.8237672306749273}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9973297730307076 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9986577181208054 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.4901315789473684\n",
      "\u001b[32m[I 2022-05-26 15:29:17,581]\u001b[0m Trial 22 finished with value: 0.989830812635062 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 9.956148108335649}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 0.99867197875166 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9946595460614153 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9960159362549801 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4953947368421053\n",
      "\u001b[32m[I 2022-05-26 15:32:00,953]\u001b[0m Trial 23 finished with value: 0.9909560774444177 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 3.0778782385370045}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4888157894736842\n",
      "Precision: 0.9986541049798116 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4888157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 15:34:41,642]\u001b[0m A new study created in memory with name: no-name-0fe7bcc8-386e-48fc-bd8a-d424a5bf4aae\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9986595174262735 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49078947368421055\n",
      "\u001b[32m[I 2022-05-26 15:34:41,548]\u001b[0m Trial 24 finished with value: 0.988474578068299 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 4.468796458318151}. Best is trial 12 with value: 0.9922680178299649.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 4 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a72bb6ad7bb416d871244e396ef6554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.996005326231691 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.9907773386034255 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4993421052631579\n",
      "Precision: 0.9933774834437086 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4967105263157895\n",
      "\u001b[32m[I 2022-05-26 15:34:45,486]\u001b[0m Trial 0 finished with value: 0.9900989638875067 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 0.16680816502640744, 'min_data_in_leaf': 45, 'max_leaves': 45}. Best is trial 0 with value: 0.9900989638875067.\u001b[0m\n",
      "Precision: 0.9566929133858267 \n",
      "Recall: 0.9592105263157895 \n",
      "Aging Rate: 0.5013157894736842\n",
      "Precision: 0.9453125 \n",
      "Recall: 0.9552631578947368 \n",
      "Aging Rate: 0.5052631578947369\n",
      "Precision: 0.9579500657030223 \n",
      "Recall: 0.9592105263157895 \n",
      "Aging Rate: 0.5006578947368421\n",
      "\u001b[32m[I 2022-05-26 15:34:47,950]\u001b[0m Trial 1 finished with value: 0.9555972424881798 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 2, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 2.6933804776234354, 'min_data_in_leaf': 55, 'max_leaves': 30}. Best is trial 0 with value: 0.9900989638875067.\u001b[0m\n",
      "Precision: 0.9920948616600791 \n",
      "Recall: 0.9907894736842106 \n",
      "Aging Rate: 0.4993421052631579\n",
      "Precision: 0.9959946595460614 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9960264900662251 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4967105263157895\n",
      "\u001b[32m[I 2022-05-26 15:34:50,164]\u001b[0m Trial 2 finished with value: 0.9909717576709921 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 3.721089554027589, 'min_data_in_leaf': 50}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9959946595460614 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9946879150066401 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9829619921363041 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.5019736842105263\n",
      "\u001b[32m[I 2022-05-26 15:34:54,479]\u001b[0m Trial 3 finished with value: 0.98790613676417 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 4, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.023794017261651996}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9036755386565273 \n",
      "Recall: 0.9381578947368421 \n",
      "Aging Rate: 0.5190789473684211\n",
      "Precision: 0.9019354838709678 \n",
      "Recall: 0.9197368421052632 \n",
      "Aging Rate: 0.5098684210526315\n",
      "Precision: 0.906578947368421 \n",
      "Recall: 0.906578947368421 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2022-05-26 15:34:55,129]\u001b[0m Trial 4 finished with value: 0.9126406882016426 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 2, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 0.014497795312978453, 'min_data_in_leaf': 40, 'max_leaves': 45}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9650537634408602 \n",
      "Recall: 0.9447368421052632 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.9613848202396804 \n",
      "Recall: 0.95 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.9402597402597402 \n",
      "Recall: 0.9526315789473684 \n",
      "Aging Rate: 0.506578947368421\n",
      "\u001b[32m[I 2022-05-26 15:35:03,518]\u001b[0m Trial 5 finished with value: 0.9522836557008366 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 2, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 3.1545708136226054}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9736495388669302 \n",
      "Recall: 0.9723684210526315 \n",
      "Aging Rate: 0.4993421052631579\n",
      "Precision: 0.9919137466307277 \n",
      "Recall: 0.968421052631579 \n",
      "Aging Rate: 0.4881578947368421\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 15:35:04,374]\u001b[0m Trial 6 finished with value: 0.9808177262794864 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 4, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 0.14922351424921576, 'min_data_in_leaf': 45}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9867724867724867 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9829172141918529 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.5006578947368421\n",
      "Precision: 0.9841897233201581 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4993421052631579\n",
      "\u001b[32m[I 2022-05-26 15:35:16,464]\u001b[0m Trial 7 finished with value: 0.9837580381185215 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 2, 'learning_rate': 0.275, 'subsample': 0.7, 'l2_leaf_reg': 0.019421623803190305}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9920424403183024 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9973404255319149 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 15:35:21,822]\u001b[0m Trial 8 finished with value: 0.9898641486047914 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.5071216436069199, 'min_data_in_leaf': 5, 'max_leaves': 25}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.996 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9946236559139785 \n",
      "Recall: 0.9736842105263158 \n",
      "Aging Rate: 0.48947368421052634\n",
      "\u001b[32m[I 2022-05-26 15:35:31,989]\u001b[0m Trial 9 finished with value: 0.9876215053731375 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.3, 'l2_leaf_reg': 0.19074848109979162}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9946949602122016 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9960159362549801 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9946949602122016 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49605263157894736\n",
      "\u001b[32m[I 2022-05-26 15:35:35,664]\u001b[0m Trial 10 finished with value: 0.9909712478641731 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 12, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 1.0637555609569331, 'min_data_in_leaf': 20}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9908015768725361 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.5006578947368421\n",
      "Precision: 0.9919678714859438 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9973297730307076 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49276315789473685\n",
      "\u001b[32m[I 2022-05-26 15:35:39,677]\u001b[0m Trial 11 finished with value: 0.988307794478072 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 12, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 8.082689307673373, 'min_data_in_leaf': 15}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9791395045632334 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.5046052631578948\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.9973262032085561 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 15:35:43,242]\u001b[0m Trial 12 finished with value: 0.9868144984117005 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 12, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 0.8363214497699601, 'min_data_in_leaf': 25}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.996042216358839 \n",
      "Recall: 0.993421052631579 \n",
      "Aging Rate: 0.4986842105263158\n",
      "Precision: 0.9933510638297872 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.993368700265252 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49605263157894736\n",
      "\u001b[32m[I 2022-05-26 15:35:46,397]\u001b[0m Trial 13 finished with value: 0.9907523713881764 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 1.7948471095442178, 'min_data_in_leaf': 20}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9894736842105263 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9882352941176471 \n",
      "Recall: 0.9947368421052631 \n",
      "Aging Rate: 0.5032894736842105\n",
      "Precision: 0.9920634920634921 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49736842105263157\n",
      "\u001b[32m[I 2022-05-26 15:35:48,644]\u001b[0m Trial 14 finished with value: 0.9901316681122765 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 9.293850886100012, 'min_data_in_leaf': 60}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9946879150066401 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9933862433862434 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.49736842105263157\n",
      "\u001b[32m[I 2022-05-26 15:35:52,260]\u001b[0m Trial 15 finished with value: 0.9896382513302214 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 10, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 1.220006108445875, 'min_data_in_leaf': 5}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9881578947368421 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.994750656167979 \n",
      "Recall: 0.9973684210526316 \n",
      "Aging Rate: 0.5013157894736842\n",
      "Precision: 0.9933510638297872 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 15:35:55,100]\u001b[0m Trial 16 finished with value: 0.9907703171639128 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 4.747731944047413, 'min_data_in_leaf': 30}. Best is trial 2 with value: 0.9909717576709921.\u001b[0m\n",
      "Precision: 0.9960474308300395 \n",
      "Recall: 0.9947368421052631 \n",
      "Aging Rate: 0.4993421052631579\n",
      "Precision: 0.996005326231691 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.996005326231691 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49407894736842106\n",
      "\u001b[32m[I 2022-05-26 15:35:57,247]\u001b[0m Trial 17 finished with value: 0.9918457680034076 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 0.4827514234491299, 'min_data_in_leaf': 35}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.9829619921363041 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.5019736842105263\n",
      "Precision: 0.9973262032085561 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9894039735099338 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4967105263157895\n",
      "\u001b[32m[I 2022-05-26 15:35:59,183]\u001b[0m Trial 18 finished with value: 0.9868089204896601 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 0.05078380107293134, 'min_data_in_leaf': 40}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.9920424403183024 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9907407407407407 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9907529722589168 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4980263157894737\n",
      "\u001b[32m[I 2022-05-26 15:36:01,367]\u001b[0m Trial 19 finished with value: 0.9883437617099705 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 0.37663182905546977, 'min_data_in_leaf': 55}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.99867197875166 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9986648865153538 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9946380697050938 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.49078947368421055\n",
      "\u001b[32m[I 2022-05-26 15:36:04,035]\u001b[0m Trial 20 finished with value: 0.9902761142226603 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 8, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.08923227812693033, 'min_data_in_leaf': 35}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.9960264900662251 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4967105263157895\n",
      "Precision: 0.9919354838709677 \n",
      "Recall: 0.9710526315789474 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.9946737683089214 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49407894736842106\n",
      "\u001b[32m[I 2022-05-26 15:36:06,218]\u001b[0m Trial 21 finished with value: 0.9876238084613621 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.7362414195101428, 'min_data_in_leaf': 15}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.9894179894179894 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9933949801849405 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4980263157894737\n",
      "Precision: 0.9894179894179894 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49736842105263157\n",
      "\u001b[32m[I 2022-05-26 15:36:08,120]\u001b[0m Trial 22 finished with value: 0.9883484101902275 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 1.5374808917813838, 'min_data_in_leaf': 30}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.9959568733153639 \n",
      "Recall: 0.9723684210526315 \n",
      "Aging Rate: 0.4881578947368421\n",
      "Precision: 0.9907407407407407 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49407894736842106\n",
      "\u001b[32m[I 2022-05-26 15:36:10,761]\u001b[0m Trial 23 finished with value: 0.9865244999997702 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 8, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 4.860813897871439, 'min_data_in_leaf': 50}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Precision: 0.990728476821192 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4967105263157895\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4953947368421053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 15:36:13,088]\u001b[0m A new study created in memory with name: no-name-0b2a3cd6-57d5-4456-8754-ea6978a02b64\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.988031914893617 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 15:36:13,010]\u001b[0m Trial 24 finished with value: 0.9863423412075468 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.7, 'l2_leaf_reg': 0.5937295602413626, 'min_data_in_leaf': 20}. Best is trial 17 with value: 0.9918457680034076.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 5 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd0c23c5df941b280468e1d62b57a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9708222811671088 \n",
      "Recall: 0.9695364238410596 \n",
      "Aging Rate: 0.4976897689768977\n",
      "Precision: 0.9670619235836627 \n",
      "Recall: 0.9721854304635762 \n",
      "Aging Rate: 0.500990099009901\n",
      "Precision: 0.9645203679369251 \n",
      "Recall: 0.9721854304635762 \n",
      "Aging Rate: 0.5023102310231023\n",
      "\u001b[32m[I 2022-05-26 15:36:14,699]\u001b[0m Trial 0 finished with value: 0.9693778553875969 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 2, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.45913347732143145, 'min_data_in_leaf': 30, 'max_leaves': 45}. Best is trial 0 with value: 0.9693778553875969.\u001b[0m\n",
      "Precision: 0.9920634920634921 \n",
      "Recall: 0.9933774834437086 \n",
      "Aging Rate: 0.499009900990099\n",
      "Precision: 0.9946737683089214 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.4957095709570957\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.4957095709570957\n",
      "\u001b[32m[I 2022-05-26 15:36:25,951]\u001b[0m Trial 1 finished with value: 0.9918185922388832 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 2.6834855975498333}. Best is trial 1 with value: 0.9918185922388832.\u001b[0m\n",
      "Precision: 0.9973404255319149 \n",
      "Recall: 0.9933774834437086 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.49504950495049505\n",
      "Precision: 0.9973368841544608 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.4957095709570957\n",
      "\u001b[32m[I 2022-05-26 15:36:36,010]\u001b[0m Trial 2 finished with value: 0.9946876195050026 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 0.031587233469087324}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9841688654353562 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.5003300330033004\n",
      "Precision: 0.9893899204244032 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.4976897689768977\n",
      "Precision: 0.9894736842105263 \n",
      "Recall: 0.9960264900662251 \n",
      "Aging Rate: 0.5016501650165016\n",
      "\u001b[32m[I 2022-05-26 15:36:40,207]\u001b[0m Trial 3 finished with value: 0.9891979419468045 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 4, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.06864809683944641}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9893048128342246 \n",
      "Recall: 0.9801324503311258 \n",
      "Aging Rate: 0.49372937293729374\n",
      "Precision: 0.9842105263157894 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.5016501650165016\n",
      "Precision: 0.9841059602649007 \n",
      "Recall: 0.9841059602649007 \n",
      "Aging Rate: 0.49834983498349833\n",
      "\u001b[32m[I 2022-05-26 15:36:50,534]\u001b[0m Trial 4 finished with value: 0.9854206594206367 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 2, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 0.7734559099729885}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9907407407407407 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.499009900990099\n",
      "Precision: 0.9907651715039578 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.5003300330033004\n",
      "Precision: 0.9881266490765171 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.5003300330033004\n",
      "\u001b[32m[I 2022-05-26 15:36:53,998]\u001b[0m Trial 5 finished with value: 0.9914040081190606 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 0.11641158094312971, 'min_data_in_leaf': 55, 'max_leaves': 45}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9946595460614153 \n",
      "Recall: 0.9867549668874173 \n",
      "Aging Rate: 0.49438943894389437\n",
      "Precision: 0.9973226238286479 \n",
      "Recall: 0.9867549668874173 \n",
      "Aging Rate: 0.49306930693069306\n",
      "Precision: 0.9960212201591512 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.4976897689768977\n",
      "\u001b[32m[I 2022-05-26 15:36:57,337]\u001b[0m Trial 6 finished with value: 0.9926877693868019 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 12, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 7.540822519560752, 'min_data_in_leaf': 45}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.996 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.49504950495049505\n",
      "Precision: 0.9960106382978723 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.981675392670157 \n",
      "Recall: 0.9933774834437086 \n",
      "Aging Rate: 0.5042904290429043\n",
      "\u001b[32m[I 2022-05-26 15:36:58,521]\u001b[0m Trial 7 finished with value: 0.9914035569141734 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 0.5272783650845906, 'min_data_in_leaf': 45}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9933862433862434 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.499009900990099\n",
      "Precision: 0.984313725490196 \n",
      "Recall: 0.9973509933774835 \n",
      "Aging Rate: 0.504950495049505\n",
      "Precision: 0.9894179894179894 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.499009900990099\n",
      "\u001b[32m[I 2022-05-26 15:37:01,439]\u001b[0m Trial 8 finished with value: 0.9916353176123631 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 4, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 0.05949756578710615, 'min_data_in_leaf': 55, 'max_leaves': 50}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9933510638297872 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.9894598155467721 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.500990099009901\n",
      "\u001b[32m[I 2022-05-26 15:37:03,623]\u001b[0m Trial 9 finished with value: 0.9918300021513223 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 8, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 0.16321765013677547}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9920212765957447 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.9946879150066401 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.99867197875166 \n",
      "Recall: 0.9960264900662251 \n",
      "Aging Rate: 0.497029702970297\n",
      "\u001b[32m[I 2022-05-26 15:37:13,995]\u001b[0m Trial 10 finished with value: 0.9935875434239391 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.5, 'l2_leaf_reg': 0.014412107538962089}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9960106382978723 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.4957095709570957\n",
      "Precision: 0.9920212765957447 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.49636963696369635\n",
      "\u001b[32m[I 2022-05-26 15:37:24,523]\u001b[0m Trial 11 finished with value: 0.9915927237007877 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.5, 'l2_leaf_reg': 0.010758455270175098}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9933949801849405 \n",
      "Recall: 0.9960264900662251 \n",
      "Aging Rate: 0.4996699669966997\n",
      "Precision: 0.9959623149394348 \n",
      "Recall: 0.9801324503311258 \n",
      "Aging Rate: 0.4904290429042904\n",
      "Precision: 0.9959946595460614 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.49438943894389437\n",
      "\u001b[32m[I 2022-05-26 15:37:33,626]\u001b[0m Trial 12 finished with value: 0.9915714166476413 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.025, 'subsample': 0.5, 'l2_leaf_reg': 0.014780798785910326}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9986631016042781 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.49372937293729374\n",
      "Precision: 0.9920424403183024 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.4976897689768977\n",
      "Precision: 0.9894039735099338 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.49834983498349833\n",
      "\u001b[32m[I 2022-05-26 15:37:50,329]\u001b[0m Trial 13 finished with value: 0.9916003242506687 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 0.02553937774186112}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9920529801324504 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.49834983498349833\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.4957095709570957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9907407407407407 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.499009900990099\n",
      "\u001b[32m[I 2022-05-26 15:38:02,165]\u001b[0m Trial 14 finished with value: 0.9913844192006267 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.3, 'l2_leaf_reg': 0.02727277659925095}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9946879150066401 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.9947089947089947 \n",
      "Recall: 0.9960264900662251 \n",
      "Aging Rate: 0.499009900990099\n",
      "Precision: 0.990728476821192 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.49834983498349833\n",
      "\u001b[32m[I 2022-05-26 15:38:10,542]\u001b[0m Trial 15 finished with value: 0.9931548278353445 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 0.010718287858992486, 'min_data_in_leaf': 5}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.99734395750332 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.9959839357429718 \n",
      "Recall: 0.9854304635761589 \n",
      "Aging Rate: 0.49306930693069306\n",
      "Precision: 0.9828496042216359 \n",
      "Recall: 0.9867549668874173 \n",
      "Aging Rate: 0.5003300330033004\n",
      "\u001b[32m[I 2022-05-26 15:38:27,846]\u001b[0m Trial 16 finished with value: 0.990499576149095 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 0.031708640946253}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9933862433862434 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.499009900990099\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.4957095709570957\n",
      "Precision: 0.9842312746386334 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.5023102310231023\n",
      "\u001b[32m[I 2022-05-26 15:38:40,522]\u001b[0m Trial 17 finished with value: 0.9909580600068223 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 6, 'learning_rate': 0.325, 'subsample': 0.5, 'l2_leaf_reg': 0.21403476778792288}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.9986648865153538 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.49438943894389437\n",
      "Precision: 0.9920212765957447 \n",
      "Recall: 0.9880794701986755 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.9920529801324504 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.49834983498349833\n",
      "\u001b[32m[I 2022-05-26 15:38:57,273]\u001b[0m Trial 18 finished with value: 0.9922600936989148 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.3, 'l2_leaf_reg': 0.06038338678990859}. Best is trial 2 with value: 0.9946876195050026.\u001b[0m\n",
      "Precision: 0.99734395750332 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.49504950495049505\n",
      "Precision: 0.99867197875166 \n",
      "Recall: 0.9960264900662251 \n",
      "Aging Rate: 0.497029702970297\n",
      "\u001b[32m[I 2022-05-26 15:39:02,935]\u001b[0m Trial 19 finished with value: 0.9957962112733565 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 0.019136825691923324, 'min_data_in_leaf': 5}. Best is trial 19 with value: 0.9957962112733565.\u001b[0m\n",
      "Precision: 0.9920318725099602 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.9920739762219286 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.4996699669966997\n",
      "Precision: 0.9946879150066401 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.497029702970297\n",
      "\u001b[32m[I 2022-05-26 15:39:04,150]\u001b[0m Trial 20 finished with value: 0.9924903746742828 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 1.4754157763157638, 'min_data_in_leaf': 5}. Best is trial 19 with value: 0.9957962112733565.\u001b[0m\n",
      "Precision: 0.9960106382978723 \n",
      "Recall: 0.9920529801324504 \n",
      "Aging Rate: 0.49636963696369635\n",
      "Precision: 0.996005326231691 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.4957095709570957\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.497029702970297\n",
      "\u001b[32m[I 2022-05-26 15:39:10,520]\u001b[0m Trial 21 finished with value: 0.9931434013389603 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.01874507900638698, 'min_data_in_leaf': 20}. Best is trial 19 with value: 0.9957962112733565.\u001b[0m\n",
      "Precision: 0.9960159362549801 \n",
      "Recall: 0.9933774834437086 \n",
      "Aging Rate: 0.497029702970297\n",
      "Precision: 0.9906790945406125 \n",
      "Recall: 0.9854304635761589 \n",
      "Aging Rate: 0.4957095709570957\n",
      "Precision: 0.9960212201591512 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.4976897689768977\n",
      "\u001b[32m[I 2022-05-26 15:39:15,375]\u001b[0m Trial 22 finished with value: 0.9927013117708211 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 0.04291147478625964, 'min_data_in_leaf': 15}. Best is trial 19 with value: 0.9957962112733565.\u001b[0m\n",
      "Precision: 0.9933774834437086 \n",
      "Recall: 0.9933774834437086 \n",
      "Aging Rate: 0.49834983498349833\n",
      "Precision: 0.9920424403183024 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.4976897689768977\n",
      "Precision: 0.9881578947368421 \n",
      "Recall: 0.9947019867549669 \n",
      "Aging Rate: 0.5016501650165016\n",
      "\u001b[32m[I 2022-05-26 15:39:23,515]\u001b[0m Trial 23 finished with value: 0.9920605495173561 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.011193529334577538, 'min_data_in_leaf': 15}. Best is trial 19 with value: 0.9957962112733565.\u001b[0m\n",
      "Precision: 0.9920424403183024 \n",
      "Recall: 0.990728476821192 \n",
      "Aging Rate: 0.4976897689768977\n",
      "Precision: 0.996031746031746 \n",
      "Recall: 0.9973509933774835 \n",
      "Aging Rate: 0.499009900990099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 15:39:39,912]\u001b[0m A new study created in memory with name: no-name-2715824d-1192-4e6a-98a3-fe0daa50ec16\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9920318725099602 \n",
      "Recall: 0.9894039735099338 \n",
      "Aging Rate: 0.497029702970297\n",
      "\u001b[32m[I 2022-05-26 15:39:39,833]\u001b[0m Trial 24 finished with value: 0.9929307122407903 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 0.0899983278502236}. Best is trial 19 with value: 0.9957962112733565.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 6 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a5b44d2fd14691896dca67561282b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9852348993288591 \n",
      "Recall: 0.9825970548862115 \n",
      "Aging Rate: 0.49435965494359657\n",
      "Precision: 0.9892183288409704 \n",
      "Recall: 0.9825970548862115 \n",
      "Aging Rate: 0.49236894492368943\n",
      "Precision: 0.9893190921228304 \n",
      "Recall: 0.9919678714859438 \n",
      "Aging Rate: 0.49701393497013935\n",
      "\u001b[32m[I 2022-05-26 15:39:45,105]\u001b[0m Trial 0 finished with value: 0.9868174984092332 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.07500000000000001, 'subsample': 0.3, 'l2_leaf_reg': 1.4307959836528163, 'min_data_in_leaf': 60}. Best is trial 0 with value: 0.9868174984092332.\u001b[0m\n",
      "Precision: 0.9814077025232404 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.4996682149966821\n",
      "Precision: 0.9736842105263158 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.504313205043132\n",
      "Precision: 0.9827127659574468 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.49900464499004643\n",
      "\u001b[32m[I 2022-05-26 15:39:53,246]\u001b[0m Trial 1 finished with value: 0.9844692011981543 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.10870597359627675}. Best is trial 0 with value: 0.9868174984092332.\u001b[0m\n",
      "Precision: 0.9840425531914894 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.49900464499004643\n",
      "Precision: 0.9802371541501976 \n",
      "Recall: 0.9959839357429718 \n",
      "Aging Rate: 0.5036496350364964\n",
      "Precision: 0.9906417112299465 \n",
      "Recall: 0.9919678714859438 \n",
      "Aging Rate: 0.49635036496350365\n",
      "\u001b[32m[I 2022-05-26 15:40:35,104]\u001b[0m Trial 2 finished with value: 0.9888923466155103 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 1.2988211101884832}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9798115746971736 \n",
      "Recall: 0.9745649263721553 \n",
      "Aging Rate: 0.4930325149303251\n",
      "Precision: 0.9918144611186903 \n",
      "Recall: 0.9732262382864793 \n",
      "Aging Rate: 0.48639681486396813\n",
      "Precision: 0.9877717391304348 \n",
      "Recall: 0.9732262382864793 \n",
      "Aging Rate: 0.48838752488387527\n",
      "\u001b[32m[I 2022-05-26 15:40:37,052]\u001b[0m Trial 3 finished with value: 0.9800195614387327 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.17500000000000002, 'subsample': 0.7, 'l2_leaf_reg': 0.25626483190317023, 'min_data_in_leaf': 50}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9748010610079576 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.5003317850033179\n",
      "Precision: 0.9798927613941019 \n",
      "Recall: 0.9785809906291834 \n",
      "Aging Rate: 0.49502322495023227\n",
      "Precision: 0.9852348993288591 \n",
      "Recall: 0.9825970548862115 \n",
      "Aging Rate: 0.49435965494359657\n",
      "\u001b[32m[I 2022-05-26 15:40:39,457]\u001b[0m Trial 4 finished with value: 0.9808325825839828 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 6, 'learning_rate': 0.07500000000000001, 'subsample': 0.3, 'l2_leaf_reg': 3.7417736649448297, 'min_data_in_leaf': 45, 'max_leaves': 25}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9698162729658792 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.5056403450564034\n",
      "Precision: 0.9836956521739131 \n",
      "Recall: 0.9692101740294511 \n",
      "Aging Rate: 0.48838752488387527\n",
      "Precision: 0.9852941176470589 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.49635036496350365\n",
      "\u001b[32m[I 2022-05-26 15:41:06,007]\u001b[0m Trial 5 finished with value: 0.9806029872858781 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 4, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 0.07927939120234184}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9775132275132276 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.5016589250165893\n",
      "Precision: 0.9865410497981157 \n",
      "Recall: 0.9812583668005355 \n",
      "Aging Rate: 0.4930325149303251\n",
      "Precision: 0.9919028340080972 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.49170537491705374\n",
      "\u001b[32m[I 2022-05-26 15:41:08,455]\u001b[0m Trial 6 finished with value: 0.9850541477963944 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 0.02168408733107976, 'min_data_in_leaf': 20}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9839786381842457 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.49701393497013935\n",
      "Precision: 0.9919786096256684 \n",
      "Recall: 0.9933065595716198 \n",
      "Aging Rate: 0.49635036496350365\n",
      "Precision: 0.9866310160427807 \n",
      "Recall: 0.9879518072289156 \n",
      "Aging Rate: 0.49635036496350365\n",
      "\u001b[32m[I 2022-05-26 15:41:12,945]\u001b[0m Trial 7 finished with value: 0.9884090760049838 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.08655702813287128, 'min_data_in_leaf': 50}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9788079470198675 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.5009953550099535\n",
      "Precision: 0.9892328398384926 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.4930325149303251\n",
      "Precision: 0.9892328398384926 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.4930325149303251\n",
      "\u001b[32m[I 2022-05-26 15:41:14,363]\u001b[0m Trial 8 finished with value: 0.985725222447624 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.5, 'l2_leaf_reg': 1.8054124103782183, 'min_data_in_leaf': 50, 'max_leaves': 30}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9840213049267643 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.49834107498341074\n",
      "Precision: 0.9919137466307277 \n",
      "Recall: 0.9852744310575636 \n",
      "Aging Rate: 0.49236894492368943\n",
      "Precision: 0.9747675962815405 \n",
      "Recall: 0.9825970548862115 \n",
      "Aging Rate: 0.4996682149966821\n",
      "\u001b[32m[I 2022-05-26 15:41:15,610]\u001b[0m Trial 9 finished with value: 0.9846328244639097 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 0.0768033336086054, 'min_data_in_leaf': 15, 'max_leaves': 20}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9814323607427056 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.5003317850033179\n",
      "Precision: 0.9788079470198675 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.5009953550099535\n",
      "Precision: 0.9814323607427056 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.5003317850033179\n",
      "\u001b[32m[I 2022-05-26 15:41:57,449]\u001b[0m Trial 10 finished with value: 0.9853466530524257 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.9, 'l2_leaf_reg': 9.799269094126533}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9919246298788694 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.4930325149303251\n",
      "Precision: 0.9787798408488063 \n",
      "Recall: 0.9879518072289156 \n",
      "Aging Rate: 0.5003317850033179\n",
      "Precision: 0.985254691689008 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.49502322495023227\n",
      "\u001b[32m[I 2022-05-26 15:42:04,179]\u001b[0m Trial 11 finished with value: 0.9857336525426575 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.42919626129029936, 'min_data_in_leaf': 35}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9879679144385026 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.49635036496350365\n",
      "Precision: 0.9892904953145917 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.49568679495686796\n",
      "Precision: 0.9826666666666667 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.49767750497677504\n",
      "\u001b[32m[I 2022-05-26 15:42:17,975]\u001b[0m Trial 12 finished with value: 0.9875183988000505 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 0.612540235886614}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9174560216508796 \n",
      "Recall: 0.9076305220883534 \n",
      "Aging Rate: 0.49037823490378235\n",
      "Precision: 0.9242219215155616 \n",
      "Recall: 0.9143239625167336 \n",
      "Aging Rate: 0.49037823490378235\n",
      "Precision: 0.9139784946236559 \n",
      "Recall: 0.9103078982597055 \n",
      "Aging Rate: 0.4936960849369608\n",
      "\u001b[32m[I 2022-05-26 15:42:20,112]\u001b[0m Trial 13 finished with value: 0.9146342087217487 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 2, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.012512282177919252, 'min_data_in_leaf': 60}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9787798408488063 \n",
      "Recall: 0.9879518072289156 \n",
      "Aging Rate: 0.5003317850033179\n",
      "Precision: 0.9879032258064516 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.4936960849369608\n",
      "Precision: 0.9800531914893617 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.49900464499004643\n",
      "\u001b[32m[I 2022-05-26 15:42:22,973]\u001b[0m Trial 14 finished with value: 0.9841940482698638 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 0.03785412874896762}. Best is trial 2 with value: 0.9888923466155103.\u001b[0m\n",
      "Precision: 0.9867197875166003 \n",
      "Recall: 0.9946452476572959 \n",
      "Aging Rate: 0.4996682149966821\n",
      "Precision: 0.9801587301587301 \n",
      "Recall: 0.9919678714859438 \n",
      "Aging Rate: 0.5016589250165893\n",
      "Precision: 0.9893758300132802 \n",
      "Recall: 0.9973226238286479 \n",
      "Aging Rate: 0.4996682149966821\n",
      "\u001b[32m[I 2022-05-26 15:43:31,519]\u001b[0m Trial 15 finished with value: 0.9900093147039254 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.20071322198535757}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9854304635761589 \n",
      "Recall: 0.9959839357429718 \n",
      "Aging Rate: 0.5009953550099535\n",
      "Precision: 0.9879356568364611 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.49502322495023227\n",
      "Precision: 0.9840637450199203 \n",
      "Recall: 0.9919678714859438 \n",
      "Aging Rate: 0.4996682149966821\n",
      "\u001b[32m[I 2022-05-26 15:44:13,741]\u001b[0m Trial 16 finished with value: 0.9886510132058796 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.9, 'l2_leaf_reg': 0.8736537868979104}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9801587301587301 \n",
      "Recall: 0.9919678714859438 \n",
      "Aging Rate: 0.5016589250165893\n",
      "Precision: 0.9827814569536424 \n",
      "Recall: 0.9933065595716198 \n",
      "Aging Rate: 0.5009953550099535\n",
      "Precision: 0.986737400530504 \n",
      "Recall: 0.9959839357429718 \n",
      "Aging Rate: 0.5003317850033179\n",
      "\u001b[32m[I 2022-05-26 15:45:22,166]\u001b[0m Trial 17 finished with value: 0.9884610100228916 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.22198720824317003}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9867021276595744 \n",
      "Recall: 0.9933065595716198 \n",
      "Aging Rate: 0.49900464499004643\n",
      "Precision: 0.9893048128342246 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.49635036496350365\n",
      "Precision: 0.9853528628495339 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.49834107498341074\n",
      "\u001b[32m[I 2022-05-26 15:45:34,628]\u001b[0m Trial 18 finished with value: 0.989314620902685 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.7, 'l2_leaf_reg': 5.600591662479662}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9892037786774629 \n",
      "Recall: 0.9812583668005355 \n",
      "Aging Rate: 0.49170537491705374\n",
      "Precision: 0.982620320855615 \n",
      "Recall: 0.9839357429718876 \n",
      "Aging Rate: 0.49635036496350365\n",
      "Precision: 0.986648865153538 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.49701393497013935\n",
      "\u001b[32m[I 2022-05-26 15:45:47,482]\u001b[0m Trial 19 finished with value: 0.9854868533917291 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.7, 'l2_leaf_reg': 8.979129154726254}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9699738903394256 \n",
      "Recall: 0.9946452476572959 \n",
      "Aging Rate: 0.5082946250829462\n",
      "Precision: 0.9892617449664429 \n",
      "Recall: 0.9866131191432396 \n",
      "Aging Rate: 0.49435965494359657\n",
      "Precision: 0.9879839786381842 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.49701393497013935\n",
      "\u001b[32m[I 2022-05-26 15:45:51,857]\u001b[0m Trial 20 finished with value: 0.9864650430957803 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 4.025864214705093}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9840848806366048 \n",
      "Recall: 0.9933065595716198 \n",
      "Aging Rate: 0.5003317850033179\n",
      "Precision: 0.9853723404255319 \n",
      "Recall: 0.9919678714859438 \n",
      "Aging Rate: 0.49900464499004643\n",
      "Precision: 0.9837618403247632 \n",
      "Recall: 0.9732262382864793 \n",
      "Aging Rate: 0.49037823490378235\n",
      "\u001b[32m[I 2022-05-26 15:46:33,924]\u001b[0m Trial 21 finished with value: 0.98526633431208 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 3.5817145285142904}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9879839786381842 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.49701393497013935\n",
      "Precision: 0.9828042328042328 \n",
      "Recall: 0.9946452476572959 \n",
      "Aging Rate: 0.5016589250165893\n",
      "Precision: 0.9840848806366048 \n",
      "Recall: 0.9933065595716198 \n",
      "Aging Rate: 0.5003317850033179\n",
      "\u001b[32m[I 2022-05-26 15:47:15,217]\u001b[0m Trial 22 finished with value: 0.9888894393710838 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.17500000000000002, 'subsample': 0.7, 'l2_leaf_reg': 1.6215690944988492}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9788359788359788 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.5016589250165893\n",
      "Precision: 0.9866666666666667 \n",
      "Recall: 0.9906291834002677 \n",
      "Aging Rate: 0.49767750497677504\n",
      "Precision: 0.9852744310575636 \n",
      "Recall: 0.9852744310575636 \n",
      "Aging Rate: 0.49568679495686796\n",
      "\u001b[32m[I 2022-05-26 15:47:27,913]\u001b[0m Trial 23 finished with value: 0.9862052192519345 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.9, 'l2_leaf_reg': 5.772611181989925}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Precision: 0.9932795698924731 \n",
      "Recall: 0.9892904953145917 \n",
      "Aging Rate: 0.4936960849369608\n",
      "Precision: 0.990578734858681 \n",
      "Recall: 0.9852744310575636 \n",
      "Aging Rate: 0.4930325149303251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 15:48:10,090]\u001b[0m A new study created in memory with name: no-name-4655dbca-0f82-4b5e-a033-3dd49eed5eac\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9853137516688919 \n",
      "Recall: 0.9879518072289156 \n",
      "Aging Rate: 0.49701393497013935\n",
      "\u001b[32m[I 2022-05-26 15:48:10,012]\u001b[0m Trial 24 finished with value: 0.9886104995266874 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 12, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 0.8436725011796831}. Best is trial 15 with value: 0.9900093147039254.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 7 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa65df30b7944e08b002ab9c89d380f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.9052631578947369 \n",
      "Aging Rate: 0.45263157894736844\n",
      "Precision: 1.0 \n",
      "Recall: 0.906578947368421 \n",
      "Aging Rate: 0.4532894736842105\n",
      "Precision: 1.0 \n",
      "Recall: 0.9184210526315789 \n",
      "Aging Rate: 0.45921052631578946\n",
      "\u001b[32m[I 2022-05-26 15:50:18,617]\u001b[0m Trial 0 finished with value: 0.9529176425793597 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 12, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 5.439228857667954}. Best is trial 0 with value: 0.9529176425793597.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 1.0 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4934210526315789\n",
      "\u001b[32m[I 2022-05-26 15:50:31,235]\u001b[0m Trial 1 finished with value: 0.9911611947464882 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.125, 'subsample': 0.7, 'l2_leaf_reg': 4.671445285373985, 'min_data_in_leaf': 40}. Best is trial 1 with value: 0.9911611947464882.\u001b[0m\n",
      "Precision: 0.9972826086956522 \n",
      "Recall: 0.9657894736842105 \n",
      "Aging Rate: 0.4842105263157895\n",
      "Precision: 0.9973082099596231 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.4888157894736842\n",
      "Precision: 1.0 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4888157894736842\n",
      "\u001b[32m[I 2022-05-26 15:50:35,635]\u001b[0m Trial 2 finished with value: 0.9853335515540517 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 8, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 7.1827791454955845, 'min_data_in_leaf': 5}. Best is trial 1 with value: 0.9911611947464882.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4881578947368421\n",
      "Precision: 0.9933510638297872 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 1.0 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4881578947368421\n",
      "\u001b[32m[I 2022-05-26 15:53:17,828]\u001b[0m Trial 3 finished with value: 0.9880423984951282 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.3, 'l2_leaf_reg': 0.7436172028746639}. Best is trial 1 with value: 0.9911611947464882.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9723684210526315 \n",
      "Aging Rate: 0.4861842105263158\n",
      "Precision: 0.9973262032085561 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9986631016042781 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 15:53:21,334]\u001b[0m Trial 4 finished with value: 0.9886989204120166 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 12, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 4.567460220830193, 'min_data_in_leaf': 5, 'max_leaves': 40}. Best is trial 1 with value: 0.9911611947464882.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9657894736842105 \n",
      "Aging Rate: 0.48289473684210527\n",
      "Precision: 0.9972714870395635 \n",
      "Recall: 0.9618421052631579 \n",
      "Aging Rate: 0.48223684210526313\n",
      "Precision: 0.994572591587517 \n",
      "Recall: 0.9644736842105263 \n",
      "Aging Rate: 0.48486842105263156\n",
      "\u001b[32m[I 2022-05-26 15:53:22,574]\u001b[0m Trial 5 finished with value: 0.9803751362528339 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 4, 'learning_rate': 0.275, 'subsample': 0.7, 'l2_leaf_reg': 6.628786211358591, 'min_data_in_leaf': 60}. Best is trial 1 with value: 0.9911611947464882.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 1.0 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 1.0 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4934210526315789\n",
      "\u001b[32m[I 2022-05-26 15:55:31,669]\u001b[0m Trial 6 finished with value: 0.9913715190988969 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 0.7322674194839618}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9986631016042781 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9932885906040269 \n",
      "Recall: 0.9736842105263158 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 15:55:32,893]\u001b[0m Trial 7 finished with value: 0.9873895150559143 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 4, 'learning_rate': 0.275, 'subsample': 0.3, 'l2_leaf_reg': 0.1624526491887282, 'min_data_in_leaf': 15, 'max_leaves': 20}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9986684420772304 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 1.0 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4888157894736842\n",
      "Precision: 0.9960212201591512 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.49605263157894736\n",
      "\u001b[32m[I 2022-05-26 15:55:38,237]\u001b[0m Trial 8 finished with value: 0.9911611057524946 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 0.10978823580577511, 'min_data_in_leaf': 25, 'max_leaves': 10}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9959839357429718 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9973262032085561 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 15:55:46,003]\u001b[0m Trial 9 finished with value: 0.9882819169252595 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 0.01808932478861928, 'min_data_in_leaf': 5}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9945130315500685 \n",
      "Recall: 0.9539473684210527 \n",
      "Aging Rate: 0.47960526315789476\n",
      "Precision: 0.9904371584699454 \n",
      "Recall: 0.9539473684210527 \n",
      "Aging Rate: 0.48157894736842105\n",
      "Precision: 0.9945054945054945 \n",
      "Recall: 0.9526315789473684 \n",
      "Aging Rate: 0.4789473684210526\n",
      "\u001b[32m[I 2022-05-26 15:55:47,792]\u001b[0m Trial 10 finished with value: 0.9729253567677892 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 2, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 0.8025410392732746}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9973404255319149 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.994572591587517 \n",
      "Recall: 0.9644736842105263 \n",
      "Aging Rate: 0.48486842105263156\n",
      "Precision: 0.9933510638297872 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 15:56:10,527]\u001b[0m Trial 11 finished with value: 0.9864835491087995 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.025, 'subsample': 0.9, 'l2_leaf_reg': 1.2415695700921625, 'min_data_in_leaf': 50}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9973154362416108 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 1.0 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.48486842105263156\n",
      "\u001b[32m[I 2022-05-26 15:57:47,793]\u001b[0m Trial 12 finished with value: 0.9877949812466275 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 10, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 2.0091533257610714}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9959731543624161 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9960159362549801 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9973154362416108 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4901315789473684\n",
      "\u001b[32m[I 2022-05-26 15:58:10,198]\u001b[0m Trial 13 finished with value: 0.9882765753283284 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 0.29877848074023294, 'min_data_in_leaf': 40}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.997289972899729 \n",
      "Recall: 0.968421052631579 \n",
      "Aging Rate: 0.4855263157894737\n",
      "Precision: 1.0 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9986449864498645 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.4855263157894737\n",
      "\u001b[32m[I 2022-05-26 15:58:25,501]\u001b[0m Trial 14 finished with value: 0.9871103858642799 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 8, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 0.048524739443999584}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9933510638297872 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9986559139784946 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.998661311914324 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49144736842105263\n",
      "\u001b[32m[I 2022-05-26 15:58:29,138]\u001b[0m Trial 15 finished with value: 0.9887245342964399 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 1.7416101978140834, 'min_data_in_leaf': 35}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9946949602122016 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9973333333333333 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9973226238286479 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49144736842105263\n",
      "\u001b[32m[I 2022-05-26 16:09:16,157]\u001b[0m Trial 16 finished with value: 0.9900669196557672 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 12, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 0.42031225351332346}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9971711456859972 \n",
      "Recall: 0.9276315789473685 \n",
      "Aging Rate: 0.4651315789473684\n",
      "Precision: 1.0 \n",
      "Recall: 0.9473684210526315 \n",
      "Aging Rate: 0.47368421052631576\n",
      "Precision: 1.0 \n",
      "Recall: 0.9539473684210527 \n",
      "Aging Rate: 0.4769736842105263\n",
      "\u001b[32m[I 2022-05-26 16:09:22,334]\u001b[0m Trial 17 finished with value: 0.9701830478926595 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 8, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 2.9591759670338416}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9986648865153538 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9973190348525469 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 0.99734395750332 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.4953947368421053\n",
      "\u001b[32m[I 2022-05-26 16:09:34,507]\u001b[0m Trial 18 finished with value: 0.9907208360330758 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 0.4456634209094727, 'min_data_in_leaf': 50}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9513157894736842 \n",
      "Aging Rate: 0.4756578947368421\n",
      "Precision: 0.9986357435197817 \n",
      "Recall: 0.9631578947368421 \n",
      "Aging Rate: 0.48223684210526313\n",
      "Precision: 1.0 \n",
      "Recall: 0.9618421052631579 \n",
      "Aging Rate: 0.48092105263157897\n",
      "\u001b[32m[I 2022-05-26 16:09:38,969]\u001b[0m Trial 19 finished with value: 0.9787255203537746 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 12, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 9.989232638239917, 'min_data_in_leaf': 25, 'max_leaves': 50}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.998661311914324 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9986595174262735 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 0.9986559139784946 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.48947368421052634\n",
      "\u001b[32m[I 2022-05-26 16:10:06,300]\u001b[0m Trial 20 finished with value: 0.9891513982691205 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 500, 'depth': 8, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.16640134638217155}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9986468200270636 \n",
      "Recall: 0.9710526315789474 \n",
      "Aging Rate: 0.4861842105263158\n",
      "Precision: 0.996005326231691 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.9986754966887417 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.4967105263157895\n",
      "\u001b[32m[I 2022-05-26 16:10:12,802]\u001b[0m Trial 21 finished with value: 0.9900362583498094 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 0.0592323235968658, 'min_data_in_leaf': 25, 'max_leaves': 10}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.48947368421052634\n",
      "Precision: 0.9986702127659575 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9986631016042781 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 16:10:18,782]\u001b[0m Trial 22 finished with value: 0.9911547086284186 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 0.07473685828626647, 'min_data_in_leaf': 25, 'max_leaves': 10}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9919893190921228 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.49276315789473685\n",
      "Precision: 0.9973368841544608 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.996005326231691 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49407894736842106\n",
      "\u001b[32m[I 2022-05-26 16:10:24,154]\u001b[0m Trial 23 finished with value: 0.9887424478790906 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 0.1733405503497826, 'min_data_in_leaf': 45, 'max_leaves': 25}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Precision: 0.9960369881109643 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.4980263157894737\n",
      "Precision: 0.9932614555256065 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.4881578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 16:10:31,398]\u001b[0m A new study created in memory with name: no-name-d21bd0c3-ca9d-479b-9bc1-ba55134a605d\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9986559139784946 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.48947368421052634\n",
      "\u001b[32m[I 2022-05-26 16:10:31,319]\u001b[0m Trial 24 finished with value: 0.9878191139815063 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.17500000000000002, 'subsample': 0.7, 'l2_leaf_reg': 0.02471791930342291, 'min_data_in_leaf': 15, 'max_leaves': 10}. Best is trial 6 with value: 0.9913715190988969.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 8 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c6eba250b74287906a2e632438a899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9933333333333333 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9842312746386334 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.5006578947368421\n",
      "Precision: 0.9841897233201581 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4993421052631579\n",
      "\u001b[32m[I 2022-05-26 16:10:32,610]\u001b[0m Trial 0 finished with value: 0.9850583800664908 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.9, 'l2_leaf_reg': 0.43674958487911997, 'min_data_in_leaf': 30, 'max_leaves': 15}. Best is trial 0 with value: 0.9850583800664908.\u001b[0m\n",
      "Precision: 0.8873239436619719 \n",
      "Recall: 0.9118421052631579 \n",
      "Aging Rate: 0.5138157894736842\n",
      "Precision: 0.8901515151515151 \n",
      "Recall: 0.9276315789473685 \n",
      "Aging Rate: 0.5210526315789473\n",
      "Precision: 0.8754669987546699 \n",
      "Recall: 0.925 \n",
      "Aging Rate: 0.5282894736842105\n",
      "\u001b[32m[I 2022-05-26 16:10:34,651]\u001b[0m Trial 1 finished with value: 0.9024910872044253 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 2, 'learning_rate': 0.025, 'subsample': 0.9, 'l2_leaf_reg': 6.538727199258157, 'min_data_in_leaf': 35}. Best is trial 0 with value: 0.9850583800664908.\u001b[0m\n",
      "Precision: 0.9959785522788204 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 0.9959731543624161 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4901315789473684\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 16:10:38,459]\u001b[0m Trial 2 finished with value: 0.986943319873987 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 0.015287056237678181}. Best is trial 2 with value: 0.986943319873987.\u001b[0m\n",
      "Precision: 0.9892904953145917 \n",
      "Recall: 0.9723684210526315 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.988110964332893 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4980263157894737\n",
      "Precision: 0.9920318725099602 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4953947368421053\n",
      "\u001b[32m[I 2022-05-26 16:10:42,690]\u001b[0m Trial 3 finished with value: 0.9847851754272883 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 0.08881476320214637, 'min_data_in_leaf': 60, 'max_leaves': 15}. Best is trial 2 with value: 0.986943319873987.\u001b[0m\n",
      "Precision: 0.9960369881109643 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.4980263157894737\n",
      "Precision: 0.9946452476572959 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9933422103861518 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49407894736842106\n",
      "\u001b[32m[I 2022-05-26 16:10:44,351]\u001b[0m Trial 4 finished with value: 0.989185937942119 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 8, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 5.490903003743441, 'min_data_in_leaf': 5}. Best is trial 4 with value: 0.989185937942119.\u001b[0m\n",
      "Precision: 0.9221938775510204 \n",
      "Recall: 0.9513157894736842 \n",
      "Aging Rate: 0.5157894736842106\n",
      "Precision: 0.9397116644823067 \n",
      "Recall: 0.9434210526315789 \n",
      "Aging Rate: 0.5019736842105263\n",
      "Precision: 0.9377431906614786 \n",
      "Recall: 0.9513157894736842 \n",
      "Aging Rate: 0.5072368421052632\n",
      "\u001b[32m[I 2022-05-26 16:10:45,087]\u001b[0m Trial 5 finished with value: 0.9408573113814883 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 2, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 0.03395256930339191, 'min_data_in_leaf': 55}. Best is trial 4 with value: 0.989185937942119.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4881578947368421\n",
      "Precision: 0.9907407407407407 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9986522911051213 \n",
      "Recall: 0.975 \n",
      "Aging Rate: 0.4881578947368421\n",
      "\u001b[32m[I 2022-05-26 16:10:56,459]\u001b[0m Trial 6 finished with value: 0.987609016181298 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 10, 'learning_rate': 0.125, 'subsample': 0.5, 'l2_leaf_reg': 0.010034348935509231}. Best is trial 4 with value: 0.989185937942119.\u001b[0m\n",
      "Precision: 0.9959839357429718 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9894179894179894 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49736842105263157\n",
      "\u001b[32m[I 2022-05-26 16:11:00,301]\u001b[0m Trial 7 finished with value: 0.9852433002482114 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 100, 'depth': 4, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.02861459319213753}. Best is trial 4 with value: 0.989185937942119.\u001b[0m\n",
      "Precision: 0.9645669291338582 \n",
      "Recall: 0.9671052631578947 \n",
      "Aging Rate: 0.5013157894736842\n",
      "Precision: 0.9451697127937336 \n",
      "Recall: 0.9526315789473684 \n",
      "Aging Rate: 0.5039473684210526\n",
      "Precision: 0.9528178243774574 \n",
      "Recall: 0.9565789473684211 \n",
      "Aging Rate: 0.5019736842105263\n",
      "\u001b[32m[I 2022-05-26 16:11:01,508]\u001b[0m Trial 8 finished with value: 0.9564716954473971 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 8, 'learning_rate': 0.025, 'subsample': 0.3, 'l2_leaf_reg': 6.659967697847167, 'min_data_in_leaf': 55, 'max_leaves': 15}. Best is trial 4 with value: 0.989185937942119.\u001b[0m\n",
      "Precision: 0.9907651715039578 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.4986842105263158\n",
      "Precision: 0.9893758300132802 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9920739762219286 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.4980263157894737\n",
      "\u001b[32m[I 2022-05-26 16:11:18,282]\u001b[0m Trial 9 finished with value: 0.9881234308590295 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 6.384695013523413}. Best is trial 4 with value: 0.989185937942119.\u001b[0m\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.993421052631579 \n",
      "Recall: 0.993421052631579 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9829842931937173 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.5026315789473684\n",
      "\u001b[32m[I 2022-05-26 16:11:22,102]\u001b[0m Trial 10 finished with value: 0.9892498006791138 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 1.1305703470584973, 'min_data_in_leaf': 5}. Best is trial 10 with value: 0.9892498006791138.\u001b[0m\n",
      "Precision: 0.9906666666666667 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9946666666666667 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9934123847167325 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.4993421052631579\n",
      "\u001b[32m[I 2022-05-26 16:11:25,885]\u001b[0m Trial 11 finished with value: 0.9883146080478763 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 1.3733682814351502, 'min_data_in_leaf': 5}. Best is trial 10 with value: 0.9892498006791138.\u001b[0m\n",
      "Precision: 0.9906914893617021 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9933333333333333 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9946666666666667 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.4934210526315789\n",
      "\u001b[32m[I 2022-05-26 16:11:29,578]\u001b[0m Trial 12 finished with value: 0.986761390845276 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 1.5372630375545753, 'min_data_in_leaf': 5}. Best is trial 10 with value: 0.9892498006791138.\u001b[0m\n",
      "Precision: 0.9881889763779528 \n",
      "Recall: 0.9907894736842106 \n",
      "Aging Rate: 0.5013157894736842\n",
      "Precision: 0.9919246298788694 \n",
      "Recall: 0.9697368421052631 \n",
      "Aging Rate: 0.4888157894736842\n",
      "Precision: 0.992 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.4934210526315789\n",
      "\u001b[32m[I 2022-05-26 16:11:33,952]\u001b[0m Trial 13 finished with value: 0.9852077453854241 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 8, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 1.7903956937002312, 'min_data_in_leaf': 20}. Best is trial 10 with value: 0.9892498006791138.\u001b[0m\n",
      "Precision: 0.9973368841544608 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49407894736842106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9947229551451188 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.4986842105263158\n",
      "Precision: 0.9933949801849405 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4980263157894737\n",
      "\u001b[32m[I 2022-05-26 16:11:40,199]\u001b[0m Trial 14 finished with value: 0.9920797552565436 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.527935741809418, 'min_data_in_leaf': 15}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9973190348525469 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 0.9894039735099338 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.4967105263157895\n",
      "Precision: 0.9973154362416108 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4901315789473684\n",
      "\u001b[32m[I 2022-05-26 16:11:46,529]\u001b[0m Trial 15 finished with value: 0.9871872793029062 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.7, 'l2_leaf_reg': 0.2638752225853466, 'min_data_in_leaf': 20}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9947019867549669 \n",
      "Recall: 0.9881578947368421 \n",
      "Aging Rate: 0.4967105263157895\n",
      "Precision: 0.9986541049798116 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.4888157894736842\n",
      "Precision: 0.9973118279569892 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.48947368421052634\n",
      "\u001b[32m[I 2022-05-26 16:11:52,900]\u001b[0m Trial 16 finished with value: 0.9884932952249658 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.3696885057050208, 'min_data_in_leaf': 15}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9905660377358491 \n",
      "Recall: 0.9671052631578947 \n",
      "Aging Rate: 0.4881578947368421\n",
      "Precision: 0.9920634920634921 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.49736842105263157\n",
      "Precision: 0.9880952380952381 \n",
      "Recall: 0.9828947368421053 \n",
      "Aging Rate: 0.49736842105263157\n",
      "\u001b[32m[I 2022-05-26 16:11:56,011]\u001b[0m Trial 17 finished with value: 0.9845430367249999 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 4, 'learning_rate': 0.225, 'subsample': 0.7, 'l2_leaf_reg': 0.7456109255145519, 'min_data_in_leaf': 15}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9933155080213903 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9906914893617021 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.9973118279569892 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.48947368421052634\n",
      "\u001b[32m[I 2022-05-26 16:11:59,673]\u001b[0m Trial 18 finished with value: 0.9858543345642882 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.7, 'l2_leaf_reg': 0.1335656038275431, 'min_data_in_leaf': 35}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9973226238286479 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49144736842105263\n",
      "Precision: 0.9973190348525469 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.49078947368421055\n",
      "Precision: 0.9906914893617021 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 16:12:06,434]\u001b[0m Trial 19 finished with value: 0.9874056180339562 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.225, 'subsample': 0.9, 'l2_leaf_reg': 2.841986698107906, 'min_data_in_leaf': 10}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4953947368421053\n",
      "Precision: 0.9946524064171123 \n",
      "Recall: 0.9789473684210527 \n",
      "Aging Rate: 0.4921052631578947\n",
      "Precision: 0.9919463087248322 \n",
      "Recall: 0.9723684210526315 \n",
      "Aging Rate: 0.4901315789473684\n",
      "\u001b[32m[I 2022-05-26 16:12:10,710]\u001b[0m Trial 20 finished with value: 0.9858537487129254 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 6, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 0.82606414426831, 'min_data_in_leaf': 25, 'max_leaves': 50}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9933949801849405 \n",
      "Recall: 0.9894736842105263 \n",
      "Aging Rate: 0.4980263157894737\n",
      "Precision: 0.996005326231691 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.49407894736842106\n",
      "Precision: 0.9933598937583001 \n",
      "Recall: 0.9842105263157894 \n",
      "Aging Rate: 0.4953947368421053\n",
      "\u001b[32m[I 2022-05-26 16:12:12,179]\u001b[0m Trial 21 finished with value: 0.990089099753153 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 8, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 3.0247604551464073, 'min_data_in_leaf': 5}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.993368700265252 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49605263157894736\n",
      "Precision: 0.9906666666666667 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.4934210526315789\n",
      "Precision: 0.9959893048128342 \n",
      "Recall: 0.9802631578947368 \n",
      "Aging Rate: 0.4921052631578947\n",
      "\u001b[32m[I 2022-05-26 16:12:13,481]\u001b[0m Trial 22 finished with value: 0.9872005296794196 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 8, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 3.1852398639908763, 'min_data_in_leaf': 10}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9920212765957447 \n",
      "Recall: 0.9815789473684211 \n",
      "Aging Rate: 0.49473684210526314\n",
      "Precision: 0.984251968503937 \n",
      "Recall: 0.9868421052631579 \n",
      "Aging Rate: 0.5013157894736842\n",
      "Precision: 0.9960106382978723 \n",
      "Recall: 0.9855263157894737 \n",
      "Aging Rate: 0.49473684210526314\n",
      "\u001b[32m[I 2022-05-26 16:12:16,680]\u001b[0m Trial 23 finished with value: 0.9876861875328805 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 4, 'learning_rate': 0.325, 'subsample': 0.9, 'l2_leaf_reg': 0.8882134358698774, 'min_data_in_leaf': 5}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Precision: 0.9908015768725361 \n",
      "Recall: 0.9921052631578947 \n",
      "Aging Rate: 0.5006578947368421\n",
      "Precision: 0.9986559139784946 \n",
      "Recall: 0.9776315789473684 \n",
      "Aging Rate: 0.48947368421052634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 16:12:20,234]\u001b[0m A new study created in memory with name: no-name-24b7c567-8a52-4548-96b1-e6d3aeb3c932\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9906542056074766 \n",
      "Recall: 0.9763157894736842 \n",
      "Aging Rate: 0.49276315789473685\n",
      "\u001b[32m[I 2022-05-26 16:12:20,165]\u001b[0m Trial 24 finished with value: 0.9876392144194902 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.275, 'subsample': 0.9, 'l2_leaf_reg': 0.21535572071038972, 'min_data_in_leaf': 45}. Best is trial 14 with value: 0.9920797552565436.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 9 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59d6ca5c712491ba21615e0eb48ff20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.3815789473684211 \n",
      "Aging Rate: 0.034688995215311005\n",
      "Precision: 1.0 \n",
      "Recall: 0.47368421052631576 \n",
      "Aging Rate: 0.0430622009569378\n",
      "Precision: 0.9736842105263158 \n",
      "Recall: 0.4868421052631579 \n",
      "Aging Rate: 0.045454545454545456\n",
      "\u001b[32m[I 2022-05-26 16:12:21,889]\u001b[0m Trial 0 finished with value: 0.6147869674185465 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 4, 'learning_rate': 0.025, 'subsample': 0.7, 'l2_leaf_reg': 0.1201179502787437, 'min_data_in_leaf': 35}. Best is trial 0 with value: 0.6147869674185465.\u001b[0m\n",
      "Precision: 0.9705882352941176 \n",
      "Recall: 0.868421052631579 \n",
      "Aging Rate: 0.08133971291866028\n",
      "Precision: 0.9523809523809523 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.07535885167464115\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.07416267942583732\n",
      "\u001b[32m[I 2022-05-26 16:12:30,609]\u001b[0m Trial 1 finished with value: 0.8831804121919856 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 8, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 2.865486405216162}. Best is trial 1 with value: 0.8831804121919856.\u001b[0m\n",
      "Precision: 0.9166666666666666 \n",
      "Recall: 0.4342105263157895 \n",
      "Aging Rate: 0.0430622009569378\n",
      "Precision: 0.9166666666666666 \n",
      "Recall: 0.4342105263157895 \n",
      "Aging Rate: 0.0430622009569378\n",
      "Precision: 0.8837209302325582 \n",
      "Recall: 0.5 \n",
      "Aging Rate: 0.05143540669856459\n",
      "\u001b[32m[I 2022-05-26 16:12:31,862]\u001b[0m Trial 2 finished with value: 0.6057422969187675 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 2, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 0.987714869191767, 'min_data_in_leaf': 60}. Best is trial 1 with value: 0.8831804121919856.\u001b[0m\n",
      "Precision: 0.9558823529411765 \n",
      "Recall: 0.8552631578947368 \n",
      "Aging Rate: 0.08133971291866028\n",
      "Precision: 0.9436619718309859 \n",
      "Recall: 0.881578947368421 \n",
      "Aging Rate: 0.08492822966507177\n",
      "Precision: 0.984375 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07655502392344497\n",
      "\u001b[32m[I 2022-05-26 16:12:34,965]\u001b[0m Trial 3 finished with value: 0.9047808012093727 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.3, 'l2_leaf_reg': 0.9149254234229193, 'min_data_in_leaf': 15}. Best is trial 3 with value: 0.9047808012093727.\u001b[0m\n",
      "Precision: 0.9666666666666667 \n",
      "Recall: 0.7631578947368421 \n",
      "Aging Rate: 0.07177033492822966\n",
      "Precision: 0.9672131147540983 \n",
      "Recall: 0.7763157894736842 \n",
      "Aging Rate: 0.0729665071770335\n",
      "Precision: 0.9545454545454546 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07894736842105263\n",
      "\u001b[32m[I 2022-05-26 16:12:54,598]\u001b[0m Trial 4 finished with value: 0.8671929962485664 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 4, 'learning_rate': 0.17500000000000002, 'subsample': 0.3, 'l2_leaf_reg': 0.04462004175967422}. Best is trial 3 with value: 0.9047808012093727.\u001b[0m\n",
      "Precision: 0.9846153846153847 \n",
      "Recall: 0.8421052631578947 \n",
      "Aging Rate: 0.07775119617224881\n",
      "Precision: 0.9365079365079365 \n",
      "Recall: 0.7763157894736842 \n",
      "Aging Rate: 0.07535885167464115\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.07416267942583732\n",
      "\u001b[32m[I 2022-05-26 16:12:58,409]\u001b[0m Trial 5 finished with value: 0.8754291663801244 and parameters: {'grow_policy': 'Lossguide', 'iterations': 300, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 0.02330964261383136, 'min_data_in_leaf': 40, 'max_leaves': 30}. Best is trial 3 with value: 0.9047808012093727.\u001b[0m\n",
      "Precision: 0.927536231884058 \n",
      "Recall: 0.8421052631578947 \n",
      "Aging Rate: 0.08253588516746412\n",
      "Precision: 0.9577464788732394 \n",
      "Recall: 0.8947368421052632 \n",
      "Aging Rate: 0.08492822966507177\n",
      "Precision: 1.0 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07535885167464115\n",
      "\u001b[32m[I 2022-05-26 16:13:01,453]\u001b[0m Trial 6 finished with value: 0.9048011696202503 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 12, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 0.02409060693676228, 'min_data_in_leaf': 55}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.8888888888888888 \n",
      "Recall: 0.21052631578947367 \n",
      "Aging Rate: 0.0215311004784689\n",
      "Precision: 1.0 \n",
      "Recall: 0.3684210526315789 \n",
      "Aging Rate: 0.03349282296650718\n",
      "Precision: 0.95 \n",
      "Recall: 0.25 \n",
      "Aging Rate: 0.023923444976076555\n",
      "\u001b[32m[I 2022-05-26 16:13:21,085]\u001b[0m Trial 7 finished with value: 0.4249068012365884 and parameters: {'grow_policy': 'SymmetricTree', 'iterations': 300, 'depth': 4, 'learning_rate': 0.025, 'subsample': 0.3, 'l2_leaf_reg': 1.0915495631782532}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.9402985074626866 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.08014354066985646\n",
      "Precision: 0.9565217391304348 \n",
      "Recall: 0.868421052631579 \n",
      "Aging Rate: 0.08253588516746412\n",
      "Precision: 0.9516129032258065 \n",
      "Recall: 0.7763157894736842 \n",
      "Aging Rate: 0.07416267942583732\n",
      "\u001b[32m[I 2022-05-26 16:13:22,321]\u001b[0m Trial 8 finished with value: 0.8821787241577347 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 8, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.047668501957922875, 'min_data_in_leaf': 45, 'max_leaves': 25}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.8904109589041096 \n",
      "Recall: 0.8552631578947368 \n",
      "Aging Rate: 0.08732057416267942\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8026315789473685 \n",
      "Aging Rate: 0.07535885167464115\n",
      "Precision: 0.9821428571428571 \n",
      "Recall: 0.7236842105263158 \n",
      "Aging Rate: 0.06698564593301436\n",
      "\u001b[32m[I 2022-05-26 16:13:24,318]\u001b[0m Trial 9 finished with value: 0.8611714655121542 and parameters: {'grow_policy': 'Depthwise', 'iterations': 300, 'depth': 6, 'learning_rate': 0.125, 'subsample': 0.9, 'l2_leaf_reg': 0.935350624182741, 'min_data_in_leaf': 25}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.7368421052631579 \n",
      "Aging Rate: 0.06698564593301436\n",
      "Precision: 0.9666666666666667 \n",
      "Recall: 0.7631578947368421 \n",
      "Aging Rate: 0.07177033492822966\n",
      "Precision: 0.8985507246376812 \n",
      "Recall: 0.8157894736842105 \n",
      "Aging Rate: 0.08253588516746412\n",
      "\u001b[32m[I 2022-05-26 16:13:28,499]\u001b[0m Trial 10 finished with value: 0.8521994795828466 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.325, 'subsample': 0.5, 'l2_leaf_reg': 0.14758035076725654, 'min_data_in_leaf': 60}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.9324324324324325 \n",
      "Recall: 0.9078947368421053 \n",
      "Aging Rate: 0.08851674641148326\n",
      "Precision: 0.9365079365079365 \n",
      "Recall: 0.7763157894736842 \n",
      "Aging Rate: 0.07535885167464115\n",
      "Precision: 1.0 \n",
      "Recall: 0.7368421052631579 \n",
      "Aging Rate: 0.06698564593301436\n",
      "\u001b[32m[I 2022-05-26 16:13:33,953]\u001b[0m Trial 11 finished with value: 0.872468570598067 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 12, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 8.557450888755158, 'min_data_in_leaf': 5}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.9117647058823529 \n",
      "Recall: 0.8157894736842105 \n",
      "Aging Rate: 0.08133971291866028\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.07416267942583732\n",
      "Precision: 0.9655172413793104 \n",
      "Recall: 0.7368421052631579 \n",
      "Aging Rate: 0.06937799043062201\n",
      "\u001b[32m[I 2022-05-26 16:13:36,826]\u001b[0m Trial 12 finished with value: 0.8554990746749344 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 6, 'learning_rate': 0.225, 'subsample': 0.5, 'l2_leaf_reg': 0.010593296003931723, 'min_data_in_leaf': 15}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.9692307692307692 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07775119617224881\n",
      "Precision: 1.0 \n",
      "Recall: 0.7631578947368421 \n",
      "Aging Rate: 0.06937799043062201\n",
      "Precision: 0.9523809523809523 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.07535885167464115\n",
      "\u001b[32m[I 2022-05-26 16:13:38,159]\u001b[0m Trial 13 finished with value: 0.874199338528542 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.225, 'subsample': 0.3, 'l2_leaf_reg': 0.34715905761580457, 'min_data_in_leaf': 5}. Best is trial 6 with value: 0.9048011696202503.\u001b[0m\n",
      "Precision: 0.9344262295081968 \n",
      "Recall: 0.75 \n",
      "Aging Rate: 0.0729665071770335\n",
      "Precision: 0.9857142857142858 \n",
      "Recall: 0.9078947368421053 \n",
      "Aging Rate: 0.08373205741626795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9466666666666667 \n",
      "Recall: 0.9342105263157895 \n",
      "Aging Rate: 0.08971291866028708\n",
      "\u001b[32m[I 2022-05-26 16:13:42,288]\u001b[0m Trial 14 finished with value: 0.9059065395888668 and parameters: {'grow_policy': 'Depthwise', 'iterations': 500, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 3.2137527562541335, 'min_data_in_leaf': 20}. Best is trial 14 with value: 0.9059065395888668.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.881578947368421 \n",
      "Aging Rate: 0.08014354066985646\n",
      "Precision: 0.9206349206349206 \n",
      "Recall: 0.7631578947368421 \n",
      "Aging Rate: 0.07535885167464115\n",
      "Precision: 1.0 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.07177033492822966\n",
      "\u001b[32m[I 2022-05-26 16:13:47,453]\u001b[0m Trial 15 finished with value: 0.8846494174467091 and parameters: {'grow_policy': 'Lossguide', 'iterations': 500, 'depth': 10, 'learning_rate': 0.325, 'subsample': 0.7, 'l2_leaf_reg': 9.827363081404332, 'min_data_in_leaf': 50, 'max_leaves': 50}. Best is trial 14 with value: 0.9059065395888668.\u001b[0m\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.0729665071770335\n",
      "Precision: 0.9855072463768116 \n",
      "Recall: 0.8947368421052632 \n",
      "Aging Rate: 0.08253588516746412\n",
      "Precision: 0.9701492537313433 \n",
      "Recall: 0.8552631578947368 \n",
      "Aging Rate: 0.08014354066985646\n",
      "\u001b[32m[I 2022-05-26 16:13:48,818]\u001b[0m Trial 16 finished with value: 0.9076447841109306 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 12, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 3.8769393281676847, 'min_data_in_leaf': 25}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9696969696969697 \n",
      "Recall: 0.8421052631578947 \n",
      "Aging Rate: 0.07894736842105263\n",
      "Precision: 0.9692307692307692 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07775119617224881\n",
      "Precision: 0.9850746268656716 \n",
      "Recall: 0.868421052631579 \n",
      "Aging Rate: 0.08014354066985646\n",
      "\u001b[32m[I 2022-05-26 16:13:49,934]\u001b[0m Trial 17 finished with value: 0.9060341316859146 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 3.3620199713119883, 'min_data_in_leaf': 25}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9672131147540983 \n",
      "Recall: 0.7763157894736842 \n",
      "Aging Rate: 0.0729665071770335\n",
      "Precision: 1.0 \n",
      "Recall: 0.8157894736842105 \n",
      "Aging Rate: 0.07416267942583732\n",
      "Precision: 0.984375 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07655502392344497\n",
      "\u001b[32m[I 2022-05-26 16:13:51,418]\u001b[0m Trial 18 finished with value: 0.8866215310836066 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 12, 'learning_rate': 0.07500000000000001, 'subsample': 0.7, 'l2_leaf_reg': 4.001905496756388, 'min_data_in_leaf': 30}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9333333333333333 \n",
      "Recall: 0.7368421052631579 \n",
      "Aging Rate: 0.07177033492822966\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.7894736842105263 \n",
      "Aging Rate: 0.0729665071770335\n",
      "Precision: 0.9508196721311475 \n",
      "Recall: 0.7631578947368421 \n",
      "Aging Rate: 0.0729665071770335\n",
      "\u001b[32m[I 2022-05-26 16:13:52,231]\u001b[0m Trial 19 finished with value: 0.8487190496636611 and parameters: {'grow_policy': 'Lossguide', 'iterations': 100, 'depth': 8, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 5.182085853649494, 'min_data_in_leaf': 30, 'max_leaves': 10}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9305555555555556 \n",
      "Recall: 0.881578947368421 \n",
      "Aging Rate: 0.0861244019138756\n",
      "Precision: 0.9848484848484849 \n",
      "Recall: 0.8552631578947368 \n",
      "Aging Rate: 0.07894736842105263\n",
      "Precision: 1.0 \n",
      "Recall: 0.7631578947368421 \n",
      "Aging Rate: 0.06937799043062201\n",
      "\u001b[32m[I 2022-05-26 16:13:53,523]\u001b[0m Trial 20 finished with value: 0.8955233349809765 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.7, 'l2_leaf_reg': 1.594018942579115, 'min_data_in_leaf': 20}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9538461538461539 \n",
      "Recall: 0.8157894736842105 \n",
      "Aging Rate: 0.07775119617224881\n",
      "Precision: 0.9315068493150684 \n",
      "Recall: 0.8947368421052632 \n",
      "Aging Rate: 0.08732057416267942\n",
      "Precision: 1.0 \n",
      "Recall: 0.868421052631579 \n",
      "Aging Rate: 0.07894736842105263\n",
      "\u001b[32m[I 2022-05-26 16:13:54,761]\u001b[0m Trial 21 finished with value: 0.9072539222515189 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 2.350470920366555, 'min_data_in_leaf': 20}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9285714285714286 \n",
      "Recall: 0.8552631578947368 \n",
      "Aging Rate: 0.08373205741626795\n",
      "Precision: 0.9333333333333333 \n",
      "Recall: 0.7368421052631579 \n",
      "Aging Rate: 0.07177033492822966\n",
      "Precision: 0.9857142857142858 \n",
      "Recall: 0.9078947368421053 \n",
      "Aging Rate: 0.08373205741626795\n",
      "\u001b[32m[I 2022-05-26 16:13:56,017]\u001b[0m Trial 22 finished with value: 0.8863819500402901 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 12, 'learning_rate': 0.17500000000000002, 'subsample': 0.5, 'l2_leaf_reg': 1.925172488896947, 'min_data_in_leaf': 15}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.7105263157894737 \n",
      "Aging Rate: 0.0645933014354067\n",
      "Precision: 0.9818181818181818 \n",
      "Recall: 0.7105263157894737 \n",
      "Aging Rate: 0.06578947368421052\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.7763157894736842 \n",
      "Aging Rate: 0.07177033492822966\n",
      "\u001b[32m[I 2022-05-26 16:13:57,280]\u001b[0m Trial 23 finished with value: 0.8409479235029304 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 10, 'learning_rate': 0.07500000000000001, 'subsample': 0.5, 'l2_leaf_reg': 7.783315680181598, 'min_data_in_leaf': 25}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Precision: 0.9104477611940298 \n",
      "Recall: 0.8026315789473685 \n",
      "Aging Rate: 0.08014354066985646\n",
      "Precision: 0.9692307692307692 \n",
      "Recall: 0.8289473684210527 \n",
      "Aging Rate: 0.07775119617224881\n",
      "Precision: 0.9846153846153847 \n",
      "Recall: 0.8421052631578947 \n",
      "Aging Rate: 0.07775119617224881\n",
      "\u001b[32m[I 2022-05-26 16:13:58,278]\u001b[0m Trial 24 finished with value: 0.8848550976210551 and parameters: {'grow_policy': 'Depthwise', 'iterations': 100, 'depth': 8, 'learning_rate': 0.275, 'subsample': 0.5, 'l2_leaf_reg': 0.4564325552085415, 'min_data_in_leaf': 10}. Best is trial 16 with value: 0.9076447841109306.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "\n",
      "Starting for NeuralNetwork:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8004515f32e41278c220ea1aeac4cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 16:13:58,380]\u001b[0m A new study created in memory with name: no-name-b361d955-a1b5-4988-9940-778086d449b0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338500e30bef4831baa1f733c9222aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae2e7b491db44f6bb8c2424103a705f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5194177457560664\n",
      "Epoch 2: Train Loss = 0.514245212285415\n",
      "Epoch 3: Train Loss = 0.5142056990706403\n",
      "Epoch 4: Train Loss = 0.5137086047296938\n",
      "Epoch 5: Train Loss = 0.5138762758089148\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5141302203095478\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5143830234071483\n",
      "Epoch 7: Train Loss = 0.5142100178677103\n",
      "Epoch 8: Train Loss = 0.5149640181271926\n",
      "Epoch 9: Train Loss = 0.5136859539280767\n",
      "Epoch 10: Train Loss = 0.514611136291338\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5142469456403151\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 11: Train Loss = 0.513384305342384\n",
      "Epoch 12: Train Loss = 0.5149914588617241\n",
      "Epoch 13: Train Loss = 0.5142145448663961\n",
      "Epoch 14: Train Loss = 0.5144903215636377\n",
      "Epoch 15: Train Loss = 0.5142891768787218\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5138331578088843\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 16: Train Loss = 0.5136241831986801\n",
      "Epoch 17: Train Loss = 0.5140740508618562\n",
      "Epoch 18: Train Loss = 0.5140734483884728\n",
      "Epoch 19: Train Loss = 0.5141567171138266\n",
      "Epoch 20: Train Loss = 0.5138335161623747\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5139696599089582\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 21: Train Loss = 0.5135763279251431\n",
      "Epoch 22: Train Loss = 0.5141697345609251\n",
      "Epoch 23: Train Loss = 0.5148740609832432\n",
      "Epoch 24: Train Loss = 0.5141791606986005\n",
      "Epoch 25: Train Loss = 0.514146276038626\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5144766208399897\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 26: Train Loss = 0.5140800985046055\n",
      "Epoch 27: Train Loss = 0.5133731661672177\n",
      "Epoch 28: Train Loss = 0.5135818264795387\n",
      "Epoch 29: Train Loss = 0.5132372306740802\n",
      "Epoch 30: Train Loss = 0.5142143578114717\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5148555998180223\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 31: Train Loss = 0.5144431707133418\n",
      "Epoch 32: Train Loss = 0.513955307421477\n",
      "Epoch 33: Train Loss = 0.5140311493562616\n",
      "Epoch 34: Train Loss = 0.5146830754694731\n",
      "Epoch 35: Train Loss = 0.5138088064090065\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5140171115813048\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 36: Train Loss = 0.5148114694719729\n",
      "Epoch 37: Train Loss = 0.5139700903063235\n",
      "Epoch 38: Train Loss = 0.5137297837112261\n",
      "Epoch 39: Train Loss = 0.5136224129925604\n",
      "Epoch 40: Train Loss = 0.5149793951407723\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5143299539192863\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 41: Train Loss = 0.5135286249285159\n",
      "Epoch 42: Train Loss = 0.5139139905701513\n",
      "Epoch 43: Train Loss = 0.5140338666542716\n",
      "Epoch 44: Train Loss = 0.5139625932859337\n",
      "Epoch 45: Train Loss = 0.5144442348894865\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5139654644675877\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 46: Train Loss = 0.5139755750739056\n",
      "Epoch 47: Train Loss = 0.5138848157550978\n",
      "Epoch 48: Train Loss = 0.5142080404447472\n",
      "Epoch 49: Train Loss = 0.5140154466421708\n",
      "Epoch 50: Train Loss = 0.5140284802602685\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5141343143193619\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Epoch 51: Train Loss = 0.5141344671664031\n",
      "Epoch 52: Train Loss = 0.5141266557444697\n",
      "Epoch 53: Train Loss = 0.5147951048353444\n",
      "Epoch 54: Train Loss = 0.5140489096226899\n",
      "Epoch 55: Train Loss = 0.5147614746508391\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.503695652173913\n",
      "Validation: Test Loss = 0.5134303765711578\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.503695652173913\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.522749868645351\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.48891786179921776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1544f8d7d02145f5bd2bb1a73660c17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5203488221375838\n",
      "Epoch 2: Train Loss = 0.513160050848256\n",
      "Epoch 3: Train Loss = 0.5135725154047427\n",
      "Epoch 4: Train Loss = 0.513617028568102\n",
      "Epoch 5: Train Loss = 0.5137734803946122\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5130572950839997\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5136267707140549\n",
      "Epoch 7: Train Loss = 0.5138264741068301\n",
      "Epoch 8: Train Loss = 0.5137836580691131\n",
      "Epoch 9: Train Loss = 0.5141613861270573\n",
      "Epoch 10: Train Loss = 0.5133277889956599\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5139633301030034\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 11: Train Loss = 0.5139447979305102\n",
      "Epoch 12: Train Loss = 0.5138339070133541\n",
      "Epoch 13: Train Loss = 0.5138172037705131\n",
      "Epoch 14: Train Loss = 0.5137163417235665\n",
      "Epoch 15: Train Loss = 0.5131746965905895\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5150534320914227\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 16: Train Loss = 0.5140552426939425\n",
      "Epoch 17: Train Loss = 0.513862105037855\n",
      "Epoch 18: Train Loss = 0.5143630879858265\n",
      "Epoch 19: Train Loss = 0.5136306382262188\n",
      "Epoch 20: Train Loss = 0.5135657105238541\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5137267391578011\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 21: Train Loss = 0.5142001922234245\n",
      "Epoch 22: Train Loss = 0.5142153500474017\n",
      "Epoch 23: Train Loss = 0.5140600666792496\n",
      "Epoch 24: Train Loss = 0.5140428830229717\n",
      "Epoch 25: Train Loss = 0.5131077136164126\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5149583567743716\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 26: Train Loss = 0.5142752150867296\n",
      "Epoch 27: Train Loss = 0.5140702307742575\n",
      "Epoch 28: Train Loss = 0.5135346801902937\n",
      "Epoch 29: Train Loss = 0.5140925599699435\n",
      "Epoch 30: Train Loss = 0.5141159573845241\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5137402876563694\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 31: Train Loss = 0.514257448548856\n",
      "Epoch 32: Train Loss = 0.5138642997327059\n",
      "Epoch 33: Train Loss = 0.5140689273502516\n",
      "Epoch 34: Train Loss = 0.514078876764878\n",
      "Epoch 35: Train Loss = 0.5141135059232297\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5138939792176952\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 36: Train Loss = 0.5145046464256618\n",
      "Epoch 37: Train Loss = 0.5146412173561428\n",
      "Epoch 38: Train Loss = 0.5143898443553758\n",
      "Epoch 39: Train Loss = 0.5134784228905388\n",
      "Epoch 40: Train Loss = 0.5137639778593313\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5135566548679186\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 41: Train Loss = 0.5147240550621696\n",
      "Epoch 42: Train Loss = 0.5133450387871784\n",
      "Epoch 43: Train Loss = 0.5136722398840863\n",
      "Epoch 44: Train Loss = 0.5145367074012757\n",
      "Epoch 45: Train Loss = 0.5139649152755738\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5132168435013813\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 46: Train Loss = 0.5135872047880422\n",
      "Epoch 47: Train Loss = 0.5138728841491368\n",
      "Epoch 48: Train Loss = 0.5138567095217498\n",
      "Epoch 49: Train Loss = 0.5139118102322454\n",
      "Epoch 50: Train Loss = 0.5132938369460728\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.513705966576286\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Epoch 51: Train Loss = 0.5148312927329022\n",
      "Epoch 52: Train Loss = 0.5139529646997867\n",
      "Epoch 53: Train Loss = 0.513732707604118\n",
      "Epoch 54: Train Loss = 0.5143871684696364\n",
      "Epoch 55: Train Loss = 0.5141711118428604\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5041304347826087\n",
      "Validation: Test Loss = 0.5139410884484\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5041304347826087\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5253861424509518\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4876140808344198\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8cfa5a904e4846929cbcfa3d3e5d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5232441931703816\n",
      "Epoch 2: Train Loss = 0.5161967969977337\n",
      "Epoch 3: Train Loss = 0.5160879809960075\n",
      "Epoch 4: Train Loss = 0.5154826311443163\n",
      "Epoch 5: Train Loss = 0.5166510638983354\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5155233899925066\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5169198570044145\n",
      "Epoch 7: Train Loss = 0.5163375267256861\n",
      "Epoch 8: Train Loss = 0.5157441973686219\n",
      "Epoch 9: Train Loss = 0.5167609045816505\n",
      "Epoch 10: Train Loss = 0.5158792624266251\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5163961272136025\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 11: Train Loss = 0.5158251849464748\n",
      "Epoch 12: Train Loss = 0.5157959510969079\n",
      "Epoch 13: Train Loss = 0.5159614508566649\n",
      "Epoch 14: Train Loss = 0.5155993433620619\n",
      "Epoch 15: Train Loss = 0.5168058576791182\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5153114476411239\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 16: Train Loss = 0.5158716847585595\n",
      "Epoch 17: Train Loss = 0.5163715936826623\n",
      "Epoch 18: Train Loss = 0.5164791944234267\n",
      "Epoch 19: Train Loss = 0.5155608561764593\n",
      "Epoch 20: Train Loss = 0.5169113511624543\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5158552271386851\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 21: Train Loss = 0.516053767359775\n",
      "Epoch 22: Train Loss = 0.5163322778888371\n",
      "Epoch 23: Train Loss = 0.5150983649751414\n",
      "Epoch 24: Train Loss = 0.5157467550816743\n",
      "Epoch 25: Train Loss = 0.5158681301448657\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.516184832531473\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 26: Train Loss = 0.5165732292507006\n",
      "Epoch 27: Train Loss = 0.5162005302180415\n",
      "Epoch 28: Train Loss = 0.516069833299388\n",
      "Epoch 29: Train Loss = 0.5152802381308182\n",
      "Epoch 30: Train Loss = 0.5164841536853625\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5156029998219531\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 31: Train Loss = 0.5162914120114368\n",
      "Epoch 32: Train Loss = 0.5157751663871434\n",
      "Epoch 33: Train Loss = 0.5155740998102271\n",
      "Epoch 34: Train Loss = 0.5166764001224352\n",
      "Epoch 35: Train Loss = 0.5158934666799463\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5163803375285605\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 36: Train Loss = 0.5153974680796913\n",
      "Epoch 37: Train Loss = 0.5168894553184509\n",
      "Epoch 38: Train Loss = 0.5153730572824893\n",
      "Epoch 39: Train Loss = 0.5152178239822388\n",
      "Epoch 40: Train Loss = 0.5151637921644294\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.516270630100499\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 41: Train Loss = 0.5158561146777609\n",
      "Epoch 42: Train Loss = 0.5165599473662998\n",
      "Epoch 43: Train Loss = 0.5159402186974236\n",
      "Epoch 44: Train Loss = 0.5162316891421442\n",
      "Epoch 45: Train Loss = 0.515983736411385\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.515638300128605\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 46: Train Loss = 0.515764654615651\n",
      "Epoch 47: Train Loss = 0.515552065372467\n",
      "Epoch 48: Train Loss = 0.5156094390413035\n",
      "Epoch 49: Train Loss = 0.5151045765047488\n",
      "Epoch 50: Train Loss = 0.5166613106105639\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5152048733441726\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 51: Train Loss = 0.5152889485981154\n",
      "Epoch 52: Train Loss = 0.5152802277647931\n",
      "Epoch 53: Train Loss = 0.5157258188206216\n",
      "Epoch 54: Train Loss = 0.516356503341509\n",
      "Epoch 55: Train Loss = 0.5158661574902742\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.5154045400412186\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5187373012110027\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49674054758800523\n",
      "\u001b[32m[I 2022-05-26 16:14:16,623]\u001b[0m Trial 0 finished with value: 0.6586903055739608 and parameters: {'batch_size': 64, 'learning_rate': 0.01, 'weight_decay': 0.001, 'bad_weight': 0.8}. Best is trial 0 with value: 0.6586903055739608.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8476f779a3b644c9983daaddb70c11b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5617973848011183\n",
      "Epoch 2: Train Loss = 0.5181545178786569\n",
      "Epoch 3: Train Loss = 0.5163103988896246\n",
      "Epoch 4: Train Loss = 0.5156761098944622\n",
      "Epoch 5: Train Loss = 0.5152120936435202\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5165405814544014\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.515540726599486\n",
      "Epoch 7: Train Loss = 0.5168529711598935\n",
      "Epoch 8: Train Loss = 0.5156641434068265\n",
      "Epoch 9: Train Loss = 0.5157736860150877\n",
      "Epoch 10: Train Loss = 0.5171517115053923\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5153992557007333\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 11: Train Loss = 0.5157318586888521\n",
      "Epoch 12: Train Loss = 0.5159311918072078\n",
      "Epoch 13: Train Loss = 0.5142327716039574\n",
      "Epoch 14: Train Loss = 0.5159367239993552\n",
      "Epoch 15: Train Loss = 0.5160845966961073\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5162176640137383\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 16: Train Loss = 0.5155784754649453\n",
      "Epoch 17: Train Loss = 0.5158261466026306\n",
      "Epoch 18: Train Loss = 0.5154472382172294\n",
      "Epoch 19: Train Loss = 0.5145878164664559\n",
      "Epoch 20: Train Loss = 0.5154312287206235\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5158614246741585\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 21: Train Loss = 0.516057593304178\n",
      "Epoch 22: Train Loss = 0.5163321185112\n",
      "Epoch 23: Train Loss = 0.5148341530302296\n",
      "Epoch 24: Train Loss = 0.5156242639085521\n",
      "Epoch 25: Train Loss = 0.515800204069718\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5152590515302575\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 26: Train Loss = 0.5147641101609106\n",
      "Epoch 27: Train Loss = 0.5163706917348115\n",
      "Epoch 28: Train Loss = 0.5158785102678382\n",
      "Epoch 29: Train Loss = 0.5157005010998768\n",
      "Epoch 30: Train Loss = 0.5161934756714365\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5156180498910987\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 31: Train Loss = 0.5152055636177892\n",
      "Epoch 32: Train Loss = 0.5153220358620519\n",
      "Epoch 33: Train Loss = 0.5156501160497251\n",
      "Epoch 34: Train Loss = 0.5158082103729248\n",
      "Epoch 35: Train Loss = 0.5155756801107655\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5162388440837031\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 36: Train Loss = 0.5159947918808978\n",
      "Epoch 37: Train Loss = 0.5161152280931888\n",
      "Epoch 38: Train Loss = 0.5155486538099207\n",
      "Epoch 39: Train Loss = 0.5157978613480277\n",
      "Epoch 40: Train Loss = 0.5155735586000525\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5156063020747641\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 41: Train Loss = 0.5163356605820034\n",
      "Epoch 42: Train Loss = 0.5157293687177741\n",
      "Epoch 43: Train Loss = 0.5164739733156951\n",
      "Epoch 44: Train Loss = 0.5164888114514559\n",
      "Epoch 45: Train Loss = 0.5163037542674852\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5156062648607337\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 46: Train Loss = 0.5151295186125714\n",
      "Epoch 47: Train Loss = 0.5154070240518321\n",
      "Epoch 48: Train Loss = 0.5149615299701691\n",
      "Epoch 49: Train Loss = 0.5162943135137144\n",
      "Epoch 50: Train Loss = 0.5162318834014561\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5159610247612\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Epoch 51: Train Loss = 0.5153363519647847\n",
      "Epoch 52: Train Loss = 0.5165787355277849\n",
      "Epoch 53: Train Loss = 0.5162674137820368\n",
      "Epoch 54: Train Loss = 0.5154365850531537\n",
      "Epoch 55: Train Loss = 0.5160562951668449\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5008695652173913\n",
      "Validation: Test Loss = 0.5154964101314544\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5008695652173913\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5187189104970462\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4973924380704042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcdd663c76a4acabe9ead45e6e11834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5692766340919163\n",
      "Epoch 2: Train Loss = 0.5198032857024152\n",
      "Epoch 3: Train Loss = 0.5184904624068218\n",
      "Epoch 4: Train Loss = 0.5163513528782389\n",
      "Epoch 5: Train Loss = 0.5165061819553375\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5166843737726626\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5160872712342636\n",
      "Epoch 7: Train Loss = 0.51607940207357\n",
      "Epoch 8: Train Loss = 0.5168371338947959\n",
      "Epoch 9: Train Loss = 0.5174936083607051\n",
      "Epoch 10: Train Loss = 0.5170384509148805\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5170708421002264\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 11: Train Loss = 0.5167082311796105\n",
      "Epoch 12: Train Loss = 0.5167044491353242\n",
      "Epoch 13: Train Loss = 0.5162086259800455\n",
      "Epoch 14: Train Loss = 0.5168598305660745\n",
      "Epoch 15: Train Loss = 0.5157085291199062\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5169068855824678\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 16: Train Loss = 0.5162855948572573\n",
      "Epoch 17: Train Loss = 0.5163066860903864\n",
      "Epoch 18: Train Loss = 0.5156304643465125\n",
      "Epoch 19: Train Loss = 0.5168698912081511\n",
      "Epoch 20: Train Loss = 0.5158768804176994\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5163458725680475\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 21: Train Loss = 0.5174415507005609\n",
      "Epoch 22: Train Loss = 0.5163599452765092\n",
      "Epoch 23: Train Loss = 0.5156741391057553\n",
      "Epoch 24: Train Loss = 0.5161602965645168\n",
      "Epoch 25: Train Loss = 0.5159845656933992\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5163476314751998\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 26: Train Loss = 0.5166489664368007\n",
      "Epoch 27: Train Loss = 0.5175931800966678\n",
      "Epoch 28: Train Loss = 0.5171722615283468\n",
      "Epoch 29: Train Loss = 0.516421854806983\n",
      "Epoch 30: Train Loss = 0.5164185232701509\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5167547194854073\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 31: Train Loss = 0.5157006903834965\n",
      "Epoch 32: Train Loss = 0.5173109862078791\n",
      "Epoch 33: Train Loss = 0.5169868849671405\n",
      "Epoch 34: Train Loss = 0.5157559254376785\n",
      "Epoch 35: Train Loss = 0.5162979051341181\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5165354739064756\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 36: Train Loss = 0.5177105263005132\n",
      "Epoch 37: Train Loss = 0.5160743080014768\n",
      "Epoch 38: Train Loss = 0.5163317951948746\n",
      "Epoch 39: Train Loss = 0.5165785864125128\n",
      "Epoch 40: Train Loss = 0.5168739148844843\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5174799930531045\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 41: Train Loss = 0.5168703702221746\n",
      "Epoch 42: Train Loss = 0.5164073236610578\n",
      "Epoch 43: Train Loss = 0.5167612138001815\n",
      "Epoch 44: Train Loss = 0.5164251394893812\n",
      "Epoch 45: Train Loss = 0.5167103374522666\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5169397482664688\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 46: Train Loss = 0.5163837420422098\n",
      "Epoch 47: Train Loss = 0.5166818491272305\n",
      "Epoch 48: Train Loss = 0.5163939473939979\n",
      "Epoch 49: Train Loss = 0.516814936036649\n",
      "Epoch 50: Train Loss = 0.5165250128248463\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5163153209375299\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Epoch 51: Train Loss = 0.5161882943692415\n",
      "Epoch 52: Train Loss = 0.5164702762728152\n",
      "Epoch 53: Train Loss = 0.5157691762240036\n",
      "Epoch 54: Train Loss = 0.5166920475337816\n",
      "Epoch 55: Train Loss = 0.5160262620967367\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49956521739130433\n",
      "Validation: Test Loss = 0.5166178049730218\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49956521739130433\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5148754926517392\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5013037809647979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5a62e4ce7a4d7eb7ee6b4569924dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.572321936669557\n",
      "Epoch 2: Train Loss = 0.5206015093430229\n",
      "Epoch 3: Train Loss = 0.5178701651614646\n",
      "Epoch 4: Train Loss = 0.5170370100892109\n",
      "Epoch 5: Train Loss = 0.5175934334423231\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5175211670087732\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5170215114303257\n",
      "Epoch 7: Train Loss = 0.5172661751249562\n",
      "Epoch 8: Train Loss = 0.5162553114994712\n",
      "Epoch 9: Train Loss = 0.5173183030667512\n",
      "Epoch 10: Train Loss = 0.516248664441316\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5165029330357261\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 11: Train Loss = 0.517617882852969\n",
      "Epoch 12: Train Loss = 0.5163631571894106\n",
      "Epoch 13: Train Loss = 0.5168799529904905\n",
      "Epoch 14: Train Loss = 0.5169606148160022\n",
      "Epoch 15: Train Loss = 0.516551977655162\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5169856845814249\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 16: Train Loss = 0.5173229176065196\n",
      "Epoch 17: Train Loss = 0.5163678757004115\n",
      "Epoch 18: Train Loss = 0.517004888990651\n",
      "Epoch 19: Train Loss = 0.5157895537044691\n",
      "Epoch 20: Train Loss = 0.5163192258710446\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5170844259469406\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 21: Train Loss = 0.5169496481314949\n",
      "Epoch 22: Train Loss = 0.5166477918106577\n",
      "Epoch 23: Train Loss = 0.5164043956217559\n",
      "Epoch 24: Train Loss = 0.5169604271391164\n",
      "Epoch 25: Train Loss = 0.516139286549195\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5166977636710457\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 26: Train Loss = 0.5163673761616583\n",
      "Epoch 27: Train Loss = 0.5162831472313922\n",
      "Epoch 28: Train Loss = 0.5182853445799455\n",
      "Epoch 29: Train Loss = 0.5167535201362942\n",
      "Epoch 30: Train Loss = 0.5169561119701551\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5164225463245226\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 31: Train Loss = 0.5167964477124422\n",
      "Epoch 32: Train Loss = 0.5162009785486305\n",
      "Epoch 33: Train Loss = 0.5168972347093665\n",
      "Epoch 34: Train Loss = 0.5171743810695151\n",
      "Epoch 35: Train Loss = 0.5180688023567199\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5171748795716659\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 36: Train Loss = 0.5162655041528784\n",
      "Epoch 37: Train Loss = 0.5170388075579767\n",
      "Epoch 38: Train Loss = 0.516610886739648\n",
      "Epoch 39: Train Loss = 0.5177611544857854\n",
      "Epoch 40: Train Loss = 0.5168453012342038\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5182143836954366\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 41: Train Loss = 0.5178582536655923\n",
      "Epoch 42: Train Loss = 0.5156568754237632\n",
      "Epoch 43: Train Loss = 0.5163586293096127\n",
      "Epoch 44: Train Loss = 0.5167177344405133\n",
      "Epoch 45: Train Loss = 0.5172595085268435\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5167794020797896\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 46: Train Loss = 0.516618002238481\n",
      "Epoch 47: Train Loss = 0.5174104483231254\n",
      "Epoch 48: Train Loss = 0.5161721922003705\n",
      "Epoch 49: Train Loss = 0.5170476713387863\n",
      "Epoch 50: Train Loss = 0.5171401340028514\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5171833521386852\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Epoch 51: Train Loss = 0.5162364003969275\n",
      "Epoch 52: Train Loss = 0.5164265439821326\n",
      "Epoch 53: Train Loss = 0.5170990643293961\n",
      "Epoch 54: Train Loss = 0.5170736912022467\n",
      "Epoch 55: Train Loss = 0.5170475539953813\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991304347826087\n",
      "Validation: Test Loss = 0.5170494939969934\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991304347826087\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.515798424648648\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026075619295959\n",
      "\u001b[32m[I 2022-05-26 16:14:35,825]\u001b[0m Trial 1 finished with value: 0.667049950474877 and parameters: {'batch_size': 64, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'bad_weight': 0.8}. Best is trial 1 with value: 0.667049950474877.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78571d5762ce47d9885c9baa3fdd1f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6823216948301896\n",
      "Epoch 2: Train Loss = 0.6733775862403538\n",
      "Epoch 3: Train Loss = 0.665519287897193\n",
      "Epoch 4: Train Loss = 0.657919688017472\n",
      "Epoch 5: Train Loss = 0.6505613148730734\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.6476639911402827\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6447965038341025\n",
      "Epoch 7: Train Loss = 0.6391868709481281\n",
      "Epoch 8: Train Loss = 0.6343865095014157\n",
      "Epoch 9: Train Loss = 0.6301088770576145\n",
      "Epoch 10: Train Loss = 0.6262040938501773\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.6243533377025439\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 11: Train Loss = 0.6227999815733536\n",
      "Epoch 12: Train Loss = 0.6199452257156373\n",
      "Epoch 13: Train Loss = 0.6172006402844968\n",
      "Epoch 14: Train Loss = 0.6141161313264266\n",
      "Epoch 15: Train Loss = 0.6111437579859857\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5010869565217392\n",
      "Validation: Test Loss = 0.6095475451842598\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5010869565217392\n",
      "\n",
      "Epoch 16: Train Loss = 0.6077411497157553\n",
      "Epoch 17: Train Loss = 0.6045789794299914\n",
      "Epoch 18: Train Loss = 0.6003504962506502\n",
      "Epoch 19: Train Loss = 0.5959021439759628\n",
      "Epoch 20: Train Loss = 0.5916105541975601\n",
      "Recall = 1.0, Aging Rate = 0.9993478260869565, Precision = 0.5014139656297585\n",
      "Validation: Test Loss = 0.5885019002790036\n",
      "Recall = 1.0, Aging Rate = 0.9978260869565218, precision = 0.5021786492374728\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5866173012360283\n",
      "Epoch 22: Train Loss = 0.5812135485980822\n",
      "Epoch 23: Train Loss = 0.5756145120703656\n",
      "Epoch 24: Train Loss = 0.5703422746451005\n",
      "Epoch 25: Train Loss = 0.565093757795251\n",
      "Recall = 0.9939262472885032, Aging Rate = 0.9030434782608696, Precision = 0.5515166104959076\n",
      "Validation: Test Loss = 0.5622743259305539\n",
      "Recall = 0.9908893709327549, Aging Rate = 0.878695652173913, precision = 0.5650667986145472\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5599892934508945\n",
      "Epoch 27: Train Loss = 0.5549403493300729\n",
      "Epoch 28: Train Loss = 0.5504838270726411\n",
      "Epoch 29: Train Loss = 0.5457357123623724\n",
      "Epoch 30: Train Loss = 0.5415094153777412\n",
      "Recall = 0.9774403470715836, Aging Rate = 0.7902173913043479, Precision = 0.6198074277854195\n",
      "Validation: Test Loss = 0.5392962895268979\n",
      "Recall = 0.9774403470715836, Aging Rate = 0.7823913043478261, precision = 0.6260072242289525\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.5376233507239301\n",
      "Epoch 32: Train Loss = 0.5344550348364788\n",
      "Epoch 33: Train Loss = 0.5307621443789938\n",
      "Epoch 34: Train Loss = 0.5273822086790334\n",
      "Epoch 35: Train Loss = 0.5244600680600042\n",
      "Recall = 0.9661605206073752, Aging Rate = 0.7397826086956522, Precision = 0.6544225683220688\n",
      "Validation: Test Loss = 0.5230833242250525\n",
      "Recall = 0.9661605206073752, Aging Rate = 0.7232608695652174, precision = 0.6693718064322213\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.5219542006824327\n",
      "Epoch 37: Train Loss = 0.5192610890968986\n",
      "Epoch 38: Train Loss = 0.5168528208006983\n",
      "Epoch 39: Train Loss = 0.5146925246197245\n",
      "Epoch 40: Train Loss = 0.5126397737212803\n",
      "Recall = 0.9639913232104121, Aging Rate = 0.7132608695652174, Precision = 0.6772325510515087\n",
      "Validation: Test Loss = 0.5114978846259739\n",
      "Recall = 0.9522776572668112, Aging Rate = 0.6878260869565217, precision = 0.6937420986093552\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.5109801820050115\n",
      "Epoch 42: Train Loss = 0.5090831798055898\n",
      "Epoch 43: Train Loss = 0.5071633262219636\n",
      "Epoch 44: Train Loss = 0.5055905077250108\n",
      "Epoch 45: Train Loss = 0.5043384763468867\n",
      "Recall = 0.9518438177874187, Aging Rate = 0.6791304347826087, Precision = 0.7023047375160051\n",
      "Validation: Test Loss = 0.5032595689400383\n",
      "Recall = 0.9492407809110629, Aging Rate = 0.6719565217391305, precision = 0.7078615334843092\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.5027048363892929\n",
      "Epoch 47: Train Loss = 0.5010180887968644\n",
      "Epoch 48: Train Loss = 0.4995922735463018\n",
      "Epoch 49: Train Loss = 0.4985563958209494\n",
      "Epoch 50: Train Loss = 0.49728924243346506\n",
      "Recall = 0.9483731019522776, Aging Rate = 0.667608695652174, Precision = 0.7118202539889287\n",
      "Validation: Test Loss = 0.4965917804448501\n",
      "Recall = 0.9483731019522776, Aging Rate = 0.6721739130434783, precision = 0.7069857697283312\n",
      "\n",
      "Epoch 51: Train Loss = 0.4961962868856347\n",
      "Epoch 52: Train Loss = 0.4950831715956978\n",
      "Epoch 53: Train Loss = 0.4940750609273496\n",
      "Epoch 54: Train Loss = 0.49288414011830867\n",
      "Epoch 55: Train Loss = 0.4920368192506873\n",
      "Recall = 0.9470715835140998, Aging Rate = 0.6615217391304348, Precision = 0.7173841603680579\n",
      "Validation: Test Loss = 0.49121089261511097\n",
      "Recall = 0.9466377440347071, Aging Rate = 0.6593478260869565, precision = 0.7194197164523574\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.49113693413527115\n",
      "Epoch 57: Train Loss = 0.4901007518042689\n",
      "Epoch 58: Train Loss = 0.48932237640671106\n",
      "Epoch 59: Train Loss = 0.4885681604820749\n",
      "Epoch 60: Train Loss = 0.4876951977999314\n",
      "Recall = 0.9488069414316703, Aging Rate = 0.6569565217391304, Precision = 0.7236929185969556\n",
      "Validation: Test Loss = 0.48700373343799425\n",
      "Recall = 0.9509761388286334, Aging Rate = 0.6578260869565218, precision = 0.7243886318572372\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.48695883305176446\n",
      "Epoch 62: Train Loss = 0.48610784483992536\n",
      "Epoch 63: Train Loss = 0.485235216099283\n",
      "Epoch 64: Train Loss = 0.48450766910677373\n",
      "Epoch 65: Train Loss = 0.48391261359919674\n",
      "Recall = 0.9496746203904556, Aging Rate = 0.6534782608695652, Precision = 0.7282102461743181\n",
      "Validation: Test Loss = 0.48320292576499607\n",
      "Recall = 0.9470715835140998, Aging Rate = 0.6495652173913044, precision = 0.7305890227576974\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.48302644200946976\n",
      "Epoch 67: Train Loss = 0.48289828300476073\n",
      "Epoch 68: Train Loss = 0.48209073403607244\n",
      "Epoch 69: Train Loss = 0.4814675761305768\n",
      "Epoch 70: Train Loss = 0.4807233314928801\n",
      "Recall = 0.9475054229934924, Aging Rate = 0.6430434782608696, Precision = 0.7383367139959433\n",
      "Validation: Test Loss = 0.4801798766073973\n",
      "Recall = 0.9535791757049892, Aging Rate = 0.6563043478260869, precision = 0.728055647565419\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.48000540795533553\n",
      "Epoch 72: Train Loss = 0.4795576734128206\n",
      "Epoch 73: Train Loss = 0.47896559995153676\n",
      "Epoch 74: Train Loss = 0.4783204344044561\n",
      "Epoch 75: Train Loss = 0.47804620561392414\n",
      "Recall = 0.9605206073752711, Aging Rate = 0.6556521739130434, Precision = 0.7340848806366048\n",
      "Validation: Test Loss = 0.4774343906796497\n",
      "Recall = 0.9587852494577006, Aging Rate = 0.6476086956521739, precision = 0.7418596844578718\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.47771111586819526\n",
      "Epoch 77: Train Loss = 0.47701141041258105\n",
      "Epoch 78: Train Loss = 0.47644690985264987\n",
      "Epoch 79: Train Loss = 0.47599788033443946\n",
      "Epoch 80: Train Loss = 0.47539461047753045\n",
      "Recall = 0.9587852494577006, Aging Rate = 0.6434782608695652, Precision = 0.7466216216216216\n",
      "Validation: Test Loss = 0.4748406175945116\n",
      "Recall = 0.9605206073752711, Aging Rate = 0.65, precision = 0.7404682274247492\n",
      "\n",
      "Epoch 81: Train Loss = 0.4747542742024297\n",
      "Epoch 82: Train Loss = 0.4745806354543437\n",
      "Epoch 83: Train Loss = 0.4743751961770265\n",
      "Epoch 84: Train Loss = 0.47371727155602494\n",
      "Epoch 85: Train Loss = 0.4732904340391574\n",
      "Recall = 0.958351409978308, Aging Rate = 0.6410869565217391, Precision = 0.749067480501865\n",
      "Validation: Test Loss = 0.47275728344917295\n",
      "Recall = 0.9600867678958785, Aging Rate = 0.6476086956521739, precision = 0.7428667338032897\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.47272711453230487\n",
      "Epoch 87: Train Loss = 0.4724666134170864\n",
      "Epoch 88: Train Loss = 0.47204427102337715\n",
      "Epoch 89: Train Loss = 0.47169021414673845\n",
      "Epoch 90: Train Loss = 0.4713777209883151\n",
      "Recall = 0.9592190889370933, Aging Rate = 0.6384782608695653, Precision = 0.7528089887640449\n",
      "Validation: Test Loss = 0.47090929316437763\n",
      "Recall = 0.9592190889370933, Aging Rate = 0.6408695652173914, precision = 0.75\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.4709158420562744\n",
      "Epoch 92: Train Loss = 0.4705457207430964\n",
      "Epoch 93: Train Loss = 0.47039257541946744\n",
      "Epoch 94: Train Loss = 0.4698553259476371\n",
      "Epoch 95: Train Loss = 0.469705706782963\n",
      "Recall = 0.9596529284164859, Aging Rate = 0.6384782608695653, Precision = 0.7531494722505958\n",
      "Validation: Test Loss = 0.4692970057674076\n",
      "Recall = 0.9574837310195228, Aging Rate = 0.6304347826086957, precision = 0.7610344827586207\n",
      "Model in epoch 95 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: Train Loss = 0.46956205855245176\n",
      "Epoch 97: Train Loss = 0.46924506161523905\n",
      "Epoch 98: Train Loss = 0.46882736724355945\n",
      "Epoch 99: Train Loss = 0.4685934019088745\n",
      "Epoch 100: Train Loss = 0.4681322941054469\n",
      "Recall = 0.9613882863340564, Aging Rate = 0.6358695652173914, Precision = 0.7576068376068376\n",
      "Validation: Test Loss = 0.46783831062524217\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6334782608695653, precision = 0.7601235415236788\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4677281429456628\n",
      "Epoch 102: Train Loss = 0.467487932132638\n",
      "Epoch 103: Train Loss = 0.467515345863674\n",
      "Epoch 104: Train Loss = 0.46699420602425284\n",
      "Epoch 105: Train Loss = 0.4666702887286311\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6317391304347826, Precision = 0.7622161046111493\n",
      "Validation: Test Loss = 0.466489394488542\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.633695652173913, precision = 0.7598627787307033\n",
      "\n",
      "Epoch 106: Train Loss = 0.4665500114793363\n",
      "Epoch 107: Train Loss = 0.46633598877036053\n",
      "Epoch 108: Train Loss = 0.4661017575989599\n",
      "Epoch 109: Train Loss = 0.4658888112462085\n",
      "Epoch 110: Train Loss = 0.46559937207595165\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6326086956521739, Precision = 0.761168384879725\n",
      "Validation: Test Loss = 0.4651851607405621\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6330434782608696, precision = 0.7606456043956044\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.46530244848002555\n",
      "Epoch 112: Train Loss = 0.46513306280841\n",
      "Epoch 113: Train Loss = 0.4650017033970874\n",
      "Epoch 114: Train Loss = 0.4646751111486684\n",
      "Epoch 115: Train Loss = 0.46442809488462367\n",
      "Recall = 0.9635574837310196, Aging Rate = 0.6376086956521739, Precision = 0.7572451414933515\n",
      "Validation: Test Loss = 0.46427203302798065\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6276086956521739, precision = 0.7672324211984759\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.4645736428447392\n",
      "Epoch 117: Train Loss = 0.4643012990640557\n",
      "Epoch 118: Train Loss = 0.46404080593067665\n",
      "Epoch 119: Train Loss = 0.4637966085516888\n",
      "Epoch 120: Train Loss = 0.4636805882661239\n",
      "Recall = 0.9626898047722343, Aging Rate = 0.6315217391304347, Precision = 0.763855421686747\n",
      "Validation: Test Loss = 0.46332502111144686\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6273913043478261, precision = 0.7674982674982676\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.46339176701462786\n",
      "Epoch 122: Train Loss = 0.46321730390838955\n",
      "Epoch 123: Train Loss = 0.46338360040084176\n",
      "Epoch 124: Train Loss = 0.4629525113105774\n",
      "Epoch 125: Train Loss = 0.4628394581442294\n",
      "Recall = 0.9618221258134491, Aging Rate = 0.6232608695652174, Precision = 0.7732821764911056\n",
      "Validation: Test Loss = 0.4625014871618022\n",
      "Recall = 0.9635574837310196, Aging Rate = 0.6332608695652174, precision = 0.7624442155853073\n",
      "\n",
      "Epoch 126: Train Loss = 0.4627537267104439\n",
      "Epoch 127: Train Loss = 0.46256350045618805\n",
      "Epoch 128: Train Loss = 0.4624380727954533\n",
      "Epoch 129: Train Loss = 0.4621964164402174\n",
      "Epoch 130: Train Loss = 0.46199769377708433\n",
      "Recall = 0.9618221258134491, Aging Rate = 0.6273913043478261, Precision = 0.7681912681912682\n",
      "Validation: Test Loss = 0.46141082364579905\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6217391304347826, precision = 0.7744755244755245\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.46198534561240157\n",
      "Epoch 132: Train Loss = 0.4616034402536309\n",
      "Epoch 133: Train Loss = 0.4615192999528802\n",
      "Epoch 134: Train Loss = 0.4613068439649499\n",
      "Epoch 135: Train Loss = 0.4611639512103537\n",
      "Recall = 0.9605206073752711, Aging Rate = 0.6239130434782608, Precision = 0.7714285714285715\n",
      "Validation: Test Loss = 0.46103566760602205\n",
      "Recall = 0.9635574837310196, Aging Rate = 0.6317391304347826, precision = 0.764280798348245\n",
      "\n",
      "Epoch 136: Train Loss = 0.46114283888236335\n",
      "Epoch 137: Train Loss = 0.4612350601735322\n",
      "Epoch 138: Train Loss = 0.4612906539958456\n",
      "Epoch 139: Train Loss = 0.460959559523541\n",
      "Epoch 140: Train Loss = 0.4607980762357297\n",
      "Recall = 0.9609544468546638, Aging Rate = 0.6215217391304347, Precision = 0.7747464148303602\n",
      "Validation: Test Loss = 0.4603322823151298\n",
      "Recall = 0.9635574837310196, Aging Rate = 0.6302173913043478, precision = 0.7661262504311832\n",
      "\n",
      "Epoch 141: Train Loss = 0.46071580612141155\n",
      "Epoch 142: Train Loss = 0.4604188701899155\n",
      "Epoch 143: Train Loss = 0.46034087616464364\n",
      "Epoch 144: Train Loss = 0.4601712922427965\n",
      "Epoch 145: Train Loss = 0.46027579146882763\n",
      "Recall = 0.9631236442516269, Aging Rate = 0.6280434782608696, Precision = 0.7684319833852544\n",
      "Validation: Test Loss = 0.4600447676492774\n",
      "Recall = 0.9635574837310196, Aging Rate = 0.6245652173913043, precision = 0.7730595196658545\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.4603099850468014\n",
      "Epoch 147: Train Loss = 0.46004356327264206\n",
      "Epoch 148: Train Loss = 0.4598744363370149\n",
      "Epoch 149: Train Loss = 0.45977674593096196\n",
      "Epoch 150: Train Loss = 0.45964449768481047\n",
      "Recall = 0.9639913232104121, Aging Rate = 0.6280434782608696, Precision = 0.7691242644513673\n",
      "Validation: Test Loss = 0.45922815011895224\n",
      "Recall = 0.9587852494577006, Aging Rate = 0.6156521739130435, precision = 0.780367231638418\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.4696898439794662\n",
      "Recall = 0.9396325459317585, Aging Rate = 0.60625814863103, precision = 0.7698924731182796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170c1d6c1cd94b27bc2a94db8c1c7941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6699499354155167\n",
      "Epoch 2: Train Loss = 0.6620696380864018\n",
      "Epoch 3: Train Loss = 0.65363987321439\n",
      "Epoch 4: Train Loss = 0.6465039154757624\n",
      "Epoch 5: Train Loss = 0.6402157206120699\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5013043478260869\n",
      "Validation: Test Loss = 0.6367645914658256\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5013043478260869\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6342854048894799\n",
      "Epoch 7: Train Loss = 0.6292502978573675\n",
      "Epoch 8: Train Loss = 0.6251831413351971\n",
      "Epoch 9: Train Loss = 0.6216256055624588\n",
      "Epoch 10: Train Loss = 0.6180572857027469\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5013043478260869\n",
      "Validation: Test Loss = 0.6163624112502388\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5013043478260869\n",
      "\n",
      "Epoch 11: Train Loss = 0.6149673059712285\n",
      "Epoch 12: Train Loss = 0.6115428013386933\n",
      "Epoch 13: Train Loss = 0.6078296964064889\n",
      "Epoch 14: Train Loss = 0.6044016596545344\n",
      "Epoch 15: Train Loss = 0.6002550488969554\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5013043478260869\n",
      "Validation: Test Loss = 0.5982244809814121\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5013043478260869\n",
      "\n",
      "Epoch 16: Train Loss = 0.5960381974344668\n",
      "Epoch 17: Train Loss = 0.5918426273180091\n",
      "Epoch 18: Train Loss = 0.5874613773304483\n",
      "Epoch 19: Train Loss = 0.5827562316604282\n",
      "Epoch 20: Train Loss = 0.577870962827102\n",
      "Recall = 0.9978317432784042, Aging Rate = 0.9619565217391305, Precision = 0.52\n",
      "Validation: Test Loss = 0.5753421407160552\n",
      "Recall = 0.9978317432784042, Aging Rate = 0.948695652173913, precision = 0.5272685609532539\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.572869404917178\n",
      "Epoch 22: Train Loss = 0.5681053298452626\n",
      "Epoch 23: Train Loss = 0.5634134415958238\n",
      "Epoch 24: Train Loss = 0.5589239454269409\n",
      "Epoch 25: Train Loss = 0.5546112076095913\n",
      "Recall = 0.9908933217692975, Aging Rate = 0.8465217391304348, Precision = 0.5868002054442732\n",
      "Validation: Test Loss = 0.5523490162517714\n",
      "Recall = 0.9904596704249783, Aging Rate = 0.8369565217391305, precision = 0.5932467532467532\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5503098209007926\n",
      "Epoch 27: Train Loss = 0.5460577540812285\n",
      "Epoch 28: Train Loss = 0.5423283015126767\n",
      "Epoch 29: Train Loss = 0.5388948520370151\n",
      "Epoch 30: Train Loss = 0.535237290651902\n",
      "Recall = 0.9744145706851691, Aging Rate = 0.7715217391304348, Precision = 0.6331360946745562\n",
      "Validation: Test Loss = 0.5337241979267286\n",
      "Recall = 0.9744145706851691, Aging Rate = 0.7693478260869565, precision = 0.634925120090421\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.5319833029871401\n",
      "Epoch 32: Train Loss = 0.5289392333963643\n",
      "Epoch 33: Train Loss = 0.5261796558421591\n",
      "Epoch 34: Train Loss = 0.5237305472208106\n",
      "Epoch 35: Train Loss = 0.5208075954603112\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.7186956521739131, Precision = 0.6702964307320024\n",
      "Validation: Test Loss = 0.5193931699835735\n",
      "Recall = 0.9614050303555941, Aging Rate = 0.7239130434782609, precision = 0.6657657657657657\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.5182936494246773\n",
      "Epoch 37: Train Loss = 0.5162477326393128\n",
      "Epoch 38: Train Loss = 0.5140327856851661\n",
      "Epoch 39: Train Loss = 0.5119358968734741\n",
      "Epoch 40: Train Loss = 0.5101660022528275\n",
      "Recall = 0.9544666088464874, Aging Rate = 0.6947826086956522, Precision = 0.6886733416770964\n",
      "Validation: Test Loss = 0.5089763682821523\n",
      "Recall = 0.9492627927146574, Aging Rate = 0.6893478260869565, precision = 0.6903185115105644\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.5083003312608471\n",
      "Epoch 42: Train Loss = 0.5067752335900846\n",
      "Epoch 43: Train Loss = 0.5049316353383272\n",
      "Epoch 44: Train Loss = 0.5032260463548743\n",
      "Epoch 45: Train Loss = 0.5020573531026425\n",
      "Recall = 0.9483954900260191, Aging Rate = 0.6815217391304348, Precision = 0.6976076555023923\n",
      "Validation: Test Loss = 0.5008633700142736\n",
      "Recall = 0.9440589765828274, Aging Rate = 0.6743478260869565, precision = 0.7018052869116699\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.500573208850363\n",
      "Epoch 47: Train Loss = 0.49913188187972357\n",
      "Epoch 48: Train Loss = 0.4977913795346799\n",
      "Epoch 49: Train Loss = 0.49685916014339615\n",
      "Epoch 50: Train Loss = 0.49532005755797676\n",
      "Recall = 0.9470945359930616, Aging Rate = 0.6745652173913044, Precision = 0.7038349983886562\n",
      "Validation: Test Loss = 0.49459838276324064\n",
      "Recall = 0.9457935819601041, Aging Rate = 0.6715217391304348, precision = 0.706053739074134\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.49448701091434644\n",
      "Epoch 52: Train Loss = 0.49310116151104805\n",
      "Epoch 53: Train Loss = 0.4920579520515774\n",
      "Epoch 54: Train Loss = 0.49120542339656664\n",
      "Epoch 55: Train Loss = 0.49023675141127215\n",
      "Recall = 0.9475281873373808, Aging Rate = 0.667608695652174, Precision = 0.7114946271572777\n",
      "Validation: Test Loss = 0.48948406359423763\n",
      "Recall = 0.9470945359930616, Aging Rate = 0.6623913043478261, precision = 0.7167705940269117\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.48951490982719087\n",
      "Epoch 57: Train Loss = 0.4884240284691686\n",
      "Epoch 58: Train Loss = 0.4873098349571228\n",
      "Epoch 59: Train Loss = 0.48677122976468956\n",
      "Epoch 60: Train Loss = 0.4856320771445399\n",
      "Recall = 0.9540329575021682, Aging Rate = 0.6604347826086957, Precision = 0.7241606319947334\n",
      "Validation: Test Loss = 0.48525807033414425\n",
      "Recall = 0.953599306157849, Aging Rate = 0.6582608695652173, precision = 0.7262219286657859\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.48482587150905443\n",
      "Epoch 62: Train Loss = 0.48412227516588957\n",
      "Epoch 63: Train Loss = 0.4834781021138896\n",
      "Epoch 64: Train Loss = 0.48282012498897053\n",
      "Epoch 65: Train Loss = 0.48222273272016775\n",
      "Recall = 0.9562012142237641, Aging Rate = 0.6547826086956522, Precision = 0.7320717131474104\n",
      "Validation: Test Loss = 0.4815208057216976\n",
      "Recall = 0.9614050303555941, Aging Rate = 0.662608695652174, precision = 0.7273622047244095\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4813269605325616\n",
      "Epoch 67: Train Loss = 0.4807603875450466\n",
      "Epoch 68: Train Loss = 0.4799433269708053\n",
      "Epoch 69: Train Loss = 0.47942668640095254\n",
      "Epoch 70: Train Loss = 0.47903957403224445\n",
      "Recall = 0.9596704249783174, Aging Rate = 0.6541304347826087, Precision = 0.735460285809239\n",
      "Validation: Test Loss = 0.47816568110300145\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.6545652173913044, precision = 0.7359681169046828\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.4785013895449431\n",
      "Epoch 72: Train Loss = 0.4776434840845025\n",
      "Epoch 73: Train Loss = 0.4771632483731145\n",
      "Epoch 74: Train Loss = 0.4764985488290372\n",
      "Epoch 75: Train Loss = 0.47632953109948534\n",
      "Recall = 0.9592367736339983, Aging Rate = 0.6484782608695652, Precision = 0.7415353670801207\n",
      "Validation: Test Loss = 0.4758277543731358\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.6517391304347826, precision = 0.7391594396264176\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.47593051879302317\n",
      "Epoch 77: Train Loss = 0.4749873244762421\n",
      "Epoch 78: Train Loss = 0.47481032708416815\n",
      "Epoch 79: Train Loss = 0.4741135863635851\n",
      "Epoch 80: Train Loss = 0.47364878265754035\n",
      "Recall = 0.9618386816999133, Aging Rate = 0.6497826086956522, Precision = 0.7420541987286718\n",
      "Validation: Test Loss = 0.4732341331502666\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.6417391304347826, precision = 0.7506775067750677\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.473057010329288\n",
      "Epoch 82: Train Loss = 0.47256973028182986\n",
      "Epoch 83: Train Loss = 0.47235012064809384\n",
      "Epoch 84: Train Loss = 0.4715439828064131\n",
      "Epoch 85: Train Loss = 0.47149703300517537\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6502173913043479, Precision = 0.7428953527248412\n",
      "Validation: Test Loss = 0.4710694782630257\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.6430434782608696, precision = 0.7491548343475322\n",
      "\n",
      "Epoch 86: Train Loss = 0.47105597822562506\n",
      "Epoch 87: Train Loss = 0.4707026359827622\n",
      "Epoch 88: Train Loss = 0.4704928942348646\n",
      "Epoch 89: Train Loss = 0.46982365929562114\n",
      "Epoch 90: Train Loss = 0.469551531387412\n",
      "Recall = 0.9605377276669558, Aging Rate = 0.6367391304347826, Precision = 0.7562307954933425\n",
      "Validation: Test Loss = 0.4689550060292949\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.6397826086956522, precision = 0.7529731566428814\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.46919532527094304\n",
      "Epoch 92: Train Loss = 0.4688427063693171\n",
      "Epoch 93: Train Loss = 0.4685756273373314\n",
      "Epoch 94: Train Loss = 0.46813134960506275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train Loss = 0.46776858262393783\n",
      "Recall = 0.9631396357328708, Aging Rate = 0.6404347826086957, Precision = 0.7539035980991174\n",
      "Validation: Test Loss = 0.4676322801216789\n",
      "Recall = 0.9605377276669558, Aging Rate = 0.6315217391304347, precision = 0.7624784853700516\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.4675014359536378\n",
      "Epoch 97: Train Loss = 0.46734453605568926\n",
      "Epoch 98: Train Loss = 0.46686386326084967\n",
      "Epoch 99: Train Loss = 0.4668139465477156\n",
      "Epoch 100: Train Loss = 0.4662969298984693\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6428260869565218, Precision = 0.7514372675008455\n",
      "Validation: Test Loss = 0.46608834520630216\n",
      "Recall = 0.9609713790112749, Aging Rate = 0.6295652173913043, precision = 0.7651933701657458\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4662494316826696\n",
      "Epoch 102: Train Loss = 0.465720503848532\n",
      "Epoch 103: Train Loss = 0.46549002072085505\n",
      "Epoch 104: Train Loss = 0.46512231106343477\n",
      "Epoch 105: Train Loss = 0.4649216683014579\n",
      "Recall = 0.9627059843885516, Aging Rate = 0.6317391304347826, Precision = 0.7639366827253957\n",
      "Validation: Test Loss = 0.46469129977018936\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6358695652173914, precision = 0.7596581196581197\n",
      "\n",
      "Epoch 106: Train Loss = 0.46454148230345355\n",
      "Epoch 107: Train Loss = 0.4644267781921055\n",
      "Epoch 108: Train Loss = 0.46418410321940545\n",
      "Epoch 109: Train Loss = 0.4640044030935868\n",
      "Epoch 110: Train Loss = 0.4638187927266826\n",
      "Recall = 0.9627059843885516, Aging Rate = 0.6308695652173913, Precision = 0.7649896623018608\n",
      "Validation: Test Loss = 0.46353393565053524\n",
      "Recall = 0.9618386816999133, Aging Rate = 0.6269565217391304, precision = 0.7690707350901526\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.463591961135035\n",
      "Epoch 112: Train Loss = 0.46339887598286505\n",
      "Epoch 113: Train Loss = 0.4630292853065159\n",
      "Epoch 114: Train Loss = 0.4630033763076948\n",
      "Epoch 115: Train Loss = 0.46303193190823433\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6367391304347826, Precision = 0.7586206896551724\n",
      "Validation: Test Loss = 0.46239352148512136\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6280434782608696, precision = 0.7691242644513673\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.46261233464531276\n",
      "Epoch 117: Train Loss = 0.46242425763088724\n",
      "Epoch 118: Train Loss = 0.46227397260458575\n",
      "Epoch 119: Train Loss = 0.4618899075881295\n",
      "Epoch 120: Train Loss = 0.46186068301615507\n",
      "Recall = 0.9622723330442324, Aging Rate = 0.6271739130434782, Precision = 0.7691507798960139\n",
      "Validation: Test Loss = 0.461267329869063\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6291304347826087, precision = 0.7677954388389772\n",
      "\n",
      "Epoch 121: Train Loss = 0.4616936329136724\n",
      "Epoch 122: Train Loss = 0.46141949632893436\n",
      "Epoch 123: Train Loss = 0.4613572804824166\n",
      "Epoch 124: Train Loss = 0.4609234504596047\n",
      "Epoch 125: Train Loss = 0.46081130587536356\n",
      "Recall = 0.9640069384215091, Aging Rate = 0.6302173913043478, Precision = 0.7668161434977578\n",
      "Validation: Test Loss = 0.46060644030570985\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6278260869565218, precision = 0.7693905817174516\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.4606329049753106\n",
      "Epoch 127: Train Loss = 0.4606804677714472\n",
      "Epoch 128: Train Loss = 0.46057690382003785\n",
      "Epoch 129: Train Loss = 0.4600359575126482\n",
      "Epoch 130: Train Loss = 0.46013029720472254\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6276086956521739, Precision = 0.7696570834776585\n",
      "Validation: Test Loss = 0.4596514065887617\n",
      "Recall = 0.9605377276669558, Aging Rate = 0.6206521739130435, precision = 0.7758318739054291\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.4601511261774146\n",
      "Epoch 132: Train Loss = 0.45969176380530646\n",
      "Epoch 133: Train Loss = 0.4598901635667552\n",
      "Epoch 134: Train Loss = 0.45931976463483726\n",
      "Epoch 135: Train Loss = 0.45944309903227765\n",
      "Recall = 0.9631396357328708, Aging Rate = 0.6269565217391304, Precision = 0.7701109570041609\n",
      "Validation: Test Loss = 0.4589648955801259\n",
      "Recall = 0.9635732870771899, Aging Rate = 0.6265217391304347, precision = 0.7709923664122137\n",
      "\n",
      "Epoch 136: Train Loss = 0.4592973357698192\n",
      "Epoch 137: Train Loss = 0.4591489251281904\n",
      "Epoch 138: Train Loss = 0.4590009501705999\n",
      "Epoch 139: Train Loss = 0.45881174154903576\n",
      "Epoch 140: Train Loss = 0.4588161877445553\n",
      "Recall = 0.9627059843885516, Aging Rate = 0.6243478260869565, Precision = 0.7729805013927576\n",
      "Validation: Test Loss = 0.45841353421625886\n",
      "Recall = 0.9670424978317432, Aging Rate = 0.6302173913043478, precision = 0.7692307692307693\n",
      "\n",
      "Epoch 141: Train Loss = 0.458709877781246\n",
      "Epoch 142: Train Loss = 0.45842376206232155\n",
      "Epoch 143: Train Loss = 0.4583853920128034\n",
      "Epoch 144: Train Loss = 0.4582576624725176\n",
      "Epoch 145: Train Loss = 0.4583667319753896\n",
      "Recall = 0.9622723330442324, Aging Rate = 0.6217391304347826, Precision = 0.7758741258741259\n",
      "Validation: Test Loss = 0.45781960632490076\n",
      "Recall = 0.9670424978317432, Aging Rate = 0.6295652173913043, precision = 0.7700276243093923\n",
      "\n",
      "Epoch 146: Train Loss = 0.45808580906494806\n",
      "Epoch 147: Train Loss = 0.4580458658156188\n",
      "Epoch 148: Train Loss = 0.45803074266599575\n",
      "Epoch 149: Train Loss = 0.4577270390158114\n",
      "Epoch 150: Train Loss = 0.4576834029736726\n",
      "Recall = 0.9644405897658282, Aging Rate = 0.6243478260869565, Precision = 0.7743732590529248\n",
      "Validation: Test Loss = 0.4573064049948817\n",
      "Recall = 0.9661751951431049, Aging Rate = 0.6254347826086957, precision = 0.7744177963156066\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.4730859896194826\n",
      "Recall = 0.9592641261498029, Aging Rate = 0.6375488917861799, precision = 0.7464212678936605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03eb52f40554c14a9b592892641b594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6729835940443951\n",
      "Epoch 2: Train Loss = 0.6632465374988058\n",
      "Epoch 3: Train Loss = 0.6554800522845724\n",
      "Epoch 4: Train Loss = 0.648709075761878\n",
      "Epoch 5: Train Loss = 0.6425302930500196\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4956521739130435\n",
      "Validation: Test Loss = 0.6396217027954433\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4956521739130435\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6372427517434825\n",
      "Epoch 7: Train Loss = 0.6328547892363175\n",
      "Epoch 8: Train Loss = 0.6286918354034424\n",
      "Epoch 9: Train Loss = 0.6249188762125761\n",
      "Epoch 10: Train Loss = 0.6213001996537914\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4956521739130435\n",
      "Validation: Test Loss = 0.6197946593035822\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4956521739130435\n",
      "\n",
      "Epoch 11: Train Loss = 0.6178895182194917\n",
      "Epoch 12: Train Loss = 0.6144783576675084\n",
      "Epoch 13: Train Loss = 0.6109130060154458\n",
      "Epoch 14: Train Loss = 0.6070876243840093\n",
      "Epoch 15: Train Loss = 0.6028687994376473\n",
      "Recall = 1.0, Aging Rate = 0.9991304347826087, Precision = 0.4960835509138381\n",
      "Validation: Test Loss = 0.6007728099822998\n",
      "Recall = 1.0, Aging Rate = 0.9989130434782608, precision = 0.4961915125136017\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5986292400567428\n",
      "Epoch 17: Train Loss = 0.5940737852842911\n",
      "Epoch 18: Train Loss = 0.5898169402454211\n",
      "Epoch 19: Train Loss = 0.584743159542913\n",
      "Epoch 20: Train Loss = 0.5797382509190103\n",
      "Recall = 0.9947368421052631, Aging Rate = 0.946304347826087, Precision = 0.5210199862164024\n",
      "Validation: Test Loss = 0.5771774414311285\n",
      "Recall = 0.9947368421052631, Aging Rate = 0.9339130434782609, precision = 0.5279329608938548\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.574877103411633\n",
      "Epoch 22: Train Loss = 0.5701714849472046\n",
      "Epoch 23: Train Loss = 0.5652268769430078\n",
      "Epoch 24: Train Loss = 0.5602905646614407\n",
      "Epoch 25: Train Loss = 0.5560549844866214\n",
      "Recall = 0.9824561403508771, Aging Rate = 0.8189130434782609, Precision = 0.5946376426864879\n",
      "Validation: Test Loss = 0.553446934223175\n",
      "Recall = 0.9824561403508771, Aging Rate = 0.8108695652173913, precision = 0.6005361930294906\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5519384353057198\n",
      "Epoch 27: Train Loss = 0.5477318548119586\n",
      "Epoch 28: Train Loss = 0.543823319621708\n",
      "Epoch 29: Train Loss = 0.5401478252203569\n",
      "Epoch 30: Train Loss = 0.5368622023126354\n",
      "Recall = 0.9644736842105263, Aging Rate = 0.7491304347826087, Precision = 0.6381311665699362\n",
      "Validation: Test Loss = 0.5350852236540421\n",
      "Recall = 0.9657894736842105, Aging Rate = 0.7558695652173913, precision = 0.633304572907679\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.5338353688820549\n",
      "Epoch 32: Train Loss = 0.5307844642970873\n",
      "Epoch 33: Train Loss = 0.5278056049346924\n",
      "Epoch 34: Train Loss = 0.5253096717336904\n",
      "Epoch 35: Train Loss = 0.5226593636429828\n",
      "Recall = 0.9508771929824561, Aging Rate = 0.712391304347826, Precision = 0.6615807140677449\n",
      "Validation: Test Loss = 0.5215186735858088\n",
      "Recall = 0.9526315789473684, Aging Rate = 0.7141304347826087, precision = 0.6611872146118721\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.5204854331845823\n",
      "Epoch 37: Train Loss = 0.518218194609103\n",
      "Epoch 38: Train Loss = 0.5161770742872487\n",
      "Epoch 39: Train Loss = 0.5142041114102239\n",
      "Epoch 40: Train Loss = 0.5126161776418271\n",
      "Recall = 0.9486842105263158, Aging Rate = 0.6906521739130435, Precision = 0.6808309726156752\n",
      "Validation: Test Loss = 0.5113489270728567\n",
      "Recall = 0.9482456140350877, Aging Rate = 0.6878260869565217, precision = 0.6833122629582806\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.5105000974821008\n",
      "Epoch 42: Train Loss = 0.509023599313653\n",
      "Epoch 43: Train Loss = 0.5074954134484996\n",
      "Epoch 44: Train Loss = 0.5060933439627938\n",
      "Epoch 45: Train Loss = 0.5046805300401604\n",
      "Recall = 0.9508771929824561, Aging Rate = 0.6873913043478261, Precision = 0.6856419987349779\n",
      "Validation: Test Loss = 0.5036235276512477\n",
      "Recall = 0.9460526315789474, Aging Rate = 0.6754347826086956, precision = 0.6942388155777277\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.5033188540520875\n",
      "Epoch 47: Train Loss = 0.5017525848098423\n",
      "Epoch 48: Train Loss = 0.5003977296663368\n",
      "Epoch 49: Train Loss = 0.4992183721583823\n",
      "Epoch 50: Train Loss = 0.49835912766663926\n",
      "Recall = 0.9486842105263158, Aging Rate = 0.6715217391304348, Precision = 0.7002266105535772\n",
      "Validation: Test Loss = 0.4974879567519478\n",
      "Recall = 0.9469298245614035, Aging Rate = 0.6684782608695652, precision = 0.7021138211382114\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.4971398857365484\n",
      "Epoch 52: Train Loss = 0.49591506823249487\n",
      "Epoch 53: Train Loss = 0.49489708879719607\n",
      "Epoch 54: Train Loss = 0.49393729106239653\n",
      "Epoch 55: Train Loss = 0.49287159494731736\n",
      "Recall = 0.9469298245614035, Aging Rate = 0.6567391304347826, Precision = 0.7146640185369083\n",
      "Validation: Test Loss = 0.4926092497680498\n",
      "Recall = 0.9464912280701754, Aging Rate = 0.6541304347826087, precision = 0.7171817879694251\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.49212446585945463\n",
      "Epoch 57: Train Loss = 0.49121299106141797\n",
      "Epoch 58: Train Loss = 0.49034713164619775\n",
      "Epoch 59: Train Loss = 0.4894997028682543\n",
      "Epoch 60: Train Loss = 0.4887498164176941\n",
      "Recall = 0.9482456140350877, Aging Rate = 0.6541304347826087, Precision = 0.7185111332668661\n",
      "Validation: Test Loss = 0.488187824435856\n",
      "Recall = 0.9526315789473684, Aging Rate = 0.6563043478260869, precision = 0.7194435243458098\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.4881348009213157\n",
      "Epoch 62: Train Loss = 0.48729064956955287\n",
      "Epoch 63: Train Loss = 0.4865281659623851\n",
      "Epoch 64: Train Loss = 0.48573030611743095\n",
      "Epoch 65: Train Loss = 0.4850896285409513\n",
      "Recall = 0.9552631578947368, Aging Rate = 0.6515217391304348, Precision = 0.7267267267267268\n",
      "Validation: Test Loss = 0.48464853245279066\n",
      "Recall = 0.9565789473684211, Aging Rate = 0.6534782608695652, precision = 0.7255489021956087\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.48460625016171\n",
      "Epoch 67: Train Loss = 0.4836785151129184\n",
      "Epoch 68: Train Loss = 0.4829514861625174\n",
      "Epoch 69: Train Loss = 0.482547178009282\n",
      "Epoch 70: Train Loss = 0.4817657296553902\n",
      "Recall = 0.9583333333333334, Aging Rate = 0.6476086956521739, Precision = 0.733467606579389\n",
      "Validation: Test Loss = 0.4811530083158742\n",
      "Recall = 0.9565789473684211, Aging Rate = 0.6460869565217391, precision = 0.7338492597577388\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.4814287941870482\n",
      "Epoch 72: Train Loss = 0.4805294609069824\n",
      "Epoch 73: Train Loss = 0.48033921563107035\n",
      "Epoch 74: Train Loss = 0.47991434366806696\n",
      "Epoch 75: Train Loss = 0.4792025611711585\n",
      "Recall = 0.9587719298245614, Aging Rate = 0.6410869565217391, Precision = 0.7412682265174636\n",
      "Validation: Test Loss = 0.4786129504183064\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6508695652173913, precision = 0.7331329325317302\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.4786524582945782\n",
      "Epoch 77: Train Loss = 0.4781393082763838\n",
      "Epoch 78: Train Loss = 0.4774318644274836\n",
      "Epoch 79: Train Loss = 0.47686540764311086\n",
      "Epoch 80: Train Loss = 0.4768121594449748\n",
      "Recall = 0.9596491228070175, Aging Rate = 0.6373913043478261, Precision = 0.7462482946793997\n",
      "Validation: Test Loss = 0.4761066069810287\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6456521739130435, precision = 0.7390572390572391\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.47612409166667774\n",
      "Epoch 82: Train Loss = 0.4755898278692494\n",
      "Epoch 83: Train Loss = 0.47520661499189293\n",
      "Epoch 84: Train Loss = 0.4752296895566194\n",
      "Epoch 85: Train Loss = 0.47457632645316744\n",
      "Recall = 0.9570175438596491, Aging Rate = 0.6282608695652174, Precision = 0.7550173010380623\n",
      "Validation: Test Loss = 0.473834954759349\n",
      "Recall = 0.9592105263157895, Aging Rate = 0.6315217391304347, precision = 0.7528399311531841\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.4741487888149593\n",
      "Epoch 87: Train Loss = 0.47371249344037925\n",
      "Epoch 88: Train Loss = 0.4732431415889574\n",
      "Epoch 89: Train Loss = 0.4731795052341793\n",
      "Epoch 90: Train Loss = 0.4723490089437236\n",
      "Recall = 0.9618421052631579, Aging Rate = 0.6363043478260869, Precision = 0.7492312948411343\n",
      "Validation: Test Loss = 0.4720991212388744\n",
      "Recall = 0.962280701754386, Aging Rate = 0.6334782608695653, precision = 0.7529169526424159\n",
      "Model in epoch 90 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss = 0.4719627388663914\n",
      "Epoch 92: Train Loss = 0.47197003882864247\n",
      "Epoch 93: Train Loss = 0.47150910004325536\n",
      "Epoch 94: Train Loss = 0.4710847353935242\n",
      "Epoch 95: Train Loss = 0.47077904032624285\n",
      "Recall = 0.962280701754386, Aging Rate = 0.6319565217391304, Precision = 0.7547299621603027\n",
      "Validation: Test Loss = 0.47041253369787467\n",
      "Recall = 0.962280701754386, Aging Rate = 0.633695652173913, precision = 0.7526586620926243\n",
      "\n",
      "Epoch 96: Train Loss = 0.4704430112631425\n",
      "Epoch 97: Train Loss = 0.47018348533174265\n",
      "Epoch 98: Train Loss = 0.46986949174300485\n",
      "Epoch 99: Train Loss = 0.4694955495129461\n",
      "Epoch 100: Train Loss = 0.469353388392407\n",
      "Recall = 0.962280701754386, Aging Rate = 0.633695652173913, Precision = 0.7526586620926243\n",
      "Validation: Test Loss = 0.46854703224223593\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.6258695652173913, precision = 0.7613754775963877\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4689647508185843\n",
      "Epoch 102: Train Loss = 0.4686441441204237\n",
      "Epoch 103: Train Loss = 0.468328188813251\n",
      "Epoch 104: Train Loss = 0.46817973857340606\n",
      "Epoch 105: Train Loss = 0.46779850436293563\n",
      "Recall = 0.9609649122807018, Aging Rate = 0.6243478260869565, Precision = 0.762883008356546\n",
      "Validation: Test Loss = 0.4675415383732837\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.6284782608695653, precision = 0.7582151504669664\n",
      "\n",
      "Epoch 106: Train Loss = 0.46751456488733706\n",
      "Epoch 107: Train Loss = 0.46735509768776273\n",
      "Epoch 108: Train Loss = 0.4671182680648306\n",
      "Epoch 109: Train Loss = 0.46685233442679697\n",
      "Epoch 110: Train Loss = 0.46661774967027747\n",
      "Recall = 0.9583333333333334, Aging Rate = 0.6217391304347826, Precision = 0.763986013986014\n",
      "Validation: Test Loss = 0.4662162412767825\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.6197826086956522, precision = 0.7688530340231498\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.4662707927952642\n",
      "Epoch 112: Train Loss = 0.46611083668211234\n",
      "Epoch 113: Train Loss = 0.46604694283526876\n",
      "Epoch 114: Train Loss = 0.46590380482051685\n",
      "Epoch 115: Train Loss = 0.4657054983532947\n",
      "Recall = 0.962280701754386, Aging Rate = 0.6265217391304347, Precision = 0.7612768910478834\n",
      "Validation: Test Loss = 0.46535997976427496\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.6186956521739131, precision = 0.7702037947997189\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.46537694759990855\n",
      "Epoch 117: Train Loss = 0.4649691471327906\n",
      "Epoch 118: Train Loss = 0.465206015991128\n",
      "Epoch 119: Train Loss = 0.464862038726392\n",
      "Epoch 120: Train Loss = 0.4648399478456248\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.6186956521739131, Precision = 0.7702037947997189\n",
      "Validation: Test Loss = 0.46427348738131313\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.6178260869565217, precision = 0.7712878254750176\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.4643344246822855\n",
      "Epoch 122: Train Loss = 0.46417943757513297\n",
      "Epoch 123: Train Loss = 0.46393538578696875\n",
      "Epoch 124: Train Loss = 0.4641166623260664\n",
      "Epoch 125: Train Loss = 0.4635869792751644\n",
      "Recall = 0.9614035087719298, Aging Rate = 0.621304347826087, Precision = 0.7669699090272918\n",
      "Validation: Test Loss = 0.46321912978006446\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.621304347826087, precision = 0.7680195941217635\n",
      "\n",
      "Epoch 126: Train Loss = 0.4634129457370095\n",
      "Epoch 127: Train Loss = 0.4630354168104089\n",
      "Epoch 128: Train Loss = 0.46319573402404784\n",
      "Epoch 129: Train Loss = 0.4628452973780425\n",
      "Epoch 130: Train Loss = 0.46266501623651257\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6186956521739131, Precision = 0.7712579058327477\n",
      "Validation: Test Loss = 0.4623523833958999\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6197826086956522, precision = 0.7699052963872326\n",
      "\n",
      "Epoch 131: Train Loss = 0.46264539811922156\n",
      "Epoch 132: Train Loss = 0.46250162544457807\n",
      "Epoch 133: Train Loss = 0.4621158436588619\n",
      "Epoch 134: Train Loss = 0.4622136450332144\n",
      "Epoch 135: Train Loss = 0.46200598364290985\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6132608695652174, Precision = 0.7780928748670685\n",
      "Validation: Test Loss = 0.4617798794870791\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6145652173913043, precision = 0.7764414573753096\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.4619189858436584\n",
      "Epoch 137: Train Loss = 0.4616570996719858\n",
      "Epoch 138: Train Loss = 0.46155239722003105\n",
      "Epoch 139: Train Loss = 0.46161487765934156\n",
      "Epoch 140: Train Loss = 0.4614871748634007\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6156521739130435, Precision = 0.7750706214689266\n",
      "Validation: Test Loss = 0.4610686390814574\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6167391304347826, precision = 0.773704617553754\n",
      "\n",
      "Epoch 141: Train Loss = 0.4612126169515693\n",
      "Epoch 142: Train Loss = 0.4614477542172308\n",
      "Epoch 143: Train Loss = 0.46098889511564506\n",
      "Epoch 144: Train Loss = 0.4609626420684483\n",
      "Epoch 145: Train Loss = 0.46080803912618884\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6097826086956522, Precision = 0.7825311942959001\n",
      "Validation: Test Loss = 0.46041035595147506\n",
      "Recall = 0.9644736842105263, Aging Rate = 0.6197826086956522, precision = 0.7713083128726762\n",
      "\n",
      "Epoch 146: Train Loss = 0.4609223757101142\n",
      "Epoch 147: Train Loss = 0.4607870110221531\n",
      "Epoch 148: Train Loss = 0.4606383995387865\n",
      "Epoch 149: Train Loss = 0.46030881202739216\n",
      "Epoch 150: Train Loss = 0.4604274633656377\n",
      "Recall = 0.9627192982456141, Aging Rate = 0.6097826086956522, Precision = 0.7825311942959001\n",
      "Validation: Test Loss = 0.45994785801224086\n",
      "Recall = 0.9644736842105263, Aging Rate = 0.6152173913043478, precision = 0.7770318021201413\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.4592539357237586\n",
      "Recall = 0.9580686149936467, Aging Rate = 0.6310299869621904, precision = 0.7789256198347108\n",
      "\u001b[32m[I 2022-05-26 16:15:08,472]\u001b[0m Trial 2 finished with value: 0.8483859746271095 and parameters: {'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 0.01, 'bad_weight': 0.7}. Best is trial 2 with value: 0.8483859746271095.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02c1ce9f80c48ce9f950f30c36f7fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6492003305062004\n",
      "Epoch 2: Train Loss = 0.5939771069651064\n",
      "Epoch 3: Train Loss = 0.5587947500270346\n",
      "Epoch 4: Train Loss = 0.5437612348017485\n",
      "Epoch 5: Train Loss = 0.534895005951757\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5317060381951539\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5300867777285369\n",
      "Epoch 7: Train Loss = 0.5269957603060681\n",
      "Epoch 8: Train Loss = 0.5257455112104831\n",
      "Epoch 9: Train Loss = 0.5237733059344084\n",
      "Epoch 10: Train Loss = 0.5234263944625854\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5230480632574662\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 11: Train Loss = 0.5218195886197298\n",
      "Epoch 12: Train Loss = 0.5223848684974338\n",
      "Epoch 13: Train Loss = 0.5207964602760646\n",
      "Epoch 14: Train Loss = 0.5219921722619429\n",
      "Epoch 15: Train Loss = 0.521815722403319\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5219751034612241\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 16: Train Loss = 0.5233705323675405\n",
      "Epoch 17: Train Loss = 0.5220836216470469\n",
      "Epoch 18: Train Loss = 0.5231413321391396\n",
      "Epoch 19: Train Loss = 0.5216636718874392\n",
      "Epoch 20: Train Loss = 0.5229600435754527\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5233039932147316\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 21: Train Loss = 0.5226056348759195\n",
      "Epoch 22: Train Loss = 0.5224345907957657\n",
      "Epoch 23: Train Loss = 0.5208865770049718\n",
      "Epoch 24: Train Loss = 0.5216800480303557\n",
      "Epoch 25: Train Loss = 0.5212537552999413\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5232246131482332\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 26: Train Loss = 0.5212104543395665\n",
      "Epoch 27: Train Loss = 0.5227401798704396\n",
      "Epoch 28: Train Loss = 0.5202760142347087\n",
      "Epoch 29: Train Loss = 0.5215237006933793\n",
      "Epoch 30: Train Loss = 0.523179347826087\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5212000293317048\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 31: Train Loss = 0.5215384846148283\n",
      "Epoch 32: Train Loss = 0.5218088395699211\n",
      "Epoch 33: Train Loss = 0.5218567567804585\n",
      "Epoch 34: Train Loss = 0.5216611906756525\n",
      "Epoch 35: Train Loss = 0.5199433579652206\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5215026765284331\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 36: Train Loss = 0.5221912608457648\n",
      "Epoch 37: Train Loss = 0.5220725175608759\n",
      "Epoch 38: Train Loss = 0.521100377621858\n",
      "Epoch 39: Train Loss = 0.5219150170554285\n",
      "Epoch 40: Train Loss = 0.5226284174297167\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5242759323120117\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 41: Train Loss = 0.5223684836470562\n",
      "Epoch 42: Train Loss = 0.5199244716893072\n",
      "Epoch 43: Train Loss = 0.5200701598499132\n",
      "Epoch 44: Train Loss = 0.5225710737186929\n",
      "Epoch 45: Train Loss = 0.5234199558133664\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5217819038681362\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 46: Train Loss = 0.5222461127198261\n",
      "Epoch 47: Train Loss = 0.5221480061696924\n",
      "Epoch 48: Train Loss = 0.5219393694919089\n",
      "Epoch 49: Train Loss = 0.52256745773813\n",
      "Epoch 50: Train Loss = 0.5212395078721254\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5219189127631809\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Epoch 51: Train Loss = 0.522490292839382\n",
      "Epoch 52: Train Loss = 0.5205611453885618\n",
      "Epoch 53: Train Loss = 0.5236704595192619\n",
      "Epoch 54: Train Loss = 0.5227004456001779\n",
      "Epoch 55: Train Loss = 0.5215134708777718\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49717391304347824\n",
      "Validation: Test Loss = 0.5200209957102071\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49717391304347824\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5148005872537447\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5084745762711864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5e463572274af69a12b9b806b1cc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6559657601688219\n",
      "Epoch 2: Train Loss = 0.6072335625731426\n",
      "Epoch 3: Train Loss = 0.5699680145927097\n",
      "Epoch 4: Train Loss = 0.550119687370632\n",
      "Epoch 5: Train Loss = 0.5391038646905318\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5374706142881642\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5344803406362948\n",
      "Epoch 7: Train Loss = 0.5301551471067512\n",
      "Epoch 8: Train Loss = 0.528281627115996\n",
      "Epoch 9: Train Loss = 0.5264557358492976\n",
      "Epoch 10: Train Loss = 0.5259816705662271\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5263261580467224\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 11: Train Loss = 0.5257185048642365\n",
      "Epoch 12: Train Loss = 0.5242020387753197\n",
      "Epoch 13: Train Loss = 0.5244980802743331\n",
      "Epoch 14: Train Loss = 0.524431163850038\n",
      "Epoch 15: Train Loss = 0.5229999549492546\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5240868283873019\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 16: Train Loss = 0.5238523131868114\n",
      "Epoch 17: Train Loss = 0.5238695728260537\n",
      "Epoch 18: Train Loss = 0.5248893251626388\n",
      "Epoch 19: Train Loss = 0.5245682768199755\n",
      "Epoch 20: Train Loss = 0.5250666337427886\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5239533575721409\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 21: Train Loss = 0.5229016308162523\n",
      "Epoch 22: Train Loss = 0.5242489568565203\n",
      "Epoch 23: Train Loss = 0.5234627988545791\n",
      "Epoch 24: Train Loss = 0.523343490621318\n",
      "Epoch 25: Train Loss = 0.5217074833745542\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5243329790364141\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 26: Train Loss = 0.5240518000333205\n",
      "Epoch 27: Train Loss = 0.5236150308277296\n",
      "Epoch 28: Train Loss = 0.5240175727139349\n",
      "Epoch 29: Train Loss = 0.5228564632457235\n",
      "Epoch 30: Train Loss = 0.5242951037572777\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5229898689104163\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 31: Train Loss = 0.5235092997032663\n",
      "Epoch 32: Train Loss = 0.5223362365494604\n",
      "Epoch 33: Train Loss = 0.5226606940186542\n",
      "Epoch 34: Train Loss = 0.5232606341527856\n",
      "Epoch 35: Train Loss = 0.5232233278647713\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5233291914152063\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 36: Train Loss = 0.5231511887260105\n",
      "Epoch 37: Train Loss = 0.5238495197503463\n",
      "Epoch 38: Train Loss = 0.523314465439838\n",
      "Epoch 39: Train Loss = 0.5225561321300008\n",
      "Epoch 40: Train Loss = 0.522623228819474\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5233873256393101\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 41: Train Loss = 0.522757524303768\n",
      "Epoch 42: Train Loss = 0.5229794134264407\n",
      "Epoch 43: Train Loss = 0.5227092060835465\n",
      "Epoch 44: Train Loss = 0.5239367740568908\n",
      "Epoch 45: Train Loss = 0.5221485098548557\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5249449066493822\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 46: Train Loss = 0.5227447837332021\n",
      "Epoch 47: Train Loss = 0.5235617741294529\n",
      "Epoch 48: Train Loss = 0.5236072893246361\n",
      "Epoch 49: Train Loss = 0.5238690457136734\n",
      "Epoch 50: Train Loss = 0.5242084294816722\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.5232448418244071\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Epoch 51: Train Loss = 0.5228724268208379\n",
      "Epoch 52: Train Loss = 0.5229780367146367\n",
      "Epoch 53: Train Loss = 0.5227687407058218\n",
      "Epoch 54: Train Loss = 0.5232732537518376\n",
      "Epoch 55: Train Loss = 0.522268284196439\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4945652173913043\n",
      "Validation: Test Loss = 0.52318850538005\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4945652173913043\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5115187757173759\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.516297262059974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a463cb339094f119bc83dea9ec8fe0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6699195438882579\n",
      "Epoch 2: Train Loss = 0.6004047488129657\n",
      "Epoch 3: Train Loss = 0.5576239305993785\n",
      "Epoch 4: Train Loss = 0.538037271551464\n",
      "Epoch 5: Train Loss = 0.5313027510435685\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5279335893237073\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5250719484557276\n",
      "Epoch 7: Train Loss = 0.5227385505904322\n",
      "Epoch 8: Train Loss = 0.5209945619106293\n",
      "Epoch 9: Train Loss = 0.5204400453360184\n",
      "Epoch 10: Train Loss = 0.5194601426953854\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5199212048364722\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 11: Train Loss = 0.5200306060003198\n",
      "Epoch 12: Train Loss = 0.519678911022518\n",
      "Epoch 13: Train Loss = 0.5200131405954775\n",
      "Epoch 14: Train Loss = 0.5178768002468607\n",
      "Epoch 15: Train Loss = 0.519677905621736\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5183240106313125\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 16: Train Loss = 0.5178729862752168\n",
      "Epoch 17: Train Loss = 0.5191094634843909\n",
      "Epoch 18: Train Loss = 0.5197260144482488\n",
      "Epoch 19: Train Loss = 0.5195375509884046\n",
      "Epoch 20: Train Loss = 0.5191837337742681\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5194160702954168\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 21: Train Loss = 0.5178081970629484\n",
      "Epoch 22: Train Loss = 0.5177573953504148\n",
      "Epoch 23: Train Loss = 0.5188218560426131\n",
      "Epoch 24: Train Loss = 0.5181916680543319\n",
      "Epoch 25: Train Loss = 0.5193573072163955\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5183452381258425\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 26: Train Loss = 0.5175653027969858\n",
      "Epoch 27: Train Loss = 0.5192511229929717\n",
      "Epoch 28: Train Loss = 0.5191590222586756\n",
      "Epoch 29: Train Loss = 0.5185972091944321\n",
      "Epoch 30: Train Loss = 0.5180092720881753\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5193324284968169\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 31: Train Loss = 0.5189589413352634\n",
      "Epoch 32: Train Loss = 0.5172672950703164\n",
      "Epoch 33: Train Loss = 0.5185863910032356\n",
      "Epoch 34: Train Loss = 0.5183101692407027\n",
      "Epoch 35: Train Loss = 0.5181401086890179\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.517182121069535\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 36: Train Loss = 0.5193494083570397\n",
      "Epoch 37: Train Loss = 0.5181782267404639\n",
      "Epoch 38: Train Loss = 0.5205453223767488\n",
      "Epoch 39: Train Loss = 0.5170068908256034\n",
      "Epoch 40: Train Loss = 0.5186952343194381\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5185713330559109\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 41: Train Loss = 0.5186665859948034\n",
      "Epoch 42: Train Loss = 0.5176939429407534\n",
      "Epoch 43: Train Loss = 0.5180913348819899\n",
      "Epoch 44: Train Loss = 0.5186314623252205\n",
      "Epoch 45: Train Loss = 0.5176079611674599\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.518227363721184\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 46: Train Loss = 0.5180436301231385\n",
      "Epoch 47: Train Loss = 0.5174765716428342\n",
      "Epoch 48: Train Loss = 0.5175556660735089\n",
      "Epoch 49: Train Loss = 0.5183012035100356\n",
      "Epoch 50: Train Loss = 0.5172349342055943\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5178820984778197\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Epoch 51: Train Loss = 0.5182624190268309\n",
      "Epoch 52: Train Loss = 0.5175831970961198\n",
      "Epoch 53: Train Loss = 0.5186959925941799\n",
      "Epoch 54: Train Loss = 0.5199518449410149\n",
      "Epoch 55: Train Loss = 0.5193398357474286\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5021739130434782\n",
      "Validation: Test Loss = 0.5178943402870841\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5021739130434782\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5211081840535031\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934810951760104\n",
      "\u001b[32m[I 2022-05-26 16:15:38,866]\u001b[0m Trial 3 finished with value: 0.672000505209693 and parameters: {'batch_size': 32, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'bad_weight': 0.8}. Best is trial 2 with value: 0.8483859746271095.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8624247ee53a41bfb83502898f56f436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6309111812840338\n",
      "Epoch 2: Train Loss = 0.5126911337479301\n",
      "Epoch 3: Train Loss = 0.46853741381479347\n",
      "Epoch 4: Train Loss = 0.44106972564821656\n",
      "Epoch 5: Train Loss = 0.417484267950058\n",
      "Recall = 0.945840554592721, Aging Rate = 0.5402173913043479, Precision = 0.8784708249496982\n",
      "Validation: Test Loss = 0.4012166024290997\n",
      "Recall = 0.9519064124783362, Aging Rate = 0.5243478260869565, precision = 0.9108623548922057\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3940229581749958\n",
      "Epoch 7: Train Loss = 0.378127335206322\n",
      "Epoch 8: Train Loss = 0.3669206877895024\n",
      "Epoch 9: Train Loss = 0.3581369931801506\n",
      "Epoch 10: Train Loss = 0.3508277801327083\n",
      "Recall = 0.9805025996533796, Aging Rate = 0.5063043478260869, Precision = 0.9716616573636754\n",
      "Validation: Test Loss = 0.34531310869299847\n",
      "Recall = 0.9835355285961872, Aging Rate = 0.5028260869565218, precision = 0.9814094249891915\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.34495385579440907\n",
      "Epoch 12: Train Loss = 0.34021833160649173\n",
      "Epoch 13: Train Loss = 0.33663418868313666\n",
      "Epoch 14: Train Loss = 0.3340655281751052\n",
      "Epoch 15: Train Loss = 0.3317684485083041\n",
      "Recall = 0.9896013864818024, Aging Rate = 0.5010869565217392, Precision = 0.9908893709327549\n",
      "Validation: Test Loss = 0.33140691648358883\n",
      "Recall = 0.9896013864818024, Aging Rate = 0.4989130434782609, precision = 0.9952069716775599\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3304321554951046\n",
      "Epoch 17: Train Loss = 0.32886512398719786\n",
      "Epoch 18: Train Loss = 0.3276864561827286\n",
      "Epoch 19: Train Loss = 0.32689218904661094\n",
      "Epoch 20: Train Loss = 0.32614126547523165\n",
      "Recall = 0.9900346620450606, Aging Rate = 0.49826086956521737, Precision = 0.9969458987783595\n",
      "Validation: Test Loss = 0.3249212801974753\n",
      "Recall = 0.9900346620450606, Aging Rate = 0.4980434782608696, precision = 0.9973810563072893\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3253360332095105\n",
      "Epoch 22: Train Loss = 0.3246472232756407\n",
      "Epoch 23: Train Loss = 0.3243385263629582\n",
      "Epoch 24: Train Loss = 0.3238517289576323\n",
      "Epoch 25: Train Loss = 0.3234708458962648\n",
      "Recall = 0.9900346620450606, Aging Rate = 0.4976086956521739, Precision = 0.9982525120139799\n",
      "Validation: Test Loss = 0.3230570518452188\n",
      "Recall = 0.9900346620450606, Aging Rate = 0.4973913043478261, precision = 0.9986888111888111\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.32311878188796667\n",
      "Epoch 27: Train Loss = 0.32310140583826147\n",
      "Epoch 28: Train Loss = 0.32263875810996345\n",
      "Epoch 29: Train Loss = 0.32266917197600653\n",
      "Epoch 30: Train Loss = 0.3222544641598411\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, Precision = 0.9991258741258742\n",
      "Validation: Test Loss = 0.3218738199835238\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, precision = 0.9991258741258742\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.32203014150909753\n",
      "Epoch 32: Train Loss = 0.32194353321324226\n",
      "Epoch 33: Train Loss = 0.32173048973083496\n",
      "Epoch 34: Train Loss = 0.3215412093763766\n",
      "Epoch 35: Train Loss = 0.3214975750964621\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, Precision = 0.9991258741258742\n",
      "Validation: Test Loss = 0.32117949097052867\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, precision = 0.9991258741258742\n",
      "\n",
      "Epoch 36: Train Loss = 0.3214660704135895\n",
      "Epoch 37: Train Loss = 0.3214320510366689\n",
      "Epoch 38: Train Loss = 0.3213011213489201\n",
      "Epoch 39: Train Loss = 0.32142860365950543\n",
      "Epoch 40: Train Loss = 0.32209535946016726\n",
      "Recall = 0.9900346620450606, Aging Rate = 0.4973913043478261, Precision = 0.9986888111888111\n",
      "Validation: Test Loss = 0.3210775808666063\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, precision = 0.9991258741258742\n",
      "\n",
      "Epoch 41: Train Loss = 0.32113364743149797\n",
      "Epoch 42: Train Loss = 0.3210882351191148\n",
      "Epoch 43: Train Loss = 0.32109699358110844\n",
      "Epoch 44: Train Loss = 0.32112072644026385\n",
      "Epoch 45: Train Loss = 0.3211212700864543\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, Precision = 0.9991258741258742\n",
      "Validation: Test Loss = 0.32078503292539845\n",
      "Recall = 0.9904679376083189, Aging Rate = 0.4973913043478261, precision = 0.9991258741258742\n",
      "\n",
      "Epoch 46: Train Loss = 0.32117723537528\n",
      "Epoch 47: Train Loss = 0.3210310595450194\n",
      "Epoch 48: Train Loss = 0.3211453888727271\n",
      "Epoch 49: Train Loss = 0.3208986469455387\n",
      "Epoch 50: Train Loss = 0.32106962095136227\n",
      "Recall = 0.9909012131715771, Aging Rate = 0.4973913043478261, Precision = 0.9995629370629371\n",
      "Validation: Test Loss = 0.32087187606355416\n",
      "Recall = 0.9909012131715771, Aging Rate = 0.4973913043478261, precision = 0.9995629370629371\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3206703247194705\n",
      "Epoch 52: Train Loss = 0.3205175895794578\n",
      "Epoch 53: Train Loss = 0.3206131477459617\n",
      "Epoch 54: Train Loss = 0.3207589302373969\n",
      "Epoch 55: Train Loss = 0.3205765642808831\n",
      "Recall = 0.9909012131715771, Aging Rate = 0.4973913043478261, Precision = 0.9995629370629371\n",
      "Validation: Test Loss = 0.3207483765871628\n",
      "Recall = 0.9909012131715771, Aging Rate = 0.4973913043478261, precision = 0.9995629370629371\n",
      "\n",
      "Epoch 56: Train Loss = 0.32061469534169074\n",
      "Epoch 57: Train Loss = 0.3204632081156192\n",
      "Epoch 58: Train Loss = 0.3209697196794593\n",
      "Epoch 59: Train Loss = 0.3187509720740111\n",
      "Epoch 60: Train Loss = 0.3188503713193147\n",
      "Recall = 0.9939341421143848, Aging Rate = 0.4989130434782609, Precision = 0.9995642701525055\n",
      "Validation: Test Loss = 0.3189627962527068\n",
      "Recall = 0.9939341421143848, Aging Rate = 0.49869565217391304, precision = 1.0\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.31860268919364265\n",
      "Epoch 62: Train Loss = 0.3185601094494695\n",
      "Epoch 63: Train Loss = 0.31848962389904517\n",
      "Epoch 64: Train Loss = 0.3185617736111517\n",
      "Epoch 65: Train Loss = 0.318669506881548\n",
      "Recall = 0.9939341421143848, Aging Rate = 0.49869565217391304, Precision = 0\n",
      "Validation: Test Loss = 0.3184137309115866\n",
      "Recall = 0.9939341421143848, Aging Rate = 0.49869565217391304, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.3186688025619673\n",
      "Epoch 67: Train Loss = 0.3187462803591853\n",
      "Epoch 68: Train Loss = 0.3186130634079809\n",
      "Epoch 69: Train Loss = 0.3186117657371189\n",
      "Epoch 70: Train Loss = 0.31886692752008855\n",
      "Recall = 0.9939341421143848, Aging Rate = 0.49869565217391304, Precision = 0\n",
      "Validation: Test Loss = 0.3182993047133736\n",
      "Recall = 0.9939341421143848, Aging Rate = 0.49869565217391304, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.31868172671483913\n",
      "Epoch 72: Train Loss = 0.31894594778185303\n",
      "Epoch 73: Train Loss = 0.31820858074271163\n",
      "Epoch 74: Train Loss = 0.31745292839796646\n",
      "Epoch 75: Train Loss = 0.31724219088969025\n",
      "Recall = 0.9956672443674177, Aging Rate = 0.49956521739130433, Precision = 0\n",
      "Validation: Test Loss = 0.3172096247776695\n",
      "Recall = 0.9956672443674177, Aging Rate = 0.49956521739130433, precision = 1.0\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.3173730266612509\n",
      "Epoch 77: Train Loss = 0.31759808991266336\n",
      "Epoch 78: Train Loss = 0.31786150600599206\n",
      "Epoch 79: Train Loss = 0.3173378279416457\n",
      "Epoch 80: Train Loss = 0.3173471731724946\n",
      "Recall = 0.9961005199306759, Aging Rate = 0.49978260869565216, Precision = 0\n",
      "Validation: Test Loss = 0.3168008237299712\n",
      "Recall = 0.9965337954939342, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.3171313516989998\n",
      "Epoch 82: Train Loss = 0.31708005226176716\n",
      "Epoch 83: Train Loss = 0.31695542998935866\n",
      "Epoch 84: Train Loss = 0.3169136611793352\n",
      "Epoch 85: Train Loss = 0.3169804290584896\n",
      "Recall = 0.9965337954939342, Aging Rate = 0.5, Precision = 0\n",
      "Validation: Test Loss = 0.3167220163345337\n",
      "Recall = 0.9965337954939342, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.31689520711484165\n",
      "Epoch 87: Train Loss = 0.3172419772459113\n",
      "Epoch 88: Train Loss = 0.31760900077612503\n",
      "Epoch 89: Train Loss = 0.31681597455688143\n",
      "Epoch 90: Train Loss = 0.3166142934301625\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.3161720189322596\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.31635318367377574\n",
      "Epoch 92: Train Loss = 0.3163677956228671\n",
      "Epoch 93: Train Loss = 0.3164083312905353\n",
      "Epoch 94: Train Loss = 0.31638603200083193\n",
      "Epoch 95: Train Loss = 0.3163790067382481\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.3167559287859046\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: Train Loss = 0.3167120501787766\n",
      "Epoch 97: Train Loss = 0.31645016001618426\n",
      "Epoch 98: Train Loss = 0.3165347669435584\n",
      "Epoch 99: Train Loss = 0.3165637579689855\n",
      "Epoch 100: Train Loss = 0.31644554262575897\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.31680548745652903\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.31685499320859495\n",
      "Epoch 102: Train Loss = 0.31679699135863265\n",
      "Epoch 103: Train Loss = 0.31635004826214\n",
      "Epoch 104: Train Loss = 0.3163452020417089\n",
      "Epoch 105: Train Loss = 0.3165020199962284\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.3163771356707034\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5006521739130435, precision = 0.9995657837603127\n",
      "\n",
      "Epoch 106: Train Loss = 0.31654757385668547\n",
      "Epoch 107: Train Loss = 0.31753612704899\n",
      "Epoch 108: Train Loss = 0.3163514084919639\n",
      "Epoch 109: Train Loss = 0.31639406883198284\n",
      "Epoch 110: Train Loss = 0.31643333455790645\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.31627769397652666\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.31626081119412963\n",
      "Epoch 112: Train Loss = 0.3163097533972367\n",
      "Epoch 113: Train Loss = 0.3164322890924371\n",
      "Epoch 114: Train Loss = 0.3163834422567616\n",
      "Epoch 115: Train Loss = 0.3164910151647485\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.31638831713925236\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.3167971494923467\n",
      "Epoch 117: Train Loss = 0.3163477299006089\n",
      "Epoch 118: Train Loss = 0.3163547595687535\n",
      "Epoch 119: Train Loss = 0.3165716229832691\n",
      "Epoch 120: Train Loss = 0.3164289539274962\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.31630544444789055\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.31634232158246245\n",
      "Epoch 122: Train Loss = 0.31645833056906\n",
      "Epoch 123: Train Loss = 0.316567296774491\n",
      "Epoch 124: Train Loss = 0.31660513546155844\n",
      "Epoch 125: Train Loss = 0.31656615832577584\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.31624157128126723\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.31663779891055566\n",
      "Epoch 127: Train Loss = 0.31638968498810477\n",
      "Epoch 128: Train Loss = 0.31660917028136876\n",
      "Epoch 129: Train Loss = 0.31634285963099934\n",
      "Epoch 130: Train Loss = 0.31636458008185675\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.31622414376424707\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 131: Train Loss = 0.31638589449550797\n",
      "Epoch 132: Train Loss = 0.31649777163629944\n",
      "Epoch 133: Train Loss = 0.3164909712646318\n",
      "Epoch 134: Train Loss = 0.316398170719976\n",
      "Epoch 135: Train Loss = 0.3164187086146811\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.3162565773466359\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, precision = 1.0\n",
      "\n",
      "Epoch 136: Train Loss = 0.3167957803995713\n",
      "Epoch 137: Train Loss = 0.3169118779638539\n",
      "Epoch 138: Train Loss = 0.31660621586053267\n",
      "Epoch 139: Train Loss = 0.31621496143548383\n",
      "Epoch 140: Train Loss = 0.31661768255026446\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5004347826086957, Precision = 0\n",
      "Validation: Test Loss = 0.317181586286296\n",
      "Recall = 0.9974003466204506, Aging Rate = 0.5006521739130435, precision = 0.9995657837603127\n",
      "\n",
      "Training Finished at epoch 140.\n",
      "Validation: Test Loss = 0.3285251521691194\n",
      "Recall = 0.9868247694334651, Aging Rate = 0.49674054758800523, precision = 0.9829396325459318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca478d867c0348f3b56f1a426eaa7dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6275943464818208\n",
      "Epoch 2: Train Loss = 0.5090569707103397\n",
      "Epoch 3: Train Loss = 0.4652357115952865\n",
      "Epoch 4: Train Loss = 0.43652921396753064\n",
      "Epoch 5: Train Loss = 0.4171884881931803\n",
      "Recall = 0.9490418118466899, Aging Rate = 0.5358695652173913, Precision = 0.883975659229209\n",
      "Validation: Test Loss = 0.39882459707882095\n",
      "Recall = 0.9634146341463414, Aging Rate = 0.5313043478260869, precision = 0.9050736497545008\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3929485875627269\n",
      "Epoch 7: Train Loss = 0.3772420811134836\n",
      "Epoch 8: Train Loss = 0.36507418767265654\n",
      "Epoch 9: Train Loss = 0.3565667914307636\n",
      "Epoch 10: Train Loss = 0.3501523060902305\n",
      "Recall = 0.9878048780487805, Aging Rate = 0.5121739130434783, Precision = 0.9626485568760611\n",
      "Validation: Test Loss = 0.3446261682199395\n",
      "Recall = 0.9899825783972126, Aging Rate = 0.5076086956521739, precision = 0.9734475374732334\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.34405091607052346\n",
      "Epoch 12: Train Loss = 0.3398748655422874\n",
      "Epoch 13: Train Loss = 0.3367799917511318\n",
      "Epoch 14: Train Loss = 0.3342348210707955\n",
      "Epoch 15: Train Loss = 0.3321522962528726\n",
      "Recall = 0.9930313588850174, Aging Rate = 0.5028260869565218, Precision = 0.9857328145265889\n",
      "Validation: Test Loss = 0.33002656848534295\n",
      "Recall = 0.9930313588850174, Aging Rate = 0.5013043478260869, precision = 0.9887250650477016\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.33040450065032295\n",
      "Epoch 17: Train Loss = 0.3282864929800448\n",
      "Epoch 18: Train Loss = 0.32714923195216966\n",
      "Epoch 19: Train Loss = 0.325600664563801\n",
      "Epoch 20: Train Loss = 0.32448676632798235\n",
      "Recall = 0.9947735191637631, Aging Rate = 0.5, Precision = 0.9930434782608696\n",
      "Validation: Test Loss = 0.32295827466508614\n",
      "Recall = 0.9960801393728222, Aging Rate = 0.49978260869565216, precision = 0.994780339277947\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.32320079782734745\n",
      "Epoch 22: Train Loss = 0.3226409279263538\n",
      "Epoch 23: Train Loss = 0.3217631298044453\n",
      "Epoch 24: Train Loss = 0.3211753391182941\n",
      "Epoch 25: Train Loss = 0.3207134321979854\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4991304347826087, Precision = 0.9965156794425087\n",
      "Validation: Test Loss = 0.3200194748069929\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.49869565217391304, precision = 0.997384481255449\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.32024193888125213\n",
      "Epoch 27: Train Loss = 0.3198711197790892\n",
      "Epoch 28: Train Loss = 0.3196602119051892\n",
      "Epoch 29: Train Loss = 0.31932053249815234\n",
      "Epoch 30: Train Loss = 0.31912531552107437\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4984782608695652, Precision = 0.9978194505015264\n",
      "Validation: Test Loss = 0.3187020558896272\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.49826086956521737, precision = 0.9982547993019197\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.31890302880950594\n",
      "Epoch 32: Train Loss = 0.31875567861225296\n",
      "Epoch 33: Train Loss = 0.3188616542194201\n",
      "Epoch 34: Train Loss = 0.3186320870337279\n",
      "Epoch 35: Train Loss = 0.31847959067510523\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4980434782608696, Precision = 0.9986905281536447\n",
      "Validation: Test Loss = 0.3178937696892282\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4980434782608696, precision = 0.9986905281536447\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.31854137337726096\n",
      "Epoch 37: Train Loss = 0.31814279032790144\n",
      "Epoch 38: Train Loss = 0.3180003320652506\n",
      "Epoch 39: Train Loss = 0.31794784364492995\n",
      "Epoch 40: Train Loss = 0.31798052513081093\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4980434782608696, Precision = 0.9986905281536447\n",
      "Validation: Test Loss = 0.3176423122571862\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4980434782608696, precision = 0.9986905281536447\n",
      "\n",
      "Epoch 41: Train Loss = 0.3180602134828982\n",
      "Epoch 42: Train Loss = 0.31796694895495536\n",
      "Epoch 43: Train Loss = 0.31797788879145744\n",
      "Epoch 44: Train Loss = 0.3177490022389785\n",
      "Epoch 45: Train Loss = 0.31768456868503403\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.49782608695652175, Precision = 0.9991266375545852\n",
      "Validation: Test Loss = 0.3176928654442663\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.49782608695652175, precision = 0.9991266375545852\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3176503849029541\n",
      "Epoch 47: Train Loss = 0.3177217589772266\n",
      "Epoch 48: Train Loss = 0.31779175613237465\n",
      "Epoch 49: Train Loss = 0.31749010557713714\n",
      "Epoch 50: Train Loss = 0.31763074880060943\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4976086956521739, Precision = 0.999563128003495\n",
      "Validation: Test Loss = 0.3172416481246119\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4976086956521739, precision = 0.999563128003495\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3177120999149654\n",
      "Epoch 52: Train Loss = 0.31721405609794284\n",
      "Epoch 53: Train Loss = 0.31736575235491216\n",
      "Epoch 54: Train Loss = 0.3173846310636272\n",
      "Epoch 55: Train Loss = 0.31723487838454867\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4976086956521739, Precision = 0.999563128003495\n",
      "Validation: Test Loss = 0.3171918780389039\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4976086956521739, precision = 0.999563128003495\n",
      "\n",
      "Epoch 56: Train Loss = 0.3172481330581333\n",
      "Epoch 57: Train Loss = 0.31759457976921746\n",
      "Epoch 58: Train Loss = 0.31770016198572903\n",
      "Epoch 59: Train Loss = 0.3172777402919272\n",
      "Epoch 60: Train Loss = 0.31749755750531733\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4976086956521739, Precision = 0.999563128003495\n",
      "Validation: Test Loss = 0.3170195899320685\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4976086956521739, precision = 0.999563128003495\n",
      "\n",
      "Epoch 61: Train Loss = 0.3171785874988722\n",
      "Epoch 62: Train Loss = 0.31702649582987247\n",
      "Epoch 63: Train Loss = 0.31704366554384644\n",
      "Epoch 64: Train Loss = 0.31711747423462244\n",
      "Epoch 65: Train Loss = 0.3176077008765677\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.49782608695652175, Precision = 0.9991266375545852\n",
      "Validation: Test Loss = 0.31750179695046465\n",
      "Recall = 0.9965156794425087, Aging Rate = 0.4973913043478261, precision = 1.0\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.3175091228277787\n",
      "Epoch 67: Train Loss = 0.31735055607298146\n",
      "Epoch 68: Train Loss = 0.31725989409115\n",
      "Epoch 69: Train Loss = 0.3171977294527966\n",
      "Epoch 70: Train Loss = 0.3166071182748546\n",
      "Recall = 0.997822299651568, Aging Rate = 0.4980434782608696, Precision = 0\n",
      "Validation: Test Loss = 0.3158176419527634\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.3159435432890187\n",
      "Epoch 72: Train Loss = 0.3160294169964998\n",
      "Epoch 73: Train Loss = 0.3160246825736502\n",
      "Epoch 74: Train Loss = 0.3161487923497739\n",
      "Epoch 75: Train Loss = 0.3159772121387979\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31582597437112225\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.3160366348080013\n",
      "Epoch 77: Train Loss = 0.3160801967330601\n",
      "Epoch 78: Train Loss = 0.31685034808905227\n",
      "Epoch 79: Train Loss = 0.3161896100251571\n",
      "Epoch 80: Train Loss = 0.3158318447030109\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31569613493007165\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.3158745448485665\n",
      "Epoch 82: Train Loss = 0.3158605412296627\n",
      "Epoch 83: Train Loss = 0.3159920562868533\n",
      "Epoch 84: Train Loss = 0.3162630932227425\n",
      "Epoch 85: Train Loss = 0.3159896867689879\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31581685734831766\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.3159957490278327\n",
      "Epoch 87: Train Loss = 0.3160342082251673\n",
      "Epoch 88: Train Loss = 0.31613337558248766\n",
      "Epoch 89: Train Loss = 0.3161163578862729\n",
      "Epoch 90: Train Loss = 0.31602886091107907\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31664958295614826\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.3161754828432332\n",
      "Epoch 92: Train Loss = 0.31608847633652065\n",
      "Epoch 93: Train Loss = 0.3160027944005054\n",
      "Epoch 94: Train Loss = 0.31597417064335037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train Loss = 0.3159921683954156\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.3161310963527016\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.3159938346821329\n",
      "Epoch 97: Train Loss = 0.31598972869955977\n",
      "Epoch 98: Train Loss = 0.3160635789062666\n",
      "Epoch 99: Train Loss = 0.31608738380929696\n",
      "Epoch 100: Train Loss = 0.31626896407293237\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.3159143879102624\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.31623780120974004\n",
      "Epoch 102: Train Loss = 0.3159761229287023\n",
      "Epoch 103: Train Loss = 0.31585304177325707\n",
      "Epoch 104: Train Loss = 0.3158933510987655\n",
      "Epoch 105: Train Loss = 0.3159316384274027\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31577041465303174\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.3160376341964887\n",
      "Epoch 107: Train Loss = 0.3159422122395557\n",
      "Epoch 108: Train Loss = 0.31605120036913004\n",
      "Epoch 109: Train Loss = 0.31653684496879575\n",
      "Epoch 110: Train Loss = 0.3160351393015488\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31607138685558156\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.31587401851363806\n",
      "Epoch 112: Train Loss = 0.31603727185207864\n",
      "Epoch 113: Train Loss = 0.31602367173070495\n",
      "Epoch 114: Train Loss = 0.31608312871145167\n",
      "Epoch 115: Train Loss = 0.31597703068152716\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31598205675249513\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.3158547892259515\n",
      "Epoch 117: Train Loss = 0.3161707774452541\n",
      "Epoch 118: Train Loss = 0.3159420867588209\n",
      "Epoch 119: Train Loss = 0.3159680822621221\n",
      "Epoch 120: Train Loss = 0.31606280627457994\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, Precision = 0\n",
      "Validation: Test Loss = 0.31589443642160164\n",
      "Recall = 0.9982578397212544, Aging Rate = 0.49826086956521737, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 120.\n",
      "Validation: Test Loss = 0.32393912883594417\n",
      "Recall = 0.9922178988326849, Aging Rate = 0.5058670143415906, precision = 0.9858247422680413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7baade1ea44e7da5946b9f2935545d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.629895559704822\n",
      "Epoch 2: Train Loss = 0.5134755428459333\n",
      "Epoch 3: Train Loss = 0.47240745720656024\n",
      "Epoch 4: Train Loss = 0.44909776609876884\n",
      "Epoch 5: Train Loss = 0.42769932456638504\n",
      "Recall = 0.9347730277655355, Aging Rate = 0.5304347826086957, Precision = 0.8692622950819672\n",
      "Validation: Test Loss = 0.41202544673629427\n",
      "Recall = 0.9510797708241516, Aging Rate = 0.5239130434782608, precision = 0.8954356846473029\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.4069426968305007\n",
      "Epoch 7: Train Loss = 0.38992294290791385\n",
      "Epoch 8: Train Loss = 0.37595403199610505\n",
      "Epoch 9: Train Loss = 0.3643043640385503\n",
      "Epoch 10: Train Loss = 0.35525007429330246\n",
      "Recall = 0.9801674746584399, Aging Rate = 0.49978260869565216, Precision = 0.9673771204871683\n",
      "Validation: Test Loss = 0.349218073565027\n",
      "Recall = 0.9819303657999119, Aging Rate = 0.49630434782608696, precision = 0.9759088918090232\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.3484931141915529\n",
      "Epoch 12: Train Loss = 0.34338861128558285\n",
      "Epoch 13: Train Loss = 0.339776030830715\n",
      "Epoch 14: Train Loss = 0.3372361161397851\n",
      "Epoch 15: Train Loss = 0.3353482167098833\n",
      "Recall = 0.9841339797267519, Aging Rate = 0.49043478260869566, Precision = 0.9898049645390071\n",
      "Validation: Test Loss = 0.33291582428890726\n",
      "Recall = 0.9854561480828559, Aging Rate = 0.49043478260869566, precision = 0.9911347517730497\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.33335343195044476\n",
      "Epoch 17: Train Loss = 0.3319759499508402\n",
      "Epoch 18: Train Loss = 0.3299355124390644\n",
      "Epoch 19: Train Loss = 0.32894613120866856\n",
      "Epoch 20: Train Loss = 0.32836574637371563\n",
      "Recall = 0.9872190392243279, Aging Rate = 0.48934782608695654, Precision = 0.9951132829853399\n",
      "Validation: Test Loss = 0.3272022501800371\n",
      "Recall = 0.9876597620096959, Aging Rate = 0.4895652173913044, precision = 0.9951154529307282\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.32752687381661455\n",
      "Epoch 22: Train Loss = 0.3269921598745429\n",
      "Epoch 23: Train Loss = 0.3254197024780771\n",
      "Epoch 24: Train Loss = 0.32460968779480975\n",
      "Epoch 25: Train Loss = 0.3242571056925732\n",
      "Recall = 0.989863375936536, Aging Rate = 0.48978260869565216, Precision = 0.9968930315135375\n",
      "Validation: Test Loss = 0.32432766111000727\n",
      "Recall = 0.989863375936536, Aging Rate = 0.48934782608695654, precision = 0.9977787649933363\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.32393705342126927\n",
      "Epoch 27: Train Loss = 0.32366287039673847\n",
      "Epoch 28: Train Loss = 0.32337388142295503\n",
      "Epoch 29: Train Loss = 0.3230312609154245\n",
      "Epoch 30: Train Loss = 0.322565120510433\n",
      "Recall = 0.990304098721904, Aging Rate = 0.48934782608695654, Precision = 0.998223011994669\n",
      "Validation: Test Loss = 0.3221300132378288\n",
      "Recall = 0.990304098721904, Aging Rate = 0.48934782608695654, precision = 0.998223011994669\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3223374008614084\n",
      "Epoch 32: Train Loss = 0.3220393410972927\n",
      "Epoch 33: Train Loss = 0.32201140243074167\n",
      "Epoch 34: Train Loss = 0.32192599477975264\n",
      "Epoch 35: Train Loss = 0.3215812385082245\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, Precision = 0.9995553579368608\n",
      "Validation: Test Loss = 0.321169861347779\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, precision = 0.9995553579368608\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3216979884583017\n",
      "Epoch 37: Train Loss = 0.32151389578114387\n",
      "Epoch 38: Train Loss = 0.32139711944953253\n",
      "Epoch 39: Train Loss = 0.32126106972279755\n",
      "Epoch 40: Train Loss = 0.3211005439965621\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, Precision = 0.9995553579368608\n",
      "Validation: Test Loss = 0.32076454297355983\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 41: Train Loss = 0.3209888159710428\n",
      "Epoch 42: Train Loss = 0.32089437951212346\n",
      "Epoch 43: Train Loss = 0.3210010316060937\n",
      "Epoch 44: Train Loss = 0.3207305752194446\n",
      "Epoch 45: Train Loss = 0.3208666809227156\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, Precision = 0.9995553579368608\n",
      "Validation: Test Loss = 0.3206245762368907\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 46: Train Loss = 0.320796782348467\n",
      "Epoch 47: Train Loss = 0.32086893387462784\n",
      "Epoch 48: Train Loss = 0.32088433260503024\n",
      "Epoch 49: Train Loss = 0.3208037837173628\n",
      "Epoch 50: Train Loss = 0.3209850511861884\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, Precision = 0.9995553579368608\n",
      "Validation: Test Loss = 0.3206441028740095\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 51: Train Loss = 0.32070911112038986\n",
      "Epoch 52: Train Loss = 0.3213191989712093\n",
      "Epoch 53: Train Loss = 0.3206928872025531\n",
      "Epoch 54: Train Loss = 0.3206803771723872\n",
      "Epoch 55: Train Loss = 0.32083121320475705\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48891304347826087, Precision = 0.9995553579368608\n",
      "Validation: Test Loss = 0.3204950049130813\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.32061663777931876\n",
      "Epoch 57: Train Loss = 0.3205877037152\n",
      "Epoch 58: Train Loss = 0.32048063542531885\n",
      "Epoch 59: Train Loss = 0.32044327196867567\n",
      "Epoch 60: Train Loss = 0.32065345080002494\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.32043356631113135\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.32067877650260923\n",
      "Epoch 62: Train Loss = 0.3204881321865579\n",
      "Epoch 63: Train Loss = 0.32034985345342887\n",
      "Epoch 64: Train Loss = 0.32038138068240624\n",
      "Epoch 65: Train Loss = 0.3203577649075052\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.32017954639766527\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.3204450023692587\n",
      "Epoch 67: Train Loss = 0.32095838297968327\n",
      "Epoch 68: Train Loss = 0.32048048091971354\n",
      "Epoch 69: Train Loss = 0.3203513425847758\n",
      "Epoch 70: Train Loss = 0.3203387693218563\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.3201394149531489\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.32046883676363075\n",
      "Epoch 72: Train Loss = 0.32089659172555673\n",
      "Epoch 73: Train Loss = 0.3203548094500666\n",
      "Epoch 74: Train Loss = 0.32028563763784323\n",
      "Epoch 75: Train Loss = 0.3204231073027072\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.32022374225699385\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.3203509831946829\n",
      "Epoch 77: Train Loss = 0.32033242443333504\n",
      "Epoch 78: Train Loss = 0.32032323692155923\n",
      "Epoch 79: Train Loss = 0.3206813273740851\n",
      "Epoch 80: Train Loss = 0.3208888939152593\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.3202023789157038\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.32023823375287264\n",
      "Epoch 82: Train Loss = 0.32032203088635985\n",
      "Epoch 83: Train Loss = 0.32051939342332925\n",
      "Epoch 84: Train Loss = 0.3203605598470439\n",
      "Epoch 85: Train Loss = 0.3203232335007709\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.32033405065536497\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.3205559385859448\n",
      "Epoch 87: Train Loss = 0.3204253150069195\n",
      "Epoch 88: Train Loss = 0.32023832746174025\n",
      "Epoch 89: Train Loss = 0.32037263253460757\n",
      "Epoch 90: Train Loss = 0.32053979267244753\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.3201936207128608\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.3204234160547671\n",
      "Epoch 92: Train Loss = 0.32058256817900616\n",
      "Epoch 93: Train Loss = 0.3212495786729066\n",
      "Epoch 94: Train Loss = 0.32045961447384047\n",
      "Epoch 95: Train Loss = 0.320207513778106\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.3200608433329541\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.32022566810898156\n",
      "Epoch 97: Train Loss = 0.32025339753731435\n",
      "Epoch 98: Train Loss = 0.32048762896786565\n",
      "Epoch 99: Train Loss = 0.32061897428139396\n",
      "Epoch 100: Train Loss = 0.32021639849828637\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.3202770539988642\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.32036383504452914\n",
      "Epoch 102: Train Loss = 0.3202364799250727\n",
      "Epoch 103: Train Loss = 0.3204109079423158\n",
      "Epoch 104: Train Loss = 0.32036042223805966\n",
      "Epoch 105: Train Loss = 0.3203062250821487\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, Precision = 0\n",
      "Validation: Test Loss = 0.3206432610491048\n",
      "Recall = 0.9907448215072719, Aging Rate = 0.48869565217391303, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 105.\n",
      "Validation: Test Loss = 0.3251562470174364\n",
      "Recall = 0.9887218045112782, Aging Rate = 0.5182529335071708, precision = 0.9924528301886792\n",
      "\u001b[32m[I 2022-05-26 16:16:49,491]\u001b[0m Trial 4 finished with value: 0.9881577208826231 and parameters: {'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'bad_weight': 0.6}. Best is trial 4 with value: 0.9881577208826231.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b85a0a214c1433cac5951a83b6366f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.544128794255464\n",
      "Epoch 2: Train Loss = 0.5196738946956136\n",
      "Epoch 3: Train Loss = 0.5193407633511916\n",
      "Epoch 4: Train Loss = 0.519032603243123\n",
      "Epoch 5: Train Loss = 0.5176649329455002\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5183939711943917\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5181948838544929\n",
      "Epoch 7: Train Loss = 0.5176402230884718\n",
      "Epoch 8: Train Loss = 0.518841572067012\n",
      "Epoch 9: Train Loss = 0.5188309523333674\n",
      "Epoch 10: Train Loss = 0.5186904680210611\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5186590772089751\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 11: Train Loss = 0.519118407394575\n",
      "Epoch 12: Train Loss = 0.5184365320205688\n",
      "Epoch 13: Train Loss = 0.5181080188958541\n",
      "Epoch 14: Train Loss = 0.5174357087715812\n",
      "Epoch 15: Train Loss = 0.5191348649107892\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5175195292804552\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 16: Train Loss = 0.5168196964782217\n",
      "Epoch 17: Train Loss = 0.5182925721873408\n",
      "Epoch 18: Train Loss = 0.5187251691196276\n",
      "Epoch 19: Train Loss = 0.5177600237597589\n",
      "Epoch 20: Train Loss = 0.5187579550432122\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5189304614067077\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 21: Train Loss = 0.5173512019281802\n",
      "Epoch 22: Train Loss = 0.5179670158676479\n",
      "Epoch 23: Train Loss = 0.5183965347124183\n",
      "Epoch 24: Train Loss = 0.5181212911398514\n",
      "Epoch 25: Train Loss = 0.518669620078543\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5185375105298083\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 26: Train Loss = 0.5191731572151184\n",
      "Epoch 27: Train Loss = 0.5179498083695121\n",
      "Epoch 28: Train Loss = 0.5176963999478713\n",
      "Epoch 29: Train Loss = 0.5180790866976199\n",
      "Epoch 30: Train Loss = 0.5172922445380169\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5177858237598253\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 31: Train Loss = 0.5190051098491835\n",
      "Epoch 32: Train Loss = 0.5188172067248303\n",
      "Epoch 33: Train Loss = 0.519593233751214\n",
      "Epoch 34: Train Loss = 0.5174669596423274\n",
      "Epoch 35: Train Loss = 0.5180262782262719\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5183512827624446\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 36: Train Loss = 0.5179619499393131\n",
      "Epoch 37: Train Loss = 0.5185651942958003\n",
      "Epoch 38: Train Loss = 0.5175745587763579\n",
      "Epoch 39: Train Loss = 0.5175201431564663\n",
      "Epoch 40: Train Loss = 0.5181832673238671\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5170132323969966\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 41: Train Loss = 0.5187325742970342\n",
      "Epoch 42: Train Loss = 0.5175705215205317\n",
      "Epoch 43: Train Loss = 0.5183718998017518\n",
      "Epoch 44: Train Loss = 0.51731043079625\n",
      "Epoch 45: Train Loss = 0.5172968937003094\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5183219897228738\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 46: Train Loss = 0.517258673802666\n",
      "Epoch 47: Train Loss = 0.5182405446923297\n",
      "Epoch 48: Train Loss = 0.5173976579956386\n",
      "Epoch 49: Train Loss = 0.5180389714241028\n",
      "Epoch 50: Train Loss = 0.5180322007510973\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5180532176598258\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Epoch 51: Train Loss = 0.5176302162460659\n",
      "Epoch 52: Train Loss = 0.5177129752739617\n",
      "Epoch 53: Train Loss = 0.5167415705971096\n",
      "Epoch 54: Train Loss = 0.5171601503309996\n",
      "Epoch 55: Train Loss = 0.5193422849281975\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5026086956521739\n",
      "Validation: Test Loss = 0.5173806167685467\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026086956521739\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.522695357205815\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4921773142112125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99052d94d7334f7892a7274e06e64479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5523401068604511\n",
      "Epoch 2: Train Loss = 0.5186367016253264\n",
      "Epoch 3: Train Loss = 0.5176423129828079\n",
      "Epoch 4: Train Loss = 0.5170077146654544\n",
      "Epoch 5: Train Loss = 0.5172539143976957\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5173963633827541\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5187401789167653\n",
      "Epoch 7: Train Loss = 0.5183465325832367\n",
      "Epoch 8: Train Loss = 0.5172352178200431\n",
      "Epoch 9: Train Loss = 0.5172130662461986\n",
      "Epoch 10: Train Loss = 0.5178263235092163\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5177959233781566\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 11: Train Loss = 0.5165528783590897\n",
      "Epoch 12: Train Loss = 0.5184938437005748\n",
      "Epoch 13: Train Loss = 0.5179634796018185\n",
      "Epoch 14: Train Loss = 0.5171793393466784\n",
      "Epoch 15: Train Loss = 0.5173201758965202\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5174678584803706\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 16: Train Loss = 0.5172597616133483\n",
      "Epoch 17: Train Loss = 0.5171975143059441\n",
      "Epoch 18: Train Loss = 0.5178935677072276\n",
      "Epoch 19: Train Loss = 0.5182576712318089\n",
      "Epoch 20: Train Loss = 0.5169487089696138\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5178817160233208\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 21: Train Loss = 0.5178241438451021\n",
      "Epoch 22: Train Loss = 0.5169761778479037\n",
      "Epoch 23: Train Loss = 0.5183544796446096\n",
      "Epoch 24: Train Loss = 0.5170994787630827\n",
      "Epoch 25: Train Loss = 0.5179174662672955\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5169669346187425\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 26: Train Loss = 0.5176857864338419\n",
      "Epoch 27: Train Loss = 0.5191910679444023\n",
      "Epoch 28: Train Loss = 0.5173490339258443\n",
      "Epoch 29: Train Loss = 0.5176531112712363\n",
      "Epoch 30: Train Loss = 0.5172946582669797\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5180785590669383\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 31: Train Loss = 0.5185723199015079\n",
      "Epoch 32: Train Loss = 0.5184385494045589\n",
      "Epoch 33: Train Loss = 0.5184253052006597\n",
      "Epoch 34: Train Loss = 0.5169143244494563\n",
      "Epoch 35: Train Loss = 0.5178981164227361\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.517006776643836\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 36: Train Loss = 0.5176605769862299\n",
      "Epoch 37: Train Loss = 0.5176029799295508\n",
      "Epoch 38: Train Loss = 0.5169428852330084\n",
      "Epoch 39: Train Loss = 0.5174093920251598\n",
      "Epoch 40: Train Loss = 0.518428995401963\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.516835712712744\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 41: Train Loss = 0.5183467443093009\n",
      "Epoch 42: Train Loss = 0.5164215323717698\n",
      "Epoch 43: Train Loss = 0.5162827205657959\n",
      "Epoch 44: Train Loss = 0.5188322246074677\n",
      "Epoch 45: Train Loss = 0.517292016485463\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5182987714850384\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 46: Train Loss = 0.5181688778296761\n",
      "Epoch 47: Train Loss = 0.5163892189316127\n",
      "Epoch 48: Train Loss = 0.5191486310958863\n",
      "Epoch 49: Train Loss = 0.5179936423509017\n",
      "Epoch 50: Train Loss = 0.5179652470091115\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5176092559358348\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Epoch 51: Train Loss = 0.518601448017618\n",
      "Epoch 52: Train Loss = 0.5176498231680496\n",
      "Epoch 53: Train Loss = 0.5172488458260246\n",
      "Epoch 54: Train Loss = 0.5175970023611317\n",
      "Epoch 55: Train Loss = 0.5171380246203878\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5032608695652174\n",
      "Validation: Test Loss = 0.5166298373885777\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5032608695652174\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5255388869300505\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49022164276401564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c83769a4f34aa59d85029d455f35f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5500931582243547\n",
      "Epoch 2: Train Loss = 0.5242653579297273\n",
      "Epoch 3: Train Loss = 0.5249057046226833\n",
      "Epoch 4: Train Loss = 0.5253099018594493\n",
      "Epoch 5: Train Loss = 0.523349316638449\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5242226391253264\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5237455709084221\n",
      "Epoch 7: Train Loss = 0.5249216797040857\n",
      "Epoch 8: Train Loss = 0.523646062353383\n",
      "Epoch 9: Train Loss = 0.5239116031190624\n",
      "Epoch 10: Train Loss = 0.5242225797280021\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5252434068140777\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 11: Train Loss = 0.5243906075021495\n",
      "Epoch 12: Train Loss = 0.5257180143439252\n",
      "Epoch 13: Train Loss = 0.5247935625781184\n",
      "Epoch 14: Train Loss = 0.5247646205839903\n",
      "Epoch 15: Train Loss = 0.5225991944644762\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5252636242431143\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 16: Train Loss = 0.5237326671766198\n",
      "Epoch 17: Train Loss = 0.5234824799973031\n",
      "Epoch 18: Train Loss = 0.5242777111219323\n",
      "Epoch 19: Train Loss = 0.5251285875361899\n",
      "Epoch 20: Train Loss = 0.523320850185726\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5247237104954927\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 21: Train Loss = 0.525107433795929\n",
      "Epoch 22: Train Loss = 0.5252706624113995\n",
      "Epoch 23: Train Loss = 0.5232368673448977\n",
      "Epoch 24: Train Loss = 0.5246416226677273\n",
      "Epoch 25: Train Loss = 0.525157549744067\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5244745948003686\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 26: Train Loss = 0.5239926138131514\n",
      "Epoch 27: Train Loss = 0.5241887123688408\n",
      "Epoch 28: Train Loss = 0.523924182601597\n",
      "Epoch 29: Train Loss = 0.5249864660138669\n",
      "Epoch 30: Train Loss = 0.5253624020970385\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5226523192032524\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 31: Train Loss = 0.5237273351005886\n",
      "Epoch 32: Train Loss = 0.5237545413556306\n",
      "Epoch 33: Train Loss = 0.5250559406695159\n",
      "Epoch 34: Train Loss = 0.5239037109457928\n",
      "Epoch 35: Train Loss = 0.5243971291832302\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5236378873949465\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 36: Train Loss = 0.5230650006688159\n",
      "Epoch 37: Train Loss = 0.5229680710253508\n",
      "Epoch 38: Train Loss = 0.5247293928395147\n",
      "Epoch 39: Train Loss = 0.524218101605125\n",
      "Epoch 40: Train Loss = 0.5243998718261719\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.524277893771296\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 41: Train Loss = 0.5237616766017417\n",
      "Epoch 42: Train Loss = 0.5232713380067244\n",
      "Epoch 43: Train Loss = 0.5246005913485652\n",
      "Epoch 44: Train Loss = 0.5238969994627911\n",
      "Epoch 45: Train Loss = 0.5237717876227006\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5233456087112427\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 46: Train Loss = 0.5243689620494842\n",
      "Epoch 47: Train Loss = 0.5245795435490815\n",
      "Epoch 48: Train Loss = 0.5220969136901524\n",
      "Epoch 49: Train Loss = 0.5247162267436152\n",
      "Epoch 50: Train Loss = 0.524745672993038\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5250711489760358\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Epoch 51: Train Loss = 0.5254607245196466\n",
      "Epoch 52: Train Loss = 0.5233381230934806\n",
      "Epoch 53: Train Loss = 0.5240437057743902\n",
      "Epoch 54: Train Loss = 0.5232915055233499\n",
      "Epoch 55: Train Loss = 0.5245985081921453\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4930434782608696\n",
      "Validation: Test Loss = 0.5233658662049667\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4930434782608696\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5067731930426234\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5208604954367666\n",
      "\u001b[32m[I 2022-05-26 16:17:21,882]\u001b[0m Trial 5 finished with value: 0.6675164895243538 and parameters: {'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 0.001, 'bad_weight': 0.8}. Best is trial 4 with value: 0.9881577208826231.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffebf288b4044d0812ce2b50b864b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6671164054455965\n",
      "Epoch 2: Train Loss = 0.6396568567856499\n",
      "Epoch 3: Train Loss = 0.6142413689779199\n",
      "Epoch 4: Train Loss = 0.5906505744353585\n",
      "Epoch 5: Train Loss = 0.5725052058178446\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5647931069913118\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5595286114319511\n",
      "Epoch 7: Train Loss = 0.5496367038851199\n",
      "Epoch 8: Train Loss = 0.5418885360593381\n",
      "Epoch 9: Train Loss = 0.5354171262616697\n",
      "Epoch 10: Train Loss = 0.532782856070477\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5301299053689708\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 11: Train Loss = 0.528913864882096\n",
      "Epoch 12: Train Loss = 0.5278916187908338\n",
      "Epoch 13: Train Loss = 0.5248748134530109\n",
      "Epoch 14: Train Loss = 0.5225149684366972\n",
      "Epoch 15: Train Loss = 0.5219414867525516\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5213101520745651\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 16: Train Loss = 0.5213526304908421\n",
      "Epoch 17: Train Loss = 0.5195289327787317\n",
      "Epoch 18: Train Loss = 0.5188234259771264\n",
      "Epoch 19: Train Loss = 0.5181089002153147\n",
      "Epoch 20: Train Loss = 0.5177357890294946\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5187839093415634\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 21: Train Loss = 0.5180556969020678\n",
      "Epoch 22: Train Loss = 0.5164998039473658\n",
      "Epoch 23: Train Loss = 0.516956428963205\n",
      "Epoch 24: Train Loss = 0.516945569826209\n",
      "Epoch 25: Train Loss = 0.5156659892330999\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5157588106652965\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 26: Train Loss = 0.5164117588167605\n",
      "Epoch 27: Train Loss = 0.5149451283786608\n",
      "Epoch 28: Train Loss = 0.5158788230108178\n",
      "Epoch 29: Train Loss = 0.5167941505494325\n",
      "Epoch 30: Train Loss = 0.5148675584793091\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5150478196662406\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 31: Train Loss = 0.5157814299541971\n",
      "Epoch 32: Train Loss = 0.5152574297656184\n",
      "Epoch 33: Train Loss = 0.5160280641265538\n",
      "Epoch 34: Train Loss = 0.5151237885848335\n",
      "Epoch 35: Train Loss = 0.5141548409669295\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5147873115539551\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 36: Train Loss = 0.5147694077699081\n",
      "Epoch 37: Train Loss = 0.5151194794281669\n",
      "Epoch 38: Train Loss = 0.5147453722746476\n",
      "Epoch 39: Train Loss = 0.514353352992431\n",
      "Epoch 40: Train Loss = 0.5148867580165034\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5155380773544311\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 41: Train Loss = 0.5147126852947733\n",
      "Epoch 42: Train Loss = 0.5143503061584804\n",
      "Epoch 43: Train Loss = 0.5143100782062696\n",
      "Epoch 44: Train Loss = 0.5145872991499694\n",
      "Epoch 45: Train Loss = 0.5149037610966226\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5145345877564471\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 46: Train Loss = 0.5143259226757547\n",
      "Epoch 47: Train Loss = 0.514284567936607\n",
      "Epoch 48: Train Loss = 0.5145190117151841\n",
      "Epoch 49: Train Loss = 0.5146433904896611\n",
      "Epoch 50: Train Loss = 0.5143552147823831\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5145388024267943\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Epoch 51: Train Loss = 0.514757895262345\n",
      "Epoch 52: Train Loss = 0.5150154949271161\n",
      "Epoch 53: Train Loss = 0.5145841553936834\n",
      "Epoch 54: Train Loss = 0.5137992992608443\n",
      "Epoch 55: Train Loss = 0.5146963731102322\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5017391304347826\n",
      "Validation: Test Loss = 0.5144321995714436\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5017391304347826\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5184413732419269\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49478487614080835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30de4ca253a74a449b4e09294482d43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6975569665950277\n",
      "Epoch 2: Train Loss = 0.6686931612180627\n",
      "Epoch 3: Train Loss = 0.6413981132921965\n",
      "Epoch 4: Train Loss = 0.6150129392872686\n",
      "Epoch 5: Train Loss = 0.5923458939013274\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5821400053604789\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5746750038603078\n",
      "Epoch 7: Train Loss = 0.5620343030017355\n",
      "Epoch 8: Train Loss = 0.5529910579971645\n",
      "Epoch 9: Train Loss = 0.5455020371727322\n",
      "Epoch 10: Train Loss = 0.5398607215674027\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5382424032169839\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 11: Train Loss = 0.5364483653980753\n",
      "Epoch 12: Train Loss = 0.5329571661741838\n",
      "Epoch 13: Train Loss = 0.530747026671534\n",
      "Epoch 14: Train Loss = 0.5283716638191887\n",
      "Epoch 15: Train Loss = 0.5278849321862925\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5276018195566924\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 16: Train Loss = 0.5262157219389211\n",
      "Epoch 17: Train Loss = 0.5249497911204463\n",
      "Epoch 18: Train Loss = 0.5240826943646306\n",
      "Epoch 19: Train Loss = 0.523715686487115\n",
      "Epoch 20: Train Loss = 0.5233564732385718\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5225776105341704\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 21: Train Loss = 0.522677684555883\n",
      "Epoch 22: Train Loss = 0.5222738825756571\n",
      "Epoch 23: Train Loss = 0.5216912938200909\n",
      "Epoch 24: Train Loss = 0.5214694461615189\n",
      "Epoch 25: Train Loss = 0.5215688991546631\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5209125103121218\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 26: Train Loss = 0.5219676300235416\n",
      "Epoch 27: Train Loss = 0.5209096075140912\n",
      "Epoch 28: Train Loss = 0.5210493960587874\n",
      "Epoch 29: Train Loss = 0.5205349337536356\n",
      "Epoch 30: Train Loss = 0.5203159021294635\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5199096795268681\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 31: Train Loss = 0.5198757654687632\n",
      "Epoch 32: Train Loss = 0.5211730241775513\n",
      "Epoch 33: Train Loss = 0.5206469189602396\n",
      "Epoch 34: Train Loss = 0.5205426683633224\n",
      "Epoch 35: Train Loss = 0.5199566219163978\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5208763757995937\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 36: Train Loss = 0.5202593564987182\n",
      "Epoch 37: Train Loss = 0.5201285875361898\n",
      "Epoch 38: Train Loss = 0.5196831116987312\n",
      "Epoch 39: Train Loss = 0.5196324515342713\n",
      "Epoch 40: Train Loss = 0.5203072188729825\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5197329893319503\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 41: Train Loss = 0.5198550538394762\n",
      "Epoch 42: Train Loss = 0.5212903428077698\n",
      "Epoch 43: Train Loss = 0.520154432732126\n",
      "Epoch 44: Train Loss = 0.5198189239398293\n",
      "Epoch 45: Train Loss = 0.5198884103609168\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5194874248297319\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 46: Train Loss = 0.5193646804664446\n",
      "Epoch 47: Train Loss = 0.5200323153578716\n",
      "Epoch 48: Train Loss = 0.5199973070621491\n",
      "Epoch 49: Train Loss = 0.5199214169253473\n",
      "Epoch 50: Train Loss = 0.5200438564756642\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5194556902802508\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 51: Train Loss = 0.5199521193815314\n",
      "Epoch 52: Train Loss = 0.519509537530982\n",
      "Epoch 53: Train Loss = 0.5203877998434979\n",
      "Epoch 54: Train Loss = 0.5194279850047567\n",
      "Epoch 55: Train Loss = 0.5190200452182604\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5190353827372841\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5028232450280071\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5195567144719687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eff5c6d66f4298bd253696a91705dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6499260980149973\n",
      "Epoch 2: Train Loss = 0.6282090704337411\n",
      "Epoch 3: Train Loss = 0.6080827404105145\n",
      "Epoch 4: Train Loss = 0.589681557883387\n",
      "Epoch 5: Train Loss = 0.5735953103977701\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5661933903072192\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5608097533557725\n",
      "Epoch 7: Train Loss = 0.5510889436887658\n",
      "Epoch 8: Train Loss = 0.5437450024356013\n",
      "Epoch 9: Train Loss = 0.538596395616946\n",
      "Epoch 10: Train Loss = 0.5352057534715403\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5338550631896309\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 11: Train Loss = 0.5320223811398381\n",
      "Epoch 12: Train Loss = 0.5300563716888428\n",
      "Epoch 13: Train Loss = 0.5280184386087501\n",
      "Epoch 14: Train Loss = 0.5265148867213207\n",
      "Epoch 15: Train Loss = 0.5252738338968028\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5255650742157646\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 16: Train Loss = 0.5253055411836375\n",
      "Epoch 17: Train Loss = 0.5248525791064552\n",
      "Epoch 18: Train Loss = 0.5232422383971836\n",
      "Epoch 19: Train Loss = 0.5232904468411985\n",
      "Epoch 20: Train Loss = 0.5223812153028405\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5225386966829715\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 21: Train Loss = 0.5223025709649791\n",
      "Epoch 22: Train Loss = 0.5213737266996632\n",
      "Epoch 23: Train Loss = 0.5214970561732416\n",
      "Epoch 24: Train Loss = 0.5215317180364029\n",
      "Epoch 25: Train Loss = 0.5208015104998713\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.521403619621111\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 26: Train Loss = 0.5214041723375735\n",
      "Epoch 27: Train Loss = 0.5211300102524136\n",
      "Epoch 28: Train Loss = 0.5209868823963663\n",
      "Epoch 29: Train Loss = 0.5209685564041138\n",
      "Epoch 30: Train Loss = 0.5204854518952577\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5214584011616914\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 31: Train Loss = 0.5199505726150845\n",
      "Epoch 32: Train Loss = 0.5211015251926754\n",
      "Epoch 33: Train Loss = 0.5207068851201431\n",
      "Epoch 34: Train Loss = 0.5198964829030244\n",
      "Epoch 35: Train Loss = 0.519923872999523\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5206900040999702\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 36: Train Loss = 0.520518872115923\n",
      "Epoch 37: Train Loss = 0.5202818922892861\n",
      "Epoch 38: Train Loss = 0.5197521467830823\n",
      "Epoch 39: Train Loss = 0.5199843919795493\n",
      "Epoch 40: Train Loss = 0.5194214552381764\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5201855031303737\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 41: Train Loss = 0.5200054236080336\n",
      "Epoch 42: Train Loss = 0.5205905481006788\n",
      "Epoch 43: Train Loss = 0.5193441140133401\n",
      "Epoch 44: Train Loss = 0.5199370669282001\n",
      "Epoch 45: Train Loss = 0.5198396006874416\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5196083124824192\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 46: Train Loss = 0.5200288212817649\n",
      "Epoch 47: Train Loss = 0.5206374874840612\n",
      "Epoch 48: Train Loss = 0.519534205198288\n",
      "Epoch 49: Train Loss = 0.5195933030999225\n",
      "Epoch 50: Train Loss = 0.5203852996618852\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5193888240275175\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Epoch 51: Train Loss = 0.5201140245147373\n",
      "Epoch 52: Train Loss = 0.5199474428011024\n",
      "Epoch 53: Train Loss = 0.5198245072364807\n",
      "Epoch 54: Train Loss = 0.5202308418439782\n",
      "Epoch 55: Train Loss = 0.5196093464934307\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4934782608695652\n",
      "Validation: Test Loss = 0.5198642943216407\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4934782608695652\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.504700103553675\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5195567144719687\n",
      "\u001b[32m[I 2022-05-26 16:17:35,427]\u001b[0m Trial 6 finished with value: 0.6765560651299857 and parameters: {'batch_size': 96, 'learning_rate': 0.0001, 'weight_decay': 0.0001, 'bad_weight': 0.8}. Best is trial 4 with value: 0.9881577208826231.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63baa420924144bc865da6cacc58b42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6373566303045853\n",
      "Epoch 2: Train Loss = 0.5585145054692807\n",
      "Epoch 3: Train Loss = 0.49289602761683254\n",
      "Epoch 4: Train Loss = 0.4596277381544528\n",
      "Epoch 5: Train Loss = 0.43849658504776334\n",
      "Recall = 0.9648742411101474, Aging Rate = 0.6115217391304347, Precision = 0.7909704941343761\n",
      "Validation: Test Loss = 0.4226011574786642\n",
      "Recall = 0.971379011274935, Aging Rate = 0.5993478260869565, precision = 0.8124773304316286\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.4161816616680311\n",
      "Epoch 7: Train Loss = 0.40041457979575446\n",
      "Epoch 8: Train Loss = 0.3859372740206511\n",
      "Epoch 9: Train Loss = 0.3749950804917709\n",
      "Epoch 10: Train Loss = 0.36644794655882795\n",
      "Recall = 0.9943625325238509, Aging Rate = 0.5493478260869565, Precision = 0.9074000791452315\n",
      "Validation: Test Loss = 0.3606890433767567\n",
      "Recall = 0.9952298352124892, Aging Rate = 0.5369565217391304, precision = 0.9291497975708503\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.3585667167539182\n",
      "Epoch 12: Train Loss = 0.3525483115859654\n",
      "Epoch 13: Train Loss = 0.3475357312741487\n",
      "Epoch 14: Train Loss = 0.3429764821218408\n",
      "Epoch 15: Train Loss = 0.3396138076160265\n",
      "Recall = 0.9960971379011275, Aging Rate = 0.5193478260869565, Precision = 0.961490163248221\n",
      "Validation: Test Loss = 0.33608438719873845\n",
      "Recall = 0.9978317432784042, Aging Rate = 0.5165217391304348, precision = 0.9684343434343434\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3357701239896857\n",
      "Epoch 17: Train Loss = 0.33293142095856043\n",
      "Epoch 18: Train Loss = 0.33099738219509955\n",
      "Epoch 19: Train Loss = 0.3291325609062029\n",
      "Epoch 20: Train Loss = 0.327610547542572\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5104347826086957, Precision = 0.9812606473594548\n",
      "Validation: Test Loss = 0.32634373441986414\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5104347826086957, precision = 0.9812606473594548\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.32629927070244497\n",
      "Epoch 22: Train Loss = 0.3252851194402446\n",
      "Epoch 23: Train Loss = 0.3242892807462941\n",
      "Epoch 24: Train Loss = 0.32338668159816575\n",
      "Epoch 25: Train Loss = 0.32262924733369247\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5054347826086957, Precision = 0.9909677419354839\n",
      "Validation: Test Loss = 0.3220022886214049\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5052173913043478, precision = 0.9913941480206541\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.3221606344762056\n",
      "Epoch 27: Train Loss = 0.32162215637124103\n",
      "Epoch 28: Train Loss = 0.3211775983934817\n",
      "Epoch 29: Train Loss = 0.32062548202017077\n",
      "Epoch 30: Train Loss = 0.3200490325948466\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.503695652173913, Precision = 0.9943892965041001\n",
      "Validation: Test Loss = 0.31942665773889295\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5032608695652174, precision = 0.9952483801295896\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.31990616274916606\n",
      "Epoch 32: Train Loss = 0.3191791129630545\n",
      "Epoch 33: Train Loss = 0.3189222129013227\n",
      "Epoch 34: Train Loss = 0.3187567412853241\n",
      "Epoch 35: Train Loss = 0.3183902263641357\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5019565217391304, Precision = 0.9978345604157643\n",
      "Validation: Test Loss = 0.3180720218368199\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5023913043478261, precision = 0.9969710082215492\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3182622313499451\n",
      "Epoch 37: Train Loss = 0.3180657157690629\n",
      "Epoch 38: Train Loss = 0.3178901473853899\n",
      "Epoch 39: Train Loss = 0.31783124659372414\n",
      "Epoch 40: Train Loss = 0.3176698583105336\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5019565217391304, Precision = 0.9978345604157643\n",
      "Validation: Test Loss = 0.3172613367308741\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5019565217391304, precision = 0.9978345604157643\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3174808317163716\n",
      "Epoch 42: Train Loss = 0.31731426928354345\n",
      "Epoch 43: Train Loss = 0.3173238578568334\n",
      "Epoch 44: Train Loss = 0.31719327133634817\n",
      "Epoch 45: Train Loss = 0.3169714449281278\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5015217391304347, Precision = 0.9986996098829649\n",
      "Validation: Test Loss = 0.3167728118792824\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5015217391304347, precision = 0.9986996098829649\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3169309739962868\n",
      "Epoch 47: Train Loss = 0.3168023467063904\n",
      "Epoch 48: Train Loss = 0.3166793585860211\n",
      "Epoch 49: Train Loss = 0.3166553893814916\n",
      "Epoch 50: Train Loss = 0.3166133704392806\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5015217391304347, Precision = 0.9986996098829649\n",
      "Validation: Test Loss = 0.3163605741314266\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, precision = 0.9991326973113617\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.31659629645554915\n",
      "Epoch 52: Train Loss = 0.31648196946019713\n",
      "Epoch 53: Train Loss = 0.3163758409023285\n",
      "Epoch 54: Train Loss = 0.31638703968213955\n",
      "Epoch 55: Train Loss = 0.31636443806731185\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.31615196269491447\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, precision = 0.9991326973113617\n",
      "\n",
      "Epoch 56: Train Loss = 0.3163899933255237\n",
      "Epoch 57: Train Loss = 0.3162266313511392\n",
      "Epoch 58: Train Loss = 0.31622181529584137\n",
      "Epoch 59: Train Loss = 0.31616636778997337\n",
      "Epoch 60: Train Loss = 0.3161778362419294\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.3159658358926358\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, precision = 0.9991326973113617\n",
      "\n",
      "Epoch 61: Train Loss = 0.3161658360647119\n",
      "Epoch 62: Train Loss = 0.31609387843505193\n",
      "Epoch 63: Train Loss = 0.31611825746038685\n",
      "Epoch 64: Train Loss = 0.31610959224078966\n",
      "Epoch 65: Train Loss = 0.31601900878159894\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.3158843372697416\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, precision = 0.9991326973113617\n",
      "\n",
      "Epoch 66: Train Loss = 0.31598215004672175\n",
      "Epoch 67: Train Loss = 0.3159915644707887\n",
      "Epoch 68: Train Loss = 0.3160210950996565\n",
      "Epoch 69: Train Loss = 0.31595315420109293\n",
      "Epoch 70: Train Loss = 0.31596233015475067\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.31595138264739\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, precision = 0.9991326973113617\n",
      "\n",
      "Epoch 71: Train Loss = 0.31597107866536017\n",
      "Epoch 72: Train Loss = 0.3159506546932718\n",
      "Epoch 73: Train Loss = 0.31597788961037343\n",
      "Epoch 74: Train Loss = 0.3158626679233883\n",
      "Epoch 75: Train Loss = 0.3159641983198083\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.3158388990941255\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5015217391304347, precision = 0.9986996098829649\n",
      "\n",
      "Epoch 76: Train Loss = 0.3159020112908405\n",
      "Epoch 77: Train Loss = 0.31580845527026963\n",
      "Epoch 78: Train Loss = 0.3160107233213342\n",
      "Epoch 79: Train Loss = 0.31591171596361245\n",
      "Epoch 80: Train Loss = 0.31587568749552186\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.3156492714778237\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.3159111920646999\n",
      "Epoch 82: Train Loss = 0.3158257087935572\n",
      "Epoch 83: Train Loss = 0.31585833279982856\n",
      "Epoch 84: Train Loss = 0.31581735408824424\n",
      "Epoch 85: Train Loss = 0.31576249993365746\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.31560516518095266\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 86: Train Loss = 0.31584894926651663\n",
      "Epoch 87: Train Loss = 0.31610271199889806\n",
      "Epoch 88: Train Loss = 0.31587186419445534\n",
      "Epoch 89: Train Loss = 0.3157114994525909\n",
      "Epoch 90: Train Loss = 0.315688455571299\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.31553107334219893\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 91: Train Loss = 0.3157264559165291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.3158494349666264\n",
      "Epoch 93: Train Loss = 0.315704721160557\n",
      "Epoch 94: Train Loss = 0.31574557568715966\n",
      "Epoch 95: Train Loss = 0.31564614498096966\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.3155623650032541\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 96: Train Loss = 0.3158039678698001\n",
      "Epoch 97: Train Loss = 0.31565528579380203\n",
      "Epoch 98: Train Loss = 0.3156803374705107\n",
      "Epoch 99: Train Loss = 0.3157357087860937\n",
      "Epoch 100: Train Loss = 0.3157649517059326\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5013043478260869, Precision = 0.9991326973113617\n",
      "Validation: Test Loss = 0.31561416993970454\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 101: Train Loss = 0.3156918414779331\n",
      "Epoch 102: Train Loss = 0.3157523385856463\n",
      "Epoch 103: Train Loss = 0.3156858433329541\n",
      "Epoch 104: Train Loss = 0.3156836921754091\n",
      "Epoch 105: Train Loss = 0.31572975314181784\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.31551784453184706\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 106: Train Loss = 0.31572098960047185\n",
      "Epoch 107: Train Loss = 0.3157993887300077\n",
      "Epoch 108: Train Loss = 0.3157547496194425\n",
      "Epoch 109: Train Loss = 0.31569828774618064\n",
      "Epoch 110: Train Loss = 0.3157608027561851\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.3156453169428784\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 111: Train Loss = 0.3157961778018786\n",
      "Epoch 112: Train Loss = 0.3156446515995523\n",
      "Epoch 113: Train Loss = 0.3156235712507497\n",
      "Epoch 114: Train Loss = 0.31586259380630827\n",
      "Epoch 115: Train Loss = 0.3157375775731128\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.31555032320644544\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 116: Train Loss = 0.315649486521016\n",
      "Epoch 117: Train Loss = 0.31565829660581507\n",
      "Epoch 118: Train Loss = 0.3156655405915302\n",
      "Epoch 119: Train Loss = 0.3157004896454189\n",
      "Epoch 120: Train Loss = 0.3157398937059485\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.3156320550130761\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 121: Train Loss = 0.31562453710514565\n",
      "Epoch 122: Train Loss = 0.31559877805087877\n",
      "Epoch 123: Train Loss = 0.3156801275585009\n",
      "Epoch 124: Train Loss = 0.3157294181118841\n",
      "Epoch 125: Train Loss = 0.3156927726579749\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.3154834531182828\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Epoch 126: Train Loss = 0.31579765407935434\n",
      "Epoch 127: Train Loss = 0.31564836025238036\n",
      "Epoch 128: Train Loss = 0.31561230638752813\n",
      "Epoch 129: Train Loss = 0.31587024419204046\n",
      "Epoch 130: Train Loss = 0.3156756221729776\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, Precision = 0.9995661605206074\n",
      "Validation: Test Loss = 0.3154858652923418\n",
      "Recall = 0.9991326973113617, Aging Rate = 0.5010869565217392, precision = 0.9995661605206074\n",
      "\n",
      "Training Finished at epoch 130.\n",
      "Validation: Test Loss = 0.3249105280738766\n",
      "Recall = 0.9921156373193167, Aging Rate = 0.5019556714471969, precision = 0.9805194805194806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bd884439ef4d8e979c6e652251cc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6517112229181372\n",
      "Epoch 2: Train Loss = 0.5721337678121484\n",
      "Epoch 3: Train Loss = 0.5054796097589576\n",
      "Epoch 4: Train Loss = 0.47286062401273976\n",
      "Epoch 5: Train Loss = 0.4542317780204441\n",
      "Recall = 0.9515361315447858, Aging Rate = 0.6273913043478261, Precision = 0.761954261954262\n",
      "Validation: Test Loss = 0.4385398572424184\n",
      "Recall = 0.9627866724361748, Aging Rate = 0.6252173913043478, precision = 0.7736439499304589\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.43172851033832715\n",
      "Epoch 7: Train Loss = 0.4171114841233129\n",
      "Epoch 8: Train Loss = 0.40351498925167584\n",
      "Epoch 9: Train Loss = 0.3921647769471873\n",
      "Epoch 10: Train Loss = 0.38127338212469347\n",
      "Recall = 0.9917784508870618, Aging Rate = 0.5652173913043478, Precision = 0.8815384615384615\n",
      "Validation: Test Loss = 0.37517647877983423\n",
      "Recall = 0.9917784508870618, Aging Rate = 0.555, precision = 0.8977673325499412\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.3727322000524272\n",
      "Epoch 12: Train Loss = 0.3651361316183339\n",
      "Epoch 13: Train Loss = 0.35967676784681235\n",
      "Epoch 14: Train Loss = 0.3536398158384406\n",
      "Epoch 15: Train Loss = 0.3494673312228659\n",
      "Recall = 0.9930765902206837, Aging Rate = 0.53, Precision = 0.9413453650533224\n",
      "Validation: Test Loss = 0.3461911405687747\n",
      "Recall = 0.9943747295543055, Aging Rate = 0.5271739130434783, precision = 0.9476288659793815\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.34607236551201864\n",
      "Epoch 17: Train Loss = 0.343146048369615\n",
      "Epoch 18: Train Loss = 0.3404925898883654\n",
      "Epoch 19: Train Loss = 0.3375145701221798\n",
      "Epoch 20: Train Loss = 0.33550304086312005\n",
      "Recall = 0.9948074426655128, Aging Rate = 0.5154347826086957, Precision = 0.9696330662167861\n",
      "Validation: Test Loss = 0.3332249385895936\n",
      "Recall = 0.9948074426655128, Aging Rate = 0.5117391304347826, precision = 0.9766355140186916\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.33337145722430683\n",
      "Epoch 22: Train Loss = 0.33186192227446515\n",
      "Epoch 23: Train Loss = 0.32988351438356484\n",
      "Epoch 24: Train Loss = 0.32873333205347477\n",
      "Epoch 25: Train Loss = 0.32723499717919724\n",
      "Recall = 0.9974037213327563, Aging Rate = 0.5091304347826087, Precision = 0.984201537147737\n",
      "Validation: Test Loss = 0.3262119247084079\n",
      "Recall = 0.9974037213327563, Aging Rate = 0.5073913043478261, precision = 0.9875749785775493\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.32635054971860805\n",
      "Epoch 27: Train Loss = 0.325445788010307\n",
      "Epoch 28: Train Loss = 0.32477928125340005\n",
      "Epoch 29: Train Loss = 0.32382677866064985\n",
      "Epoch 30: Train Loss = 0.32310675253038823\n",
      "Recall = 0.9982691475551709, Aging Rate = 0.5065217391304347, Precision = 0.9901287553648068\n",
      "Validation: Test Loss = 0.3224468671238941\n",
      "Recall = 0.9982691475551709, Aging Rate = 0.5054347826086957, precision = 0.9922580645161291\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.32258291778357134\n",
      "Epoch 32: Train Loss = 0.32177460074424746\n",
      "Epoch 33: Train Loss = 0.3212857601953589\n",
      "Epoch 34: Train Loss = 0.3210064234940902\n",
      "Epoch 35: Train Loss = 0.32081173813861347\n",
      "Recall = 0.9982691475551709, Aging Rate = 0.5047826086956522, Precision = 0.9935400516795866\n",
      "Validation: Test Loss = 0.3198372613865396\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5039130434782608, precision = 0.9956859361518551\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.32017957822136256\n",
      "Epoch 37: Train Loss = 0.3199344333358433\n",
      "Epoch 38: Train Loss = 0.3193973762056102\n",
      "Epoch 39: Train Loss = 0.3191993382702703\n",
      "Epoch 40: Train Loss = 0.3190007811007292\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5032608695652174, Precision = 0.9969762419006479\n",
      "Validation: Test Loss = 0.3183844942113628\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5030434782608696, precision = 0.9974070872947277\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3185341631329578\n",
      "Epoch 42: Train Loss = 0.3183625422871631\n",
      "Epoch 43: Train Loss = 0.3182389039578645\n",
      "Epoch 44: Train Loss = 0.3179924752401269\n",
      "Epoch 45: Train Loss = 0.31777010715526083\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5023913043478261, Precision = 0.9987018606663782\n",
      "Validation: Test Loss = 0.31737293279689294\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, precision = 0.9991341991341991\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.31762577663297237\n",
      "Epoch 47: Train Loss = 0.3174148878843888\n",
      "Epoch 48: Train Loss = 0.31742009074791616\n",
      "Epoch 49: Train Loss = 0.31728616932164067\n",
      "Epoch 50: Train Loss = 0.31720034588938173\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, Precision = 0.9991341991341991\n",
      "Validation: Test Loss = 0.31693932657656465\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, precision = 0.9991341991341991\n",
      "\n",
      "Epoch 51: Train Loss = 0.3171065194710441\n",
      "Epoch 52: Train Loss = 0.3170860683399698\n",
      "Epoch 53: Train Loss = 0.31696323669475057\n",
      "Epoch 54: Train Loss = 0.31682729156120965\n",
      "Epoch 55: Train Loss = 0.3168340900669927\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, Precision = 0.9991341991341991\n",
      "Validation: Test Loss = 0.31664749451305557\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, precision = 0.9991341991341991\n",
      "\n",
      "Epoch 56: Train Loss = 0.3166885824307151\n",
      "Epoch 57: Train Loss = 0.3167105067812878\n",
      "Epoch 58: Train Loss = 0.31672687727472054\n",
      "Epoch 59: Train Loss = 0.3167408079168071\n",
      "Epoch 60: Train Loss = 0.31667675013127533\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, Precision = 0.9991341991341991\n",
      "Validation: Test Loss = 0.316406430731649\n",
      "Recall = 0.9987018606663782, Aging Rate = 0.5021739130434782, precision = 0.9991341991341991\n",
      "\n",
      "Epoch 61: Train Loss = 0.31651247091915297\n",
      "Epoch 62: Train Loss = 0.31609077681665837\n",
      "Epoch 63: Train Loss = 0.31599452205326245\n",
      "Epoch 64: Train Loss = 0.31592303364173224\n",
      "Epoch 65: Train Loss = 0.315906257422074\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5026086956521739, Precision = 0.9991349480968859\n",
      "Validation: Test Loss = 0.31594396135081415\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5026086956521739, precision = 0.9991349480968859\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.31584586895030475\n",
      "Epoch 67: Train Loss = 0.31580161172410715\n",
      "Epoch 68: Train Loss = 0.3158359017579452\n",
      "Epoch 69: Train Loss = 0.31574395936468375\n",
      "Epoch 70: Train Loss = 0.31577295505482217\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, Precision = 0.9995672868887927\n",
      "Validation: Test Loss = 0.31554547035175823\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, precision = 0.9995672868887927\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.3157503197504127\n",
      "Epoch 72: Train Loss = 0.3156419324356577\n",
      "Epoch 73: Train Loss = 0.315622042313866\n",
      "Epoch 74: Train Loss = 0.31555872201919555\n",
      "Epoch 75: Train Loss = 0.3155893986639769\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, Precision = 0.9995672868887927\n",
      "Validation: Test Loss = 0.31538660324138146\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, precision = 0.9995672868887927\n",
      "\n",
      "Epoch 76: Train Loss = 0.31553854113039764\n",
      "Epoch 77: Train Loss = 0.3156304962220399\n",
      "Epoch 78: Train Loss = 0.31563582928284356\n",
      "Epoch 79: Train Loss = 0.315543599076893\n",
      "Epoch 80: Train Loss = 0.3155232048034668\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, Precision = 0.9995672868887927\n",
      "Validation: Test Loss = 0.31540564262348675\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, precision = 0.9995672868887927\n",
      "\n",
      "Epoch 81: Train Loss = 0.3154881442111471\n",
      "Epoch 82: Train Loss = 0.31542685467263926\n",
      "Epoch 83: Train Loss = 0.31550630802693574\n",
      "Epoch 84: Train Loss = 0.31549659345460973\n",
      "Epoch 85: Train Loss = 0.31546687442323434\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, Precision = 0.9995672868887927\n",
      "Validation: Test Loss = 0.31523336861444556\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, precision = 0.9995672868887927\n",
      "\n",
      "Epoch 86: Train Loss = 0.31543255085530486\n",
      "Epoch 87: Train Loss = 0.31548995365267213\n",
      "Epoch 88: Train Loss = 0.31538613946541494\n",
      "Epoch 89: Train Loss = 0.31546931520752286\n",
      "Epoch 90: Train Loss = 0.3154627234002818\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5023913043478261, Precision = 0.9995672868887927\n",
      "Validation: Test Loss = 0.3154912457258805\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.3153855919837952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.3153827967332757\n",
      "Epoch 93: Train Loss = 0.31535386204719545\n",
      "Epoch 94: Train Loss = 0.3154225361347198\n",
      "Epoch 95: Train Loss = 0.31542001568752787\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.31526147479596345\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.31538899758587713\n",
      "Epoch 97: Train Loss = 0.31538602051527603\n",
      "Epoch 98: Train Loss = 0.3154175718970921\n",
      "Epoch 99: Train Loss = 0.3153570701764977\n",
      "Epoch 100: Train Loss = 0.3152998671842658\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.3152537997390913\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.31536138467166736\n",
      "Epoch 102: Train Loss = 0.31538221426632096\n",
      "Epoch 103: Train Loss = 0.3154576029466546\n",
      "Epoch 104: Train Loss = 0.3153979366758595\n",
      "Epoch 105: Train Loss = 0.3153745231939399\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.31519921406455664\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.3153084133500638\n",
      "Epoch 107: Train Loss = 0.31523739109868587\n",
      "Epoch 108: Train Loss = 0.3153464099116947\n",
      "Epoch 109: Train Loss = 0.3152462606844695\n",
      "Epoch 110: Train Loss = 0.31531877761301785\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.31522077669268067\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.3152674963163293\n",
      "Epoch 112: Train Loss = 0.3153423783053523\n",
      "Epoch 113: Train Loss = 0.3153989258538122\n",
      "Epoch 114: Train Loss = 0.31533935966699017\n",
      "Epoch 115: Train Loss = 0.3153167269022568\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.3151696661762569\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.3154269466192826\n",
      "Epoch 117: Train Loss = 0.3153397714055103\n",
      "Epoch 118: Train Loss = 0.31531279195909917\n",
      "Epoch 119: Train Loss = 0.31533762921457703\n",
      "Epoch 120: Train Loss = 0.3152569832490838\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.3152000227700109\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.3152360397836436\n",
      "Epoch 122: Train Loss = 0.3153064573847729\n",
      "Epoch 123: Train Loss = 0.3152829015773276\n",
      "Epoch 124: Train Loss = 0.31535443062367646\n",
      "Epoch 125: Train Loss = 0.31533332057621166\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.31511474801146466\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.3152699564332547\n",
      "Epoch 127: Train Loss = 0.3153095679179482\n",
      "Epoch 128: Train Loss = 0.3152430996687516\n",
      "Epoch 129: Train Loss = 0.31529778024424676\n",
      "Epoch 130: Train Loss = 0.3153469492559848\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.31533959000006967\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 131: Train Loss = 0.31542125199152077\n",
      "Epoch 132: Train Loss = 0.3152812558153401\n",
      "Epoch 133: Train Loss = 0.3153030204254648\n",
      "Epoch 134: Train Loss = 0.3153227458829465\n",
      "Epoch 135: Train Loss = 0.3153311537141385\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.3151918139146722\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Epoch 136: Train Loss = 0.3153604062743809\n",
      "Epoch 137: Train Loss = 0.3152659421900044\n",
      "Epoch 138: Train Loss = 0.3153300172349681\n",
      "Epoch 139: Train Loss = 0.31525270451670107\n",
      "Epoch 140: Train Loss = 0.3153151048784671\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, Precision = 0\n",
      "Validation: Test Loss = 0.3151607074426568\n",
      "Recall = 0.9995672868887927, Aging Rate = 0.5021739130434782, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 140.\n",
      "Validation: Test Loss = 0.32376836883353444\n",
      "Recall = 0.9947089947089947, Aging Rate = 0.4980443285528031, precision = 0.9842931937172775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7413cb0de6604c62ba72c0f5bea16e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6372759519452634\n",
      "Epoch 2: Train Loss = 0.5650986183207968\n",
      "Epoch 3: Train Loss = 0.4992404026052226\n",
      "Epoch 4: Train Loss = 0.4657566267511119\n",
      "Epoch 5: Train Loss = 0.44814041842585023\n",
      "Recall = 0.956691208315288, Aging Rate = 0.6210869565217392, Precision = 0.7731886594329717\n",
      "Validation: Test Loss = 0.433096052667369\n",
      "Recall = 0.9744478129060199, Aging Rate = 0.6276086956521739, precision = 0.7793557325943886\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.42631057785904924\n",
      "Epoch 7: Train Loss = 0.41305306906285494\n",
      "Epoch 8: Train Loss = 0.4000034911217897\n",
      "Epoch 9: Train Loss = 0.38862637773804043\n",
      "Epoch 10: Train Loss = 0.380665473005046\n",
      "Recall = 0.9857080987440451, Aging Rate = 0.5591304347826087, Precision = 0.8849144634525661\n",
      "Validation: Test Loss = 0.37430044790972833\n",
      "Recall = 0.9887397141619749, Aging Rate = 0.5510869565217391, precision = 0.9005917159763314\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.372296126096145\n",
      "Epoch 12: Train Loss = 0.36538734171701515\n",
      "Epoch 13: Train Loss = 0.35905153912046683\n",
      "Epoch 14: Train Loss = 0.3534287037020144\n",
      "Epoch 15: Train Loss = 0.3485397908480271\n",
      "Recall = 0.9943698570809875, Aging Rate = 0.5280434782608696, Precision = 0.9452449567723343\n",
      "Validation: Test Loss = 0.34536005963449895\n",
      "Recall = 0.9952360329146817, Aging Rate = 0.5241304347826087, precision = 0.9531314807133969\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.34484279948732127\n",
      "Epoch 17: Train Loss = 0.3413874160206836\n",
      "Epoch 18: Train Loss = 0.33855956476667654\n",
      "Epoch 19: Train Loss = 0.3357340613655422\n",
      "Epoch 20: Train Loss = 0.3343016146576923\n",
      "Recall = 0.996535296665223, Aging Rate = 0.5145652173913043, Precision = 0.9721166032953105\n",
      "Validation: Test Loss = 0.33235831644224084\n",
      "Recall = 0.996535296665223, Aging Rate = 0.5139130434782608, precision = 0.9733502538071066\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.332460878620977\n",
      "Epoch 22: Train Loss = 0.33014188553975976\n",
      "Epoch 23: Train Loss = 0.32883115726968515\n",
      "Epoch 24: Train Loss = 0.32784576913584834\n",
      "Epoch 25: Train Loss = 0.32651046934335126\n",
      "Recall = 0.9982676483326115, Aging Rate = 0.5095652173913043, Precision = 0.9833617747440273\n",
      "Validation: Test Loss = 0.3255571030533832\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5097826086956522, precision = 0.9833688699360341\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.3258088652983956\n",
      "Epoch 27: Train Loss = 0.32491438927857774\n",
      "Epoch 28: Train Loss = 0.3243100022751352\n",
      "Epoch 29: Train Loss = 0.32369710787482886\n",
      "Epoch 30: Train Loss = 0.3229889406846917\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5071739130434783, Precision = 0.9884269181311616\n",
      "Validation: Test Loss = 0.3222432089888531\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5065217391304347, precision = 0.9896995708154507\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3223931932967642\n",
      "Epoch 32: Train Loss = 0.3217336750030518\n",
      "Epoch 33: Train Loss = 0.32140977227169537\n",
      "Epoch 34: Train Loss = 0.32089177800261454\n",
      "Epoch 35: Train Loss = 0.3205452512139859\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5047826086956522, Precision = 0.9931093884582257\n",
      "Validation: Test Loss = 0.31988604892855105\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5039130434782608, precision = 0.994823123382226\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3203063089432924\n",
      "Epoch 37: Train Loss = 0.32015328754549444\n",
      "Epoch 38: Train Loss = 0.319422789397447\n",
      "Epoch 39: Train Loss = 0.319234865333723\n",
      "Epoch 40: Train Loss = 0.31895006464875264\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5034782608695653, Precision = 0.9956822107081175\n",
      "Validation: Test Loss = 0.3189644968509674\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5032608695652174, precision = 0.9961123110151188\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.31901544679766114\n",
      "Epoch 42: Train Loss = 0.31866004684697025\n",
      "Epoch 43: Train Loss = 0.3182973256836767\n",
      "Epoch 44: Train Loss = 0.3182534811289414\n",
      "Epoch 45: Train Loss = 0.31797032174856765\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5023913043478261, Precision = 0.9978364344439636\n",
      "Validation: Test Loss = 0.3176954354410586\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5023913043478261, precision = 0.9978364344439636\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3178389767978502\n",
      "Epoch 47: Train Loss = 0.31763314817262733\n",
      "Epoch 48: Train Loss = 0.3175185201479041\n",
      "Epoch 49: Train Loss = 0.3175449221030526\n",
      "Epoch 50: Train Loss = 0.31727905890215996\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5019565217391304, Precision = 0.9987007362494587\n",
      "Validation: Test Loss = 0.3172605579832326\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5019565217391304, precision = 0.9987007362494587\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3173126007681308\n",
      "Epoch 52: Train Loss = 0.31716311926427093\n",
      "Epoch 53: Train Loss = 0.3171753538173178\n",
      "Epoch 54: Train Loss = 0.3170203779573026\n",
      "Epoch 55: Train Loss = 0.3170174059142237\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5019565217391304, Precision = 0.9987007362494587\n",
      "Validation: Test Loss = 0.31685384517130644\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5019565217391304, precision = 0.9987007362494587\n",
      "\n",
      "Epoch 56: Train Loss = 0.31690698255663335\n",
      "Epoch 57: Train Loss = 0.3168840242987094\n",
      "Epoch 58: Train Loss = 0.3167947943832563\n",
      "Epoch 59: Train Loss = 0.31682330639465994\n",
      "Epoch 60: Train Loss = 0.31668688566788383\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5019565217391304, Precision = 0.9987007362494587\n",
      "Validation: Test Loss = 0.3165334807789844\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.3166626897065536\n",
      "Epoch 62: Train Loss = 0.3166373360675314\n",
      "Epoch 63: Train Loss = 0.31670616035876065\n",
      "Epoch 64: Train Loss = 0.31662225832109864\n",
      "Epoch 65: Train Loss = 0.316484287718068\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, Precision = 0.9991334488734835\n",
      "Validation: Test Loss = 0.31633996564409006\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "\n",
      "Epoch 66: Train Loss = 0.31647606678630996\n",
      "Epoch 67: Train Loss = 0.31643385710923566\n",
      "Epoch 68: Train Loss = 0.31649379740590633\n",
      "Epoch 69: Train Loss = 0.31641366087872047\n",
      "Epoch 70: Train Loss = 0.3163370496812074\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, Precision = 0.9991334488734835\n",
      "Validation: Test Loss = 0.3162203373598016\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "\n",
      "Epoch 71: Train Loss = 0.3163278867887414\n",
      "Epoch 72: Train Loss = 0.3164326210643934\n",
      "Epoch 73: Train Loss = 0.3163205775488978\n",
      "Epoch 74: Train Loss = 0.31634952125342\n",
      "Epoch 75: Train Loss = 0.3163323818600696\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, Precision = 0.9991334488734835\n",
      "Validation: Test Loss = 0.31616510494895605\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "\n",
      "Epoch 76: Train Loss = 0.31621765655020007\n",
      "Epoch 77: Train Loss = 0.3162918088228806\n",
      "Epoch 78: Train Loss = 0.3162793693853461\n",
      "Epoch 79: Train Loss = 0.3163021602837936\n",
      "Epoch 80: Train Loss = 0.3163589307017948\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, Precision = 0.9991334488734835\n",
      "Validation: Test Loss = 0.31621143460273743\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "\n",
      "Epoch 81: Train Loss = 0.31633197603018387\n",
      "Epoch 82: Train Loss = 0.31632688097331835\n",
      "Epoch 83: Train Loss = 0.3162724064225736\n",
      "Epoch 84: Train Loss = 0.31623916304629784\n",
      "Epoch 85: Train Loss = 0.31629557086073834\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, Precision = 0.9991334488734835\n",
      "Validation: Test Loss = 0.3161120643823043\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "\n",
      "Epoch 86: Train Loss = 0.3161781640156456\n",
      "Epoch 87: Train Loss = 0.31622261938841445\n",
      "Epoch 88: Train Loss = 0.31614422186561253\n",
      "Epoch 89: Train Loss = 0.316105845026348\n",
      "Epoch 90: Train Loss = 0.31628684017969216\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, Precision = 0.9991334488734835\n",
      "Validation: Test Loss = 0.31631067395210266\n",
      "Recall = 0.9987007362494587, Aging Rate = 0.5017391304347826, precision = 0.9991334488734835\n",
      "\n",
      "Epoch 91: Train Loss = 0.31620631202407506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.31624172179595283\n",
      "Epoch 93: Train Loss = 0.3160011104915453\n",
      "Epoch 94: Train Loss = 0.3163051262627477\n",
      "Epoch 95: Train Loss = 0.31589536283327185\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.315739464500676\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.3157693060066389\n",
      "Epoch 97: Train Loss = 0.31593355847441634\n",
      "Epoch 98: Train Loss = 0.3158188245089158\n",
      "Epoch 99: Train Loss = 0.315885987955591\n",
      "Epoch 100: Train Loss = 0.31585677535637563\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.3156160004760908\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 101: Train Loss = 0.3157860970497131\n",
      "Epoch 102: Train Loss = 0.3158609239951424\n",
      "Epoch 103: Train Loss = 0.31583758297173875\n",
      "Epoch 104: Train Loss = 0.31575570287911786\n",
      "Epoch 105: Train Loss = 0.3159708327314128\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.3156670842481696\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 106: Train Loss = 0.3158549408808998\n",
      "Epoch 107: Train Loss = 0.3159585171678792\n",
      "Epoch 108: Train Loss = 0.31600249005400616\n",
      "Epoch 109: Train Loss = 0.31602952687636665\n",
      "Epoch 110: Train Loss = 0.31577930092811585\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.31604266234066175\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 111: Train Loss = 0.3157286265103713\n",
      "Epoch 112: Train Loss = 0.3158096082314201\n",
      "Epoch 113: Train Loss = 0.3157771915456523\n",
      "Epoch 114: Train Loss = 0.31580363491307134\n",
      "Epoch 115: Train Loss = 0.31581903379896414\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.3156381707087807\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 116: Train Loss = 0.3157501140884731\n",
      "Epoch 117: Train Loss = 0.31580176700716434\n",
      "Epoch 118: Train Loss = 0.3157326367108718\n",
      "Epoch 119: Train Loss = 0.31586236901905224\n",
      "Epoch 120: Train Loss = 0.3159131695913232\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.3157836767901545\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 121: Train Loss = 0.3158381379687268\n",
      "Epoch 122: Train Loss = 0.31586182615031366\n",
      "Epoch 123: Train Loss = 0.3158822559792063\n",
      "Epoch 124: Train Loss = 0.3158374679606894\n",
      "Epoch 125: Train Loss = 0.3158219009378682\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.3155836341692054\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 126: Train Loss = 0.31579029316487517\n",
      "Epoch 127: Train Loss = 0.31586748216463173\n",
      "Epoch 128: Train Loss = 0.31590460994969244\n",
      "Epoch 129: Train Loss = 0.3157635770673337\n",
      "Epoch 130: Train Loss = 0.31585731641105985\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.31568026361258134\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 131: Train Loss = 0.3158395927885304\n",
      "Epoch 132: Train Loss = 0.31573167251503986\n",
      "Epoch 133: Train Loss = 0.3157776678645092\n",
      "Epoch 134: Train Loss = 0.31586510741192364\n",
      "Epoch 135: Train Loss = 0.31583409267923107\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.3156158142504485\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 136: Train Loss = 0.3157218021413554\n",
      "Epoch 137: Train Loss = 0.3157350907118424\n",
      "Epoch 138: Train Loss = 0.3158057813540749\n",
      "Epoch 139: Train Loss = 0.3158250824783159\n",
      "Epoch 140: Train Loss = 0.3158238933397376\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.31576070116913835\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Epoch 141: Train Loss = 0.3157747209072113\n",
      "Epoch 142: Train Loss = 0.31589966286783633\n",
      "Epoch 143: Train Loss = 0.31580613799717117\n",
      "Epoch 144: Train Loss = 0.3159303642874179\n",
      "Epoch 145: Train Loss = 0.31575612659039703\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, Precision = 0.9991338241663058\n",
      "Validation: Test Loss = 0.31563175092572754\n",
      "Recall = 0.9991338241663058, Aging Rate = 0.5019565217391304, precision = 0.9991338241663058\n",
      "\n",
      "Training Finished at epoch 145.\n",
      "Validation: Test Loss = 0.32306799008917775\n",
      "Recall = 0.9934036939313984, Aging Rate = 0.49674054758800523, precision = 0.9881889763779528\n",
      "\u001b[32m[I 2022-05-26 16:18:23,695]\u001b[0m Trial 7 finished with value: 0.9888488775825913 and parameters: {'batch_size': 64, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'bad_weight': 0.7}. Best is trial 7 with value: 0.9888488775825913.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6959c1109cf48489ab0bbb7c441d114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6874295577795609\n",
      "Epoch 2: Train Loss = 0.6811176437916963\n",
      "Epoch 3: Train Loss = 0.6746947849315146\n",
      "Epoch 4: Train Loss = 0.6685222796771837\n",
      "Epoch 5: Train Loss = 0.6634660395332005\n",
      "Recall = 0.2813321647677476, Aging Rate = 0.15065217391304347, Precision = 0.9264069264069265\n",
      "Validation: Test Loss = 0.6590500325741975\n",
      "Recall = 0.261612620508326, Aging Rate = 0.13717391304347826, precision = 0.9461172741679873\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6558009345635124\n",
      "Epoch 7: Train Loss = 0.6481774447275245\n",
      "Epoch 8: Train Loss = 0.6398255943215412\n",
      "Epoch 9: Train Loss = 0.6307247667727263\n",
      "Epoch 10: Train Loss = 0.6211428767701854\n",
      "Recall = 0.5902716914986854, Aging Rate = 0.33021739130434785, Precision = 0.8867676102699145\n",
      "Validation: Test Loss = 0.6159195942464082\n",
      "Recall = 0.6126205083260298, Aging Rate = 0.34391304347826085, precision = 0.8836915297092288\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.6113145127503768\n",
      "Epoch 12: Train Loss = 0.6016177073768948\n",
      "Epoch 13: Train Loss = 0.592178622017736\n",
      "Epoch 14: Train Loss = 0.5832391562669174\n",
      "Epoch 15: Train Loss = 0.5747527831533681\n",
      "Recall = 0.7322524101665207, Aging Rate = 0.4265217391304348, Precision = 0.8516819571865444\n",
      "Validation: Test Loss = 0.5703333214054936\n",
      "Recall = 0.7248028045574058, Aging Rate = 0.4210869565217391, precision = 0.8538977800722767\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5667121288050776\n",
      "Epoch 17: Train Loss = 0.5593785322230795\n",
      "Epoch 18: Train Loss = 0.5523845382358716\n",
      "Epoch 19: Train Loss = 0.5460239598025446\n",
      "Epoch 20: Train Loss = 0.5400718342739603\n",
      "Recall = 0.7738825591586328, Aging Rate = 0.4519565217391304, Precision = 0.8494468494468495\n",
      "Validation: Test Loss = 0.5370335810080819\n",
      "Recall = 0.7874671340929009, Aging Rate = 0.4617391304347826, precision = 0.846045197740113\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5346117109837739\n",
      "Epoch 22: Train Loss = 0.5296077397595281\n",
      "Epoch 23: Train Loss = 0.524967591866203\n",
      "Epoch 24: Train Loss = 0.5205366774227308\n",
      "Epoch 25: Train Loss = 0.5163648898705192\n",
      "Recall = 0.8137598597721297, Aging Rate = 0.47608695652173916, Precision = 0.8479452054794521\n",
      "Validation: Test Loss = 0.5141419259361599\n",
      "Recall = 0.8207712532865907, Aging Rate = 0.48, precision = 0.8482789855072463\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5126158851644267\n",
      "Epoch 27: Train Loss = 0.5088007401383441\n",
      "Epoch 28: Train Loss = 0.5052689639900042\n",
      "Epoch 29: Train Loss = 0.5020495230218639\n",
      "Epoch 30: Train Loss = 0.4987632512009662\n",
      "Recall = 0.8347940403155127, Aging Rate = 0.48130434782608694, Precision = 0.8604336043360433\n",
      "Validation: Test Loss = 0.4968706348667974\n",
      "Recall = 0.8418054338299737, Aging Rate = 0.48934782608695654, precision = 0.8533984895601955\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4956951252273891\n",
      "Epoch 32: Train Loss = 0.4927492302397023\n",
      "Epoch 33: Train Loss = 0.48996415957160616\n",
      "Epoch 34: Train Loss = 0.4872866011184195\n",
      "Epoch 35: Train Loss = 0.48462815740834114\n",
      "Recall = 0.8531989482909729, Aging Rate = 0.49043478260869566, Precision = 0.863031914893617\n",
      "Validation: Test Loss = 0.48311019192571225\n",
      "Recall = 0.8523225241016652, Aging Rate = 0.4867391304347826, precision = 0.868691380080393\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4821389300926872\n",
      "Epoch 37: Train Loss = 0.4798429031994032\n",
      "Epoch 38: Train Loss = 0.47735021658565685\n",
      "Epoch 39: Train Loss = 0.47504525277925574\n",
      "Epoch 40: Train Loss = 0.47287870510764746\n",
      "Recall = 0.8628396143733567, Aging Rate = 0.48891304347826087, Precision = 0.8755002223210315\n",
      "Validation: Test Loss = 0.4714808681736822\n",
      "Recall = 0.8593339176161262, Aging Rate = 0.48586956521739133, precision = 0.8774049217002237\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.4706850870277571\n",
      "Epoch 42: Train Loss = 0.46863103897675223\n",
      "Epoch 43: Train Loss = 0.4666151319379392\n",
      "Epoch 44: Train Loss = 0.46464018153107683\n",
      "Epoch 45: Train Loss = 0.46263971919598784\n",
      "Recall = 0.871603856266433, Aging Rate = 0.4895652173913044, Precision = 0.883214920071048\n",
      "Validation: Test Loss = 0.46140612058017566\n",
      "Recall = 0.871603856266433, Aging Rate = 0.48934782608695654, precision = 0.8836072856508218\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.46079067981761435\n",
      "Epoch 47: Train Loss = 0.4588982337454091\n",
      "Epoch 48: Train Loss = 0.4571192449072133\n",
      "Epoch 49: Train Loss = 0.4553466306043708\n",
      "Epoch 50: Train Loss = 0.4536531176774398\n",
      "Recall = 0.8939526730937774, Aging Rate = 0.4973913043478261, Precision = 0.8916083916083916\n",
      "Validation: Test Loss = 0.45245454969613447\n",
      "Recall = 0.8913234005258545, Aging Rate = 0.4930434782608696, precision = 0.8968253968253969\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.45192047652990924\n",
      "Epoch 52: Train Loss = 0.45017833901488263\n",
      "Epoch 53: Train Loss = 0.4485536252934\n",
      "Epoch 54: Train Loss = 0.4469856970724852\n",
      "Epoch 55: Train Loss = 0.4453532596774723\n",
      "Recall = 0.9049079754601227, Aging Rate = 0.49956521739130433, Precision = 0.8986074847693647\n",
      "Validation: Test Loss = 0.444332731547563\n",
      "Recall = 0.9049079754601227, Aging Rate = 0.49630434782608696, precision = 0.9045116075339465\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.443890258592108\n",
      "Epoch 57: Train Loss = 0.44229130900424457\n",
      "Epoch 58: Train Loss = 0.44077852803727857\n",
      "Epoch 59: Train Loss = 0.43923590841500654\n",
      "Epoch 60: Train Loss = 0.43773658177127006\n",
      "Recall = 0.9066608238387379, Aging Rate = 0.4945652173913043, Precision = 0.9094505494505495\n",
      "Validation: Test Loss = 0.43672060225320897\n",
      "Recall = 0.9070990359333918, Aging Rate = 0.495, precision = 0.9090909090909091\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.43633955981420436\n",
      "Epoch 62: Train Loss = 0.43482198678928874\n",
      "Epoch 63: Train Loss = 0.4333788542643837\n",
      "Epoch 64: Train Loss = 0.4319950939261395\n",
      "Epoch 65: Train Loss = 0.4306520594202954\n",
      "Recall = 0.9070990359333918, Aging Rate = 0.4910869565217391, Precision = 0.9163346613545816\n",
      "Validation: Test Loss = 0.4295760692720828\n",
      "Recall = 0.908851884312007, Aging Rate = 0.49130434782608695, precision = 0.9176991150442478\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4290928608438243\n",
      "Epoch 67: Train Loss = 0.4277218468811201\n",
      "Epoch 68: Train Loss = 0.4263648864497309\n",
      "Epoch 69: Train Loss = 0.4250248478806537\n",
      "Epoch 70: Train Loss = 0.4236954691617385\n",
      "Recall = 0.9119193689745837, Aging Rate = 0.4917391304347826, Precision = 0.9199823165340407\n",
      "Validation: Test Loss = 0.42276051796000935\n",
      "Recall = 0.9119193689745837, Aging Rate = 0.4908695652173913, precision = 0.9216120460584588\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.42234811316365783\n",
      "Epoch 72: Train Loss = 0.4211642092725505\n",
      "Epoch 73: Train Loss = 0.4199771531250166\n",
      "Epoch 74: Train Loss = 0.4185450315993765\n",
      "Epoch 75: Train Loss = 0.41729774791261426\n",
      "Recall = 0.9189307624890447, Aging Rate = 0.49195652173913046, Precision = 0.926646045072912\n",
      "Validation: Test Loss = 0.41648250061532727\n",
      "Recall = 0.9189307624890447, Aging Rate = 0.4884782608695652, precision = 0.9332443257676902\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.41611373538556307\n",
      "Epoch 77: Train Loss = 0.41504201505495153\n",
      "Epoch 78: Train Loss = 0.4139365685504416\n",
      "Epoch 79: Train Loss = 0.41260029611380206\n",
      "Epoch 80: Train Loss = 0.41144341836804926\n",
      "Recall = 0.9338299737072743, Aging Rate = 0.49434782608695654, Precision = 0.9371152154793315\n",
      "Validation: Test Loss = 0.41062656070875087\n",
      "Recall = 0.932077125328659, Aging Rate = 0.49217391304347824, precision = 0.9394876325088339\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.41037357423616494\n",
      "Epoch 82: Train Loss = 0.40942335678183517\n",
      "Epoch 83: Train Loss = 0.4081025661592898\n",
      "Epoch 84: Train Loss = 0.4070832986417024\n",
      "Epoch 85: Train Loss = 0.4060255487587141\n",
      "Recall = 0.932077125328659, Aging Rate = 0.4891304347826087, Precision = 0.9453333333333334\n",
      "Validation: Test Loss = 0.40523590269296067\n",
      "Recall = 0.9325153374233128, Aging Rate = 0.49043478260869566, precision = 0.9432624113475178\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.40499763939691624\n",
      "Epoch 87: Train Loss = 0.4040056305346282\n",
      "Epoch 88: Train Loss = 0.40300846193147744\n",
      "Epoch 89: Train Loss = 0.40199149841847626\n",
      "Epoch 90: Train Loss = 0.40118948578834535\n",
      "Recall = 0.9347063978965819, Aging Rate = 0.4880434782608696, Precision = 0.9501113585746103\n",
      "Validation: Test Loss = 0.40029529359029686\n",
      "Recall = 0.9338299737072743, Aging Rate = 0.48717391304347823, precision = 0.9509147701918786\n",
      "Model in epoch 90 is saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: Train Loss = 0.40012972696967747\n",
      "Epoch 92: Train Loss = 0.39913733891818837\n",
      "Epoch 93: Train Loss = 0.3981830868513688\n",
      "Epoch 94: Train Loss = 0.397299507856369\n",
      "Epoch 95: Train Loss = 0.3963837288255277\n",
      "Recall = 0.9351446099912357, Aging Rate = 0.4860869565217391, Precision = 0.9543828264758497\n",
      "Validation: Test Loss = 0.3956434206340624\n",
      "Recall = 0.9355828220858896, Aging Rate = 0.48543478260869566, precision = 0.9561128526645768\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.39550407238628554\n",
      "Epoch 97: Train Loss = 0.39457247759984887\n",
      "Epoch 98: Train Loss = 0.39371368346006974\n",
      "Epoch 99: Train Loss = 0.39285935204962025\n",
      "Epoch 100: Train Loss = 0.3920555559966875\n",
      "Recall = 0.9395267309377738, Aging Rate = 0.4865217391304348, Precision = 0.9579982126899017\n",
      "Validation: Test Loss = 0.3913547521052153\n",
      "Recall = 0.936897458369851, Aging Rate = 0.48282608695652174, precision = 0.9626294461954075\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.39121688391851345\n",
      "Epoch 102: Train Loss = 0.3903500896433125\n",
      "Epoch 103: Train Loss = 0.3894994916604913\n",
      "Epoch 104: Train Loss = 0.3887232746766961\n",
      "Epoch 105: Train Loss = 0.3879572167603866\n",
      "Recall = 0.9452234881682734, Aging Rate = 0.4860869565217391, Precision = 0.9646690518783542\n",
      "Validation: Test Loss = 0.38728465111359306\n",
      "Recall = 0.9452234881682734, Aging Rate = 0.485, precision = 0.9668310174809502\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.38720149320104846\n",
      "Epoch 107: Train Loss = 0.38651144032892976\n",
      "Epoch 108: Train Loss = 0.38563333640927855\n",
      "Epoch 109: Train Loss = 0.38483903200730035\n",
      "Epoch 110: Train Loss = 0.3840834809904513\n",
      "Recall = 0.9513584574934268, Aging Rate = 0.48695652173913045, Precision = 0.9691964285714286\n",
      "Validation: Test Loss = 0.3835462194422017\n",
      "Recall = 0.950920245398773, Aging Rate = 0.4834782608695652, precision = 0.9757194244604317\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.38341306758963545\n",
      "Epoch 112: Train Loss = 0.38260099431742794\n",
      "Epoch 113: Train Loss = 0.3818872620230136\n",
      "Epoch 114: Train Loss = 0.38121982279031175\n",
      "Epoch 115: Train Loss = 0.380487472648206\n",
      "Recall = 0.9522348816827344, Aging Rate = 0.48456521739130437, Precision = 0.9748766262898161\n",
      "Validation: Test Loss = 0.3798670442208\n",
      "Recall = 0.9522348816827344, Aging Rate = 0.4834782608695652, precision = 0.977068345323741\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.37973391242649246\n",
      "Epoch 117: Train Loss = 0.379076601681502\n",
      "Epoch 118: Train Loss = 0.3783894792847011\n",
      "Epoch 119: Train Loss = 0.37776534723198935\n",
      "Epoch 120: Train Loss = 0.3770914618865303\n",
      "Recall = 0.9526730937773883, Aging Rate = 0.4834782608695652, Precision = 0.9775179856115108\n",
      "Validation: Test Loss = 0.37647516815558724\n",
      "Recall = 0.9522348816827344, Aging Rate = 0.4826086956521739, precision = 0.9788288288288288\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.37637255005214526\n",
      "Epoch 122: Train Loss = 0.37569199702014094\n",
      "Epoch 123: Train Loss = 0.3752114188152811\n",
      "Epoch 124: Train Loss = 0.37447765454002047\n",
      "Epoch 125: Train Loss = 0.3739549392202626\n",
      "Recall = 0.9583698510078879, Aging Rate = 0.48434782608695653, Precision = 0.9815978456014363\n",
      "Validation: Test Loss = 0.373419892217802\n",
      "Recall = 0.9539877300613497, Aging Rate = 0.48130434782608694, precision = 0.9832881662149955\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.3733528234129367\n",
      "Epoch 127: Train Loss = 0.37273121569467627\n",
      "Epoch 128: Train Loss = 0.37211734865022744\n",
      "Epoch 129: Train Loss = 0.3715213604077049\n",
      "Epoch 130: Train Loss = 0.37100788987201194\n",
      "Recall = 0.9675723049956179, Aging Rate = 0.4884782608695652, Precision = 0.9826435246995995\n",
      "Validation: Test Loss = 0.37043287691862686\n",
      "Recall = 0.9666958808063103, Aging Rate = 0.48717391304347823, precision = 0.9843819723337796\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.3704786427642988\n",
      "Epoch 132: Train Loss = 0.3698975279538528\n",
      "Epoch 133: Train Loss = 0.369284485163896\n",
      "Epoch 134: Train Loss = 0.36882475583449653\n",
      "Epoch 135: Train Loss = 0.3683130406815073\n",
      "Recall = 0.9693251533742331, Aging Rate = 0.4882608695652174, Precision = 0.9848619768477292\n",
      "Validation: Test Loss = 0.3678011218879534\n",
      "Recall = 0.9715162138475022, Aging Rate = 0.4891304347826087, precision = 0.9853333333333333\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.36786096811294555\n",
      "Epoch 137: Train Loss = 0.3674415695667267\n",
      "Epoch 138: Train Loss = 0.36675229425015654\n",
      "Epoch 139: Train Loss = 0.3662955873427184\n",
      "Epoch 140: Train Loss = 0.3658142371799635\n",
      "Recall = 0.9715162138475022, Aging Rate = 0.48869565217391303, Precision = 0.9862099644128114\n",
      "Validation: Test Loss = 0.3653606296622235\n",
      "Recall = 0.9702015775635408, Aging Rate = 0.4876086956521739, precision = 0.9870708872046366\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.36532186513361725\n",
      "Epoch 142: Train Loss = 0.3648427103395047\n",
      "Epoch 143: Train Loss = 0.3644891251170117\n",
      "Epoch 144: Train Loss = 0.36391463569972826\n",
      "Epoch 145: Train Loss = 0.36349514017934387\n",
      "Recall = 0.9710780017528484, Aging Rate = 0.4880434782608696, Precision = 0.9870824053452116\n",
      "Validation: Test Loss = 0.3629970784809278\n",
      "Recall = 0.9715162138475022, Aging Rate = 0.4882608695652174, precision = 0.9870881567230633\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.36303087980850884\n",
      "Epoch 147: Train Loss = 0.36259502042894776\n",
      "Epoch 148: Train Loss = 0.3622157069392826\n",
      "Epoch 149: Train Loss = 0.36186105634855187\n",
      "Epoch 150: Train Loss = 0.36139534608177515\n",
      "Recall = 0.9741454864154251, Aging Rate = 0.48934782608695654, Precision = 0.9875610839626833\n",
      "Validation: Test Loss = 0.36089724276376806\n",
      "Recall = 0.9741454864154251, Aging Rate = 0.4895652173913044, precision = 0.9871225577264654\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.36766258554321846\n",
      "Recall = 0.9681528662420382, Aging Rate = 0.5065189048239895, precision = 0.9781209781209781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078db349477e4bf39a0ef4df64bdb022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6954831685190616\n",
      "Epoch 2: Train Loss = 0.6917199330744536\n",
      "Epoch 3: Train Loss = 0.6873131528108016\n",
      "Epoch 4: Train Loss = 0.6827039013738218\n",
      "Epoch 5: Train Loss = 0.6780290034542913\n",
      "Recall = 0.07695652173913044, Aging Rate = 0.042391304347826085, Precision = 0.9076923076923077\n",
      "Validation: Test Loss = 0.6741771284393642\n",
      "Recall = 0.06956521739130435, Aging Rate = 0.035869565217391305, precision = 0.9696969696969697\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6710773685704107\n",
      "Epoch 7: Train Loss = 0.6635334616122038\n",
      "Epoch 8: Train Loss = 0.6545457318554754\n",
      "Epoch 9: Train Loss = 0.6441418747279954\n",
      "Epoch 10: Train Loss = 0.6327060070245162\n",
      "Recall = 0.5552173913043478, Aging Rate = 0.31369565217391304, Precision = 0.884961884961885\n",
      "Validation: Test Loss = 0.6264098758282869\n",
      "Recall = 0.6043478260869565, Aging Rate = 0.3421739130434783, precision = 0.8831003811944091\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.6207667310341545\n",
      "Epoch 12: Train Loss = 0.6090844244542329\n",
      "Epoch 13: Train Loss = 0.5978846258702486\n",
      "Epoch 14: Train Loss = 0.5874184634374535\n",
      "Epoch 15: Train Loss = 0.5778038450945978\n",
      "Recall = 0.7386956521739131, Aging Rate = 0.43717391304347825, Precision = 0.8448533068125311\n",
      "Validation: Test Loss = 0.5728955586060234\n",
      "Recall = 0.7582608695652174, Aging Rate = 0.4489130434782609, precision = 0.8445520581113801\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5689331754394199\n",
      "Epoch 17: Train Loss = 0.5608839669434921\n",
      "Epoch 18: Train Loss = 0.553571426557458\n",
      "Epoch 19: Train Loss = 0.5468124296354211\n",
      "Epoch 20: Train Loss = 0.5406475901603699\n",
      "Recall = 0.7865217391304348, Aging Rate = 0.46043478260869564, Precision = 0.8541076487252125\n",
      "Validation: Test Loss = 0.5374055289185565\n",
      "Recall = 0.8121739130434783, Aging Rate = 0.48, precision = 0.8460144927536232\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5350162334027497\n",
      "Epoch 22: Train Loss = 0.529827375722968\n",
      "Epoch 23: Train Loss = 0.5249691473919412\n",
      "Epoch 24: Train Loss = 0.5205156988682954\n",
      "Epoch 25: Train Loss = 0.5163063016663427\n",
      "Recall = 0.8256521739130435, Aging Rate = 0.483695652173913, Precision = 0.8534831460674157\n",
      "Validation: Test Loss = 0.514040425653043\n",
      "Recall = 0.8330434782608696, Aging Rate = 0.4902173913043478, precision = 0.8496674057649667\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5123702360236126\n",
      "Epoch 27: Train Loss = 0.5086414175448211\n",
      "Epoch 28: Train Loss = 0.5052842537216519\n",
      "Epoch 29: Train Loss = 0.5018267476558685\n",
      "Epoch 30: Train Loss = 0.4987900724618331\n",
      "Recall = 0.8360869565217391, Aging Rate = 0.4852173913043478, Precision = 0.8615591397849462\n",
      "Validation: Test Loss = 0.49702470390693\n",
      "Recall = 0.8434782608695652, Aging Rate = 0.49369565217391304, precision = 0.8542492294143549\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4958274592005688\n",
      "Epoch 32: Train Loss = 0.492881409396296\n",
      "Epoch 33: Train Loss = 0.4902117682539898\n",
      "Epoch 34: Train Loss = 0.48760395744572516\n",
      "Epoch 35: Train Loss = 0.4850223738214244\n",
      "Recall = 0.8586956521739131, Aging Rate = 0.49782608695652175, Precision = 0.8624454148471615\n",
      "Validation: Test Loss = 0.48346416157224903\n",
      "Recall = 0.8547826086956521, Aging Rate = 0.49391304347826087, precision = 0.8653169014084507\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.48259239943131155\n",
      "Epoch 37: Train Loss = 0.48016513430553937\n",
      "Epoch 38: Train Loss = 0.4779060726062111\n",
      "Epoch 39: Train Loss = 0.47561585540356843\n",
      "Epoch 40: Train Loss = 0.47346000090889306\n",
      "Recall = 0.8673913043478261, Aging Rate = 0.4980434782608696, Precision = 0.8707987778262767\n",
      "Validation: Test Loss = 0.4721362953600676\n",
      "Recall = 0.8673913043478261, Aging Rate = 0.49521739130434783, precision = 0.8757682177348551\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.47132147006366565\n",
      "Epoch 42: Train Loss = 0.46921973233637604\n",
      "Epoch 43: Train Loss = 0.467233603467112\n",
      "Epoch 44: Train Loss = 0.4653389001929242\n",
      "Epoch 45: Train Loss = 0.46333358324092366\n",
      "Recall = 0.8821739130434783, Aging Rate = 0.49978260869565216, Precision = 0.882557633753806\n",
      "Validation: Test Loss = 0.462094029550967\n",
      "Recall = 0.8817391304347826, Aging Rate = 0.5004347826086957, precision = 0.8809730668983493\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.46143772576166237\n",
      "Epoch 47: Train Loss = 0.45960823406343876\n",
      "Epoch 48: Train Loss = 0.4578270589787027\n",
      "Epoch 49: Train Loss = 0.455942643829014\n",
      "Epoch 50: Train Loss = 0.4542012086121932\n",
      "Recall = 0.9008695652173913, Aging Rate = 0.5067391304347826, Precision = 0.8888888888888888\n",
      "Validation: Test Loss = 0.45301373414371326\n",
      "Recall = 0.9017391304347826, Aging Rate = 0.5054347826086957, precision = 0.8920430107526882\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.4524767871524977\n",
      "Epoch 52: Train Loss = 0.4507940920021223\n",
      "Epoch 53: Train Loss = 0.4490907725043919\n",
      "Epoch 54: Train Loss = 0.4474981685306715\n",
      "Epoch 55: Train Loss = 0.44595187067985537\n",
      "Recall = 0.9095652173913044, Aging Rate = 0.5060869565217392, Precision = 0.8986254295532646\n",
      "Validation: Test Loss = 0.4448027282694112\n",
      "Recall = 0.9073913043478261, Aging Rate = 0.5008695652173913, precision = 0.9058159722222222\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.44420008648996767\n",
      "Epoch 57: Train Loss = 0.4426526805628901\n",
      "Epoch 58: Train Loss = 0.4412150389215221\n",
      "Epoch 59: Train Loss = 0.43963113364966017\n",
      "Epoch 60: Train Loss = 0.4380933658454729\n",
      "Recall = 0.9156521739130434, Aging Rate = 0.5043478260869565, Precision = 0.9077586206896552\n",
      "Validation: Test Loss = 0.43707496539406154\n",
      "Recall = 0.9156521739130434, Aging Rate = 0.5026086956521739, precision = 0.9108996539792388\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.43662037574726603\n",
      "Epoch 62: Train Loss = 0.43524281776469687\n",
      "Epoch 63: Train Loss = 0.43378553660019586\n",
      "Epoch 64: Train Loss = 0.43244082222814145\n",
      "Epoch 65: Train Loss = 0.43101391833761465\n",
      "Recall = 0.9208695652173913, Aging Rate = 0.5028260869565218, Precision = 0.9156939040207522\n",
      "Validation: Test Loss = 0.4299675160387288\n",
      "Recall = 0.921304347826087, Aging Rate = 0.5026086956521739, precision = 0.9165224913494809\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4296443638075953\n",
      "Epoch 67: Train Loss = 0.4283048983760502\n",
      "Epoch 68: Train Loss = 0.4269105233316836\n",
      "Epoch 69: Train Loss = 0.4256142349346824\n",
      "Epoch 70: Train Loss = 0.42432314406270566\n",
      "Recall = 0.922608695652174, Aging Rate = 0.5010869565217392, Precision = 0.9206073752711497\n",
      "Validation: Test Loss = 0.4233773978896763\n",
      "Recall = 0.9239130434782609, Aging Rate = 0.5015217391304347, precision = 0.9211096662332033\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.4230416140349015\n",
      "Epoch 72: Train Loss = 0.42176685556121496\n",
      "Epoch 73: Train Loss = 0.4205687998688739\n",
      "Epoch 74: Train Loss = 0.41934738889984463\n",
      "Epoch 75: Train Loss = 0.4180826761411584\n",
      "Recall = 0.9291304347826087, Aging Rate = 0.5004347826086957, Precision = 0.9283231972198088\n",
      "Validation: Test Loss = 0.41722428798675537\n",
      "Recall = 0.9343478260869565, Aging Rate = 0.5041304347826087, precision = 0.9266925398878827\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.4169105879638506\n",
      "Epoch 77: Train Loss = 0.41575135205103003\n",
      "Epoch 78: Train Loss = 0.4146136291649031\n",
      "Epoch 79: Train Loss = 0.41352468692738076\n",
      "Epoch 80: Train Loss = 0.4123640303508095\n",
      "Recall = 0.9334782608695652, Aging Rate = 0.5015217391304347, Precision = 0.9306458604247941\n",
      "Validation: Test Loss = 0.41151118195575215\n",
      "Recall = 0.9317391304347826, Aging Rate = 0.5, precision = 0.9317391304347826\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.4112870677139448\n",
      "Epoch 82: Train Loss = 0.410106516558191\n",
      "Epoch 83: Train Loss = 0.4090690730965656\n",
      "Epoch 84: Train Loss = 0.40803300634674405\n",
      "Epoch 85: Train Loss = 0.40705249226611595\n",
      "Recall = 0.9317391304347826, Aging Rate = 0.49826086956521737, Precision = 0.9349912739965096\n",
      "Validation: Test Loss = 0.40630007536514945\n",
      "Recall = 0.9339130434782609, Aging Rate = 0.5019565217391304, precision = 0.9302728453876137\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.40596170197362486\n",
      "Epoch 87: Train Loss = 0.4049204953338789\n",
      "Epoch 88: Train Loss = 0.4039053175242051\n",
      "Epoch 89: Train Loss = 0.4028544939600903\n",
      "Epoch 90: Train Loss = 0.40189315987669905\n",
      "Recall = 0.9343478260869565, Aging Rate = 0.49543478260869567, Precision = 0.9429574374725757\n",
      "Validation: Test Loss = 0.4011037528514862\n",
      "Recall = 0.9378260869565217, Aging Rate = 0.4980434782608696, precision = 0.9415102575294632\n",
      "Model in epoch 90 is saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: Train Loss = 0.4008920967578888\n",
      "Epoch 92: Train Loss = 0.40003613062526866\n",
      "Epoch 93: Train Loss = 0.3990657079219818\n",
      "Epoch 94: Train Loss = 0.3980558910577194\n",
      "Epoch 95: Train Loss = 0.3971073844640151\n",
      "Recall = 0.9391304347826087, Aging Rate = 0.49391304347826087, Precision = 0.9507042253521126\n",
      "Validation: Test Loss = 0.3963698778981748\n",
      "Recall = 0.9369565217391305, Aging Rate = 0.49195652173913046, precision = 0.9522757401679187\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.39611348375030186\n",
      "Epoch 97: Train Loss = 0.3952759172605432\n",
      "Epoch 98: Train Loss = 0.39436588624249336\n",
      "Epoch 99: Train Loss = 0.3934295828446098\n",
      "Epoch 100: Train Loss = 0.3925777654544167\n",
      "Recall = 0.9443478260869566, Aging Rate = 0.49521739130434783, Precision = 0.9534679543459175\n",
      "Validation: Test Loss = 0.3919285435780235\n",
      "Recall = 0.9430434782608695, Aging Rate = 0.4923913043478261, precision = 0.9576158940397351\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.39171929898469343\n",
      "Epoch 102: Train Loss = 0.3909538087637528\n",
      "Epoch 103: Train Loss = 0.3900145659757697\n",
      "Epoch 104: Train Loss = 0.38917968936588454\n",
      "Epoch 105: Train Loss = 0.3883250808197519\n",
      "Recall = 0.9452173913043478, Aging Rate = 0.49195652173913046, Precision = 0.9606716747680071\n",
      "Validation: Test Loss = 0.3877192703537319\n",
      "Recall = 0.9460869565217391, Aging Rate = 0.49369565217391304, precision = 0.9581682078379569\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.38758240150368733\n",
      "Epoch 107: Train Loss = 0.38675576712774196\n",
      "Epoch 108: Train Loss = 0.38608833178229957\n",
      "Epoch 109: Train Loss = 0.3851346433162689\n",
      "Epoch 110: Train Loss = 0.3844418337034142\n",
      "Recall = 0.9508695652173913, Aging Rate = 0.49478260869565216, Precision = 0.960896309314587\n",
      "Validation: Test Loss = 0.3837967488558396\n",
      "Recall = 0.9508695652173913, Aging Rate = 0.49369565217391304, precision = 0.9630118890356671\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3837670464101045\n",
      "Epoch 112: Train Loss = 0.3829621432138526\n",
      "Epoch 113: Train Loss = 0.3822675025981406\n",
      "Epoch 114: Train Loss = 0.3815036611971648\n",
      "Epoch 115: Train Loss = 0.3808134809265966\n",
      "Recall = 0.957391304347826, Aging Rate = 0.4945652173913043, Precision = 0.9679120879120879\n",
      "Validation: Test Loss = 0.38015604739603787\n",
      "Recall = 0.957391304347826, Aging Rate = 0.49434782608695654, precision = 0.9683377308707124\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.38009233609489773\n",
      "Epoch 117: Train Loss = 0.379431856766991\n",
      "Epoch 118: Train Loss = 0.37873024924941684\n",
      "Epoch 119: Train Loss = 0.3781275031359299\n",
      "Epoch 120: Train Loss = 0.37749681073686353\n",
      "Recall = 0.9595652173913043, Aging Rate = 0.49478260869565216, Precision = 0.9696836555360281\n",
      "Validation: Test Loss = 0.37704190715499547\n",
      "Recall = 0.9604347826086956, Aging Rate = 0.4960869565217391, precision = 0.9680105170902716\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3767771310909935\n",
      "Epoch 122: Train Loss = 0.37617274071859275\n",
      "Epoch 123: Train Loss = 0.3755381543221681\n",
      "Epoch 124: Train Loss = 0.3748773346258246\n",
      "Epoch 125: Train Loss = 0.3742577810909437\n",
      "Recall = 0.9621739130434782, Aging Rate = 0.49391304347826087, Precision = 0.9740316901408451\n",
      "Validation: Test Loss = 0.3737193743560625\n",
      "Recall = 0.9621739130434782, Aging Rate = 0.4941304347826087, precision = 0.9736031676198856\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.3737589879657911\n",
      "Epoch 127: Train Loss = 0.3730921878503716\n",
      "Epoch 128: Train Loss = 0.37254273813703787\n",
      "Epoch 129: Train Loss = 0.3719723921755086\n",
      "Epoch 130: Train Loss = 0.3713130125273829\n",
      "Recall = 0.9621739130434782, Aging Rate = 0.4934782608695652, Precision = 0.9748898678414097\n",
      "Validation: Test Loss = 0.3708324807104857\n",
      "Recall = 0.9626086956521739, Aging Rate = 0.49369565217391304, precision = 0.9749009247027741\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.37089131179063217\n",
      "Epoch 132: Train Loss = 0.3703065209803374\n",
      "Epoch 133: Train Loss = 0.36976656618325604\n",
      "Epoch 134: Train Loss = 0.36918860264446424\n",
      "Epoch 135: Train Loss = 0.3686534037797347\n",
      "Recall = 0.9660869565217391, Aging Rate = 0.4941304347826087, Precision = 0.9775626924769028\n",
      "Validation: Test Loss = 0.3681836133936177\n",
      "Recall = 0.9669565217391304, Aging Rate = 0.4945652173913043, precision = 0.9775824175824176\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.36820546881012295\n",
      "Epoch 137: Train Loss = 0.36762039873910984\n",
      "Epoch 138: Train Loss = 0.3671521444424339\n",
      "Epoch 139: Train Loss = 0.36675698446190874\n",
      "Epoch 140: Train Loss = 0.3662002978635871\n",
      "Recall = 0.9669565217391304, Aging Rate = 0.4934782608695652, Precision = 0.9797356828193833\n",
      "Validation: Test Loss = 0.36574067183162856\n",
      "Recall = 0.9669565217391304, Aging Rate = 0.49369565217391304, precision = 0.979304271246147\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.3657662696423738\n",
      "Epoch 142: Train Loss = 0.36529457823089934\n",
      "Epoch 143: Train Loss = 0.36488884252050646\n",
      "Epoch 144: Train Loss = 0.3644605445343515\n",
      "Epoch 145: Train Loss = 0.36399138020432514\n",
      "Recall = 0.9686956521739131, Aging Rate = 0.49369565217391304, Precision = 0.9810656098634962\n",
      "Validation: Test Loss = 0.3636939786828082\n",
      "Recall = 0.9721739130434782, Aging Rate = 0.4965217391304348, precision = 0.978984238178634\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.363530920225641\n",
      "Epoch 147: Train Loss = 0.36310489405756413\n",
      "Epoch 148: Train Loss = 0.3627157600029655\n",
      "Epoch 149: Train Loss = 0.3623133785309999\n",
      "Epoch 150: Train Loss = 0.3619413795678512\n",
      "Recall = 0.9721739130434782, Aging Rate = 0.49434782608695654, Precision = 0.9832893579595426\n",
      "Validation: Test Loss = 0.3614964319312054\n",
      "Recall = 0.9747826086956521, Aging Rate = 0.4958695652173913, precision = 0.9829022358614643\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.37376759077921373\n",
      "Recall = 0.9647979139504563, Aging Rate = 0.5, precision = 0.9647979139504563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbbfb20234b4b5796e4a52b133b231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6908706773882327\n",
      "Epoch 2: Train Loss = 0.6861831646380218\n",
      "Epoch 3: Train Loss = 0.6807043170928955\n",
      "Epoch 4: Train Loss = 0.6760486698150635\n",
      "Epoch 5: Train Loss = 0.6693955945968628\n",
      "Recall = 0.2668977469670711, Aging Rate = 0.15760869565217392, Precision = 0.8496551724137931\n",
      "Validation: Test Loss = 0.6658261303279711\n",
      "Recall = 0.28292894280762565, Aging Rate = 0.1617391304347826, precision = 0.8776881720430108\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6623464850757433\n",
      "Epoch 7: Train Loss = 0.654226059809975\n",
      "Epoch 8: Train Loss = 0.6450610316318014\n",
      "Epoch 9: Train Loss = 0.6351307536208112\n",
      "Epoch 10: Train Loss = 0.6247072351497153\n",
      "Recall = 0.5953206239168111, Aging Rate = 0.34891304347826085, Precision = 0.8560747663551402\n",
      "Validation: Test Loss = 0.618998962485272\n",
      "Recall = 0.6117850953206239, Aging Rate = 0.35891304347826086, precision = 0.8552392489400363\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.6139460263044938\n",
      "Epoch 12: Train Loss = 0.6033268633096115\n",
      "Epoch 13: Train Loss = 0.5930718255043029\n",
      "Epoch 14: Train Loss = 0.5833702928086986\n",
      "Epoch 15: Train Loss = 0.5743605928835661\n",
      "Recall = 0.7452339688041595, Aging Rate = 0.4393478260869565, Precision = 0.851063829787234\n",
      "Validation: Test Loss = 0.5697711740369382\n",
      "Recall = 0.7521663778162911, Aging Rate = 0.4434782608695652, precision = 0.8509803921568627\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5659727569248365\n",
      "Epoch 17: Train Loss = 0.5581941175460815\n",
      "Epoch 18: Train Loss = 0.5510143478020377\n",
      "Epoch 19: Train Loss = 0.5444350909150165\n",
      "Epoch 20: Train Loss = 0.5383943896708281\n",
      "Recall = 0.7803292894280762, Aging Rate = 0.4597826086956522, Precision = 0.8515366430260047\n",
      "Validation: Test Loss = 0.5352142478590426\n",
      "Recall = 0.7859618717504333, Aging Rate = 0.46347826086956523, precision = 0.850844277673546\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5328106089260267\n",
      "Epoch 22: Train Loss = 0.527680265903473\n",
      "Epoch 23: Train Loss = 0.5228441634385482\n",
      "Epoch 24: Train Loss = 0.5183333078674648\n",
      "Epoch 25: Train Loss = 0.5142207512648209\n",
      "Recall = 0.8145580589254766, Aging Rate = 0.47978260869565215, Precision = 0.851835070231083\n",
      "Validation: Test Loss = 0.511833173295726\n",
      "Recall = 0.8197573656845754, Aging Rate = 0.48282608695652174, precision = 0.8518685276902296\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5102184372362883\n",
      "Epoch 27: Train Loss = 0.5065346698139025\n",
      "Epoch 28: Train Loss = 0.5030922540892725\n",
      "Epoch 29: Train Loss = 0.4996901013021884\n",
      "Epoch 30: Train Loss = 0.4966215967613718\n",
      "Recall = 0.858318890814558, Aging Rate = 0.5021739130434782, Precision = 0.8575757575757575\n",
      "Validation: Test Loss = 0.49466411196667215\n",
      "Recall = 0.8587521663778163, Aging Rate = 0.4980434782608696, precision = 0.8651243998254038\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4936020717413529\n",
      "Epoch 32: Train Loss = 0.4907560200276582\n",
      "Epoch 33: Train Loss = 0.4878604008840478\n",
      "Epoch 34: Train Loss = 0.485278784710428\n",
      "Epoch 35: Train Loss = 0.48267074958137846\n",
      "Recall = 0.8704506065857885, Aging Rate = 0.5023913043478261, Precision = 0.8693206404154046\n",
      "Validation: Test Loss = 0.4811849070113638\n",
      "Recall = 0.8678509532062392, Aging Rate = 0.4956521739130435, precision = 0.8785087719298246\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4802157366275787\n",
      "Epoch 37: Train Loss = 0.47784693303315534\n",
      "Epoch 38: Train Loss = 0.47556145927180415\n",
      "Epoch 39: Train Loss = 0.4732956824613654\n",
      "Epoch 40: Train Loss = 0.4710913038253784\n",
      "Recall = 0.8778162911611785, Aging Rate = 0.49869565217391304, Precision = 0.8831734960767219\n",
      "Validation: Test Loss = 0.46974326107812964\n",
      "Recall = 0.8834488734835355, Aging Rate = 0.5010869565217392, precision = 0.8845986984815618\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.46894403804903445\n",
      "Epoch 42: Train Loss = 0.46686511863832886\n",
      "Epoch 43: Train Loss = 0.4649439928324326\n",
      "Epoch 44: Train Loss = 0.4629270307395769\n",
      "Epoch 45: Train Loss = 0.4610737780384395\n",
      "Recall = 0.8929809358752167, Aging Rate = 0.5043478260869565, Precision = 0.8883620689655173\n",
      "Validation: Test Loss = 0.45974259252133576\n",
      "Recall = 0.8929809358752167, Aging Rate = 0.5030434782608696, precision = 0.8906655142610199\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.45908399193183236\n",
      "Epoch 47: Train Loss = 0.4572780258759208\n",
      "Epoch 48: Train Loss = 0.4555032665833183\n",
      "Epoch 49: Train Loss = 0.45367874783018364\n",
      "Epoch 50: Train Loss = 0.4519561832365782\n",
      "Recall = 0.8981802426343154, Aging Rate = 0.5028260869565218, Precision = 0.8962386511024644\n",
      "Validation: Test Loss = 0.45078268299932067\n",
      "Recall = 0.8977469670710572, Aging Rate = 0.5006521739130435, precision = 0.8996960486322189\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.45024264812469483\n",
      "Epoch 52: Train Loss = 0.44857068466103595\n",
      "Epoch 53: Train Loss = 0.4468941556370777\n",
      "Epoch 54: Train Loss = 0.4452911421008732\n",
      "Epoch 55: Train Loss = 0.4436861614559008\n",
      "Recall = 0.9120450606585788, Aging Rate = 0.5065217391304347, Precision = 0.9034334763948498\n",
      "Validation: Test Loss = 0.44259995476059294\n",
      "Recall = 0.9120450606585788, Aging Rate = 0.505, precision = 0.9061558329746018\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.4421655784482541\n",
      "Epoch 57: Train Loss = 0.4405799450563348\n",
      "Epoch 58: Train Loss = 0.4390275303177212\n",
      "Epoch 59: Train Loss = 0.4375406339375869\n",
      "Epoch 60: Train Loss = 0.43599584480990533\n",
      "Recall = 0.9163778162911612, Aging Rate = 0.5045652173913043, Precision = 0.9112451529513141\n",
      "Validation: Test Loss = 0.43499776171601334\n",
      "Recall = 0.9168110918544194, Aging Rate = 0.5043478260869565, precision = 0.9120689655172414\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.4345517053811446\n",
      "Epoch 62: Train Loss = 0.4331117700493854\n",
      "Epoch 63: Train Loss = 0.4316928021804146\n",
      "Epoch 64: Train Loss = 0.4302117001492044\n",
      "Epoch 65: Train Loss = 0.428840221684912\n",
      "Recall = 0.9181109185441941, Aging Rate = 0.503695652173913, Precision = 0.9145446698316789\n",
      "Validation: Test Loss = 0.4278293796207594\n",
      "Recall = 0.9202772963604853, Aging Rate = 0.5030434782608696, precision = 0.9178910976663786\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4274624234697093\n",
      "Epoch 67: Train Loss = 0.42614175920901093\n",
      "Epoch 68: Train Loss = 0.42478392937909004\n",
      "Epoch 69: Train Loss = 0.42340553376985635\n",
      "Epoch 70: Train Loss = 0.42208762490231055\n",
      "Recall = 0.9259098786828422, Aging Rate = 0.5028260869565218, Precision = 0.9239083441418072\n",
      "Validation: Test Loss = 0.4212287895057512\n",
      "Recall = 0.9280762564991335, Aging Rate = 0.5015217391304347, precision = 0.9284785435630689\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.42084282019863956\n",
      "Epoch 72: Train Loss = 0.41961330678152003\n",
      "Epoch 73: Train Loss = 0.4182270307644554\n",
      "Epoch 74: Train Loss = 0.41699630379676816\n",
      "Epoch 75: Train Loss = 0.41575030332026275\n",
      "Recall = 0.9267764298093587, Aging Rate = 0.49869565217391304, Precision = 0.9324324324324325\n",
      "Validation: Test Loss = 0.41493119949879853\n",
      "Recall = 0.9285095320623917, Aging Rate = 0.5006521739130435, precision = 0.9305254016500217\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.41447706238083215\n",
      "Epoch 77: Train Loss = 0.4132868654831596\n",
      "Epoch 78: Train Loss = 0.41212024434753086\n",
      "Epoch 79: Train Loss = 0.4109991875938747\n",
      "Epoch 80: Train Loss = 0.4098716904287753\n",
      "Recall = 0.9345753899480069, Aging Rate = 0.5002173913043478, Precision = 0.9374185136897001\n",
      "Validation: Test Loss = 0.4090260856566222\n",
      "Recall = 0.9306759098786829, Aging Rate = 0.4941304347826087, precision = 0.945006599208095\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.40869415412778437\n",
      "Epoch 82: Train Loss = 0.4075128049435823\n",
      "Epoch 83: Train Loss = 0.40644456552422564\n",
      "Epoch 84: Train Loss = 0.4054181903859843\n",
      "Epoch 85: Train Loss = 0.4042336065872856\n",
      "Recall = 0.938474870017331, Aging Rate = 0.49717391304347824, Precision = 0.9470922606034106\n",
      "Validation: Test Loss = 0.40352391134137694\n",
      "Recall = 0.9402079722703639, Aging Rate = 0.49934782608695655, precision = 0.9447104919460165\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.4031707798916361\n",
      "Epoch 87: Train Loss = 0.4021863467278688\n",
      "Epoch 88: Train Loss = 0.4011995093200518\n",
      "Epoch 89: Train Loss = 0.40014050468154577\n",
      "Epoch 90: Train Loss = 0.3991600740992505\n",
      "Recall = 0.9406412478336221, Aging Rate = 0.4945652173913043, Precision = 0.9542857142857143\n",
      "Validation: Test Loss = 0.39840372520944345\n",
      "Recall = 0.9423743500866552, Aging Rate = 0.4969565217391304, precision = 0.9514435695538058\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.39819870451222295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.3972643163929815\n",
      "Epoch 93: Train Loss = 0.39639061440592227\n",
      "Epoch 94: Train Loss = 0.39547240640806114\n",
      "Epoch 95: Train Loss = 0.3945143096861632\n",
      "Recall = 0.9510398613518197, Aging Rate = 0.4989130434782609, Precision = 0.9564270152505446\n",
      "Validation: Test Loss = 0.3938030002946439\n",
      "Recall = 0.9510398613518197, Aging Rate = 0.49717391304347824, precision = 0.9597726278968081\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.39366126065668855\n",
      "Epoch 97: Train Loss = 0.3928174844513769\n",
      "Epoch 98: Train Loss = 0.39184370082357656\n",
      "Epoch 99: Train Loss = 0.39100895943848984\n",
      "Epoch 100: Train Loss = 0.3902571925909623\n",
      "Recall = 0.9532062391681109, Aging Rate = 0.4980434782608696, Precision = 0.9602793539938891\n",
      "Validation: Test Loss = 0.389555143273395\n",
      "Recall = 0.9532062391681109, Aging Rate = 0.4958695652173913, precision = 0.9644892590968873\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.3894345071523086\n",
      "Epoch 102: Train Loss = 0.38859895374463954\n",
      "Epoch 103: Train Loss = 0.38795953558838886\n",
      "Epoch 104: Train Loss = 0.38710362496583356\n",
      "Epoch 105: Train Loss = 0.3863719388713007\n",
      "Recall = 0.9532062391681109, Aging Rate = 0.49478260869565216, Precision = 0.9666080843585237\n",
      "Validation: Test Loss = 0.3857290542125702\n",
      "Recall = 0.9532062391681109, Aging Rate = 0.4960869565217391, precision = 0.9640666082383874\n",
      "\n",
      "Epoch 106: Train Loss = 0.3855463791411856\n",
      "Epoch 107: Train Loss = 0.3847763959221218\n",
      "Epoch 108: Train Loss = 0.3841898863730223\n",
      "Epoch 109: Train Loss = 0.3833678578293842\n",
      "Epoch 110: Train Loss = 0.38266201926314314\n",
      "Recall = 0.9532062391681109, Aging Rate = 0.49391304347826087, Precision = 0.9683098591549296\n",
      "Validation: Test Loss = 0.38207650827324907\n",
      "Recall = 0.9532062391681109, Aging Rate = 0.4923913043478261, precision = 0.9713024282560706\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3819714118605075\n",
      "Epoch 112: Train Loss = 0.3812605120824731\n",
      "Epoch 113: Train Loss = 0.38058692683344303\n",
      "Epoch 114: Train Loss = 0.37996940032295556\n",
      "Epoch 115: Train Loss = 0.3793180939425593\n",
      "Recall = 0.9562391681109186, Aging Rate = 0.4934782608695652, Precision = 0.9722466960352423\n",
      "Validation: Test Loss = 0.37873077045316283\n",
      "Recall = 0.957105719237435, Aging Rate = 0.4945652173913043, precision = 0.970989010989011\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.37863495557204535\n",
      "Epoch 117: Train Loss = 0.3780298540384873\n",
      "Epoch 118: Train Loss = 0.37740679341813793\n",
      "Epoch 119: Train Loss = 0.3767547742180202\n",
      "Epoch 120: Train Loss = 0.37610932059909985\n",
      "Recall = 0.958838821490468, Aging Rate = 0.49391304347826087, Precision = 0.9740316901408451\n",
      "Validation: Test Loss = 0.37567787699077443\n",
      "Recall = 0.9562391681109186, Aging Rate = 0.4908695652173913, precision = 0.9774136403897254\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3755556583922842\n",
      "Epoch 122: Train Loss = 0.37497641879579297\n",
      "Epoch 123: Train Loss = 0.3744290708977243\n",
      "Epoch 124: Train Loss = 0.3738199388462564\n",
      "Epoch 125: Train Loss = 0.37326388846273006\n",
      "Recall = 0.958838821490468, Aging Rate = 0.49282608695652175, Precision = 0.9761799735333039\n",
      "Validation: Test Loss = 0.3727649039807527\n",
      "Recall = 0.9566724436741768, Aging Rate = 0.49043478260869566, precision = 0.9787234042553191\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.37264495191366775\n",
      "Epoch 127: Train Loss = 0.37215497607770176\n",
      "Epoch 128: Train Loss = 0.37170721924823263\n",
      "Epoch 129: Train Loss = 0.3711072756933129\n",
      "Epoch 130: Train Loss = 0.37054478743801944\n",
      "Recall = 0.9636048526863085, Aging Rate = 0.4941304347826087, Precision = 0.9784425868895732\n",
      "Validation: Test Loss = 0.3700091875117758\n",
      "Recall = 0.9666377816291161, Aging Rate = 0.49521739130434783, precision = 0.979367866549605\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.36998319309690725\n",
      "Epoch 132: Train Loss = 0.36948610471642535\n",
      "Epoch 133: Train Loss = 0.36892163660215294\n",
      "Epoch 134: Train Loss = 0.3684103295077448\n",
      "Epoch 135: Train Loss = 0.3679761921322864\n",
      "Recall = 0.9683708838821491, Aging Rate = 0.49521739130434783, Precision = 0.9811237928007024\n",
      "Validation: Test Loss = 0.36746963008590366\n",
      "Recall = 0.970103986135182, Aging Rate = 0.4958695652173913, precision = 0.981587023235423\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.3674661916235219\n",
      "Epoch 137: Train Loss = 0.36699112306470455\n",
      "Epoch 138: Train Loss = 0.36650992061780846\n",
      "Epoch 139: Train Loss = 0.3660873704889546\n",
      "Epoch 140: Train Loss = 0.36555223413135696\n",
      "Recall = 0.9683708838821491, Aging Rate = 0.49434782608695654, Precision = 0.9828496042216359\n",
      "Validation: Test Loss = 0.3651294854412908\n",
      "Recall = 0.970103986135182, Aging Rate = 0.4956521739130435, precision = 0.9820175438596491\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.3651958607590717\n",
      "Epoch 142: Train Loss = 0.3646717794045158\n",
      "Epoch 143: Train Loss = 0.364200810815977\n",
      "Epoch 144: Train Loss = 0.363713532893554\n",
      "Epoch 145: Train Loss = 0.36340063364609426\n",
      "Recall = 0.9692374350086655, Aging Rate = 0.495, Precision = 0.9824330259112868\n",
      "Validation: Test Loss = 0.36297628496004186\n",
      "Recall = 0.9679376083188909, Aging Rate = 0.49326086956521736, precision = 0.9845747025121199\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.3629329681914786\n",
      "Epoch 147: Train Loss = 0.36241777337115744\n",
      "Epoch 148: Train Loss = 0.3620383658616439\n",
      "Epoch 149: Train Loss = 0.36160369935243025\n",
      "Epoch 150: Train Loss = 0.3612620158299156\n",
      "Recall = 0.9727036395147314, Aging Rate = 0.49630434782608696, Precision = 0.9833552343407796\n",
      "Validation: Test Loss = 0.3608006916357123\n",
      "Recall = 0.9744367417677643, Aging Rate = 0.49782608695652175, precision = 0.9820960698689957\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.37388031695625773\n",
      "Recall = 0.9591567852437418, Aging Rate = 0.4934810951760104, precision = 0.9616908850726552\n",
      "\u001b[32m[I 2022-05-26 16:19:02,347]\u001b[0m Trial 8 finished with value: 0.966110491061818 and parameters: {'batch_size': 96, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'bad_weight': 0.5}. Best is trial 7 with value: 0.9888488775825913.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b2029f2eb24a588dfa50b01227851c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5450295268970987\n",
      "Epoch 2: Train Loss = 0.49286426544189454\n",
      "Epoch 3: Train Loss = 0.48183677616326703\n",
      "Epoch 4: Train Loss = 0.47653614873471467\n",
      "Epoch 5: Train Loss = 0.4788208824655284\n",
      "Recall = 0.9367364746945899, Aging Rate = 0.633695652173913, Precision = 0.7365351629502573\n",
      "Validation: Test Loss = 0.4643689372746841\n",
      "Recall = 0.9480802792321117, Aging Rate = 0.6119565217391304, precision = 0.7719360568383659\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.46570482984833095\n",
      "Epoch 7: Train Loss = 0.4653468119579813\n",
      "Epoch 8: Train Loss = 0.46275319094243256\n",
      "Epoch 9: Train Loss = 0.45829195831132974\n",
      "Epoch 10: Train Loss = 0.462099193956541\n",
      "Recall = 0.9576788830715532, Aging Rate = 0.6273913043478261, Precision = 0.7605682605682605\n",
      "Validation: Test Loss = 0.46990537140680394\n",
      "Recall = 0.9856020942408377, Aging Rate = 0.7441304347826087, precision = 0.6599474145486416\n",
      "\n",
      "Epoch 11: Train Loss = 0.4624073555158532\n",
      "Epoch 12: Train Loss = 0.45756767744603366\n",
      "Epoch 13: Train Loss = 0.4594438834294029\n",
      "Epoch 14: Train Loss = 0.4613205347372138\n",
      "Epoch 15: Train Loss = 0.4571293308423913\n",
      "Recall = 0.9576788830715532, Aging Rate = 0.6115217391304347, Precision = 0.7803057234269464\n",
      "Validation: Test Loss = 0.4543278584272965\n",
      "Recall = 0.9537521815008726, Aging Rate = 0.59, precision = 0.8054532056005895\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.45757690880609597\n",
      "Epoch 17: Train Loss = 0.4594954748775648\n",
      "Epoch 18: Train Loss = 0.45570479709169137\n",
      "Epoch 19: Train Loss = 0.45835749807565107\n",
      "Epoch 20: Train Loss = 0.4583307244985\n",
      "Recall = 0.9528795811518325, Aging Rate = 0.6176086956521739, Precision = 0.7687434002111933\n",
      "Validation: Test Loss = 0.4530609002838964\n",
      "Recall = 0.9607329842931938, Aging Rate = 0.5897826086956521, precision = 0.8116476225580538\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.4557163811766583\n",
      "Epoch 22: Train Loss = 0.4562380158901215\n",
      "Epoch 23: Train Loss = 0.4588061825088833\n",
      "Epoch 24: Train Loss = 0.4571920042970906\n",
      "Epoch 25: Train Loss = 0.4578909063339233\n",
      "Recall = 0.9598603839441536, Aging Rate = 0.6243478260869565, Precision = 0.766016713091922\n",
      "Validation: Test Loss = 0.4510972123042397\n",
      "Recall = 0.9502617801047121, Aging Rate = 0.5758695652173913, precision = 0.8221970554926388\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.454658571585365\n",
      "Epoch 27: Train Loss = 0.4531404909880265\n",
      "Epoch 28: Train Loss = 0.45868485124214836\n",
      "Epoch 29: Train Loss = 0.45485852930856785\n",
      "Epoch 30: Train Loss = 0.45176612346068673\n",
      "Recall = 0.9611692844677138, Aging Rate = 0.6041304347826087, Precision = 0.79273119827276\n",
      "Validation: Test Loss = 0.4516085954852726\n",
      "Recall = 0.9781849912739965, Aging Rate = 0.6491304347826087, precision = 0.7508372404554589\n",
      "\n",
      "Epoch 31: Train Loss = 0.4541528022807577\n",
      "Epoch 32: Train Loss = 0.45428404326024263\n",
      "Epoch 33: Train Loss = 0.45518473423045613\n",
      "Epoch 34: Train Loss = 0.4536340239773626\n",
      "Epoch 35: Train Loss = 0.45607175941052647\n",
      "Recall = 0.962914485165794, Aging Rate = 0.6202173913043478, Precision = 0.773571678934455\n",
      "Validation: Test Loss = 0.45425301499988724\n",
      "Recall = 0.9236474694589878, Aging Rate = 0.5408695652173913, precision = 0.8508842443729904\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.45145774566608926\n",
      "Epoch 37: Train Loss = 0.46006371311519456\n",
      "Epoch 38: Train Loss = 0.45407977674318395\n",
      "Epoch 39: Train Loss = 0.4542791415297467\n",
      "Epoch 40: Train Loss = 0.45650243323782214\n",
      "Recall = 0.9594240837696335, Aging Rate = 0.6128260869565217, Precision = 0.7800638524299397\n",
      "Validation: Test Loss = 0.458671121182649\n",
      "Recall = 0.930628272251309, Aging Rate = 0.5373913043478261, precision = 0.8628640776699029\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.45164531391599905\n",
      "Epoch 42: Train Loss = 0.4589078417031661\n",
      "Epoch 43: Train Loss = 0.45270813511765523\n",
      "Epoch 44: Train Loss = 0.4512555391892143\n",
      "Epoch 45: Train Loss = 0.4508847048489944\n",
      "Recall = 0.9655322862129145, Aging Rate = 0.6065217391304348, Precision = 0.793189964157706\n",
      "Validation: Test Loss = 0.4489527438516202\n",
      "Recall = 0.974694589877836, Aging Rate = 0.648695652173913, precision = 0.7486595174262735\n",
      "\n",
      "Epoch 46: Train Loss = 0.4519396686035654\n",
      "Epoch 47: Train Loss = 0.45463405324065165\n",
      "Epoch 48: Train Loss = 0.454004793063454\n",
      "Epoch 49: Train Loss = 0.4533191171936367\n",
      "Epoch 50: Train Loss = 0.4511747628709544\n",
      "Recall = 0.9611692844677138, Aging Rate = 0.6097826086956522, Precision = 0.7853832442067736\n",
      "Validation: Test Loss = 0.4445032852628957\n",
      "Recall = 0.9659685863874345, Aging Rate = 0.5995652173913043, precision = 0.8027556200145033\n",
      "\n",
      "Epoch 51: Train Loss = 0.45210359687390533\n",
      "Epoch 52: Train Loss = 0.45467035697854086\n",
      "Epoch 53: Train Loss = 0.45156366897665934\n",
      "Epoch 54: Train Loss = 0.4509689968046935\n",
      "Epoch 55: Train Loss = 0.4493815763100334\n",
      "Recall = 0.9668411867364747, Aging Rate = 0.6030434782608696, Precision = 0.798846431146359\n",
      "Validation: Test Loss = 0.4426881608755692\n",
      "Recall = 0.962478184991274, Aging Rate = 0.5956521739130435, precision = 0.8051094890510949\n",
      "\n",
      "Epoch 56: Train Loss = 0.44974188804626464\n",
      "Epoch 57: Train Loss = 0.45073099986366605\n",
      "Epoch 58: Train Loss = 0.45413360917049905\n",
      "Epoch 59: Train Loss = 0.45561860017154526\n",
      "Epoch 60: Train Loss = 0.45648485536160677\n",
      "Recall = 0.9633507853403142, Aging Rate = 0.6160869565217392, Precision = 0.7791107974594214\n",
      "Validation: Test Loss = 0.44904754638671873\n",
      "Recall = 0.975130890052356, Aging Rate = 0.6269565217391304, precision = 0.7749653259361997\n",
      "\n",
      "Epoch 61: Train Loss = 0.4556301031941953\n",
      "Epoch 62: Train Loss = 0.452866246752117\n",
      "Epoch 63: Train Loss = 0.45065434238185054\n",
      "Epoch 64: Train Loss = 0.4522475448380346\n",
      "Epoch 65: Train Loss = 0.4512902215771053\n",
      "Recall = 0.9611692844677138, Aging Rate = 0.6045652173913043, Precision = 0.7921610931319669\n",
      "Validation: Test Loss = 0.45105773117231285\n",
      "Recall = 0.9773123909249564, Aging Rate = 0.6428260869565218, precision = 0.7575245180926615\n",
      "\n",
      "Epoch 66: Train Loss = 0.4519507352165554\n",
      "Epoch 67: Train Loss = 0.45112008773762247\n",
      "Epoch 68: Train Loss = 0.4511084811065508\n",
      "Epoch 69: Train Loss = 0.4512954777738322\n",
      "Epoch 70: Train Loss = 0.45108885236408397\n",
      "Recall = 0.9655322862129145, Aging Rate = 0.6123913043478261, Precision = 0.7855875044373447\n",
      "Validation: Test Loss = 0.4456893671595532\n",
      "Recall = 0.9725130890052356, Aging Rate = 0.6178260869565217, precision = 0.7843068261787474\n",
      "\n",
      "Epoch 71: Train Loss = 0.45139352953952294\n",
      "Epoch 72: Train Loss = 0.4508657558067985\n",
      "Epoch 73: Train Loss = 0.4514890554676885\n",
      "Epoch 74: Train Loss = 0.45102765959242114\n",
      "Epoch 75: Train Loss = 0.4504449038920195\n",
      "Recall = 0.9655322862129145, Aging Rate = 0.6067391304347826, Precision = 0.7929057685417413\n",
      "Validation: Test Loss = 0.44513793354449066\n",
      "Recall = 0.962914485165794, Aging Rate = 0.6026086956521739, precision = 0.7961760461760462\n",
      "\n",
      "Epoch 76: Train Loss = 0.4507720409268918\n",
      "Epoch 77: Train Loss = 0.45210920090260714\n",
      "Epoch 78: Train Loss = 0.4513441746131234\n",
      "Epoch 79: Train Loss = 0.45162431753200033\n",
      "Epoch 80: Train Loss = 0.4493502926826477\n",
      "Recall = 0.9707678883071553, Aging Rate = 0.6080434782608696, Precision = 0.7954951734000715\n",
      "Validation: Test Loss = 0.4500812770491061\n",
      "Recall = 0.974694589877836, Aging Rate = 0.648695652173913, precision = 0.7486595174262735\n",
      "\n",
      "Epoch 81: Train Loss = 0.44955336042072463\n",
      "Epoch 82: Train Loss = 0.45069135427474977\n",
      "Epoch 83: Train Loss = 0.4509767702351446\n",
      "Epoch 84: Train Loss = 0.4497892723394477\n",
      "Epoch 85: Train Loss = 0.45068586593088894\n",
      "Recall = 0.962478184991274, Aging Rate = 0.6121739130434782, Precision = 0.7833806818181818\n",
      "Validation: Test Loss = 0.4449501748706983\n",
      "Recall = 0.9664048865619547, Aging Rate = 0.5960869565217392, precision = 0.8078045222465354\n",
      "\n",
      "Epoch 86: Train Loss = 0.4488431035435718\n",
      "Epoch 87: Train Loss = 0.4500810070659803\n",
      "Epoch 88: Train Loss = 0.4486057200120843\n",
      "Epoch 89: Train Loss = 0.4488010823726654\n",
      "Epoch 90: Train Loss = 0.4508187915449557\n",
      "Recall = 0.9650959860383944, Aging Rate = 0.6041304347826087, Precision = 0.7959697732997482\n",
      "Validation: Test Loss = 0.4458573992874311\n",
      "Recall = 0.9707678883071553, Aging Rate = 0.6169565217391304, precision = 0.7840028188865398\n",
      "\n",
      "Training Finished at epoch 90.\n",
      "Validation: Test Loss = 0.45514386107433574\n",
      "Recall = 0.9625806451612903, Aging Rate = 0.6303780964797914, precision = 0.7714581178903827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd76c069de6d4783915b8ef22ecfc669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5602470255934674\n",
      "Epoch 2: Train Loss = 0.5026231963738151\n",
      "Epoch 3: Train Loss = 0.4879743528366089\n",
      "Epoch 4: Train Loss = 0.48316486151322074\n",
      "Epoch 5: Train Loss = 0.4815309374228768\n",
      "Recall = 0.9524847428073234, Aging Rate = 0.6619565217391304, Precision = 0.7175697865353038\n",
      "Validation: Test Loss = 0.4652001134209011\n",
      "Recall = 0.9681778552746295, Aging Rate = 0.6580434782608696, precision = 0.7337297654443343\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.4712770799450252\n",
      "Epoch 7: Train Loss = 0.4674052937652754\n",
      "Epoch 8: Train Loss = 0.4672711804120437\n",
      "Epoch 9: Train Loss = 0.46209357121716377\n",
      "Epoch 10: Train Loss = 0.4602799405740655\n",
      "Recall = 0.9629468177855275, Aging Rate = 0.6271739130434782, Precision = 0.765684575389948\n",
      "Validation: Test Loss = 0.46084801565045896\n",
      "Recall = 0.9533565823888405, Aging Rate = 0.5915217391304348, precision = 0.8037486218302095\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.4614876604598501\n",
      "Epoch 12: Train Loss = 0.46361936045729596\n",
      "Epoch 13: Train Loss = 0.459357581605082\n",
      "Epoch 14: Train Loss = 0.46053056499232414\n",
      "Epoch 15: Train Loss = 0.46061392913693966\n",
      "Recall = 0.958151700087184, Aging Rate = 0.6247826086956522, Precision = 0.7647877522616562\n",
      "Validation: Test Loss = 0.4580811047554016\n",
      "Recall = 0.9673060156931125, Aging Rate = 0.643695652173913, precision = 0.7494089834515366\n",
      "\n",
      "Epoch 16: Train Loss = 0.45994977634886036\n",
      "Epoch 17: Train Loss = 0.4553780600299006\n",
      "Epoch 18: Train Loss = 0.45926696663317473\n",
      "Epoch 19: Train Loss = 0.4606748778405397\n",
      "Epoch 20: Train Loss = 0.4590760545108629\n",
      "Recall = 0.9655623365300785, Aging Rate = 0.6291304347826087, Precision = 0.7653766413268832\n",
      "Validation: Test Loss = 0.4543810794146165\n",
      "Recall = 0.986050566695728, Aging Rate = 0.6519565217391304, precision = 0.7542514171390463\n",
      "\n",
      "Epoch 21: Train Loss = 0.45752568353777345\n",
      "Epoch 22: Train Loss = 0.46079113348670625\n",
      "Epoch 23: Train Loss = 0.4610236756697945\n",
      "Epoch 24: Train Loss = 0.45870562221692956\n",
      "Epoch 25: Train Loss = 0.4569816666064055\n",
      "Recall = 0.9655623365300785, Aging Rate = 0.6221739130434782, Precision = 0.7739343116701607\n",
      "Validation: Test Loss = 0.45203760287036066\n",
      "Recall = 0.9795117698343505, Aging Rate = 0.6369565217391304, precision = 0.7668941979522185\n",
      "\n",
      "Epoch 26: Train Loss = 0.45951410200284876\n",
      "Epoch 27: Train Loss = 0.456347424051036\n",
      "Epoch 28: Train Loss = 0.4577858426778213\n",
      "Epoch 29: Train Loss = 0.4556201623833698\n",
      "Epoch 30: Train Loss = 0.4559344215496727\n",
      "Recall = 0.9673060156931125, Aging Rate = 0.6236956521739131, Precision = 0.7734402230742419\n",
      "Validation: Test Loss = 0.4505920636135599\n",
      "Recall = 0.9755884917175239, Aging Rate = 0.625, precision = 0.7784347826086957\n",
      "\n",
      "Epoch 31: Train Loss = 0.45362981464551844\n",
      "Epoch 32: Train Loss = 0.45509028346642205\n",
      "Epoch 33: Train Loss = 0.4537018739140552\n",
      "Epoch 34: Train Loss = 0.4561313437378925\n",
      "Epoch 35: Train Loss = 0.45442401621652684\n",
      "Recall = 0.9707933740191804, Aging Rate = 0.6217391304347826, Precision = 0.7786713286713287\n",
      "Validation: Test Loss = 0.4520173581268476\n",
      "Recall = 0.9681778552746295, Aging Rate = 0.6052173913043478, precision = 0.7977729885057471\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4539791271479233\n",
      "Epoch 37: Train Loss = 0.4590332738731218\n",
      "Epoch 38: Train Loss = 0.45608022866041764\n",
      "Epoch 39: Train Loss = 0.4557347608130911\n",
      "Epoch 40: Train Loss = 0.4571619508059128\n",
      "Recall = 0.9681778552746295, Aging Rate = 0.628695652173913, Precision = 0.7679806362378977\n",
      "Validation: Test Loss = 0.45136314537214195\n",
      "Recall = 0.9734088927637314, Aging Rate = 0.6067391304347826, precision = 0.8000716589036188\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.45509079134982566\n",
      "Epoch 42: Train Loss = 0.45392285777174907\n",
      "Epoch 43: Train Loss = 0.4555252052390057\n",
      "Epoch 44: Train Loss = 0.45704596478006115\n",
      "Epoch 45: Train Loss = 0.4554445439836253\n",
      "Recall = 0.9646904969485615, Aging Rate = 0.6189130434782608, Precision = 0.7773094485423253\n",
      "Validation: Test Loss = 0.4590858651244122\n",
      "Recall = 0.9908456843940715, Aging Rate = 0.6960869565217391, precision = 0.7098688319800125\n",
      "\n",
      "Epoch 46: Train Loss = 0.45661830751792243\n",
      "Epoch 47: Train Loss = 0.45257096601569136\n",
      "Epoch 48: Train Loss = 0.4542450660207997\n",
      "Epoch 49: Train Loss = 0.4540904699201169\n",
      "Epoch 50: Train Loss = 0.4563390809556712\n",
      "Recall = 0.9638186573670444, Aging Rate = 0.6236956521739131, Precision = 0.7706517950505403\n",
      "Validation: Test Loss = 0.44840375527091647\n",
      "Recall = 0.9768962510897995, Aging Rate = 0.615, precision = 0.792152704135737\n",
      "\n",
      "Epoch 51: Train Loss = 0.45411156602527786\n",
      "Epoch 52: Train Loss = 0.45370768930601035\n",
      "Epoch 53: Train Loss = 0.4549746263545492\n",
      "Epoch 54: Train Loss = 0.4542998020545296\n",
      "Epoch 55: Train Loss = 0.45466358112252275\n",
      "Recall = 0.966870095902354, Aging Rate = 0.616304347826087, Precision = 0.7823633156966491\n",
      "Validation: Test Loss = 0.453361111153727\n",
      "Recall = 0.9864864864864865, Aging Rate = 0.662608695652174, precision = 0.7424540682414699\n",
      "\n",
      "Epoch 56: Train Loss = 0.4548664464639581\n",
      "Epoch 57: Train Loss = 0.4541947145565696\n",
      "Epoch 58: Train Loss = 0.4528587097707002\n",
      "Epoch 59: Train Loss = 0.4536623514216879\n",
      "Epoch 60: Train Loss = 0.4541715887836788\n",
      "Recall = 0.9699215344376635, Aging Rate = 0.6247826086956522, Precision = 0.7741823242867084\n",
      "Validation: Test Loss = 0.45180331276810687\n",
      "Recall = 0.9873583260680034, Aging Rate = 0.6623913043478261, precision = 0.7433541188053824\n",
      "\n",
      "Epoch 61: Train Loss = 0.45415223271950433\n",
      "Epoch 62: Train Loss = 0.4524005353450775\n",
      "Epoch 63: Train Loss = 0.4532801726590032\n",
      "Epoch 64: Train Loss = 0.4564375294809756\n",
      "Epoch 65: Train Loss = 0.4550388151148091\n",
      "Recall = 0.9664341761115954, Aging Rate = 0.6219565217391304, Precision = 0.7749038797623209\n",
      "Validation: Test Loss = 0.45088426398194353\n",
      "Recall = 0.9821272885789015, Aging Rate = 0.6306521739130435, precision = 0.7766287487073423\n",
      "\n",
      "Epoch 66: Train Loss = 0.4559209156036377\n",
      "Epoch 67: Train Loss = 0.45329307333282803\n",
      "Epoch 68: Train Loss = 0.4530352518869483\n",
      "Epoch 69: Train Loss = 0.4564964977554653\n",
      "Epoch 70: Train Loss = 0.45375086457832997\n",
      "Recall = 0.96512641673932, Aging Rate = 0.6165217391304347, Precision = 0.7806770098730607\n",
      "Validation: Test Loss = 0.4577145221440688\n",
      "Recall = 0.9908456843940715, Aging Rate = 0.6971739130434783, precision = 0.7087620829435609\n",
      "\n",
      "Epoch 71: Train Loss = 0.4541730006881382\n",
      "Epoch 72: Train Loss = 0.4519204686517301\n",
      "Epoch 73: Train Loss = 0.45201028476590693\n",
      "Epoch 74: Train Loss = 0.45261101038559626\n",
      "Epoch 75: Train Loss = 0.4547200030347575\n",
      "Recall = 0.966870095902354, Aging Rate = 0.6247826086956522, Precision = 0.7717466945024356\n",
      "Validation: Test Loss = 0.44911331156025763\n",
      "Recall = 0.9777680906713164, Aging Rate = 0.6071739130434782, precision = 0.8030791263873971\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.4542216221664263\n",
      "Epoch 77: Train Loss = 0.45824524231578995\n",
      "Epoch 78: Train Loss = 0.4566050153711568\n",
      "Epoch 79: Train Loss = 0.453968426818433\n",
      "Epoch 80: Train Loss = 0.45458319347837695\n",
      "Recall = 0.9681778552746295, Aging Rate = 0.6182608695652174, Precision = 0.7809423347398031\n",
      "Validation: Test Loss = 0.44807621961054594\n",
      "Recall = 0.979075850043592, Aging Rate = 0.6182608695652174, precision = 0.789732770745429\n",
      "\n",
      "Epoch 81: Train Loss = 0.45409777677577473\n",
      "Epoch 82: Train Loss = 0.4523139606869739\n",
      "Epoch 83: Train Loss = 0.4546062902782274\n",
      "Epoch 84: Train Loss = 0.45318322710368947\n",
      "Epoch 85: Train Loss = 0.45185909955397896\n",
      "Recall = 0.9725370531822145, Aging Rate = 0.6202173913043478, Precision = 0.7819838766211006\n",
      "Validation: Test Loss = 0.4485403467261273\n",
      "Recall = 0.9655623365300785, Aging Rate = 0.6121739130434782, precision = 0.7865767045454546\n",
      "\n",
      "Epoch 86: Train Loss = 0.4536103538326595\n",
      "Epoch 87: Train Loss = 0.45166984490726303\n",
      "Epoch 88: Train Loss = 0.4533629394614178\n",
      "Epoch 89: Train Loss = 0.45317144124404246\n",
      "Epoch 90: Train Loss = 0.4532476216813792\n",
      "Recall = 0.974716652136007, Aging Rate = 0.6230434782608696, Precision = 0.7801814375436148\n",
      "Validation: Test Loss = 0.4490598464012146\n",
      "Recall = 0.9568439407149084, Aging Rate = 0.5810869565217391, precision = 0.821174710063599\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.4535852754116058\n",
      "Epoch 92: Train Loss = 0.45383677430774855\n",
      "Epoch 93: Train Loss = 0.4567224224754002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: Train Loss = 0.45307318920674533\n",
      "Epoch 95: Train Loss = 0.45177570052768873\n",
      "Recall = 0.972972972972973, Aging Rate = 0.6202173913043478, Precision = 0.7823343848580442\n",
      "Validation: Test Loss = 0.44883858520051706\n",
      "Recall = 0.978204010462075, Aging Rate = 0.6195652173913043, precision = 0.7873684210526316\n",
      "\n",
      "Epoch 96: Train Loss = 0.4529288843403692\n",
      "Epoch 97: Train Loss = 0.4521380713711614\n",
      "Epoch 98: Train Loss = 0.4525186095030411\n",
      "Epoch 99: Train Loss = 0.4523811563719874\n",
      "Epoch 100: Train Loss = 0.45307843286058175\n",
      "Recall = 0.9699215344376635, Aging Rate = 0.6206521739130435, Precision = 0.7793345008756567\n",
      "Validation: Test Loss = 0.4507611974944239\n",
      "Recall = 0.9655623365300785, Aging Rate = 0.5767391304347826, precision = 0.8349038823972861\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4516748084710992\n",
      "Epoch 102: Train Loss = 0.45251291036605834\n",
      "Epoch 103: Train Loss = 0.45418186597202137\n",
      "Epoch 104: Train Loss = 0.4533371512267901\n",
      "Epoch 105: Train Loss = 0.45176987005316693\n",
      "Recall = 0.9716652136006975, Aging Rate = 0.6171739130434782, Precision = 0.7851356111306799\n",
      "Validation: Test Loss = 0.45357012816097425\n",
      "Recall = 0.9873583260680034, Aging Rate = 0.6721739130434783, precision = 0.732535575679172\n",
      "\n",
      "Epoch 106: Train Loss = 0.45437600643738457\n",
      "Epoch 107: Train Loss = 0.45247674993846726\n",
      "Epoch 108: Train Loss = 0.4521536143966343\n",
      "Epoch 109: Train Loss = 0.4500968400810076\n",
      "Epoch 110: Train Loss = 0.4521967099542203\n",
      "Recall = 0.9768962510897995, Aging Rate = 0.6247826086956522, Precision = 0.7797494780793319\n",
      "Validation: Test Loss = 0.4471760114379551\n",
      "Recall = 0.979075850043592, Aging Rate = 0.621304347826087, precision = 0.7858642407277817\n",
      "\n",
      "Epoch 111: Train Loss = 0.4516430902481079\n",
      "Epoch 112: Train Loss = 0.45259277566619543\n",
      "Epoch 113: Train Loss = 0.4555319858633954\n",
      "Epoch 114: Train Loss = 0.4523595009161078\n",
      "Epoch 115: Train Loss = 0.45360125945961993\n",
      "Recall = 0.9681778552746295, Aging Rate = 0.6195652173913043, Precision = 0.779298245614035\n",
      "Validation: Test Loss = 0.45431966527648593\n",
      "Recall = 0.986050566695728, Aging Rate = 0.6732608695652174, precision = 0.7303842428156281\n",
      "\n",
      "Epoch 116: Train Loss = 0.4518635992900185\n",
      "Epoch 117: Train Loss = 0.45109073053235593\n",
      "Epoch 118: Train Loss = 0.4513628871026246\n",
      "Epoch 119: Train Loss = 0.4533664865597435\n",
      "Epoch 120: Train Loss = 0.45320092367089315\n",
      "Recall = 0.9707933740191804, Aging Rate = 0.6234782608695653, Precision = 0.7764993026499303\n",
      "Validation: Test Loss = 0.45561439498611117\n",
      "Recall = 0.9463818657367045, Aging Rate = 0.5580434782608695, precision = 0.8457343202181535\n",
      "\n",
      "Epoch 121: Train Loss = 0.4537611715689949\n",
      "Epoch 122: Train Loss = 0.452848676857741\n",
      "Epoch 123: Train Loss = 0.4535906186311141\n",
      "Epoch 124: Train Loss = 0.4545576977211496\n",
      "Epoch 125: Train Loss = 0.45281946094139763\n",
      "Recall = 0.9699215344376635, Aging Rate = 0.6206521739130435, Precision = 0.7793345008756567\n",
      "Validation: Test Loss = 0.4507806770697884\n",
      "Recall = 0.983435047951177, Aging Rate = 0.6502173913043479, precision = 0.7542627883650953\n",
      "\n",
      "Epoch 126: Train Loss = 0.4546412831804027\n",
      "Epoch 127: Train Loss = 0.4538562356389087\n",
      "Epoch 128: Train Loss = 0.4532378475044085\n",
      "Epoch 129: Train Loss = 0.4522012831853784\n",
      "Epoch 130: Train Loss = 0.45643812205480494\n",
      "Recall = 0.9638186573670444, Aging Rate = 0.6221739130434782, Precision = 0.7725366876310272\n",
      "Validation: Test Loss = 0.44851581573486327\n",
      "Recall = 0.9777680906713164, Aging Rate = 0.6117391304347826, precision = 0.7970859985785359\n",
      "\n",
      "Epoch 131: Train Loss = 0.4534056721044623\n",
      "Epoch 132: Train Loss = 0.45265282465064005\n",
      "Epoch 133: Train Loss = 0.45129948849263396\n",
      "Epoch 134: Train Loss = 0.4525853979587555\n",
      "Epoch 135: Train Loss = 0.4506105755723041\n",
      "Recall = 0.9768962510897995, Aging Rate = 0.6189130434782608, Precision = 0.7871443624868283\n",
      "Validation: Test Loss = 0.4488455894200698\n",
      "Recall = 0.978204010462075, Aging Rate = 0.6347826086956522, precision = 0.7684931506849315\n",
      "\n",
      "Epoch 136: Train Loss = 0.4525473059778628\n",
      "Epoch 137: Train Loss = 0.4532022573636926\n",
      "Epoch 138: Train Loss = 0.45321761665136917\n",
      "Epoch 139: Train Loss = 0.45410224463628684\n",
      "Epoch 140: Train Loss = 0.4564612487088079\n",
      "Recall = 0.9655623365300785, Aging Rate = 0.6241304347826087, Precision = 0.7715081853012887\n",
      "Validation: Test Loss = 0.4485479047505752\n",
      "Recall = 0.9821272885789015, Aging Rate = 0.6302173913043478, precision = 0.7771645394963781\n",
      "\n",
      "Epoch 141: Train Loss = 0.4523168415608613\n",
      "Epoch 142: Train Loss = 0.4540834032970926\n",
      "Epoch 143: Train Loss = 0.45124695026356243\n",
      "Epoch 144: Train Loss = 0.4517340444481891\n",
      "Epoch 145: Train Loss = 0.4512404614427815\n",
      "Recall = 0.974716652136007, Aging Rate = 0.62, Precision = 0.7840112201963534\n",
      "Validation: Test Loss = 0.44729407419329104\n",
      "Recall = 0.979947689625109, Aging Rate = 0.6193478260869565, precision = 0.7890487890487891\n",
      "\n",
      "Epoch 146: Train Loss = 0.4499374327452286\n",
      "Epoch 147: Train Loss = 0.45263979678568633\n",
      "Epoch 148: Train Loss = 0.4511033596163211\n",
      "Epoch 149: Train Loss = 0.45247992888740873\n",
      "Epoch 150: Train Loss = 0.45218238716540127\n",
      "Recall = 0.9699215344376635, Aging Rate = 0.615, Precision = 0.7864969954047366\n",
      "Validation: Test Loss = 0.447850712330445\n",
      "Recall = 0.977332170880558, Aging Rate = 0.6189130434782608, precision = 0.7874956094134177\n",
      "\n",
      "Training Finished at epoch 150.\n",
      "Validation: Test Loss = 0.4510074617265878\n",
      "Recall = 0.9689521345407504, Aging Rate = 0.605606258148631, precision = 0.806243272335845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542e01b546274f3d95dddb0202d94dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.580109656168067\n",
      "Epoch 2: Train Loss = 0.4995813162948774\n",
      "Epoch 3: Train Loss = 0.4855009781795999\n",
      "Epoch 4: Train Loss = 0.4814593706441962\n",
      "Epoch 5: Train Loss = 0.4789864333297895\n",
      "Recall = 0.9509888220120378, Aging Rate = 0.6660869565217391, Precision = 0.7219321148825065\n",
      "Validation: Test Loss = 0.46456103708433066\n",
      "Recall = 0.944110060189166, Aging Rate = 0.601304347826087, precision = 0.7939262472885033\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.4652487974581511\n",
      "Epoch 7: Train Loss = 0.4611431356098341\n",
      "Epoch 8: Train Loss = 0.4604854304894157\n",
      "Epoch 9: Train Loss = 0.4602081547094428\n",
      "Epoch 10: Train Loss = 0.45719868022462595\n",
      "Recall = 0.9638865004299226, Aging Rate = 0.6310869565217392, Precision = 0.7723045125732001\n",
      "Validation: Test Loss = 0.4564775742655215\n",
      "Recall = 0.9514187446259673, Aging Rate = 0.58, precision = 0.8294602698650675\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.46044622136198954\n",
      "Epoch 12: Train Loss = 0.457521019137424\n",
      "Epoch 13: Train Loss = 0.4644737899303436\n",
      "Epoch 14: Train Loss = 0.4603142813495968\n",
      "Epoch 15: Train Loss = 0.4565586505247199\n",
      "Recall = 0.9595872742906277, Aging Rate = 0.6265217391304347, Precision = 0.7744621790423317\n",
      "Validation: Test Loss = 0.4530332454391148\n",
      "Recall = 0.9561478933791917, Aging Rate = 0.5895652173913043, precision = 0.8200589970501475\n",
      "\n",
      "Epoch 16: Train Loss = 0.45702951337980185\n",
      "Epoch 17: Train Loss = 0.4617598980924357\n",
      "Epoch 18: Train Loss = 0.4620415927534518\n",
      "Epoch 19: Train Loss = 0.456281912430473\n",
      "Epoch 20: Train Loss = 0.454265573543051\n",
      "Recall = 0.9638865004299226, Aging Rate = 0.6193478260869565, Precision = 0.786942786942787\n",
      "Validation: Test Loss = 0.4477645272275676\n",
      "Recall = 0.9780739466895959, Aging Rate = 0.6152173913043478, precision = 0.803886925795053\n",
      "\n",
      "Epoch 21: Train Loss = 0.45931477930234826\n",
      "Epoch 22: Train Loss = 0.4581822185930998\n",
      "Epoch 23: Train Loss = 0.4598873649991077\n",
      "Epoch 24: Train Loss = 0.45959505495817765\n",
      "Epoch 25: Train Loss = 0.4554596820603246\n",
      "Recall = 0.9621668099742047, Aging Rate = 0.6278260869565218, Precision = 0.7749307479224377\n",
      "Validation: Test Loss = 0.4524072118945744\n",
      "Recall = 0.9565778159931212, Aging Rate = 0.5921739130434782, precision = 0.816813509544787\n",
      "\n",
      "Epoch 26: Train Loss = 0.4557755027646604\n",
      "Epoch 27: Train Loss = 0.4556539114661839\n",
      "Epoch 28: Train Loss = 0.4552942669391632\n",
      "Epoch 29: Train Loss = 0.4548464592125105\n",
      "Epoch 30: Train Loss = 0.4538900606010271\n",
      "Recall = 0.9729148753224419, Aging Rate = 0.6265217391304347, Precision = 0.7852185981956974\n",
      "Validation: Test Loss = 0.44703815952591275\n",
      "Recall = 0.9604471195184867, Aging Rate = 0.6043478260869565, precision = 0.8035971223021583\n",
      "\n",
      "Epoch 31: Train Loss = 0.4544969093799591\n",
      "Epoch 32: Train Loss = 0.45584108518517535\n",
      "Epoch 33: Train Loss = 0.45358031267705173\n",
      "Epoch 34: Train Loss = 0.454504220382027\n",
      "Epoch 35: Train Loss = 0.4577638817870099\n",
      "Recall = 0.9608770421324162, Aging Rate = 0.628695652173913, Precision = 0.7728215767634855\n",
      "Validation: Test Loss = 0.4517826757742011\n",
      "Recall = 0.9539982803095443, Aging Rate = 0.5891304347826087, precision = 0.8188191881918819\n",
      "\n",
      "Epoch 36: Train Loss = 0.4560415511027626\n",
      "Epoch 37: Train Loss = 0.45464423371397933\n",
      "Epoch 38: Train Loss = 0.45476706820985546\n",
      "Epoch 39: Train Loss = 0.4540063210155653\n",
      "Epoch 40: Train Loss = 0.4539620629082555\n",
      "Recall = 0.9664660361134996, Aging Rate = 0.6282608695652174, Precision = 0.7778546712802769\n",
      "Validation: Test Loss = 0.44773818974909574\n",
      "Recall = 0.969475494411006, Aging Rate = 0.6134782608695653, precision = 0.7990786676116229\n",
      "\n",
      "Epoch 41: Train Loss = 0.4534746385657269\n",
      "Epoch 42: Train Loss = 0.4568745382454084\n",
      "Epoch 43: Train Loss = 0.45297341813211855\n",
      "Epoch 44: Train Loss = 0.45823036452998284\n",
      "Epoch 45: Train Loss = 0.45747172941332276\n",
      "Recall = 0.9613069647463457, Aging Rate = 0.6295652173913043, Precision = 0.7720994475138122\n",
      "Validation: Test Loss = 0.45640361853267836\n",
      "Recall = 0.9832330180567498, Aging Rate = 0.6845652173913044, precision = 0.7262623054938075\n",
      "\n",
      "Epoch 46: Train Loss = 0.4546524393040201\n",
      "Epoch 47: Train Loss = 0.4525601982552072\n",
      "Epoch 48: Train Loss = 0.4531929773351421\n",
      "Epoch 49: Train Loss = 0.4540807138318601\n",
      "Epoch 50: Train Loss = 0.4537052993670754\n",
      "Recall = 0.9591573516766982, Aging Rate = 0.6197826086956522, Precision = 0.7825324447562259\n",
      "Validation: Test Loss = 0.4497505754491557\n",
      "Recall = 0.9707652622527945, Aging Rate = 0.6273913043478261, precision = 0.7823977823977823\n",
      "\n",
      "Epoch 51: Train Loss = 0.4527623696949171\n",
      "Epoch 52: Train Loss = 0.4515692141263381\n",
      "Epoch 53: Train Loss = 0.455862993053768\n",
      "Epoch 54: Train Loss = 0.4532719361782074\n",
      "Epoch 55: Train Loss = 0.45174411006595777\n",
      "Recall = 0.9673258813413586, Aging Rate = 0.621304347826087, Precision = 0.7872638208537439\n",
      "Validation: Test Loss = 0.4476711895673171\n",
      "Recall = 0.9729148753224419, Aging Rate = 0.6095652173913043, precision = 0.8070613409415122\n",
      "\n",
      "Epoch 56: Train Loss = 0.452742768422417\n",
      "Epoch 57: Train Loss = 0.4539118370802506\n",
      "Epoch 58: Train Loss = 0.45342244625091555\n",
      "Epoch 59: Train Loss = 0.4513069298474685\n",
      "Epoch 60: Train Loss = 0.45264693861422334\n",
      "Recall = 0.9720550300945829, Aging Rate = 0.6302173913043478, Precision = 0.7799241117626767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-26 16:19:30,704]\u001b[0m A new study created in memory with name: no-name-48b394d7-dd1e-403d-b21e-5f23653c291e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.4477347199294878\n",
      "Recall = 0.9544282029234737, Aging Rate = 0.5884782608695652, precision = 0.8200960472848171\n",
      "\n",
      "Training Finished at epoch 60.\n",
      "Validation: Test Loss = 0.45704760622947127\n",
      "Recall = 0.9541160593792173, Aging Rate = 0.5834419817470665, precision = 0.7899441340782123\n",
      "\u001b[32m[I 2022-05-26 16:19:30,579]\u001b[0m Trial 9 finished with value: 0.866976995281735 and parameters: {'batch_size': 96, 'learning_rate': 0.01, 'weight_decay': 0.01, 'bad_weight': 0.7}. Best is trial 7 with value: 0.9888488775825913.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset 2 :\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094de0d35942494e88460220df853a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c126e999e3c4425fb05fba5cd06667f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.678721776893918\n",
      "Epoch 2: Train Loss = 0.6619653428439466\n",
      "Epoch 3: Train Loss = 0.6496682968504813\n",
      "Epoch 4: Train Loss = 0.6336108272031069\n",
      "Epoch 5: Train Loss = 0.6168873232597766\n",
      "Recall = 0.9796909492273731, Aging Rate = 0.8711041852181657, Precision = 0.5670840787119857\n",
      "Validation: Test Loss = 0.6064107077841449\n",
      "Recall = 0.9774834437086093, Aging Rate = 0.8334817453250223, precision = 0.5913461538461539\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5968907854966786\n",
      "Epoch 7: Train Loss = 0.5789256385682633\n",
      "Epoch 8: Train Loss = 0.5631926645154095\n",
      "Epoch 9: Train Loss = 0.5506627390879345\n",
      "Epoch 10: Train Loss = 0.5403151543460992\n",
      "Recall = 0.9116997792494481, Aging Rate = 0.6378005342831701, Precision = 0.7207678883071553\n",
      "Validation: Test Loss = 0.5352133399975268\n",
      "Recall = 0.9139072847682119, Aging Rate = 0.6320124666073018, precision = 0.7291299753434308\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.5316685043268722\n",
      "Epoch 12: Train Loss = 0.5249758535798702\n",
      "Epoch 13: Train Loss = 0.5188932216071913\n",
      "Epoch 14: Train Loss = 0.5138213887399367\n",
      "Epoch 15: Train Loss = 0.5093564522234543\n",
      "Recall = 0.9094922737306843, Aging Rate = 0.6104185218165628, Precision = 0.75127644055434\n",
      "Validation: Test Loss = 0.5065962380314128\n",
      "Recall = 0.9090507726269316, Aging Rate = 0.6068566340160285, precision = 0.7553191489361702\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5052594391501385\n",
      "Epoch 17: Train Loss = 0.501765105757242\n",
      "Epoch 18: Train Loss = 0.49803006444673714\n",
      "Epoch 19: Train Loss = 0.4948717522387619\n",
      "Epoch 20: Train Loss = 0.4918365086812795\n",
      "Recall = 0.9130242825607064, Aging Rate = 0.6015138023152271, Precision = 0.765358993338268\n",
      "Validation: Test Loss = 0.48981325228832073\n",
      "Recall = 0.9108167770419426, Aging Rate = 0.5955031166518254, precision = 0.7712149532710281\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.48907973430884916\n",
      "Epoch 22: Train Loss = 0.4862486745561751\n",
      "Epoch 23: Train Loss = 0.4835065114954594\n",
      "Epoch 24: Train Loss = 0.48100697935848286\n",
      "Epoch 25: Train Loss = 0.4786138914571951\n",
      "Recall = 0.9169977924944812, Aging Rate = 0.5917186108637578, Precision = 0.7814145974416855\n",
      "Validation: Test Loss = 0.4770514038345799\n",
      "Recall = 0.9130242825607064, Aging Rate = 0.5843722172751559, precision = 0.7878095238095238\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.4761657721212902\n",
      "Epoch 27: Train Loss = 0.47385389148817464\n",
      "Epoch 28: Train Loss = 0.47152454733105503\n",
      "Epoch 29: Train Loss = 0.46940075581559937\n",
      "Epoch 30: Train Loss = 0.46708239695904835\n",
      "Recall = 0.9280353200883003, Aging Rate = 0.5861531611754229, Precision = 0.7983289023927079\n",
      "Validation: Test Loss = 0.46529815736776575\n",
      "Recall = 0.9249448123620309, Aging Rate = 0.5817008014247551, precision = 0.8017604286261003\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4650274344246407\n",
      "Epoch 32: Train Loss = 0.46249017721823255\n",
      "Epoch 33: Train Loss = 0.46060569139855945\n",
      "Epoch 34: Train Loss = 0.4580377856748824\n",
      "Epoch 35: Train Loss = 0.45623906443719875\n",
      "Recall = 0.9350993377483444, Aging Rate = 0.5812555654496884, Precision = 0.81118345461509\n",
      "Validation: Test Loss = 0.45443460653110584\n",
      "Recall = 0.9381898454746137, Aging Rate = 0.5803650934995548, precision = 0.8151131568853088\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.45419084208412885\n",
      "Epoch 37: Train Loss = 0.45205892775065115\n",
      "Epoch 38: Train Loss = 0.449637362947447\n",
      "Epoch 39: Train Loss = 0.44785539438336\n",
      "Epoch 40: Train Loss = 0.4456868229810404\n",
      "Recall = 0.9421633554083885, Aging Rate = 0.5690115761353517, Precision = 0.8348982785602503\n",
      "Validation: Test Loss = 0.44422187995188805\n",
      "Recall = 0.9492273730684326, Aging Rate = 0.5774710596616207, precision = 0.8288357748650732\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.4436042634961017\n",
      "Epoch 42: Train Loss = 0.44185602911752137\n",
      "Epoch 43: Train Loss = 0.4400305376717371\n",
      "Epoch 44: Train Loss = 0.43783218355870945\n",
      "Epoch 45: Train Loss = 0.4357681984309097\n",
      "Recall = 0.9518763796909492, Aging Rate = 0.5681211041852182, Precision = 0.8448275862068966\n",
      "Validation: Test Loss = 0.4343787483997039\n",
      "Recall = 0.9501103752759382, Aging Rate = 0.5638913624220837, precision = 0.8495854717726017\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.43390175718764373\n",
      "Epoch 47: Train Loss = 0.4321703256131917\n",
      "Epoch 48: Train Loss = 0.4302773231230776\n",
      "Epoch 49: Train Loss = 0.42843952955460823\n",
      "Epoch 50: Train Loss = 0.4267604727089033\n",
      "Recall = 0.9576158940397351, Aging Rate = 0.5636687444345503, Precision = 0.8566350710900474\n",
      "Validation: Test Loss = 0.42533372258991403\n",
      "Recall = 0.9571743929359824, Aging Rate = 0.5607747105966162, precision = 0.8606589916633585\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.42505330624903087\n",
      "Epoch 52: Train Loss = 0.42362312052999346\n",
      "Epoch 53: Train Loss = 0.42183676224039374\n",
      "Epoch 54: Train Loss = 0.42018648376978623\n",
      "Epoch 55: Train Loss = 0.41844909528696633\n",
      "Recall = 0.9607064017660044, Aging Rate = 0.5545414069456812, Precision = 0.8735447611401044\n",
      "Validation: Test Loss = 0.41745683155416485\n",
      "Recall = 0.9558498896247241, Aging Rate = 0.5480854853072128, precision = 0.879366368805849\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.4171360428059409\n",
      "Epoch 57: Train Loss = 0.4155142993822972\n",
      "Epoch 58: Train Loss = 0.4139941398681325\n",
      "Epoch 59: Train Loss = 0.4126496399659083\n",
      "Epoch 60: Train Loss = 0.4109306680975699\n",
      "Recall = 0.9682119205298013, Aging Rate = 0.5534283170080142, Precision = 0.8821399839098955\n",
      "Validation: Test Loss = 0.4098315787495828\n",
      "Recall = 0.9637969094922737, Aging Rate = 0.548753339269813, precision = 0.8855983772819472\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.4097449115261051\n",
      "Epoch 62: Train Loss = 0.4082761627430377\n",
      "Epoch 63: Train Loss = 0.4070710174653112\n",
      "Epoch 64: Train Loss = 0.4056049029899302\n",
      "Epoch 65: Train Loss = 0.4041545473616355\n",
      "Recall = 0.9699779249448124, Aging Rate = 0.5471950133570792, Precision = 0.8938161106590724\n",
      "Validation: Test Loss = 0.4030561389290427\n",
      "Recall = 0.9748344370860927, Aging Rate = 0.5525378450578807, precision = 0.8896051571313457\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.40274749503853274\n",
      "Epoch 67: Train Loss = 0.40170107228567\n",
      "Epoch 68: Train Loss = 0.40049645169335396\n",
      "Epoch 69: Train Loss = 0.39927386680248056\n",
      "Epoch 70: Train Loss = 0.3979690415670271\n",
      "Recall = 0.9735099337748344, Aging Rate = 0.542520035618878, Precision = 0.9048009848173985\n",
      "Validation: Test Loss = 0.3966202089166599\n",
      "Recall = 0.9726269315673289, Aging Rate = 0.5398486197684773, precision = 0.9084536082474227\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.39674835452413515\n",
      "Epoch 72: Train Loss = 0.39572440320003915\n",
      "Epoch 73: Train Loss = 0.39453816554531496\n",
      "Epoch 74: Train Loss = 0.39346290613961665\n",
      "Epoch 75: Train Loss = 0.3925388744125808\n",
      "Recall = 0.97439293598234, Aging Rate = 0.5365093499554764, Precision = 0.9157676348547718\n",
      "Validation: Test Loss = 0.39123065569309495\n",
      "Recall = 0.9774834437086093, Aging Rate = 0.5380676758682101, precision = 0.9160115846090194\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.3914007230571414\n",
      "Epoch 77: Train Loss = 0.3903861126513324\n",
      "Epoch 78: Train Loss = 0.3892869026344585\n",
      "Epoch 79: Train Loss = 0.38836102605609096\n",
      "Epoch 80: Train Loss = 0.38747513559601293\n",
      "Recall = 0.9783664459161148, Aging Rate = 0.5371772039180766, Precision = 0.9183588893493576\n",
      "Validation: Test Loss = 0.38639695553618253\n",
      "Recall = 0.9783664459161148, Aging Rate = 0.536286731967943, precision = 0.9198837691988377\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.3866306008341476\n",
      "Epoch 82: Train Loss = 0.3856568787900444\n",
      "Epoch 83: Train Loss = 0.3847563211820217\n",
      "Epoch 84: Train Loss = 0.3837596447947613\n",
      "Epoch 85: Train Loss = 0.3830369106383591\n",
      "Recall = 0.9818984547461369, Aging Rate = 0.5358414959928762, Precision = 0.9239717490652264\n",
      "Validation: Test Loss = 0.38196915077611154\n",
      "Recall = 0.9849889624724062, Aging Rate = 0.5382902938557436, precision = 0.9226633581472291\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.3824033274092211\n",
      "Epoch 87: Train Loss = 0.38127232338314276\n",
      "Epoch 88: Train Loss = 0.3804234780590129\n",
      "Epoch 89: Train Loss = 0.37971880257076596\n",
      "Epoch 90: Train Loss = 0.3789307179720616\n",
      "Recall = 0.9832229580573951, Aging Rate = 0.5329474621549422, Precision = 0.9302422723475355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.37801104744204644\n",
      "Recall = 0.9863134657836644, Aging Rate = 0.5367319679430098, precision = 0.9265864786395687\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.37839239612818826\n",
      "Epoch 92: Train Loss = 0.37743802226661893\n",
      "Epoch 93: Train Loss = 0.3767011281059771\n",
      "Epoch 94: Train Loss = 0.37620854701511797\n",
      "Epoch 95: Train Loss = 0.37543983400558\n",
      "Recall = 0.9858719646799117, Aging Rate = 0.5325022261798753, Precision = 0.9335284280936454\n",
      "Validation: Test Loss = 0.37445060610240416\n",
      "Recall = 0.9818984547461369, Aging Rate = 0.526046304541407, precision = 0.9411764705882353\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.3745710226912002\n",
      "Epoch 97: Train Loss = 0.3739287940020659\n",
      "Epoch 98: Train Loss = 0.3733224265031908\n",
      "Epoch 99: Train Loss = 0.37274348520850453\n",
      "Epoch 100: Train Loss = 0.3723305707145565\n",
      "Recall = 0.9854304635761589, Aging Rate = 0.5287177203918076, Precision = 0.9397894736842105\n",
      "Validation: Test Loss = 0.37104365407412115\n",
      "Recall = 0.9849889624724062, Aging Rate = 0.5262689225289403, precision = 0.9437394247038917\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.3713315547521053\n",
      "Epoch 102: Train Loss = 0.370691501977501\n",
      "Epoch 103: Train Loss = 0.3701430043522418\n",
      "Epoch 104: Train Loss = 0.36959437684936386\n",
      "Epoch 105: Train Loss = 0.36899450596484984\n",
      "Recall = 0.9858719646799117, Aging Rate = 0.5256010685663401, Precision = 0.9457856840321898\n",
      "Validation: Test Loss = 0.36828123551867203\n",
      "Recall = 0.98719646799117, Aging Rate = 0.5267141585040072, precision = 0.945054945054945\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.3684517710949837\n",
      "Epoch 107: Train Loss = 0.3681111914551056\n",
      "Epoch 108: Train Loss = 0.36762301378343537\n",
      "Epoch 109: Train Loss = 0.3669371697802896\n",
      "Epoch 110: Train Loss = 0.3666331178093211\n",
      "Recall = 0.98719646799117, Aging Rate = 0.5242653606411398, Precision = 0.9494692144373673\n",
      "Validation: Test Loss = 0.3656106174575996\n",
      "Recall = 0.9885209713024282, Aging Rate = 0.52493321460374, precision = 0.9495335029686175\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3658504271390495\n",
      "Epoch 112: Train Loss = 0.3654980286889488\n",
      "Epoch 113: Train Loss = 0.36498370028879634\n",
      "Epoch 114: Train Loss = 0.364704166393883\n",
      "Epoch 115: Train Loss = 0.3641066160567191\n",
      "Recall = 0.9880794701986755, Aging Rate = 0.523820124666073, Precision = 0.9511262218444538\n",
      "Validation: Test Loss = 0.3633671367104735\n",
      "Recall = 0.9876379690949227, Aging Rate = 0.5229296527159395, precision = 0.9523201362281822\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.3636378898274548\n",
      "Epoch 117: Train Loss = 0.3631437869604846\n",
      "Epoch 118: Train Loss = 0.362816855563506\n",
      "Epoch 119: Train Loss = 0.3623787359478\n",
      "Epoch 120: Train Loss = 0.36198600383603136\n",
      "Recall = 0.9889624724061811, Aging Rate = 0.5229296527159395, Precision = 0.9535972754363559\n",
      "Validation: Test Loss = 0.3611436125590986\n",
      "Recall = 0.9894039735099338, Aging Rate = 0.5235975066785397, precision = 0.9528061224489796\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.36157278659507197\n",
      "Epoch 122: Train Loss = 0.3612390226905938\n",
      "Epoch 123: Train Loss = 0.3608626504615281\n",
      "Epoch 124: Train Loss = 0.360382169856626\n",
      "Epoch 125: Train Loss = 0.36008659758848055\n",
      "Recall = 0.9885209713024282, Aging Rate = 0.5213713268032057, Precision = 0.9560204953031597\n",
      "Validation: Test Loss = 0.3592966844328577\n",
      "Recall = 0.9902869757174393, Aging Rate = 0.5227070347284061, precision = 0.9552810902896082\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.35967482691456565\n",
      "Epoch 127: Train Loss = 0.35943597918732934\n",
      "Epoch 128: Train Loss = 0.3589261074949351\n",
      "Epoch 129: Train Loss = 0.3586155985841131\n",
      "Epoch 130: Train Loss = 0.3583422748244244\n",
      "Recall = 0.9898454746136865, Aging Rate = 0.5195903829029386, Precision = 0.9605826906598115\n",
      "Validation: Test Loss = 0.35755879981965755\n",
      "Recall = 0.9898454746136865, Aging Rate = 0.519813000890472, precision = 0.9601713062098501\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.35793563455529015\n",
      "Epoch 132: Train Loss = 0.35754443305683054\n",
      "Epoch 133: Train Loss = 0.357162563055621\n",
      "Epoch 134: Train Loss = 0.3568562027713813\n",
      "Epoch 135: Train Loss = 0.356608987809926\n",
      "Recall = 0.990728476821192, Aging Rate = 0.5209260908281389, Precision = 0.958974358974359\n",
      "Validation: Test Loss = 0.3559797755438413\n",
      "Recall = 0.9894039735099338, Aging Rate = 0.517586821015138, precision = 0.9638709677419355\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.3562386696984061\n",
      "Epoch 137: Train Loss = 0.3560828700834157\n",
      "Epoch 138: Train Loss = 0.35580697018762625\n",
      "Epoch 139: Train Loss = 0.3554068150153037\n",
      "Epoch 140: Train Loss = 0.35518504364726167\n",
      "Recall = 0.9920529801324504, Aging Rate = 0.518699910952805, Precision = 0.9643776824034335\n",
      "Validation: Test Loss = 0.35441315341293017\n",
      "Recall = 0.9920529801324504, Aging Rate = 0.5180320569902048, precision = 0.9656209712075634\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.35470827270169825\n",
      "Epoch 142: Train Loss = 0.3546225974735997\n",
      "Epoch 143: Train Loss = 0.3543717048577931\n",
      "Epoch 144: Train Loss = 0.35414435833241403\n",
      "Epoch 145: Train Loss = 0.35377288639598514\n",
      "Recall = 0.9920529801324504, Aging Rate = 0.5180320569902048, Precision = 0.9656209712075634\n",
      "Validation: Test Loss = 0.35303159112497196\n",
      "Recall = 0.9933774834437086, Aging Rate = 0.5180320569902048, precision = 0.9669101847872797\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.3533834589716588\n",
      "Epoch 147: Train Loss = 0.3531566597421787\n",
      "Epoch 148: Train Loss = 0.35311319962316395\n",
      "Epoch 149: Train Loss = 0.3527202911920454\n",
      "Epoch 150: Train Loss = 0.3524336093308558\n",
      "Recall = 0.9924944812362031, Aging Rate = 0.5158058771148709, Precision = 0.9702201122140699\n",
      "Validation: Test Loss = 0.3524017034368863\n",
      "Recall = 0.9947019867549669, Aging Rate = 0.5200356188780053, precision = 0.9644691780821918\n",
      "\n",
      "Validation: Test Loss = 0.36957864143023983\n",
      "Recall = 0.9808219178082191, Aging Rate = 0.5106809078771696, precision = 0.9359477124183007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b1eeaa7a62495c9e8d077f6118b26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6793392552187266\n",
      "Epoch 2: Train Loss = 0.6665506665449745\n",
      "Epoch 3: Train Loss = 0.654568826260571\n",
      "Epoch 4: Train Loss = 0.6405398033075002\n",
      "Epoch 5: Train Loss = 0.6232213716048379\n",
      "Recall = 0.989718372820742, Aging Rate = 0.8922528940338379, Precision = 0.5523952095808383\n",
      "Validation: Test Loss = 0.6133836689848827\n",
      "Recall = 0.9812248547161377, Aging Rate = 0.8450578806767587, precision = 0.5782402528977871\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6044614387747441\n",
      "Epoch 7: Train Loss = 0.5861852180713644\n",
      "Epoch 8: Train Loss = 0.5697475805520374\n",
      "Epoch 9: Train Loss = 0.5559227693749663\n",
      "Epoch 10: Train Loss = 0.5450760376103096\n",
      "Recall = 0.9097004917299956, Aging Rate = 0.6333481745325023, Precision = 0.7152899824253075\n",
      "Validation: Test Loss = 0.5393720401893847\n",
      "Recall = 0.9114886008046491, Aging Rate = 0.6329029385574354, precision = 0.7172001406964474\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.5359079078277731\n",
      "Epoch 12: Train Loss = 0.5279858014144116\n",
      "Epoch 13: Train Loss = 0.5215034674876308\n",
      "Epoch 14: Train Loss = 0.5161101426912649\n",
      "Epoch 15: Train Loss = 0.5112089274296884\n",
      "Recall = 0.9092534644613322, Aging Rate = 0.5999554764024934, Precision = 0.7547309833024118\n",
      "Validation: Test Loss = 0.5081512148584092\n",
      "Recall = 0.9034421099687081, Aging Rate = 0.5886019590382903, precision = 0.7643721633888049\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5066183461531506\n",
      "Epoch 17: Train Loss = 0.5024336384856478\n",
      "Epoch 18: Train Loss = 0.4990462448484433\n",
      "Epoch 19: Train Loss = 0.4953737535969232\n",
      "Epoch 20: Train Loss = 0.4925718672659815\n",
      "Recall = 0.9146177916852928, Aging Rate = 0.590160284951024, Precision = 0.7717842323651453\n",
      "Validation: Test Loss = 0.4901859443153745\n",
      "Recall = 0.9141707644166294, Aging Rate = 0.5870436331255565, precision = 0.7755024649222602\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.48937592385394807\n",
      "Epoch 22: Train Loss = 0.4863525001682135\n",
      "Epoch 23: Train Loss = 0.4837224729233094\n",
      "Epoch 24: Train Loss = 0.4809605153352155\n",
      "Epoch 25: Train Loss = 0.47834542956292786\n",
      "Recall = 0.9204291461779168, Aging Rate = 0.583926981300089, Precision = 0.7849790316431567\n",
      "Validation: Test Loss = 0.47626589454929424\n",
      "Recall = 0.9217702279839071, Aging Rate = 0.5830365093499554, precision = 0.787323405880107\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.47594138640223715\n",
      "Epoch 27: Train Loss = 0.4737614859774616\n",
      "Epoch 28: Train Loss = 0.4710258761900191\n",
      "Epoch 29: Train Loss = 0.4686887604470563\n",
      "Epoch 30: Train Loss = 0.4665166394634111\n",
      "Recall = 0.9231113097898972, Aging Rate = 0.5712377560106857, Precision = 0.8047544816835541\n",
      "Validation: Test Loss = 0.46495930244109507\n",
      "Recall = 0.9351810460438087, Aging Rate = 0.5830365093499554, precision = 0.7987781596029019\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4643930853315474\n",
      "Epoch 32: Train Loss = 0.4624399589250264\n",
      "Epoch 33: Train Loss = 0.45997165808270046\n",
      "Epoch 34: Train Loss = 0.4579653829893467\n",
      "Epoch 35: Train Loss = 0.4559670059360358\n",
      "Recall = 0.9307107733571748, Aging Rate = 0.5636687444345503, Precision = 0.8222748815165877\n",
      "Validation: Test Loss = 0.45413880133777457\n",
      "Recall = 0.937863209655789, Aging Rate = 0.5705699020480854, precision = 0.8185719859539602\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4539550909588407\n",
      "Epoch 37: Train Loss = 0.45178382707724907\n",
      "Epoch 38: Train Loss = 0.44982984463019976\n",
      "Epoch 39: Train Loss = 0.4478962162405067\n",
      "Epoch 40: Train Loss = 0.4459611070230404\n",
      "Recall = 0.9374161823871257, Aging Rate = 0.5616651825467498, Precision = 0.83115338882283\n",
      "Validation: Test Loss = 0.44429178202884706\n",
      "Recall = 0.9342869915064819, Aging Rate = 0.5529830810329475, precision = 0.8413848631239935\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.4443684576455759\n",
      "Epoch 42: Train Loss = 0.44237762801050606\n",
      "Epoch 43: Train Loss = 0.4403260982365663\n",
      "Epoch 44: Train Loss = 0.4384670965340343\n",
      "Epoch 45: Train Loss = 0.4367352605184485\n",
      "Recall = 0.9427805096110863, Aging Rate = 0.5523152270703473, Precision = 0.8500604594921403\n",
      "Validation: Test Loss = 0.43507587766392575\n",
      "Recall = 0.9409924005364327, Aging Rate = 0.5454140694568121, precision = 0.8591836734693877\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.43503406349294227\n",
      "Epoch 47: Train Loss = 0.4332721515733646\n",
      "Epoch 48: Train Loss = 0.4314493715020024\n",
      "Epoch 49: Train Loss = 0.4299368076789411\n",
      "Epoch 50: Train Loss = 0.42825194155649743\n",
      "Recall = 0.9490388913723737, Aging Rate = 0.5474176313446126, Precision = 0.8633590890605938\n",
      "Validation: Test Loss = 0.4266978542346139\n",
      "Recall = 0.9503799731783639, Aging Rate = 0.5480854853072128, precision = 0.8635255889520714\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.4266027170743458\n",
      "Epoch 52: Train Loss = 0.42499339315154566\n",
      "Epoch 53: Train Loss = 0.4238382650345219\n",
      "Epoch 54: Train Loss = 0.42206674394921756\n",
      "Epoch 55: Train Loss = 0.42057528996297744\n",
      "Recall = 0.9526151095216808, Aging Rate = 0.543633125556545, Precision = 0.8726453726453727\n",
      "Validation: Test Loss = 0.4193018343919956\n",
      "Recall = 0.951721054984354, Aging Rate = 0.5382902938557436, precision = 0.880479735318445\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.41909297576252097\n",
      "Epoch 57: Train Loss = 0.41789754105272403\n",
      "Epoch 58: Train Loss = 0.41636147211411123\n",
      "Epoch 59: Train Loss = 0.4150368414971835\n",
      "Epoch 60: Train Loss = 0.4138453122450641\n",
      "Recall = 0.9548502458649978, Aging Rate = 0.5376224398931434, Precision = 0.884472049689441\n",
      "Validation: Test Loss = 0.41261473361764334\n",
      "Recall = 0.9579794367456415, Aging Rate = 0.5394033837934105, precision = 0.8844407758976476\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.4125990691136167\n",
      "Epoch 62: Train Loss = 0.411307456948772\n",
      "Epoch 63: Train Loss = 0.4100628048844987\n",
      "Epoch 64: Train Loss = 0.4086035540723843\n",
      "Epoch 65: Train Loss = 0.40735882175679944\n",
      "Recall = 0.9584264640143049, Aging Rate = 0.5345057880676759, Precision = 0.8929612661391088\n",
      "Validation: Test Loss = 0.4062344975643464\n",
      "Recall = 0.9602145730889584, Aging Rate = 0.5338379341050757, precision = 0.8957464553794829\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.40619212457991455\n",
      "Epoch 67: Train Loss = 0.4051724117405162\n",
      "Epoch 68: Train Loss = 0.4037046328200258\n",
      "Epoch 69: Train Loss = 0.40287707588127014\n",
      "Epoch 70: Train Loss = 0.40155314624256466\n",
      "Recall = 0.9611086276262852, Aging Rate = 0.5307212822796082, Precision = 0.9018456375838926\n",
      "Validation: Test Loss = 0.4001951983807242\n",
      "Recall = 0.9633437639696022, Aging Rate = 0.5298308103294747, precision = 0.9054621848739496\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.4003760199512845\n",
      "Epoch 72: Train Loss = 0.3993325291843363\n",
      "Epoch 73: Train Loss = 0.39813384708292443\n",
      "Epoch 74: Train Loss = 0.3972856402397156\n",
      "Epoch 75: Train Loss = 0.3961541656394782\n",
      "Recall = 0.9646848457755923, Aging Rate = 0.5253784505788067, Precision = 0.9144067796610169\n",
      "Validation: Test Loss = 0.39479733466146677\n",
      "Recall = 0.9664729548502459, Aging Rate = 0.5273820124666073, precision = 0.912621359223301\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.39484585329346217\n",
      "Epoch 77: Train Loss = 0.3938502921446667\n",
      "Epoch 78: Train Loss = 0.392765788950245\n",
      "Epoch 79: Train Loss = 0.3920433925977267\n",
      "Epoch 80: Train Loss = 0.3911380531367508\n",
      "Recall = 0.9691551184622262, Aging Rate = 0.52493321460374, Precision = 0.9194232400339271\n",
      "Validation: Test Loss = 0.39011018303283496\n",
      "Recall = 0.9704962002682164, Aging Rate = 0.526046304541407, precision = 0.9187473550571308\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.39015310631197475\n",
      "Epoch 82: Train Loss = 0.3890655644483473\n",
      "Epoch 83: Train Loss = 0.3882246405178165\n",
      "Epoch 84: Train Loss = 0.3874534360553575\n",
      "Epoch 85: Train Loss = 0.3864801819316001\n",
      "Recall = 0.9709432275368798, Aging Rate = 0.5220391807658059, Precision = 0.926226012793177\n",
      "Validation: Test Loss = 0.3858014651153731\n",
      "Recall = 0.9673670093875727, Aging Rate = 0.5162511130899377, precision = 0.933160845191893\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.38583938277202956\n",
      "Epoch 87: Train Loss = 0.3848823885352932\n",
      "Epoch 88: Train Loss = 0.3839954639903584\n",
      "Epoch 89: Train Loss = 0.38307079802850263\n",
      "Epoch 90: Train Loss = 0.3825116828415719\n",
      "Recall = 0.9727313366115333, Aging Rate = 0.5209260908281389, Precision = 0.9299145299145299\n",
      "Validation: Test Loss = 0.38146543330309757\n",
      "Recall = 0.9727313366115333, Aging Rate = 0.5191451469278717, precision = 0.9331046312178388\n",
      "Model in epoch 90 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss = 0.38167915974253536\n",
      "Epoch 92: Train Loss = 0.38099018368249693\n",
      "Epoch 93: Train Loss = 0.3802427215864907\n",
      "Epoch 94: Train Loss = 0.3794355884207643\n",
      "Epoch 95: Train Loss = 0.3785787218931203\n",
      "Recall = 0.9763075547608404, Aging Rate = 0.5200356188780053, Precision = 0.934931506849315\n",
      "Validation: Test Loss = 0.37769224409110186\n",
      "Recall = 0.975860527492177, Aging Rate = 0.5191451469278717, precision = 0.9361063464837049\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.3780413605173252\n",
      "Epoch 97: Train Loss = 0.3772358610124749\n",
      "Epoch 98: Train Loss = 0.3765520131810903\n",
      "Epoch 99: Train Loss = 0.3760305910586037\n",
      "Epoch 100: Train Loss = 0.3754053857850901\n",
      "Recall = 0.9785426911041574, Aging Rate = 0.519813000890472, Precision = 0.9374732334047109\n",
      "Validation: Test Loss = 0.37459260923248894\n",
      "Recall = 0.9749664729548503, Aging Rate = 0.5131344612644702, precision = 0.9462039045553146\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.374736853189375\n",
      "Epoch 102: Train Loss = 0.37414296697211497\n",
      "Epoch 103: Train Loss = 0.3736035300755437\n",
      "Epoch 104: Train Loss = 0.37283050321301286\n",
      "Epoch 105: Train Loss = 0.37232421016119893\n",
      "Recall = 0.9812248547161377, Aging Rate = 0.5166963490650045, Precision = 0.9457130547177941\n",
      "Validation: Test Loss = 0.3714504088287574\n",
      "Recall = 0.9825659365221279, Aging Rate = 0.5164737310774711, precision = 0.9474137931034483\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.37192266815916725\n",
      "Epoch 107: Train Loss = 0.371282910621283\n",
      "Epoch 108: Train Loss = 0.37069445275241314\n",
      "Epoch 109: Train Loss = 0.37014290552423773\n",
      "Epoch 110: Train Loss = 0.36981322456446486\n",
      "Recall = 0.9821189092534645, Aging Rate = 0.5155832591273375, Precision = 0.9486183074265976\n",
      "Validation: Test Loss = 0.3689223222454424\n",
      "Recall = 0.9834599910594546, Aging Rate = 0.5138023152270703, precision = 0.9532062391681109\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3691623423477421\n",
      "Epoch 112: Train Loss = 0.3687418690772324\n",
      "Epoch 113: Train Loss = 0.36830117526270933\n",
      "Epoch 114: Train Loss = 0.3676914404464002\n",
      "Epoch 115: Train Loss = 0.367342770126178\n",
      "Recall = 0.9843540455967814, Aging Rate = 0.5142475512021372, Precision = 0.9532467532467532\n",
      "Validation: Test Loss = 0.36661340732820735\n",
      "Recall = 0.9888243182834153, Aging Rate = 0.5173642030276047, precision = 0.9518072289156626\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.36680151722522475\n",
      "Epoch 117: Train Loss = 0.3664456637226251\n",
      "Epoch 118: Train Loss = 0.36592749727165075\n",
      "Epoch 119: Train Loss = 0.365547733840832\n",
      "Epoch 120: Train Loss = 0.36508451350437987\n",
      "Recall = 0.9874832364774251, Aging Rate = 0.5153606411398041, Precision = 0.9542116630669546\n",
      "Validation: Test Loss = 0.36431941297154075\n",
      "Recall = 0.9870362092087618, Aging Rate = 0.5142475512021372, precision = 0.9558441558441558\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3646499055735468\n",
      "Epoch 122: Train Loss = 0.3642724713415093\n",
      "Epoch 123: Train Loss = 0.363859422829994\n",
      "Epoch 124: Train Loss = 0.3635164178967582\n",
      "Epoch 125: Train Loss = 0.36304353250527316\n",
      "Recall = 0.9879302637460885, Aging Rate = 0.5140249332146037, Precision = 0.9571242962321351\n",
      "Validation: Test Loss = 0.3623792936997231\n",
      "Recall = 0.989718372820742, Aging Rate = 0.5149154051647373, precision = 0.9571984435797666\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.3625643796031218\n",
      "Epoch 127: Train Loss = 0.3622589681367203\n",
      "Epoch 128: Train Loss = 0.36192576625680456\n",
      "Epoch 129: Train Loss = 0.36173792866756105\n",
      "Epoch 130: Train Loss = 0.3611279941783457\n",
      "Recall = 0.9888243182834153, Aging Rate = 0.5140249332146037, Precision = 0.9579904720658293\n",
      "Validation: Test Loss = 0.36061959277808614\n",
      "Recall = 0.9910594546267323, Aging Rate = 0.5160284951024042, precision = 0.956427955133736\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.360855777669144\n",
      "Epoch 132: Train Loss = 0.36057876786905424\n",
      "Epoch 133: Train Loss = 0.3602226215977809\n",
      "Epoch 134: Train Loss = 0.35987586427881374\n",
      "Epoch 135: Train Loss = 0.3594734644316608\n",
      "Recall = 0.9892713455520786, Aging Rate = 0.5131344612644702, Precision = 0.9600867678958785\n",
      "Validation: Test Loss = 0.3589827355431958\n",
      "Recall = 0.9906124273580689, Aging Rate = 0.5149154051647373, precision = 0.958063121487246\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.3592701877267469\n",
      "Epoch 137: Train Loss = 0.35879461580053146\n",
      "Epoch 138: Train Loss = 0.35865116055693247\n",
      "Epoch 139: Train Loss = 0.35819718602610184\n",
      "Epoch 140: Train Loss = 0.3578996397986119\n",
      "Recall = 0.989718372820742, Aging Rate = 0.5122439893143366, Precision = 0.9621903520208605\n",
      "Validation: Test Loss = 0.3573597509311122\n",
      "Recall = 0.9910594546267323, Aging Rate = 0.5133570792520036, precision = 0.9614050303555941\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.3576705015512417\n",
      "Epoch 142: Train Loss = 0.3573936283694562\n",
      "Epoch 143: Train Loss = 0.3571909320534921\n",
      "Epoch 144: Train Loss = 0.35678861808373474\n",
      "Epoch 145: Train Loss = 0.3565735844555224\n",
      "Recall = 0.9906124273580689, Aging Rate = 0.5126892252894034, Precision = 0.9622231871471993\n",
      "Validation: Test Loss = 0.35588304259261166\n",
      "Recall = 0.9915064818953956, Aging Rate = 0.5129118432769367, precision = 0.9626736111111112\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.3562575147444928\n",
      "Epoch 147: Train Loss = 0.35599954159580377\n",
      "Epoch 148: Train Loss = 0.35571245829228937\n",
      "Epoch 149: Train Loss = 0.35546833425362195\n",
      "Epoch 150: Train Loss = 0.35523751863707204\n",
      "Recall = 0.9915064818953956, Aging Rate = 0.5111308993766697, Precision = 0.9660278745644599\n",
      "Validation: Test Loss = 0.3547234454275982\n",
      "Recall = 0.9924005364327224, Aging Rate = 0.5131344612644702, precision = 0.9631236442516269\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.3700024861996578\n",
      "Recall = 0.9828496042216359, Aging Rate = 0.5340453938584779, precision = 0.93125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d290af607c48f18894ef2076e9d626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6787659359742569\n",
      "Epoch 2: Train Loss = 0.6639101504218016\n",
      "Epoch 3: Train Loss = 0.6502024995781753\n",
      "Epoch 4: Train Loss = 0.6358290677185262\n",
      "Epoch 5: Train Loss = 0.6182718998814734\n",
      "Recall = 0.9845132743362832, Aging Rate = 0.8844612644701692, Precision = 0.5600302038761641\n",
      "Validation: Test Loss = 0.6071455712203776\n",
      "Recall = 0.977433628318584, Aging Rate = 0.8386019590382903, precision = 0.5864082824528802\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.598623535246692\n",
      "Epoch 7: Train Loss = 0.5813633991158231\n",
      "Epoch 8: Train Loss = 0.5666069400066366\n",
      "Epoch 9: Train Loss = 0.5541651645146197\n",
      "Epoch 10: Train Loss = 0.5441299846739187\n",
      "Recall = 0.9128318584070797, Aging Rate = 0.6442564559216385, Precision = 0.7128541810642709\n",
      "Validation: Test Loss = 0.5387669398863191\n",
      "Recall = 0.9092920353982301, Aging Rate = 0.6333481745325023, precision = 0.7223198594024605\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.5355824039434601\n",
      "Epoch 12: Train Loss = 0.5290140867339429\n",
      "Epoch 13: Train Loss = 0.5230683895592592\n",
      "Epoch 14: Train Loss = 0.517957752714811\n",
      "Epoch 15: Train Loss = 0.5136335312736321\n",
      "Recall = 0.9039823008849558, Aging Rate = 0.6126447016918967, Precision = 0.7423691860465116\n",
      "Validation: Test Loss = 0.5107622772375601\n",
      "Recall = 0.9053097345132743, Aging Rate = 0.6081923419412288, precision = 0.7489019033674963\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5093139274939829\n",
      "Epoch 17: Train Loss = 0.5058272487864575\n",
      "Epoch 18: Train Loss = 0.5029583819084473\n",
      "Epoch 19: Train Loss = 0.49979260754924953\n",
      "Epoch 20: Train Loss = 0.4966395117030649\n",
      "Recall = 0.9106194690265487, Aging Rate = 0.6046304541406946, Precision = 0.7577319587628866\n",
      "Validation: Test Loss = 0.4948990559429328\n",
      "Recall = 0.9061946902654867, Aging Rate = 0.5957257346393589, precision = 0.7653213751868461\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.4941143131319795\n",
      "Epoch 22: Train Loss = 0.49158764174763686\n",
      "Epoch 23: Train Loss = 0.4889398591388896\n",
      "Epoch 24: Train Loss = 0.4868474901784029\n",
      "Epoch 25: Train Loss = 0.4842368068964696\n",
      "Recall = 0.9172566371681415, Aging Rate = 0.5992876224398932, Precision = 0.7700594353640416\n",
      "Validation: Test Loss = 0.4827469217384488\n",
      "Recall = 0.911504424778761, Aging Rate = 0.5868210151380232, precision = 0.7814871016691958\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.4822778693024643\n",
      "Epoch 27: Train Loss = 0.47983811887268074\n",
      "Epoch 28: Train Loss = 0.47766224746712593\n",
      "Epoch 29: Train Loss = 0.475547558398302\n",
      "Epoch 30: Train Loss = 0.4735789882319375\n",
      "Recall = 0.9203539823008849, Aging Rate = 0.5874888691006234, Precision = 0.7881773399014779\n",
      "Validation: Test Loss = 0.47208473258854977\n",
      "Recall = 0.9305309734513274, Aging Rate = 0.5959483526268923, precision = 0.7855808741128129\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.47146212208090565\n",
      "Epoch 32: Train Loss = 0.46964859858964664\n",
      "Epoch 33: Train Loss = 0.46730930977173396\n",
      "Epoch 34: Train Loss = 0.4656960497662518\n",
      "Epoch 35: Train Loss = 0.4635561145742557\n",
      "Recall = 0.9278761061946903, Aging Rate = 0.5801424755120214, Precision = 0.8046815042210284\n",
      "Validation: Test Loss = 0.46182479897144113\n",
      "Recall = 0.9327433628318584, Aging Rate = 0.5817008014247551, precision = 0.8067355530042097\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.46141016658458556\n",
      "Epoch 37: Train Loss = 0.4596208111048806\n",
      "Epoch 38: Train Loss = 0.45737364211468856\n",
      "Epoch 39: Train Loss = 0.45543569087557567\n",
      "Epoch 40: Train Loss = 0.45344335252115586\n",
      "Recall = 0.9376106194690266, Aging Rate = 0.5768032056990204, Precision = 0.8178309532998842\n",
      "Validation: Test Loss = 0.45189954840701707\n",
      "Recall = 0.9384955752212389, Aging Rate = 0.5750222617987534, precision = 0.8211382113821138\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.4515952406880692\n",
      "Epoch 42: Train Loss = 0.4497262667250867\n",
      "Epoch 43: Train Loss = 0.44777127760707114\n",
      "Epoch 44: Train Loss = 0.44575691679600404\n",
      "Epoch 45: Train Loss = 0.4436641408467229\n",
      "Recall = 0.9407079646017699, Aging Rate = 0.5621104185218165, Precision = 0.841980198019802\n",
      "Validation: Test Loss = 0.4423979437733376\n",
      "Recall = 0.9482300884955752, Aging Rate = 0.5705699020480854, precision = 0.8361295357003512\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.442018245652862\n",
      "Epoch 47: Train Loss = 0.440031003203651\n",
      "Epoch 48: Train Loss = 0.43820630461110244\n",
      "Epoch 49: Train Loss = 0.4366859846473908\n",
      "Epoch 50: Train Loss = 0.43478872796200474\n",
      "Recall = 0.945575221238938, Aging Rate = 0.5567675868210151, Precision = 0.8544582167133147\n",
      "Validation: Test Loss = 0.4331611938383148\n",
      "Recall = 0.9495575221238938, Aging Rate = 0.5598842386464826, precision = 0.8532803180914513\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.43303106503622507\n",
      "Epoch 52: Train Loss = 0.4311181797263671\n",
      "Epoch 53: Train Loss = 0.42965493160809987\n",
      "Epoch 54: Train Loss = 0.4279831645484493\n",
      "Epoch 55: Train Loss = 0.42614777894393735\n",
      "Recall = 0.95, Aging Rate = 0.553873552983081, Precision = 0.8629421221864951\n",
      "Validation: Test Loss = 0.42483059325498446\n",
      "Recall = 0.95, Aging Rate = 0.5491985752448798, precision = 0.8702877989460883\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.4245091831694728\n",
      "Epoch 57: Train Loss = 0.4233208738685397\n",
      "Epoch 58: Train Loss = 0.4213692928836055\n",
      "Epoch 59: Train Loss = 0.4200546609060316\n",
      "Epoch 60: Train Loss = 0.41843184584607424\n",
      "Recall = 0.9557522123893806, Aging Rate = 0.5478628673196795, Precision = 0.8776919951239334\n",
      "Validation: Test Loss = 0.41707869875887515\n",
      "Recall = 0.9606194690265487, Aging Rate = 0.5529830810329475, precision = 0.8739935587761675\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.41712878146876425\n",
      "Epoch 62: Train Loss = 0.41566170087268284\n",
      "Epoch 63: Train Loss = 0.4144290796317697\n",
      "Epoch 64: Train Loss = 0.41297829148499754\n",
      "Epoch 65: Train Loss = 0.4117062438946161\n",
      "Recall = 0.9601769911504425, Aging Rate = 0.5458593054318789, Precision = 0.8849918433931484\n",
      "Validation: Test Loss = 0.41081004395404247\n",
      "Recall = 0.965929203539823, Aging Rate = 0.5529830810329475, precision = 0.8788244766505636\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4105449710057871\n",
      "Epoch 67: Train Loss = 0.4091574624247139\n",
      "Epoch 68: Train Loss = 0.40777593407902674\n",
      "Epoch 69: Train Loss = 0.4066490326358714\n",
      "Epoch 70: Train Loss = 0.4051801627327689\n",
      "Recall = 0.9654867256637168, Aging Rate = 0.5409617097061442, Precision = 0.8979423868312757\n",
      "Validation: Test Loss = 0.4043636599344965\n",
      "Recall = 0.9610619469026549, Aging Rate = 0.5329474621549422, precision = 0.9072681704260651\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.40414240953652647\n",
      "Epoch 72: Train Loss = 0.4033572387981924\n",
      "Epoch 73: Train Loss = 0.40208616919105644\n",
      "Epoch 74: Train Loss = 0.4008268020509718\n",
      "Epoch 75: Train Loss = 0.3998070192835953\n",
      "Recall = 0.970353982300885, Aging Rate = 0.5389581478183437, Precision = 0.9058240396530359\n",
      "Validation: Test Loss = 0.39851931788405454\n",
      "Recall = 0.9716814159292035, Aging Rate = 0.5391807658058771, precision = 0.9066886870355079\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.3985592641951458\n",
      "Epoch 77: Train Loss = 0.3975268828741483\n",
      "Epoch 78: Train Loss = 0.39661310187218346\n",
      "Epoch 79: Train Loss = 0.3956169108142836\n",
      "Epoch 80: Train Loss = 0.39457758836521173\n",
      "Recall = 0.970353982300885, Aging Rate = 0.5351736420302761, Precision = 0.9122296173044925\n",
      "Validation: Test Loss = 0.3934292799377696\n",
      "Recall = 0.9699115044247788, Aging Rate = 0.5333926981300089, precision = 0.9148580968280468\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.3933943283950656\n",
      "Epoch 82: Train Loss = 0.39250941124752176\n",
      "Epoch 83: Train Loss = 0.39168977679042866\n",
      "Epoch 84: Train Loss = 0.3907846069346553\n",
      "Epoch 85: Train Loss = 0.3896245275452428\n",
      "Recall = 0.9707964601769912, Aging Rate = 0.5327248441674087, Precision = 0.9168407856247388\n",
      "Validation: Test Loss = 0.3886783120893307\n",
      "Recall = 0.9721238938053097, Aging Rate = 0.5307212822796082, precision = 0.9215604026845637\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.38871952570238394\n",
      "Epoch 87: Train Loss = 0.3880715087812497\n",
      "Epoch 88: Train Loss = 0.38711251911581995\n",
      "Epoch 89: Train Loss = 0.3863367421597322\n",
      "Epoch 90: Train Loss = 0.38536524353341556\n",
      "Recall = 0.9734513274336283, Aging Rate = 0.5302760463045414, Precision = 0.9235936188077246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.3843332951820438\n",
      "Recall = 0.9743362831858408, Aging Rate = 0.5304986642920748, precision = 0.9240453210239195\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.3846028627875864\n",
      "Epoch 92: Train Loss = 0.3834773317735745\n",
      "Epoch 93: Train Loss = 0.3832345060864837\n",
      "Epoch 94: Train Loss = 0.3822705617517843\n",
      "Epoch 95: Train Loss = 0.3812048817891047\n",
      "Recall = 0.9743362831858408, Aging Rate = 0.5269367764915405, Precision = 0.9302915082382763\n",
      "Validation: Test Loss = 0.3804760082481381\n",
      "Recall = 0.977433628318584, Aging Rate = 0.5289403383793411, precision = 0.9297138047138047\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.3808760194510938\n",
      "Epoch 97: Train Loss = 0.3798847304904238\n",
      "Epoch 98: Train Loss = 0.37920863444743574\n",
      "Epoch 99: Train Loss = 0.3782877371922626\n",
      "Epoch 100: Train Loss = 0.3777257932262132\n",
      "Recall = 0.9783185840707964, Aging Rate = 0.5262689225289403, Precision = 0.9352791878172588\n",
      "Validation: Test Loss = 0.37679045872293404\n",
      "Recall = 0.9805309734513274, Aging Rate = 0.5271593944790739, precision = 0.9358108108108109\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.3769705532548264\n",
      "Epoch 102: Train Loss = 0.3763825187753166\n",
      "Epoch 103: Train Loss = 0.3757958509299974\n",
      "Epoch 104: Train Loss = 0.3753457143969124\n",
      "Epoch 105: Train Loss = 0.3744105656340201\n",
      "Recall = 0.9809734513274336, Aging Rate = 0.5227070347284061, Precision = 0.9442078364565588\n",
      "Validation: Test Loss = 0.3744156454040446\n",
      "Recall = 0.9880530973451327, Aging Rate = 0.5327248441674087, precision = 0.9331383201002925\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.374087654180221\n",
      "Epoch 107: Train Loss = 0.3734062844000007\n",
      "Epoch 108: Train Loss = 0.3727249914476729\n",
      "Epoch 109: Train Loss = 0.3720879416484867\n",
      "Epoch 110: Train Loss = 0.3715941435454259\n",
      "Recall = 0.9836283185840708, Aging Rate = 0.5224844167408726, Precision = 0.9471665956540264\n",
      "Validation: Test Loss = 0.37072327127970445\n",
      "Recall = 0.9853982300884956, Aging Rate = 0.5242653606411398, precision = 0.9456475583864119\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.37094156718742394\n",
      "Epoch 112: Train Loss = 0.3704940756834307\n",
      "Epoch 113: Train Loss = 0.370007783015286\n",
      "Epoch 114: Train Loss = 0.3694319908149733\n",
      "Epoch 115: Train Loss = 0.3691929339672557\n",
      "Recall = 0.9845132743362832, Aging Rate = 0.5207034728406055, Precision = 0.9512612227447628\n",
      "Validation: Test Loss = 0.36816698779830526\n",
      "Recall = 0.9876106194690265, Aging Rate = 0.52493321460374, precision = 0.9465648854961832\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.36843302397460465\n",
      "Epoch 117: Train Loss = 0.3679698238336711\n",
      "Epoch 118: Train Loss = 0.3675231900421085\n",
      "Epoch 119: Train Loss = 0.3670165139123572\n",
      "Epoch 120: Train Loss = 0.36661386755249487\n",
      "Recall = 0.9867256637168141, Aging Rate = 0.5211487088156723, Precision = 0.9525843656557027\n",
      "Validation: Test Loss = 0.36574750130238115\n",
      "Recall = 0.9876106194690265, Aging Rate = 0.5231522707034728, precision = 0.9497872340425532\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3659823785004399\n",
      "Epoch 122: Train Loss = 0.365776728251526\n",
      "Epoch 123: Train Loss = 0.3652412324533437\n",
      "Epoch 124: Train Loss = 0.3647111659323861\n",
      "Epoch 125: Train Loss = 0.364469338370983\n",
      "Recall = 0.9880530973451327, Aging Rate = 0.5211487088156723, Precision = 0.9538658692866296\n",
      "Validation: Test Loss = 0.3635902282444367\n",
      "Recall = 0.9876106194690265, Aging Rate = 0.5195903829029386, precision = 0.9562982005141388\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.3640515250705335\n",
      "Epoch 127: Train Loss = 0.3636260813405656\n",
      "Epoch 128: Train Loss = 0.3632741805335187\n",
      "Epoch 129: Train Loss = 0.3628777218310831\n",
      "Epoch 130: Train Loss = 0.3624931070970194\n",
      "Recall = 0.988495575221239, Aging Rate = 0.5209260908281389, Precision = 0.9547008547008548\n",
      "Validation: Test Loss = 0.36170210290570826\n",
      "Recall = 0.9898230088495575, Aging Rate = 0.5207034728406055, precision = 0.9563916203505771\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.3622016515300832\n",
      "Epoch 132: Train Loss = 0.36194074841237767\n",
      "Epoch 133: Train Loss = 0.36142089238468283\n",
      "Epoch 134: Train Loss = 0.36132075026538474\n",
      "Epoch 135: Train Loss = 0.36072240004238015\n",
      "Recall = 0.9907079646017699, Aging Rate = 0.5213713268032057, Precision = 0.9560204953031597\n",
      "Validation: Test Loss = 0.3600040648467607\n",
      "Recall = 0.9907079646017699, Aging Rate = 0.5213713268032057, precision = 0.9560204953031597\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.3602588472627468\n",
      "Epoch 137: Train Loss = 0.3601389298827544\n",
      "Epoch 138: Train Loss = 0.3596304723808835\n",
      "Epoch 139: Train Loss = 0.3594208679237434\n",
      "Epoch 140: Train Loss = 0.3591211145198037\n",
      "Recall = 0.9911504424778761, Aging Rate = 0.5211487088156723, Precision = 0.9568560444254592\n",
      "Validation: Test Loss = 0.35842287874816253\n",
      "Recall = 0.9907079646017699, Aging Rate = 0.5180320569902048, precision = 0.9621830683283197\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.35880268615266625\n",
      "Epoch 142: Train Loss = 0.3584914024876147\n",
      "Epoch 143: Train Loss = 0.35832928002995246\n",
      "Epoch 144: Train Loss = 0.35808397034715567\n",
      "Epoch 145: Train Loss = 0.3576848112570847\n",
      "Recall = 0.9920353982300885, Aging Rate = 0.519813000890472, Precision = 0.9601713062098501\n",
      "Validation: Test Loss = 0.35695472473771256\n",
      "Recall = 0.9929203539823008, Aging Rate = 0.519813000890472, precision = 0.9610278372591007\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.3575433894713224\n",
      "Epoch 147: Train Loss = 0.35718181535588345\n",
      "Epoch 148: Train Loss = 0.35680916464870144\n",
      "Epoch 149: Train Loss = 0.3565917909676118\n",
      "Epoch 150: Train Loss = 0.3564485835105526\n",
      "Recall = 0.9924778761061946, Aging Rate = 0.5189225289403384, Precision = 0.9622479622479623\n",
      "Validation: Test Loss = 0.3557449259985162\n",
      "Recall = 0.9938053097345133, Aging Rate = 0.5207034728406055, precision = 0.960239418554938\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.37227662546294077\n",
      "Recall = 0.9836734693877551, Aging Rate = 0.5233644859813084, precision = 0.9221938775510204\n",
      "\u001b[32m[I 2022-05-26 16:20:52,979]\u001b[0m Trial 0 finished with value: 0.9553852997690416 and parameters: {'batch_size': 32, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'bad_weight': 0.6}. Best is trial 0 with value: 0.9553852997690416.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f533b2f91740ceb0af2d64b6d4714a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6725691308002862\n",
      "Epoch 2: Train Loss = 0.5970997019527432\n",
      "Epoch 3: Train Loss = 0.542420795902225\n",
      "Epoch 4: Train Loss = 0.5128459348438684\n",
      "Epoch 5: Train Loss = 0.4951298911711624\n",
      "Recall = 0.8451641526175687, Aging Rate = 0.5140249332146037, Precision = 0.8250324815937635\n",
      "Validation: Test Loss = 0.48196924136137176\n",
      "Recall = 0.854924578527063, Aging Rate = 0.5080142475512022, precision = 0.8444347063978965\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.47644362063250795\n",
      "Epoch 7: Train Loss = 0.46249027153475414\n",
      "Epoch 8: Train Loss = 0.4489913001616725\n",
      "Epoch 9: Train Loss = 0.43776319004124226\n",
      "Epoch 10: Train Loss = 0.42646006577692175\n",
      "Recall = 0.9139307897071872, Aging Rate = 0.5060106856634016, Precision = 0.9062912450505939\n",
      "Validation: Test Loss = 0.41876924430697704\n",
      "Recall = 0.9263531499556344, Aging Rate = 0.5091273374888691, precision = 0.9129864451246174\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.415938982900932\n",
      "Epoch 12: Train Loss = 0.40716184443484854\n",
      "Epoch 13: Train Loss = 0.39911585329689303\n",
      "Epoch 14: Train Loss = 0.3917772591060968\n",
      "Epoch 15: Train Loss = 0.3851290954507469\n",
      "Recall = 0.948092280390417, Aging Rate = 0.5006678539626002, Precision = 0.9502000889284127\n",
      "Validation: Test Loss = 0.37985182639115217\n",
      "Recall = 0.9547471162377995, Aging Rate = 0.5008904719501336, precision = 0.9564444444444444\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3789558888596291\n",
      "Epoch 17: Train Loss = 0.3739257801194756\n",
      "Epoch 18: Train Loss = 0.36917128060295024\n",
      "Epoch 19: Train Loss = 0.3653755636217333\n",
      "Epoch 20: Train Loss = 0.3614147033738113\n",
      "Recall = 0.9671694764862466, Aging Rate = 0.4977738201246661, Precision = 0.9749552772808586\n",
      "Validation: Test Loss = 0.35797868404978633\n",
      "Recall = 0.9716060337178349, Aging Rate = 0.49799643811219946, precision = 0.9789897183728208\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3581886146871086\n",
      "Epoch 22: Train Loss = 0.35554480656172055\n",
      "Epoch 23: Train Loss = 0.35292301728379377\n",
      "Epoch 24: Train Loss = 0.35120037877442895\n",
      "Epoch 25: Train Loss = 0.348243799893217\n",
      "Recall = 0.9800354924578527, Aging Rate = 0.5004452359750667, Precision = 0.9826512455516014\n",
      "Validation: Test Loss = 0.3465190013656633\n",
      "Recall = 0.9787045252883763, Aging Rate = 0.4986642920747996, precision = 0.9848214285714286\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.34650838377745363\n",
      "Epoch 27: Train Loss = 0.34467329110084427\n",
      "Epoch 28: Train Loss = 0.3430219497515066\n",
      "Epoch 29: Train Loss = 0.34177906709065525\n",
      "Epoch 30: Train Loss = 0.3405088804867898\n",
      "Recall = 0.9871339840283939, Aging Rate = 0.5024487978628673, Precision = 0.9858218874612317\n",
      "Validation: Test Loss = 0.3402106710979159\n",
      "Recall = 0.9826974267968057, Aging Rate = 0.49755120213713266, precision = 0.9910514541387024\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3396307778538919\n",
      "Epoch 32: Train Loss = 0.33860186742865817\n",
      "Epoch 33: Train Loss = 0.33720285674343126\n",
      "Epoch 34: Train Loss = 0.3362723006962244\n",
      "Epoch 35: Train Loss = 0.3351497614320006\n",
      "Recall = 0.9884649511978705, Aging Rate = 0.5004452359750667, Precision = 0.9911032028469751\n",
      "Validation: Test Loss = 0.3341319646192043\n",
      "Recall = 0.9884649511978705, Aging Rate = 0.5004452359750667, precision = 0.9911032028469751\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3343622905276019\n",
      "Epoch 37: Train Loss = 0.3335167906534852\n",
      "Epoch 38: Train Loss = 0.332746591561624\n",
      "Epoch 39: Train Loss = 0.33206626820649193\n",
      "Epoch 40: Train Loss = 0.3315186717631237\n",
      "Recall = 0.9897959183673469, Aging Rate = 0.5002226179875334, Precision = 0.9928793947485536\n",
      "Validation: Test Loss = 0.33024643070975906\n",
      "Recall = 0.9911268855368234, Aging Rate = 0.5002226179875334, precision = 0.9942145082331998\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3311161888250791\n",
      "Epoch 42: Train Loss = 0.3304959952512811\n",
      "Epoch 43: Train Loss = 0.32994420206769703\n",
      "Epoch 44: Train Loss = 0.32955265249509635\n",
      "Epoch 45: Train Loss = 0.32915498880223304\n",
      "Recall = 0.9911268855368234, Aging Rate = 0.5008904719501336, Precision = 0.9928888888888889\n",
      "Validation: Test Loss = 0.32767918950091063\n",
      "Recall = 0.9920141969831411, Aging Rate = 0.5004452359750667, precision = 0.994661921708185\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.32815184305527334\n",
      "Epoch 47: Train Loss = 0.3279345762432418\n",
      "Epoch 48: Train Loss = 0.3274227222319915\n",
      "Epoch 49: Train Loss = 0.32708934639461956\n",
      "Epoch 50: Train Loss = 0.3268053765182291\n",
      "Recall = 0.9929015084294588, Aging Rate = 0.5013357079252003, Precision = 0.9937833037300178\n",
      "Validation: Test Loss = 0.32622536995638085\n",
      "Recall = 0.9955634427684117, Aging Rate = 0.5031166518254675, precision = 0.9929203539823008\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3261243810583626\n",
      "Epoch 52: Train Loss = 0.32596719376444283\n",
      "Epoch 53: Train Loss = 0.3252825885878433\n",
      "Epoch 54: Train Loss = 0.3250477220315755\n",
      "Epoch 55: Train Loss = 0.32457988567895796\n",
      "Recall = 0.9946761313220941, Aging Rate = 0.501113089937667, Precision = 0.9960017769880053\n",
      "Validation: Test Loss = 0.32359188646476184\n",
      "Recall = 0.9951197870452528, Aging Rate = 0.5013357079252003, precision = 0.9960035523978685\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.3243425794191267\n",
      "Epoch 57: Train Loss = 0.3239906256154299\n",
      "Epoch 58: Train Loss = 0.3235078636334607\n",
      "Epoch 59: Train Loss = 0.323297907599146\n",
      "Epoch 60: Train Loss = 0.32280221053350217\n",
      "Recall = 0.9955634427684117, Aging Rate = 0.5008904719501336, Precision = 0.9973333333333333\n",
      "Validation: Test Loss = 0.32218880934154787\n",
      "Recall = 0.9960070984915705, Aging Rate = 0.5008904719501336, precision = 0.9977777777777778\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.3229169279788078\n",
      "Epoch 62: Train Loss = 0.32255337908134224\n",
      "Epoch 63: Train Loss = 0.3223879829327761\n",
      "Epoch 64: Train Loss = 0.32204594512868967\n",
      "Epoch 65: Train Loss = 0.3216689306502881\n",
      "Recall = 0.9964507542147294, Aging Rate = 0.5015583259127337, Precision = 0.9968930315135375\n",
      "Validation: Test Loss = 0.32119865785829743\n",
      "Recall = 0.9955634427684117, Aging Rate = 0.5002226179875334, precision = 0.9986648865153538\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.3218554140517251\n",
      "Epoch 67: Train Loss = 0.32143464947213896\n",
      "Epoch 68: Train Loss = 0.3217008986965631\n",
      "Epoch 69: Train Loss = 0.3212906481268569\n",
      "Epoch 70: Train Loss = 0.32098765691156805\n",
      "Recall = 0.9964507542147294, Aging Rate = 0.501113089937667, Precision = 0.9977787649933363\n",
      "Validation: Test Loss = 0.3209248825522904\n",
      "Recall = 0.9977817213842058, Aging Rate = 0.5022261798753339, precision = 0.9968971631205674\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.3207920152803457\n",
      "Epoch 72: Train Loss = 0.320762204333065\n",
      "Epoch 73: Train Loss = 0.3205689174833833\n",
      "Epoch 74: Train Loss = 0.32054939467994\n",
      "Epoch 75: Train Loss = 0.32034558357665927\n",
      "Recall = 0.9964507542147294, Aging Rate = 0.5008904719501336, Precision = 0.9982222222222222\n",
      "Validation: Test Loss = 0.3199036263347415\n",
      "Recall = 0.997338065661047, Aging Rate = 0.5015583259127337, precision = 0.9977807367953839\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.3202385815625093\n",
      "Epoch 77: Train Loss = 0.3202236601792588\n",
      "Epoch 78: Train Loss = 0.31987621886647394\n",
      "Epoch 79: Train Loss = 0.3199908024799792\n",
      "Epoch 80: Train Loss = 0.3200680647221189\n",
      "Recall = 0.9968944099378882, Aging Rate = 0.5015583259127337, Precision = 0.9973368841544608\n",
      "Validation: Test Loss = 0.3216837333019355\n",
      "Recall = 0.9955634427684117, Aging Rate = 0.5002226179875334, precision = 0.9986648865153538\n",
      "\n",
      "Epoch 81: Train Loss = 0.32009497162813816\n",
      "Epoch 82: Train Loss = 0.3197220730813401\n",
      "Epoch 83: Train Loss = 0.3192444298008352\n",
      "Epoch 84: Train Loss = 0.3195677218114491\n",
      "Epoch 85: Train Loss = 0.3191105161834166\n",
      "Recall = 0.9977817213842058, Aging Rate = 0.5017809439002672, Precision = 0.9977817213842058\n",
      "Validation: Test Loss = 0.31857015745828754\n",
      "Recall = 0.9982253771073647, Aging Rate = 0.5017809439002672, precision = 0.9982253771073647\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.31909229613687984\n",
      "Epoch 87: Train Loss = 0.31882293887257257\n",
      "Epoch 88: Train Loss = 0.31901149272600243\n",
      "Epoch 89: Train Loss = 0.31873159862902156\n",
      "Epoch 90: Train Loss = 0.31870468047083433\n",
      "Recall = 0.9977817213842058, Aging Rate = 0.5013357079252003, Precision = 0.9986678507992895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.31826425254185714\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5022261798753339, precision = 0.9977836879432624\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.31867887285386254\n",
      "Epoch 92: Train Loss = 0.3186155391238358\n",
      "Epoch 93: Train Loss = 0.318756746614181\n",
      "Epoch 94: Train Loss = 0.31833500038908724\n",
      "Epoch 95: Train Loss = 0.31842774457944256\n",
      "Recall = 0.9982253771073647, Aging Rate = 0.5020035618878005, Precision = 0.9977827050997783\n",
      "Validation: Test Loss = 0.3178997015645222\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5020035618878005, precision = 0.9982261640798226\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.31834903937944425\n",
      "Epoch 97: Train Loss = 0.31854447616282466\n",
      "Epoch 98: Train Loss = 0.31823189207622227\n",
      "Epoch 99: Train Loss = 0.3182032592097034\n",
      "Epoch 100: Train Loss = 0.3181636156986997\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5024487978628673, Precision = 0.9977846699158175\n",
      "Validation: Test Loss = 0.3177847332542535\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5020035618878005, precision = 0.9982261640798226\n",
      "\n",
      "Epoch 101: Train Loss = 0.31816340204019794\n",
      "Epoch 102: Train Loss = 0.31814500965503106\n",
      "Epoch 103: Train Loss = 0.3178986273009333\n",
      "Epoch 104: Train Loss = 0.3179779157825378\n",
      "Epoch 105: Train Loss = 0.3180760780775643\n",
      "Recall = 0.9982253771073647, Aging Rate = 0.5020035618878005, Precision = 0.9977827050997783\n",
      "Validation: Test Loss = 0.31778692355245114\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5024487978628673, precision = 0.9977846699158175\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.3179149794610398\n",
      "Epoch 107: Train Loss = 0.31786930083379294\n",
      "Epoch 108: Train Loss = 0.3179965559277063\n",
      "Epoch 109: Train Loss = 0.31793635781068197\n",
      "Epoch 110: Train Loss = 0.3177963440055202\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5022261798753339, Precision = 0.9977836879432624\n",
      "Validation: Test Loss = 0.3173261213440611\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3176759466037937\n",
      "Epoch 112: Train Loss = 0.3179294682196603\n",
      "Epoch 113: Train Loss = 0.3178218132347691\n",
      "Epoch 114: Train Loss = 0.31762526500787674\n",
      "Epoch 115: Train Loss = 0.317576629820405\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, Precision = 0.99822695035461\n",
      "Validation: Test Loss = 0.3172156380577376\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 116: Train Loss = 0.3175457210436742\n",
      "Epoch 117: Train Loss = 0.3175362737074773\n",
      "Epoch 118: Train Loss = 0.31754969581895287\n",
      "Epoch 119: Train Loss = 0.3175003134238327\n",
      "Epoch 120: Train Loss = 0.3174868032853729\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, Precision = 0.99822695035461\n",
      "Validation: Test Loss = 0.3170064351490748\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 121: Train Loss = 0.31746640103476875\n",
      "Epoch 122: Train Loss = 0.317357939556042\n",
      "Epoch 123: Train Loss = 0.31760466250052755\n",
      "Epoch 124: Train Loss = 0.3174247809636837\n",
      "Epoch 125: Train Loss = 0.317320248535034\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5024487978628673, Precision = 0.9977846699158175\n",
      "Validation: Test Loss = 0.31702715423843847\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 126: Train Loss = 0.3173994084659264\n",
      "Epoch 127: Train Loss = 0.3174648944915456\n",
      "Epoch 128: Train Loss = 0.3175163305134828\n",
      "Epoch 129: Train Loss = 0.3174059154566971\n",
      "Epoch 130: Train Loss = 0.3173862690386343\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5020035618878005, Precision = 0.9982261640798226\n",
      "Validation: Test Loss = 0.31706783809729805\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 131: Train Loss = 0.31727969537647516\n",
      "Epoch 132: Train Loss = 0.3171953570598592\n",
      "Epoch 133: Train Loss = 0.31726375386742534\n",
      "Epoch 134: Train Loss = 0.3172249828240538\n",
      "Epoch 135: Train Loss = 0.31713215814354373\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5020035618878005, Precision = 0.9982261640798226\n",
      "Validation: Test Loss = 0.31683984900412554\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 136: Train Loss = 0.31727635340295723\n",
      "Epoch 137: Train Loss = 0.3173060671941361\n",
      "Epoch 138: Train Loss = 0.31726109421582277\n",
      "Epoch 139: Train Loss = 0.31723630160279076\n",
      "Epoch 140: Train Loss = 0.31722683709642235\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5020035618878005, Precision = 0.9982261640798226\n",
      "Validation: Test Loss = 0.31690821561129734\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 141: Train Loss = 0.3172363195691062\n",
      "Epoch 142: Train Loss = 0.3173520954081976\n",
      "Epoch 143: Train Loss = 0.3173987392273102\n",
      "Epoch 144: Train Loss = 0.31735679631984776\n",
      "Epoch 145: Train Loss = 0.3171742677900904\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5020035618878005, Precision = 0.9982261640798226\n",
      "Validation: Test Loss = 0.3170637651043923\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Epoch 146: Train Loss = 0.31719741092551956\n",
      "Epoch 147: Train Loss = 0.31737834313355273\n",
      "Epoch 148: Train Loss = 0.3171324950982074\n",
      "Epoch 149: Train Loss = 0.31733741530964443\n",
      "Epoch 150: Train Loss = 0.3172505046253425\n",
      "Recall = 0.9986690328305236, Aging Rate = 0.5022261798753339, Precision = 0.9977836879432624\n",
      "Validation: Test Loss = 0.3168252403462241\n",
      "Recall = 0.9991126885536823, Aging Rate = 0.5022261798753339, precision = 0.99822695035461\n",
      "\n",
      "Validation: Test Loss = 0.3362224387310853\n",
      "Recall = 0.9770580296896086, Aging Rate = 0.4939919893190921, precision = 0.9783783783783784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d766e19e540d4525bdcc7f0e78fc2aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6677720964963375\n",
      "Epoch 2: Train Loss = 0.5928725370953153\n",
      "Epoch 3: Train Loss = 0.5389500644625241\n",
      "Epoch 4: Train Loss = 0.5124045773371988\n",
      "Epoch 5: Train Loss = 0.49326097299133365\n",
      "Recall = 0.8502862175253192, Aging Rate = 0.519813000890472, Precision = 0.8269807280513919\n",
      "Validation: Test Loss = 0.4777969440390569\n",
      "Recall = 0.8687802730074857, Aging Rate = 0.5149154051647373, precision = 0.8530047557284911\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.47240737651143877\n",
      "Epoch 7: Train Loss = 0.45658390891413975\n",
      "Epoch 8: Train Loss = 0.4431762719196819\n",
      "Epoch 9: Train Loss = 0.43141008766230365\n",
      "Epoch 10: Train Loss = 0.42074436902044504\n",
      "Recall = 0.9304271246147072, Aging Rate = 0.518699910952805, Precision = 0.9068669527896995\n",
      "Validation: Test Loss = 0.4139827392988723\n",
      "Recall = 0.9308674592690445, Aging Rate = 0.5126892252894034, precision = 0.9179331306990881\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.4113886080091804\n",
      "Epoch 12: Train Loss = 0.4033532801580981\n",
      "Epoch 13: Train Loss = 0.39562024286254005\n",
      "Epoch 14: Train Loss = 0.38983668019383055\n",
      "Epoch 15: Train Loss = 0.3846550297758352\n",
      "Recall = 0.9515631880228974, Aging Rate = 0.5082368655387355, Precision = 0.9465615418309242\n",
      "Validation: Test Loss = 0.3797732351724314\n",
      "Recall = 0.9603698811096433, Aging Rate = 0.511353517364203, precision = 0.9494993469743144\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.37901276971013653\n",
      "Epoch 17: Train Loss = 0.37596676249856514\n",
      "Epoch 18: Train Loss = 0.3705658581993565\n",
      "Epoch 19: Train Loss = 0.3672326740842787\n",
      "Epoch 20: Train Loss = 0.363857696206041\n",
      "Recall = 0.9652135623073536, Aging Rate = 0.5031166518254675, Precision = 0.9699115044247788\n",
      "Validation: Test Loss = 0.3606468015553584\n",
      "Recall = 0.9726992514310876, Aging Rate = 0.5069011576135352, precision = 0.9701361440491876\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3608673075852602\n",
      "Epoch 22: Train Loss = 0.3581366746264277\n",
      "Epoch 23: Train Loss = 0.35558846186869714\n",
      "Epoch 24: Train Loss = 0.3532384656840738\n",
      "Epoch 25: Train Loss = 0.35133943412841057\n",
      "Recall = 0.9766622633201233, Aging Rate = 0.5035618878005342, Precision = 0.9805481874447391\n",
      "Validation: Test Loss = 0.3489005879058651\n",
      "Recall = 0.9762219286657859, Aging Rate = 0.5006678539626002, precision = 0.9857714539795465\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.3492719588116249\n",
      "Epoch 27: Train Loss = 0.34791455662685744\n",
      "Epoch 28: Train Loss = 0.3459286225871751\n",
      "Epoch 29: Train Loss = 0.3444031723779116\n",
      "Epoch 30: Train Loss = 0.343002853468923\n",
      "Recall = 0.9828269484808454, Aging Rate = 0.5033392698130009, Precision = 0.9871738168951791\n",
      "Validation: Test Loss = 0.34167184717190235\n",
      "Recall = 0.9854689564068693, Aging Rate = 0.5057880676758683, precision = 0.9850352112676056\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3418080908107842\n",
      "Epoch 32: Train Loss = 0.3401568653534696\n",
      "Epoch 33: Train Loss = 0.33851569044409535\n",
      "Epoch 34: Train Loss = 0.3374321941860213\n",
      "Epoch 35: Train Loss = 0.33629876484004706\n",
      "Recall = 0.9898723029502422, Aging Rate = 0.506233303650935, Precision = 0.9885664028144239\n",
      "Validation: Test Loss = 0.33602357229163154\n",
      "Recall = 0.988110964332893, Aging Rate = 0.5042297417631345, precision = 0.990728476821192\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3355122178341805\n",
      "Epoch 37: Train Loss = 0.33437453071028655\n",
      "Epoch 38: Train Loss = 0.3334776219362036\n",
      "Epoch 39: Train Loss = 0.3327316742652458\n",
      "Epoch 40: Train Loss = 0.33203398279069474\n",
      "Recall = 0.992514310876266, Aging Rate = 0.5066785396260017, Precision = 0.9903339191564148\n",
      "Validation: Test Loss = 0.33084865215625064\n",
      "Recall = 0.9929546455306033, Aging Rate = 0.5066785396260017, precision = 0.9907732864674869\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3313848417408214\n",
      "Epoch 42: Train Loss = 0.33076712473205233\n",
      "Epoch 43: Train Loss = 0.3301461625874096\n",
      "Epoch 44: Train Loss = 0.3294828506526624\n",
      "Epoch 45: Train Loss = 0.3290070466770196\n",
      "Recall = 0.9947159841479525, Aging Rate = 0.507346393588602, Precision = 0.991224221149627\n",
      "Validation: Test Loss = 0.3283344249266763\n",
      "Recall = 0.9964773227653017, Aging Rate = 0.5084594835262689, precision = 0.9908056042031523\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.328480761176438\n",
      "Epoch 47: Train Loss = 0.32767135658544405\n",
      "Epoch 48: Train Loss = 0.32707396661821264\n",
      "Epoch 49: Train Loss = 0.32668512430662355\n",
      "Epoch 50: Train Loss = 0.3263920440221618\n",
      "Recall = 0.9964773227653017, Aging Rate = 0.507346393588602, Precision = 0.9929793769197016\n",
      "Validation: Test Loss = 0.3253061152523581\n",
      "Recall = 0.9977983267283135, Aging Rate = 0.5080142475512022, precision = 0.992988606485539\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3257121494967069\n",
      "Epoch 52: Train Loss = 0.3255299413352807\n",
      "Epoch 53: Train Loss = 0.3250836199240714\n",
      "Epoch 54: Train Loss = 0.324770998286012\n",
      "Epoch 55: Train Loss = 0.3244141614925405\n",
      "Recall = 0.9977983267283135, Aging Rate = 0.5075690115761353, Precision = 0.993859649122807\n",
      "Validation: Test Loss = 0.32413558859646796\n",
      "Recall = 0.9982386613826508, Aging Rate = 0.5077916295636687, precision = 0.9938623410784744\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.3241769154742692\n",
      "Epoch 57: Train Loss = 0.32421880854311946\n",
      "Epoch 58: Train Loss = 0.32385011130109603\n",
      "Epoch 59: Train Loss = 0.32363188816093486\n",
      "Epoch 60: Train Loss = 0.32338693642446426\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5077916295636687, Precision = 0.9943007452871547\n",
      "Validation: Test Loss = 0.3228635325404224\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5077916295636687, precision = 0.9943007452871547\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.32306637845183733\n",
      "Epoch 62: Train Loss = 0.32291235230060317\n",
      "Epoch 63: Train Loss = 0.3226846179734992\n",
      "Epoch 64: Train Loss = 0.3225349195016248\n",
      "Epoch 65: Train Loss = 0.3221916013918917\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5077916295636687, Precision = 0.9943007452871547\n",
      "Validation: Test Loss = 0.3218397538468759\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5077916295636687, precision = 0.9943007452871547\n",
      "\n",
      "Epoch 66: Train Loss = 0.32213822046986457\n",
      "Epoch 67: Train Loss = 0.32194432571119\n",
      "Epoch 68: Train Loss = 0.3218275908370795\n",
      "Epoch 69: Train Loss = 0.321756839699121\n",
      "Epoch 70: Train Loss = 0.3217897583943653\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5077916295636687, Precision = 0.9943007452871547\n",
      "Validation: Test Loss = 0.32109206514175936\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5077916295636687, precision = 0.9943007452871547\n",
      "\n",
      "Epoch 71: Train Loss = 0.3215136994290118\n",
      "Epoch 72: Train Loss = 0.3214029040807926\n",
      "Epoch 73: Train Loss = 0.3212151927599818\n",
      "Epoch 74: Train Loss = 0.32099686038886877\n",
      "Epoch 75: Train Loss = 0.32098075675836973\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5075690115761353, Precision = 0.9947368421052631\n",
      "Validation: Test Loss = 0.32078454803380174\n",
      "Recall = 0.9986789960369881, Aging Rate = 0.5075690115761353, precision = 0.9947368421052631\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.3209366797604735\n",
      "Epoch 77: Train Loss = 0.32074809217707767\n",
      "Epoch 78: Train Loss = 0.32072923415384436\n",
      "Epoch 79: Train Loss = 0.32044307453655707\n",
      "Epoch 80: Train Loss = 0.32020200649649566\n",
      "Recall = 0.9991193306913254, Aging Rate = 0.507346393588602, Precision = 0.9956121105748135\n",
      "Validation: Test Loss = 0.3198861682510546\n",
      "Recall = 0.9991193306913254, Aging Rate = 0.507346393588602, precision = 0.9956121105748135\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.3199886240177885\n",
      "Epoch 82: Train Loss = 0.3198584860865813\n",
      "Epoch 83: Train Loss = 0.319866331873042\n",
      "Epoch 84: Train Loss = 0.3197049732093607\n",
      "Epoch 85: Train Loss = 0.31951898328449085\n",
      "Recall = 0.9991193306913254, Aging Rate = 0.5071237756010686, Precision = 0.9960491659350307\n",
      "Validation: Test Loss = 0.31916223234187674\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, precision = 0.9960508995173322\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.3194371524232897\n",
      "Epoch 87: Train Loss = 0.3193922327866219\n",
      "Epoch 88: Train Loss = 0.3194606876065449\n",
      "Epoch 89: Train Loss = 0.31934267611558803\n",
      "Epoch 90: Train Loss = 0.3193217051262741\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, Precision = 0.9960508995173322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.31910772880378413\n",
      "Recall = 0.9991193306913254, Aging Rate = 0.5071237756010686, precision = 0.9960491659350307\n",
      "\n",
      "Epoch 91: Train Loss = 0.31931925576283055\n",
      "Epoch 92: Train Loss = 0.31932356818276436\n",
      "Epoch 93: Train Loss = 0.31898993673010373\n",
      "Epoch 94: Train Loss = 0.3189328195840678\n",
      "Epoch 95: Train Loss = 0.31889184359344963\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, Precision = 0.9960508995173322\n",
      "Validation: Test Loss = 0.3187620901467008\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, precision = 0.9960508995173322\n",
      "\n",
      "Epoch 96: Train Loss = 0.3188797529796777\n",
      "Epoch 97: Train Loss = 0.3188564573826794\n",
      "Epoch 98: Train Loss = 0.3188634797970525\n",
      "Epoch 99: Train Loss = 0.31878230758575277\n",
      "Epoch 100: Train Loss = 0.31871632136941913\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, Precision = 0.9960508995173322\n",
      "Validation: Test Loss = 0.31842257274863767\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, precision = 0.9960508995173322\n",
      "\n",
      "Epoch 101: Train Loss = 0.31864030352683337\n",
      "Epoch 102: Train Loss = 0.31869556151748446\n",
      "Epoch 103: Train Loss = 0.31864928631302725\n",
      "Epoch 104: Train Loss = 0.3185678139023768\n",
      "Epoch 105: Train Loss = 0.3186591430795267\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, Precision = 0.9960508995173322\n",
      "Validation: Test Loss = 0.31836855785717205\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.507346393588602, precision = 0.9960508995173322\n",
      "\n",
      "Epoch 106: Train Loss = 0.3185936129740701\n",
      "Epoch 107: Train Loss = 0.3186633701528276\n",
      "Epoch 108: Train Loss = 0.318518441220213\n",
      "Epoch 109: Train Loss = 0.31855078862908687\n",
      "Epoch 110: Train Loss = 0.31848929956462485\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.3179936014374557\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3182583311149719\n",
      "Epoch 112: Train Loss = 0.3181517380799763\n",
      "Epoch 113: Train Loss = 0.31826746776925596\n",
      "Epoch 114: Train Loss = 0.31813933107859305\n",
      "Epoch 115: Train Loss = 0.3181591269328779\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.3180616799997413\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 116: Train Loss = 0.3183186546777469\n",
      "Epoch 117: Train Loss = 0.31812542127586324\n",
      "Epoch 118: Train Loss = 0.31804755075319685\n",
      "Epoch 119: Train Loss = 0.31815682926139766\n",
      "Epoch 120: Train Loss = 0.31818204619262436\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.3177814189334693\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 121: Train Loss = 0.31806960935478007\n",
      "Epoch 122: Train Loss = 0.3180518863143819\n",
      "Epoch 123: Train Loss = 0.3179985366808126\n",
      "Epoch 124: Train Loss = 0.3179079569935905\n",
      "Epoch 125: Train Loss = 0.3179093883012091\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.318201184246228\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 126: Train Loss = 0.31802915627151757\n",
      "Epoch 127: Train Loss = 0.3179666500269889\n",
      "Epoch 128: Train Loss = 0.317970241087415\n",
      "Epoch 129: Train Loss = 0.31791517207480285\n",
      "Epoch 130: Train Loss = 0.3181039081655861\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.31784008874600206\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 131: Train Loss = 0.31790935759659017\n",
      "Epoch 132: Train Loss = 0.31791227649920556\n",
      "Epoch 133: Train Loss = 0.31827357167233766\n",
      "Epoch 134: Train Loss = 0.31806662660141877\n",
      "Epoch 135: Train Loss = 0.3177843789967276\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.31775192965488824\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 136: Train Loss = 0.3177611346714006\n",
      "Epoch 137: Train Loss = 0.31801125290451154\n",
      "Epoch 138: Train Loss = 0.3177948281474126\n",
      "Epoch 139: Train Loss = 0.3178482674002541\n",
      "Epoch 140: Train Loss = 0.3177602407078815\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.3175062550196983\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 141: Train Loss = 0.31779792660703005\n",
      "Epoch 142: Train Loss = 0.3177846059242955\n",
      "Epoch 143: Train Loss = 0.3181105239361604\n",
      "Epoch 144: Train Loss = 0.31794490051503066\n",
      "Epoch 145: Train Loss = 0.31783483294854714\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.31765280537911006\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, precision = 0.9964881474978051\n",
      "\n",
      "Epoch 146: Train Loss = 0.31764843054148945\n",
      "Epoch 147: Train Loss = 0.31773322228863105\n",
      "Epoch 148: Train Loss = 0.3176210631192633\n",
      "Epoch 149: Train Loss = 0.3178743025020946\n",
      "Epoch 150: Train Loss = 0.31779266061893235\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5071237756010686, Precision = 0.9964881474978051\n",
      "Validation: Test Loss = 0.3172964679271328\n",
      "Recall = 0.9995596653456627, Aging Rate = 0.5069011576135352, precision = 0.9969257795344751\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.33476502510988826\n",
      "Recall = 0.9875690607734806, Aging Rate = 0.4893190921228304, precision = 0.975443383356071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c391a9f4db4eefacd766d9204d312c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6768532912325668\n",
      "Epoch 2: Train Loss = 0.6079549306965468\n",
      "Epoch 3: Train Loss = 0.5439820688745324\n",
      "Epoch 4: Train Loss = 0.5170172861879252\n",
      "Epoch 5: Train Loss = 0.49508976872648813\n",
      "Recall = 0.8302139037433155, Aging Rate = 0.4995547640249332, Precision = 0.8302139037433155\n",
      "Validation: Test Loss = 0.4812603773331918\n",
      "Recall = 0.8667557932263814, Aging Rate = 0.5169189670525378, precision = 0.8376399655469423\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.47416577839150775\n",
      "Epoch 7: Train Loss = 0.456751916016836\n",
      "Epoch 8: Train Loss = 0.43994206487336335\n",
      "Epoch 9: Train Loss = 0.4250614838682958\n",
      "Epoch 10: Train Loss = 0.41129567898073477\n",
      "Recall = 0.9340463458110517, Aging Rate = 0.5024487978628673, Precision = 0.9286663712893221\n",
      "Validation: Test Loss = 0.40402932315136\n",
      "Recall = 0.9367201426024956, Aging Rate = 0.4986642920747996, precision = 0.9383928571428571\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.40179393218228143\n",
      "Epoch 12: Train Loss = 0.3921162453169071\n",
      "Epoch 13: Train Loss = 0.3843152124172646\n",
      "Epoch 14: Train Loss = 0.377894964317923\n",
      "Epoch 15: Train Loss = 0.3721609392658685\n",
      "Recall = 0.9652406417112299, Aging Rate = 0.5024487978628673, Precision = 0.9596809924678777\n",
      "Validation: Test Loss = 0.36782102566792935\n",
      "Recall = 0.9763814616755794, Aging Rate = 0.5069011576135352, precision = 0.9622310057092666\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3668354616988905\n",
      "Epoch 17: Train Loss = 0.3629147259656171\n",
      "Epoch 18: Train Loss = 0.35954284474240383\n",
      "Epoch 19: Train Loss = 0.3554113034892061\n",
      "Epoch 20: Train Loss = 0.3524573565272487\n",
      "Recall = 0.981729055258467, Aging Rate = 0.5008904719501336, Precision = 0.9791111111111112\n",
      "Validation: Test Loss = 0.34980340905733015\n",
      "Recall = 0.982620320855615, Aging Rate = 0.5004452359750667, precision = 0.9808718861209964\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.34959190476184854\n",
      "Epoch 22: Train Loss = 0.3471996152337279\n",
      "Epoch 23: Train Loss = 0.34531749799542416\n",
      "Epoch 24: Train Loss = 0.343216520303716\n",
      "Epoch 25: Train Loss = 0.3416069508766555\n",
      "Recall = 0.9884135472370766, Aging Rate = 0.5024487978628673, Precision = 0.9827204253433761\n",
      "Validation: Test Loss = 0.3397870217172142\n",
      "Recall = 0.9897504456327986, Aging Rate = 0.5022261798753339, precision = 0.9844858156028369\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.3400359375606343\n",
      "Epoch 27: Train Loss = 0.3389772364101767\n",
      "Epoch 28: Train Loss = 0.3374334294033815\n",
      "Epoch 29: Train Loss = 0.336265940992087\n",
      "Epoch 30: Train Loss = 0.335001117744726\n",
      "Recall = 0.9919786096256684, Aging Rate = 0.5013357079252003, Precision = 0.9884547069271759\n",
      "Validation: Test Loss = 0.3336884736059398\n",
      "Recall = 0.9933155080213903, Aging Rate = 0.5022261798753339, precision = 0.988031914893617\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3339305498561791\n",
      "Epoch 32: Train Loss = 0.33343229945069003\n",
      "Epoch 33: Train Loss = 0.33259551246996344\n",
      "Epoch 34: Train Loss = 0.33136085450702335\n",
      "Epoch 35: Train Loss = 0.33091143460965855\n",
      "Recall = 0.9928698752228164, Aging Rate = 0.5013357079252003, Precision = 0.9893428063943162\n",
      "Validation: Test Loss = 0.32925144965799386\n",
      "Recall = 0.9950980392156863, Aging Rate = 0.5020035618878005, precision = 0.9902439024390244\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3297674970656554\n",
      "Epoch 37: Train Loss = 0.32899929393325017\n",
      "Epoch 38: Train Loss = 0.3284596356290425\n",
      "Epoch 39: Train Loss = 0.327886423465936\n",
      "Epoch 40: Train Loss = 0.32754922276719384\n",
      "Recall = 0.9950980392156863, Aging Rate = 0.5006678539626002, Precision = 0.9928857269897733\n",
      "Validation: Test Loss = 0.3265436940021209\n",
      "Recall = 0.9973262032085561, Aging Rate = 0.5017809439002672, precision = 0.9929015084294588\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3266471087242595\n",
      "Epoch 42: Train Loss = 0.3261616916021702\n",
      "Epoch 43: Train Loss = 0.32617480953357525\n",
      "Epoch 44: Train Loss = 0.32545533184482917\n",
      "Epoch 45: Train Loss = 0.32523830735248216\n",
      "Recall = 0.9968805704099821, Aging Rate = 0.501113089937667, Precision = 0.9937805419813416\n",
      "Validation: Test Loss = 0.3242965652968134\n",
      "Recall = 0.9968805704099821, Aging Rate = 0.501113089937667, precision = 0.9937805419813416\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3246594761910443\n",
      "Epoch 47: Train Loss = 0.32424971774446043\n",
      "Epoch 48: Train Loss = 0.3241138322907479\n",
      "Epoch 49: Train Loss = 0.3239861059857604\n",
      "Epoch 50: Train Loss = 0.32376426583087986\n",
      "Recall = 0.9968805704099821, Aging Rate = 0.501113089937667, Precision = 0.9937805419813416\n",
      "Validation: Test Loss = 0.3230844320512094\n",
      "Recall = 0.9977718360071302, Aging Rate = 0.5013357079252003, precision = 0.9942273534635879\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3233646870667873\n",
      "Epoch 52: Train Loss = 0.3231479100690606\n",
      "Epoch 53: Train Loss = 0.32301953403098393\n",
      "Epoch 54: Train Loss = 0.3225730281266801\n",
      "Epoch 55: Train Loss = 0.3224806420418798\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5017809439002672, Precision = 0.9942324755989352\n",
      "Validation: Test Loss = 0.32254261011962687\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5024487978628673, precision = 0.9933540097474524\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.32252057696811237\n",
      "Epoch 57: Train Loss = 0.3220859731506898\n",
      "Epoch 58: Train Loss = 0.3220578129283042\n",
      "Epoch 59: Train Loss = 0.32185358836092487\n",
      "Epoch 60: Train Loss = 0.3216285219918588\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5020035618878005, Precision = 0.9937915742793791\n",
      "Validation: Test Loss = 0.320993477068941\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5015583259127337, precision = 0.9946737683089214\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.3213874654162705\n",
      "Epoch 62: Train Loss = 0.3212241896379238\n",
      "Epoch 63: Train Loss = 0.3212584386897321\n",
      "Epoch 64: Train Loss = 0.3208438973552079\n",
      "Epoch 65: Train Loss = 0.32082030445151527\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5015583259127337, Precision = 0.9951176209498447\n",
      "Validation: Test Loss = 0.32027655428578994\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.501113089937667, precision = 0.9955575299866726\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.32062040262740316\n",
      "Epoch 67: Train Loss = 0.32057279043184894\n",
      "Epoch 68: Train Loss = 0.3203775781079901\n",
      "Epoch 69: Train Loss = 0.3202830665045727\n",
      "Epoch 70: Train Loss = 0.3202444525404053\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.501113089937667, Precision = 0.9955575299866726\n",
      "Validation: Test Loss = 0.31999243799852456\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, precision = 0.9955595026642984\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.32010376617936076\n",
      "Epoch 72: Train Loss = 0.31994832822603514\n",
      "Epoch 73: Train Loss = 0.3198409226730479\n",
      "Epoch 74: Train Loss = 0.3199244706044745\n",
      "Epoch 75: Train Loss = 0.3198115752938594\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5008904719501336, Precision = 0.996\n",
      "Validation: Test Loss = 0.31944389058772943\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, precision = 0.9955595026642984\n",
      "\n",
      "Epoch 76: Train Loss = 0.31978669322078396\n",
      "Epoch 77: Train Loss = 0.3196371125516781\n",
      "Epoch 78: Train Loss = 0.3195705622408078\n",
      "Epoch 79: Train Loss = 0.3195357700595873\n",
      "Epoch 80: Train Loss = 0.31940873279066245\n",
      "Recall = 0.9982174688057041, Aging Rate = 0.5006678539626002, Precision = 0.9959982214317474\n",
      "Validation: Test Loss = 0.31920849504687376\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5008904719501336, precision = 0.996\n",
      "\n",
      "Epoch 81: Train Loss = 0.3194629980428667\n",
      "Epoch 82: Train Loss = 0.3194731145380654\n",
      "Epoch 83: Train Loss = 0.31934336862814605\n",
      "Epoch 84: Train Loss = 0.3192280956866586\n",
      "Epoch 85: Train Loss = 0.31911474461016226\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5008904719501336, Precision = 0.996\n",
      "Validation: Test Loss = 0.3188374575131298\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, precision = 0.9955595026642984\n",
      "\n",
      "Epoch 86: Train Loss = 0.31911767372995853\n",
      "Epoch 87: Train Loss = 0.31901747805670977\n",
      "Epoch 88: Train Loss = 0.3192308657899671\n",
      "Epoch 89: Train Loss = 0.31912172936692584\n",
      "Epoch 90: Train Loss = 0.31927160559120926\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.501113089937667, Precision = 0.9955575299866726\n",
      "Validation: Test Loss = 0.3185954271473209\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, precision = 0.9955595026642984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: Train Loss = 0.3187235978333311\n",
      "Epoch 92: Train Loss = 0.31890587265854525\n",
      "Epoch 93: Train Loss = 0.31887181011672966\n",
      "Epoch 94: Train Loss = 0.3188840435587288\n",
      "Epoch 95: Train Loss = 0.3187889660296436\n",
      "Recall = 0.9982174688057041, Aging Rate = 0.5008904719501336, Precision = 0.9955555555555555\n",
      "Validation: Test Loss = 0.31833587025598237\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, precision = 0.9955595026642984\n",
      "\n",
      "Epoch 96: Train Loss = 0.3188926266806527\n",
      "Epoch 97: Train Loss = 0.3186691305197888\n",
      "Epoch 98: Train Loss = 0.3186780603624091\n",
      "Epoch 99: Train Loss = 0.31879270113905095\n",
      "Epoch 100: Train Loss = 0.31859483749656303\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, Precision = 0.9955595026642984\n",
      "Validation: Test Loss = 0.3181345493052543\n",
      "Recall = 0.999554367201426, Aging Rate = 0.5015583259127337, precision = 0.9955614735907679\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.318483549520997\n",
      "Epoch 102: Train Loss = 0.3183831927612862\n",
      "Epoch 103: Train Loss = 0.31839677038621605\n",
      "Epoch 104: Train Loss = 0.3185750258520896\n",
      "Epoch 105: Train Loss = 0.3183975726604886\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, Precision = 0.9955595026642984\n",
      "Validation: Test Loss = 0.3184948442561862\n",
      "Recall = 0.9986631016042781, Aging Rate = 0.5004452359750667, precision = 0.9968861209964412\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.318462355982907\n",
      "Epoch 107: Train Loss = 0.31849085947603173\n",
      "Epoch 108: Train Loss = 0.3183692631876479\n",
      "Epoch 109: Train Loss = 0.31834080547280114\n",
      "Epoch 110: Train Loss = 0.3181894406450718\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5013357079252003, Precision = 0.9955595026642984\n",
      "Validation: Test Loss = 0.3177973767369322\n",
      "Recall = 1.0, Aging Rate = 0.5017809439002672, precision = 0.9955634427684117\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.3181074077225326\n",
      "Epoch 112: Train Loss = 0.3181705357820353\n",
      "Epoch 113: Train Loss = 0.31830343739432304\n",
      "Epoch 114: Train Loss = 0.31820199198735577\n",
      "Epoch 115: Train Loss = 0.31821014036159906\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.501113089937667, Precision = 0.9960017769880053\n",
      "Validation: Test Loss = 0.31783204596698655\n",
      "Recall = 1.0, Aging Rate = 0.5017809439002672, precision = 0.9955634427684117\n",
      "\n",
      "Epoch 116: Train Loss = 0.3181400033956326\n",
      "Epoch 117: Train Loss = 0.3180690248524198\n",
      "Epoch 118: Train Loss = 0.3181824834879657\n",
      "Epoch 119: Train Loss = 0.3180222426634862\n",
      "Epoch 120: Train Loss = 0.3179057811055986\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.501113089937667, Precision = 0.9960017769880053\n",
      "Validation: Test Loss = 0.31745842977601935\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.996005326231691\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3179060456376997\n",
      "Epoch 122: Train Loss = 0.3181111867525486\n",
      "Epoch 123: Train Loss = 0.3178546320404841\n",
      "Epoch 124: Train Loss = 0.3179872115063858\n",
      "Epoch 125: Train Loss = 0.3177802228333158\n",
      "Recall = 0.999554367201426, Aging Rate = 0.5008904719501336, Precision = 0.9968888888888889\n",
      "Validation: Test Loss = 0.31744197700985394\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.996005326231691\n",
      "\n",
      "Epoch 126: Train Loss = 0.31798018240440346\n",
      "Epoch 127: Train Loss = 0.31785342421047624\n",
      "Epoch 128: Train Loss = 0.31775858881530866\n",
      "Epoch 129: Train Loss = 0.3178497759869135\n",
      "Epoch 130: Train Loss = 0.3176656922695367\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, Precision = 0.996005326231691\n",
      "Validation: Test Loss = 0.3172812169933574\n",
      "Recall = 0.999554367201426, Aging Rate = 0.5008904719501336, precision = 0.9968888888888889\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.31774727080617754\n",
      "Epoch 132: Train Loss = 0.3178407222112161\n",
      "Epoch 133: Train Loss = 0.31771272312076837\n",
      "Epoch 134: Train Loss = 0.3176439333896178\n",
      "Epoch 135: Train Loss = 0.31778544153046206\n",
      "Recall = 0.999554367201426, Aging Rate = 0.5013357079252003, Precision = 0.9960035523978685\n",
      "Validation: Test Loss = 0.31726909540958526\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.996005326231691\n",
      "\n",
      "Epoch 136: Train Loss = 0.31762565801107556\n",
      "Epoch 137: Train Loss = 0.31764214498489324\n",
      "Epoch 138: Train Loss = 0.317802234037795\n",
      "Epoch 139: Train Loss = 0.3177816871278326\n",
      "Epoch 140: Train Loss = 0.31777649210270026\n",
      "Recall = 0.999554367201426, Aging Rate = 0.501113089937667, Precision = 0.9964460239893381\n",
      "Validation: Test Loss = 0.31754114441009684\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.996005326231691\n",
      "\n",
      "Epoch 141: Train Loss = 0.31781795172423843\n",
      "Epoch 142: Train Loss = 0.3176092989510547\n",
      "Epoch 143: Train Loss = 0.3175512133224246\n",
      "Epoch 144: Train Loss = 0.3175088760317805\n",
      "Epoch 145: Train Loss = 0.3176789721897003\n",
      "Recall = 0.9991087344028521, Aging Rate = 0.5006678539626002, Precision = 0.9968875055580257\n",
      "Validation: Test Loss = 0.31769744026162\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.996005326231691\n",
      "\n",
      "Epoch 146: Train Loss = 0.31780784058974243\n",
      "Epoch 147: Train Loss = 0.317681516188123\n",
      "Epoch 148: Train Loss = 0.31751226155437323\n",
      "Epoch 149: Train Loss = 0.3174889310862693\n",
      "Epoch 150: Train Loss = 0.3174857524549547\n",
      "Recall = 1.0, Aging Rate = 0.5013357079252003, Precision = 0.9964476021314387\n",
      "Validation: Test Loss = 0.3172904972720125\n",
      "Recall = 0.999554367201426, Aging Rate = 0.5008904719501336, precision = 0.9968888888888889\n",
      "\n",
      "Validation: Test Loss = 0.3376314950005235\n",
      "Recall = 0.9813581890812251, Aging Rate = 0.5066755674232309, precision = 0.9710144927536232\n",
      "\u001b[32m[I 2022-05-26 16:21:31,818]\u001b[0m Trial 1 finished with value: 0.97844849003899 and parameters: {'batch_size': 96, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'bad_weight': 0.5}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc79286fcfc439bad51d8d9530ad897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5405750447156062\n",
      "Epoch 2: Train Loss = 0.5110704484839367\n",
      "Epoch 3: Train Loss = 0.4666747465424525\n",
      "Epoch 4: Train Loss = 0.4415221231278838\n",
      "Epoch 5: Train Loss = 0.4323857100755109\n",
      "Recall = 0.9787985865724381, Aging Rate = 0.7350845948352627, Precision = 0.6711084191399153\n",
      "Validation: Test Loss = 0.4229669802335364\n",
      "Recall = 0.991166077738516, Aging Rate = 0.7404274265360641, precision = 0.6746843054720385\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.42192031248063777\n",
      "Epoch 7: Train Loss = 0.415711535769494\n",
      "Epoch 8: Train Loss = 0.4098684195419135\n",
      "Epoch 9: Train Loss = 0.4025109648598376\n",
      "Epoch 10: Train Loss = 0.3958468813505538\n",
      "Recall = 0.9880742049469965, Aging Rate = 0.6605075690115761, Precision = 0.7539602291877318\n",
      "Validation: Test Loss = 0.39145895886506127\n",
      "Recall = 0.9907243816254417, Aging Rate = 0.6589492430988424, precision = 0.7577702702702702\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.3894178346131173\n",
      "Epoch 12: Train Loss = 0.38202186728946247\n",
      "Epoch 13: Train Loss = 0.37443650376552573\n",
      "Epoch 14: Train Loss = 0.3684952606978633\n",
      "Epoch 15: Train Loss = 0.362636490767276\n",
      "Recall = 0.9946996466431095, Aging Rate = 0.5899376669634907, Precision = 0.849811320754717\n",
      "Validation: Test Loss = 0.3584167264660659\n",
      "Recall = 0.9982332155477032, Aging Rate = 0.5919412288512912, precision = 0.849943587814968\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3579336079973678\n",
      "Epoch 17: Train Loss = 0.353779701096398\n",
      "Epoch 18: Train Loss = 0.349445169272427\n",
      "Epoch 19: Train Loss = 0.34638302077274713\n",
      "Epoch 20: Train Loss = 0.3439199770813209\n",
      "Recall = 0.9969081272084805, Aging Rate = 0.5496438112199465, Precision = 0.9141352774402592\n",
      "Validation: Test Loss = 0.3399558952103103\n",
      "Recall = 0.9982332155477032, Aging Rate = 0.5485307212822796, precision = 0.9172077922077922\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.33998973366413815\n",
      "Epoch 22: Train Loss = 0.33802599967428326\n",
      "Epoch 23: Train Loss = 0.3355960964572716\n",
      "Epoch 24: Train Loss = 0.33321791887283325\n",
      "Epoch 25: Train Loss = 0.33205681893301137\n",
      "Recall = 0.9986749116607774, Aging Rate = 0.5293855743544078, Precision = 0.9507989907485281\n",
      "Validation: Test Loss = 0.33033778931769747\n",
      "Recall = 0.9991166077738516, Aging Rate = 0.5293855743544078, precision = 0.9512195121951219\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.33068908741722547\n",
      "Epoch 27: Train Loss = 0.32963184371444654\n",
      "Epoch 28: Train Loss = 0.32886275274883076\n",
      "Epoch 29: Train Loss = 0.3278826172343345\n",
      "Epoch 30: Train Loss = 0.32725086407478854\n",
      "Recall = 0.9986749116607774, Aging Rate = 0.5227070347284061, Precision = 0.962947189097104\n",
      "Validation: Test Loss = 0.3255944873006452\n",
      "Recall = 0.9991166077738516, Aging Rate = 0.5200356188780053, precision = 0.9683219178082192\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.32666263116966476\n",
      "Epoch 32: Train Loss = 0.32586795394163737\n",
      "Epoch 33: Train Loss = 0.32551193497378383\n",
      "Epoch 34: Train Loss = 0.3248328430675972\n",
      "Epoch 35: Train Loss = 0.32425678837861954\n",
      "Recall = 1.0, Aging Rate = 0.5191451469278717, Precision = 0.9708404802744426\n",
      "Validation: Test Loss = 0.3230097842927714\n",
      "Recall = 0.9995583038869258, Aging Rate = 0.517586821015138, precision = 0.9733333333333334\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3235324804844436\n",
      "Epoch 37: Train Loss = 0.32348080357588094\n",
      "Epoch 38: Train Loss = 0.32294651568732086\n",
      "Epoch 39: Train Loss = 0.3223944055057485\n",
      "Epoch 40: Train Loss = 0.32187063271513183\n",
      "Recall = 1.0, Aging Rate = 0.5164737310774711, Precision = 0.9758620689655172\n",
      "Validation: Test Loss = 0.3215499429010644\n",
      "Recall = 1.0, Aging Rate = 0.5146927871772039, precision = 0.9792387543252595\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3217969384187051\n",
      "Epoch 42: Train Loss = 0.3215595269086418\n",
      "Epoch 43: Train Loss = 0.32119042254513325\n",
      "Epoch 44: Train Loss = 0.32139110936719395\n",
      "Epoch 45: Train Loss = 0.321034147366815\n",
      "Recall = 1.0, Aging Rate = 0.5146927871772039, Precision = 0.9792387543252595\n",
      "Validation: Test Loss = 0.3202183587096784\n",
      "Recall = 1.0, Aging Rate = 0.5144701691896705, precision = 0.9796624837732584\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.32094868218803235\n",
      "Epoch 47: Train Loss = 0.3207335882917219\n",
      "Epoch 48: Train Loss = 0.3204655279459321\n",
      "Epoch 49: Train Loss = 0.32018381558882797\n",
      "Epoch 50: Train Loss = 0.31987295788733533\n",
      "Recall = 1.0, Aging Rate = 0.5133570792520036, Precision = 0.981786643538595\n",
      "Validation: Test Loss = 0.31924723634099916\n",
      "Recall = 1.0, Aging Rate = 0.5122439893143366, precision = 0.9839200347674923\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3199577567997726\n",
      "Epoch 52: Train Loss = 0.31941644551492865\n",
      "Epoch 53: Train Loss = 0.3197366209138427\n",
      "Epoch 54: Train Loss = 0.3191281808664623\n",
      "Epoch 55: Train Loss = 0.3187318824012685\n",
      "Recall = 1.0, Aging Rate = 0.5106856634016028, Precision = 0.986922406277245\n",
      "Validation: Test Loss = 0.31876506151520345\n",
      "Recall = 1.0, Aging Rate = 0.5109082813891362, precision = 0.9864923747276688\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.31878258045082736\n",
      "Epoch 57: Train Loss = 0.31847617414946655\n",
      "Epoch 58: Train Loss = 0.3183755981051699\n",
      "Epoch 59: Train Loss = 0.31857439631451906\n",
      "Epoch 60: Train Loss = 0.31812477624002355\n",
      "Recall = 1.0, Aging Rate = 0.5097951914514692, Precision = 0.988646288209607\n",
      "Validation: Test Loss = 0.3178904112544531\n",
      "Recall = 1.0, Aging Rate = 0.5093499554764025, precision = 0.9895104895104895\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.31811004747053606\n",
      "Epoch 62: Train Loss = 0.3184639022006377\n",
      "Epoch 63: Train Loss = 0.3180699092356732\n",
      "Epoch 64: Train Loss = 0.3179579572229649\n",
      "Epoch 65: Train Loss = 0.31839076241848197\n",
      "Recall = 1.0, Aging Rate = 0.5091273374888691, Precision = 0.989943156974202\n",
      "Validation: Test Loss = 0.31860680263804625\n",
      "Recall = 1.0, Aging Rate = 0.5106856634016028, precision = 0.986922406277245\n",
      "\n",
      "Epoch 66: Train Loss = 0.3182392378577354\n",
      "Epoch 67: Train Loss = 0.3177987353035306\n",
      "Epoch 68: Train Loss = 0.31766346280105606\n",
      "Epoch 69: Train Loss = 0.3177838514451989\n",
      "Epoch 70: Train Loss = 0.31781273870625243\n",
      "Recall = 1.0, Aging Rate = 0.5086821015138023, Precision = 0.9908096280087527\n",
      "Validation: Test Loss = 0.3178908223301199\n",
      "Recall = 1.0, Aging Rate = 0.5084594835262689, precision = 0.9912434325744308\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.3178326984069651\n",
      "Epoch 72: Train Loss = 0.3175573487262692\n",
      "Epoch 73: Train Loss = 0.3176575282377955\n",
      "Epoch 74: Train Loss = 0.3176056855985976\n",
      "Epoch 75: Train Loss = 0.3174637549176135\n",
      "Recall = 1.0, Aging Rate = 0.5084594835262689, Precision = 0.9912434325744308\n",
      "Validation: Test Loss = 0.3174782486388647\n",
      "Recall = 1.0, Aging Rate = 0.5084594835262689, precision = 0.9912434325744308\n",
      "\n",
      "Epoch 76: Train Loss = 0.31760544755155234\n",
      "Epoch 77: Train Loss = 0.3177875932952918\n",
      "Epoch 78: Train Loss = 0.3174972547714135\n",
      "Epoch 79: Train Loss = 0.3174947198755701\n",
      "Epoch 80: Train Loss = 0.31752746663025627\n",
      "Recall = 1.0, Aging Rate = 0.5082368655387355, Precision = 0.9916776171703898\n",
      "Validation: Test Loss = 0.3171965610418379\n",
      "Recall = 1.0, Aging Rate = 0.5084594835262689, precision = 0.9912434325744308\n",
      "\n",
      "Epoch 81: Train Loss = 0.31737369795198006\n",
      "Epoch 82: Train Loss = 0.3175904758520033\n",
      "Epoch 83: Train Loss = 0.3174966556200043\n",
      "Epoch 84: Train Loss = 0.3176661587833615\n",
      "Epoch 85: Train Loss = 0.3178279922673029\n",
      "Recall = 1.0, Aging Rate = 0.5082368655387355, Precision = 0.9916776171703898\n",
      "Validation: Test Loss = 0.3170720968305905\n",
      "Recall = 1.0, Aging Rate = 0.5080142475512022, precision = 0.9921121822962313\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.3173525185321764\n",
      "Epoch 87: Train Loss = 0.31738096027106766\n",
      "Epoch 88: Train Loss = 0.3173089049962197\n",
      "Epoch 89: Train Loss = 0.31710938054434656\n",
      "Epoch 90: Train Loss = 0.31731200597695974\n",
      "Recall = 1.0, Aging Rate = 0.5080142475512022, Precision = 0.9921121822962313\n",
      "Validation: Test Loss = 0.31699545860608985\n",
      "Recall = 1.0, Aging Rate = 0.5080142475512022, precision = 0.9921121822962313\n",
      "\n",
      "Epoch 91: Train Loss = 0.317306473466401\n",
      "Epoch 92: Train Loss = 0.317291624293408\n",
      "Epoch 93: Train Loss = 0.31722568496995385\n",
      "Epoch 94: Train Loss = 0.31741070975603425\n",
      "Epoch 95: Train Loss = 0.31728544167822637\n",
      "Recall = 1.0, Aging Rate = 0.5082368655387355, Precision = 0.9916776171703898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.31699989036800386\n",
      "Recall = 1.0, Aging Rate = 0.5080142475512022, precision = 0.9921121822962313\n",
      "\n",
      "Epoch 96: Train Loss = 0.31725375793496097\n",
      "Epoch 97: Train Loss = 0.3174273319416352\n",
      "Epoch 98: Train Loss = 0.3173878433867851\n",
      "Epoch 99: Train Loss = 0.31708606525605426\n",
      "Epoch 100: Train Loss = 0.31792056353518927\n",
      "Recall = 1.0, Aging Rate = 0.5084594835262689, Precision = 0.9912434325744308\n",
      "Validation: Test Loss = 0.3245047000456578\n",
      "Recall = 0.997791519434629, Aging Rate = 0.506233303650935, precision = 0.9934036939313984\n",
      "\n",
      "Epoch 101: Train Loss = 0.31789274045323435\n",
      "Epoch 102: Train Loss = 0.31690714519573765\n",
      "Epoch 103: Train Loss = 0.317016424420468\n",
      "Epoch 104: Train Loss = 0.31692355568665853\n",
      "Epoch 105: Train Loss = 0.31716501741991016\n",
      "Recall = 1.0, Aging Rate = 0.5077916295636687, Precision = 0.9925471284524331\n",
      "Validation: Test Loss = 0.3169757396327738\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, precision = 0.9934181658622203\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.3171117888798378\n",
      "Epoch 107: Train Loss = 0.3169575677529468\n",
      "Epoch 108: Train Loss = 0.3170361904193542\n",
      "Epoch 109: Train Loss = 0.31724296580545625\n",
      "Epoch 110: Train Loss = 0.31708288896009523\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9934181658622203\n",
      "Validation: Test Loss = 0.31662803238136683\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, precision = 0.9934181658622203\n",
      "\n",
      "Epoch 111: Train Loss = 0.3169728560789504\n",
      "Epoch 112: Train Loss = 0.31690490588160997\n",
      "Epoch 113: Train Loss = 0.3171869554587591\n",
      "Epoch 114: Train Loss = 0.3169203417065519\n",
      "Epoch 115: Train Loss = 0.31704054482261307\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, Precision = 0.9938542581211589\n",
      "Validation: Test Loss = 0.31671302950711305\n",
      "Recall = 1.0, Aging Rate = 0.5075690115761353, precision = 0.9929824561403509\n",
      "\n",
      "Epoch 116: Train Loss = 0.31699725255728406\n",
      "Epoch 117: Train Loss = 0.31685623346113884\n",
      "Epoch 118: Train Loss = 0.31707577987218266\n",
      "Epoch 119: Train Loss = 0.3168553664869001\n",
      "Epoch 120: Train Loss = 0.31709530341869363\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9934181658622203\n",
      "Validation: Test Loss = 0.3166000301404819\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, precision = 0.9938542581211589\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3168868238864365\n",
      "Epoch 122: Train Loss = 0.31689584878864613\n",
      "Epoch 123: Train Loss = 0.31692158262961057\n",
      "Epoch 124: Train Loss = 0.31689946389261997\n",
      "Epoch 125: Train Loss = 0.3169895341538576\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9934181658622203\n",
      "Validation: Test Loss = 0.31640761865108863\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, precision = 0.9934181658622203\n",
      "\n",
      "Epoch 126: Train Loss = 0.31728703279847664\n",
      "Epoch 127: Train Loss = 0.31681904361275615\n",
      "Epoch 128: Train Loss = 0.3170147311549471\n",
      "Epoch 129: Train Loss = 0.31686743966511927\n",
      "Epoch 130: Train Loss = 0.3168289716606785\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9934181658622203\n",
      "Validation: Test Loss = 0.31646704219965033\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, precision = 0.9938542581211589\n",
      "\n",
      "Epoch 131: Train Loss = 0.3169813471931702\n",
      "Epoch 132: Train Loss = 0.31661952770616153\n",
      "Epoch 133: Train Loss = 0.31677919987154557\n",
      "Epoch 134: Train Loss = 0.3168875676706659\n",
      "Epoch 135: Train Loss = 0.31677834905902935\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, Precision = 0.9938542581211589\n",
      "Validation: Test Loss = 0.31663657163575837\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, precision = 0.9938542581211589\n",
      "\n",
      "Epoch 136: Train Loss = 0.31684061088736104\n",
      "Epoch 137: Train Loss = 0.316998490322305\n",
      "Epoch 138: Train Loss = 0.31713571167905097\n",
      "Epoch 139: Train Loss = 0.31674632913070605\n",
      "Epoch 140: Train Loss = 0.3167131789698732\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, Precision = 0.9938542581211589\n",
      "Validation: Test Loss = 0.3173481647127564\n",
      "Recall = 1.0, Aging Rate = 0.5082368655387355, precision = 0.9916776171703898\n",
      "\n",
      "Epoch 141: Train Loss = 0.3168687164411944\n",
      "Epoch 142: Train Loss = 0.3167664253966467\n",
      "Epoch 143: Train Loss = 0.3168184642587935\n",
      "Epoch 144: Train Loss = 0.31690848855598636\n",
      "Epoch 145: Train Loss = 0.31715196811611485\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9934181658622203\n",
      "Validation: Test Loss = 0.3167468727378047\n",
      "Recall = 1.0, Aging Rate = 0.5075690115761353, precision = 0.9929824561403509\n",
      "\n",
      "Epoch 146: Train Loss = 0.3168015861925225\n",
      "Epoch 147: Train Loss = 0.316810539176394\n",
      "Epoch 148: Train Loss = 0.31687232583946984\n",
      "Epoch 149: Train Loss = 0.31683968465347323\n",
      "Epoch 150: Train Loss = 0.31686357924265196\n",
      "Recall = 1.0, Aging Rate = 0.5075690115761353, Precision = 0.9929824561403509\n",
      "Validation: Test Loss = 0.31662139429222336\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, precision = 0.9938542581211589\n",
      "\n",
      "Validation: Test Loss = 0.3337618723332643\n",
      "Recall = 0.9863201094391245, Aging Rate = 0.4986648865153538, precision = 0.965194109772423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcdb527563f469589bf925e015e7719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5464749989080726\n",
      "Epoch 2: Train Loss = 0.512003707333749\n",
      "Epoch 3: Train Loss = 0.46487103918781264\n",
      "Epoch 4: Train Loss = 0.4440624218430778\n",
      "Epoch 5: Train Loss = 0.4313080875287604\n",
      "Recall = 0.9811574697173621, Aging Rate = 0.7246215494211933, Precision = 0.6718894009216589\n",
      "Validation: Test Loss = 0.42188164528097727\n",
      "Recall = 0.9914759982054733, Aging Rate = 0.7288512911843277, precision = 0.6750152718387293\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.420782254035199\n",
      "Epoch 7: Train Loss = 0.41117231068819204\n",
      "Epoch 8: Train Loss = 0.40277019554445603\n",
      "Epoch 9: Train Loss = 0.3952995718374702\n",
      "Epoch 10: Train Loss = 0.38636216951180863\n",
      "Recall = 0.9901301031852848, Aging Rate = 0.6295636687444346, Precision = 0.7804101838755304\n",
      "Validation: Test Loss = 0.3800589490287132\n",
      "Recall = 0.9955136832660386, Aging Rate = 0.6208815672306323, precision = 0.7956256722839727\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.37740118909073217\n",
      "Epoch 12: Train Loss = 0.3708487231871536\n",
      "Epoch 13: Train Loss = 0.36535236961270484\n",
      "Epoch 14: Train Loss = 0.3600231711872964\n",
      "Epoch 15: Train Loss = 0.35708059053387053\n",
      "Recall = 0.9955136832660386, Aging Rate = 0.5692341941228851, Precision = 0.8678138443488463\n",
      "Validation: Test Loss = 0.3526851045480288\n",
      "Recall = 0.9982054733064154, Aging Rate = 0.5678984861976848, precision = 0.8722069776558212\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.35260476595253576\n",
      "Epoch 17: Train Loss = 0.3496110381978172\n",
      "Epoch 18: Train Loss = 0.34655959475603043\n",
      "Epoch 19: Train Loss = 0.3437083884476554\n",
      "Epoch 20: Train Loss = 0.34189011964645116\n",
      "Recall = 0.9986541049798116, Aging Rate = 0.5420747996438112, Precision = 0.9141683778234087\n",
      "Validation: Test Loss = 0.33900184453011406\n",
      "Recall = 0.9991027366532077, Aging Rate = 0.5356188780053428, precision = 0.9256026600166251\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3397604767913173\n",
      "Epoch 22: Train Loss = 0.33780691507981486\n",
      "Epoch 23: Train Loss = 0.3360995427371133\n",
      "Epoch 24: Train Loss = 0.333981632920959\n",
      "Epoch 25: Train Loss = 0.3325323916819089\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5233748886910062, Precision = 0.9476818375159507\n",
      "Validation: Test Loss = 0.3300861593133514\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5202582368655387, precision = 0.9533590072742832\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.33102473393679727\n",
      "Epoch 27: Train Loss = 0.32967584868785216\n",
      "Epoch 28: Train Loss = 0.32857495368535455\n",
      "Epoch 29: Train Loss = 0.3272800811782864\n",
      "Epoch 30: Train Loss = 0.326660885703638\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5129118432769367, Precision = 0.9670138888888888\n",
      "Validation: Test Loss = 0.3262399989159534\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5146927871772039, precision = 0.9636678200692042\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.326094859535951\n",
      "Epoch 32: Train Loss = 0.3250590068415033\n",
      "Epoch 33: Train Loss = 0.32479734189363857\n",
      "Epoch 34: Train Loss = 0.3242317132406328\n",
      "Epoch 35: Train Loss = 0.3235505297581001\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5086821015138023, Precision = 0.975054704595186\n",
      "Validation: Test Loss = 0.32321844640737757\n",
      "Recall = 1.0, Aging Rate = 0.5100178094390027, precision = 0.9729375818419904\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3231673773281933\n",
      "Epoch 37: Train Loss = 0.32270170197567555\n",
      "Epoch 38: Train Loss = 0.3222183583574643\n",
      "Epoch 39: Train Loss = 0.3219105593083909\n",
      "Epoch 40: Train Loss = 0.32160283297379955\n",
      "Recall = 1.0, Aging Rate = 0.506233303650935, Precision = 0.9802110817941952\n",
      "Validation: Test Loss = 0.3213928130622008\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5037845057880677, precision = 0.9845338046840477\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.32134262279326214\n",
      "Epoch 42: Train Loss = 0.3207987103999351\n",
      "Epoch 43: Train Loss = 0.3207305489059866\n",
      "Epoch 44: Train Loss = 0.3204734951974235\n",
      "Epoch 45: Train Loss = 0.3201948929501769\n",
      "Recall = 0.9995513683266039, Aging Rate = 0.5037845057880677, Precision = 0.9845338046840477\n",
      "Validation: Test Loss = 0.32035071177877494\n",
      "Recall = 1.0, Aging Rate = 0.5035618878005342, precision = 0.9854111405835544\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.320278290357318\n",
      "Epoch 47: Train Loss = 0.3197115142750931\n",
      "Epoch 48: Train Loss = 0.31956063116753836\n",
      "Epoch 49: Train Loss = 0.3193811869791121\n",
      "Epoch 50: Train Loss = 0.3192041785931439\n",
      "Recall = 1.0, Aging Rate = 0.5028940338379341, Precision = 0.9867197875166003\n",
      "Validation: Test Loss = 0.3189145675060055\n",
      "Recall = 1.0, Aging Rate = 0.5020035618878005, precision = 0.9884700665188471\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.31900477454477616\n",
      "Epoch 52: Train Loss = 0.3189961735361512\n",
      "Epoch 53: Train Loss = 0.3189192885335598\n",
      "Epoch 54: Train Loss = 0.31884785061527976\n",
      "Epoch 55: Train Loss = 0.318721563499736\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, Precision = 0.9893475366178429\n",
      "Validation: Test Loss = 0.31817090827125794\n",
      "Recall = 1.0, Aging Rate = 0.5017809439002672, precision = 0.9889086069210293\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.3184676965696941\n",
      "Epoch 57: Train Loss = 0.3183481443543999\n",
      "Epoch 58: Train Loss = 0.3188103026348039\n",
      "Epoch 59: Train Loss = 0.3185321162400878\n",
      "Epoch 60: Train Loss = 0.3182946571206155\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, Precision = 0.9893475366178429\n",
      "Validation: Test Loss = 0.31789349007585277\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.9893475366178429\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.31830464063323405\n",
      "Epoch 62: Train Loss = 0.318311001053901\n",
      "Epoch 63: Train Loss = 0.31838249637416083\n",
      "Epoch 64: Train Loss = 0.318367004951832\n",
      "Epoch 65: Train Loss = 0.3179534071540578\n",
      "Recall = 1.0, Aging Rate = 0.501113089937667, Precision = 0.9902265659706797\n",
      "Validation: Test Loss = 0.3175921841646451\n",
      "Recall = 1.0, Aging Rate = 0.5008904719501336, precision = 0.9906666666666667\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.3178367010738206\n",
      "Epoch 67: Train Loss = 0.31812703893424565\n",
      "Epoch 68: Train Loss = 0.3182444973439907\n",
      "Epoch 69: Train Loss = 0.3176926089150504\n",
      "Epoch 70: Train Loss = 0.3176170316850725\n",
      "Recall = 1.0, Aging Rate = 0.5004452359750667, Precision = 0.9915480427046264\n",
      "Validation: Test Loss = 0.3173440364674596\n",
      "Recall = 1.0, Aging Rate = 0.5002226179875334, precision = 0.9919893190921228\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.31757414088860647\n",
      "Epoch 72: Train Loss = 0.3175497380411211\n",
      "Epoch 73: Train Loss = 0.31775477475495817\n",
      "Epoch 74: Train Loss = 0.3174165362560208\n",
      "Epoch 75: Train Loss = 0.31739922378706487\n",
      "Recall = 1.0, Aging Rate = 0.5002226179875334, Precision = 0.9919893190921228\n",
      "Validation: Test Loss = 0.3171731646795519\n",
      "Recall = 1.0, Aging Rate = 0.5004452359750667, precision = 0.9915480427046264\n",
      "\n",
      "Epoch 76: Train Loss = 0.31758784114624494\n",
      "Epoch 77: Train Loss = 0.31752873934599724\n",
      "Epoch 78: Train Loss = 0.3173856682949372\n",
      "Epoch 79: Train Loss = 0.31728214885332917\n",
      "Epoch 80: Train Loss = 0.31750232254620225\n",
      "Recall = 1.0, Aging Rate = 0.5, Precision = 0.9924309884238647\n",
      "Validation: Test Loss = 0.31847087557254256\n",
      "Recall = 1.0, Aging Rate = 0.5015583259127337, precision = 0.9893475366178429\n",
      "\n",
      "Epoch 81: Train Loss = 0.31762922765522905\n",
      "Epoch 82: Train Loss = 0.3174454078861145\n",
      "Epoch 83: Train Loss = 0.3172271312185408\n",
      "Epoch 84: Train Loss = 0.31722972964667256\n",
      "Epoch 85: Train Loss = 0.3170723986222292\n",
      "Recall = 1.0, Aging Rate = 0.49977738201246663, Precision = 0.9928730512249443\n",
      "Validation: Test Loss = 0.31705806142288134\n",
      "Recall = 1.0, Aging Rate = 0.49977738201246663, precision = 0.9928730512249443\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.3173724762690672\n",
      "Epoch 87: Train Loss = 0.3172746023373846\n",
      "Epoch 88: Train Loss = 0.3172476798004055\n",
      "Epoch 89: Train Loss = 0.31738383047621055\n",
      "Epoch 90: Train Loss = 0.31720565642932647\n",
      "Recall = 1.0, Aging Rate = 0.49977738201246663, Precision = 0.9928730512249443\n",
      "Validation: Test Loss = 0.31737213684635723\n",
      "Recall = 1.0, Aging Rate = 0.49977738201246663, precision = 0.9928730512249443\n",
      "\n",
      "Epoch 91: Train Loss = 0.3171616050141896\n",
      "Epoch 92: Train Loss = 0.31740348912835226\n",
      "Epoch 93: Train Loss = 0.3172282601241437\n",
      "Epoch 94: Train Loss = 0.31718348732296103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train Loss = 0.31709188827954543\n",
      "Recall = 1.0, Aging Rate = 0.49977738201246663, Precision = 0.9928730512249443\n",
      "Validation: Test Loss = 0.31684194867460197\n",
      "Recall = 1.0, Aging Rate = 0.4995547640249332, precision = 0.9933155080213903\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.31725965430984937\n",
      "Epoch 97: Train Loss = 0.31704408118794886\n",
      "Epoch 98: Train Loss = 0.3170285312490387\n",
      "Epoch 99: Train Loss = 0.31705025361248773\n",
      "Epoch 100: Train Loss = 0.31732703720153915\n",
      "Recall = 1.0, Aging Rate = 0.49977738201246663, Precision = 0.9928730512249443\n",
      "Validation: Test Loss = 0.3165962743313315\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, precision = 0.9937583593401694\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.31714074219430227\n",
      "Epoch 102: Train Loss = 0.3173878070826204\n",
      "Epoch 103: Train Loss = 0.3169354426733426\n",
      "Epoch 104: Train Loss = 0.31686630539881366\n",
      "Epoch 105: Train Loss = 0.3170918419174286\n",
      "Recall = 1.0, Aging Rate = 0.4995547640249332, Precision = 0.9933155080213903\n",
      "Validation: Test Loss = 0.3167569205523174\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, precision = 0.9937583593401694\n",
      "\n",
      "Epoch 106: Train Loss = 0.3167834498207164\n",
      "Epoch 107: Train Loss = 0.31693231305795805\n",
      "Epoch 108: Train Loss = 0.3170456818884648\n",
      "Epoch 109: Train Loss = 0.3169127271999128\n",
      "Epoch 110: Train Loss = 0.31684236389021736\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, Precision = 0.9937583593401694\n",
      "Validation: Test Loss = 0.3165035968313022\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, precision = 0.9937583593401694\n",
      "\n",
      "Epoch 111: Train Loss = 0.31716208609744895\n",
      "Epoch 112: Train Loss = 0.3167503631125575\n",
      "Epoch 113: Train Loss = 0.31681458939958235\n",
      "Epoch 114: Train Loss = 0.3169614296084733\n",
      "Epoch 115: Train Loss = 0.31716420304424936\n",
      "Recall = 1.0, Aging Rate = 0.4995547640249332, Precision = 0.9933155080213903\n",
      "Validation: Test Loss = 0.3165799794670523\n",
      "Recall = 1.0, Aging Rate = 0.4995547640249332, precision = 0.9933155080213903\n",
      "\n",
      "Epoch 116: Train Loss = 0.31702363321214305\n",
      "Epoch 117: Train Loss = 0.31691951507027405\n",
      "Epoch 118: Train Loss = 0.3172418981233242\n",
      "Epoch 119: Train Loss = 0.31711288952233\n",
      "Epoch 120: Train Loss = 0.3171529585722824\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, Precision = 0.9937583593401694\n",
      "Validation: Test Loss = 0.31659497337901793\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, precision = 0.9937583593401694\n",
      "\n",
      "Epoch 121: Train Loss = 0.31707938523666196\n",
      "Epoch 122: Train Loss = 0.3167301970778675\n",
      "Epoch 123: Train Loss = 0.31657224561843295\n",
      "Epoch 124: Train Loss = 0.3166289009478935\n",
      "Epoch 125: Train Loss = 0.3166939387444609\n",
      "Recall = 1.0, Aging Rate = 0.49888691006233304, Precision = 0.9946452476572959\n",
      "Validation: Test Loss = 0.3165371652861737\n",
      "Recall = 1.0, Aging Rate = 0.49933214603739984, precision = 0.9937583593401694\n",
      "\n",
      "Epoch 126: Train Loss = 0.31682955433190774\n",
      "Epoch 127: Train Loss = 0.31707184750483913\n",
      "Epoch 128: Train Loss = 0.31684691908098395\n",
      "Epoch 129: Train Loss = 0.31672201229330693\n",
      "Epoch 130: Train Loss = 0.3166022690830757\n",
      "Recall = 1.0, Aging Rate = 0.49888691006233304, Precision = 0.9946452476572959\n",
      "Validation: Test Loss = 0.31644220707253057\n",
      "Recall = 1.0, Aging Rate = 0.4991095280498664, precision = 0.9942016057091883\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.3168658114710984\n",
      "Epoch 132: Train Loss = 0.316893130991996\n",
      "Epoch 133: Train Loss = 0.31654605959423504\n",
      "Epoch 134: Train Loss = 0.3167396469086488\n",
      "Epoch 135: Train Loss = 0.3165928704443513\n",
      "Recall = 1.0, Aging Rate = 0.4986642920747996, Precision = 0.9950892857142857\n",
      "Validation: Test Loss = 0.3162800739200011\n",
      "Recall = 1.0, Aging Rate = 0.4986642920747996, precision = 0.9950892857142857\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.316753214953313\n",
      "Epoch 137: Train Loss = 0.3168455666977703\n",
      "Epoch 138: Train Loss = 0.3166289543956916\n",
      "Epoch 139: Train Loss = 0.3163761646749712\n",
      "Epoch 140: Train Loss = 0.3164011018942853\n",
      "Recall = 1.0, Aging Rate = 0.4982190560997329, Precision = 0.9959785522788204\n",
      "Validation: Test Loss = 0.31609491577980675\n",
      "Recall = 1.0, Aging Rate = 0.49844167408726625, precision = 0.9955337204108977\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.31636879564605436\n",
      "Epoch 142: Train Loss = 0.31659667625134263\n",
      "Epoch 143: Train Loss = 0.3163285685615251\n",
      "Epoch 144: Train Loss = 0.3164449589972186\n",
      "Epoch 145: Train Loss = 0.3163930538341814\n",
      "Recall = 1.0, Aging Rate = 0.4982190560997329, Precision = 0.9959785522788204\n",
      "Validation: Test Loss = 0.31607809508897744\n",
      "Recall = 1.0, Aging Rate = 0.49844167408726625, precision = 0.9955337204108977\n",
      "\n",
      "Epoch 146: Train Loss = 0.31640515779663386\n",
      "Epoch 147: Train Loss = 0.3163286190370524\n",
      "Epoch 148: Train Loss = 0.31643877391399067\n",
      "Epoch 149: Train Loss = 0.31654400872738364\n",
      "Epoch 150: Train Loss = 0.31625534178737225\n",
      "Recall = 1.0, Aging Rate = 0.49799643811219946, Precision = 0.9964237818506929\n",
      "Validation: Test Loss = 0.31596993112925004\n",
      "Recall = 1.0, Aging Rate = 0.49799643811219946, precision = 0.9964237818506929\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.3359771599279386\n",
      "Recall = 0.9856396866840731, Aging Rate = 0.5226969292389854, precision = 0.9642401021711366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd2933426b14b119a1b972927948eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5445661754451049\n",
      "Epoch 2: Train Loss = 0.5060314956458678\n",
      "Epoch 3: Train Loss = 0.4636030867231391\n",
      "Epoch 4: Train Loss = 0.43739187032753934\n",
      "Epoch 5: Train Loss = 0.42456556682807467\n",
      "Recall = 0.9818423383525243, Aging Rate = 0.718833481745325, Precision = 0.6865902756271292\n",
      "Validation: Test Loss = 0.41486718343074896\n",
      "Recall = 0.9898139946855624, Aging Rate = 0.7056990204808549, precision = 0.7050473186119873\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.41242089971090573\n",
      "Epoch 7: Train Loss = 0.405889422046427\n",
      "Epoch 8: Train Loss = 0.3975365217625829\n",
      "Epoch 9: Train Loss = 0.39024296594005764\n",
      "Epoch 10: Train Loss = 0.3832344846436729\n",
      "Recall = 0.9924712134632419, Aging Rate = 0.6353517364203027, Precision = 0.7852137351086195\n",
      "Validation: Test Loss = 0.37683872341154306\n",
      "Recall = 0.9920283436669619, Aging Rate = 0.6164292074799644, precision = 0.8089563019140484\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.37512345130382\n",
      "Epoch 12: Train Loss = 0.3692078601804463\n",
      "Epoch 13: Train Loss = 0.36296316784933436\n",
      "Epoch 14: Train Loss = 0.35826918514626216\n",
      "Epoch 15: Train Loss = 0.354976164530349\n",
      "Recall = 0.9937998228520815, Aging Rate = 0.5694568121104185, Precision = 0.8772478498827209\n",
      "Validation: Test Loss = 0.3541370313664366\n",
      "Recall = 0.9986713906111603, Aging Rate = 0.5799198575244879, precision = 0.8656429942418427\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3517905689464121\n",
      "Epoch 17: Train Loss = 0.34915490553300504\n",
      "Epoch 18: Train Loss = 0.3461086874228976\n",
      "Epoch 19: Train Loss = 0.3442177785260276\n",
      "Epoch 20: Train Loss = 0.3420121687815217\n",
      "Recall = 0.9977856510186005, Aging Rate = 0.5476402493321461, Precision = 0.9158536585365854\n",
      "Validation: Test Loss = 0.3397534497327711\n",
      "Recall = 0.9982285208148804, Aging Rate = 0.5382902938557436, precision = 0.9321753515301903\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3395815447623031\n",
      "Epoch 22: Train Loss = 0.33842460024282534\n",
      "Epoch 23: Train Loss = 0.33700177736932\n",
      "Epoch 24: Train Loss = 0.33499756811032416\n",
      "Epoch 25: Train Loss = 0.33362054612524045\n",
      "Recall = 0.9982285208148804, Aging Rate = 0.5329474621549422, Precision = 0.9415204678362573\n",
      "Validation: Test Loss = 0.3318185753728912\n",
      "Recall = 0.9991142604074402, Aging Rate = 0.5296081923419412, precision = 0.9482976040353089\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.33246612739902676\n",
      "Epoch 27: Train Loss = 0.3316480327905127\n",
      "Epoch 28: Train Loss = 0.3307474568181025\n",
      "Epoch 29: Train Loss = 0.32992661731750544\n",
      "Epoch 30: Train Loss = 0.3288776281149602\n",
      "Recall = 0.9991142604074402, Aging Rate = 0.5247105966162066, Precision = 0.9571489181162495\n",
      "Validation: Test Loss = 0.3272658811545436\n",
      "Recall = 0.9995571302037201, Aging Rate = 0.523820124666073, precision = 0.9592010199745007\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.32825337508270386\n",
      "Epoch 32: Train Loss = 0.3277768300562593\n",
      "Epoch 33: Train Loss = 0.32741498840991023\n",
      "Epoch 34: Train Loss = 0.32707743403748113\n",
      "Epoch 35: Train Loss = 0.32608377129396793\n",
      "Recall = 0.9995571302037201, Aging Rate = 0.5215939447907391, Precision = 0.9632949210413999\n",
      "Validation: Test Loss = 0.3252846496187993\n",
      "Recall = 0.9995571302037201, Aging Rate = 0.518699910952805, precision = 0.9686695278969957\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3255007011966841\n",
      "Epoch 37: Train Loss = 0.3253551847894385\n",
      "Epoch 38: Train Loss = 0.32476420746991813\n",
      "Epoch 39: Train Loss = 0.3244443460171072\n",
      "Epoch 40: Train Loss = 0.32375693995296584\n",
      "Recall = 1.0, Aging Rate = 0.5171415850400712, Precision = 0.9720189410245372\n",
      "Validation: Test Loss = 0.32303617285599373\n",
      "Recall = 1.0, Aging Rate = 0.5164737310774711, precision = 0.9732758620689655\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.32409727308013453\n",
      "Epoch 42: Train Loss = 0.32320267121280183\n",
      "Epoch 43: Train Loss = 0.32308135373403424\n",
      "Epoch 44: Train Loss = 0.3224015684926075\n",
      "Epoch 45: Train Loss = 0.3219128250544982\n",
      "Recall = 1.0, Aging Rate = 0.5144701691896705, Precision = 0.9770662051060147\n",
      "Validation: Test Loss = 0.32098720321566104\n",
      "Recall = 1.0, Aging Rate = 0.5131344612644702, precision = 0.9796095444685466\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3217005569384975\n",
      "Epoch 47: Train Loss = 0.32205355536268954\n",
      "Epoch 48: Train Loss = 0.32096345248969665\n",
      "Epoch 49: Train Loss = 0.32143061140341517\n",
      "Epoch 50: Train Loss = 0.32098080341240603\n",
      "Recall = 1.0, Aging Rate = 0.5122439893143366, Precision = 0.9813124728378966\n",
      "Validation: Test Loss = 0.3200817663597402\n",
      "Recall = 1.0, Aging Rate = 0.5117987533392698, precision = 0.982166159199652\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.32094062497227294\n",
      "Epoch 52: Train Loss = 0.3206269187207532\n",
      "Epoch 53: Train Loss = 0.32034837169935954\n",
      "Epoch 54: Train Loss = 0.32033221318800326\n",
      "Epoch 55: Train Loss = 0.3201575747103109\n",
      "Recall = 1.0, Aging Rate = 0.511353517364203, Precision = 0.9830213321723987\n",
      "Validation: Test Loss = 0.31972301977612777\n",
      "Recall = 1.0, Aging Rate = 0.5115761353517364, precision = 0.9825935596170583\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.3198019875709435\n",
      "Epoch 57: Train Loss = 0.320407326239512\n",
      "Epoch 58: Train Loss = 0.3196474113903827\n",
      "Epoch 59: Train Loss = 0.31980595759589653\n",
      "Epoch 60: Train Loss = 0.3196859264734698\n",
      "Recall = 1.0, Aging Rate = 0.5111308993766697, Precision = 0.9834494773519163\n",
      "Validation: Test Loss = 0.3190238169814897\n",
      "Recall = 1.0, Aging Rate = 0.5106856634016028, precision = 0.984306887532694\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.319501527031935\n",
      "Epoch 62: Train Loss = 0.31941552480097235\n",
      "Epoch 63: Train Loss = 0.3194902473704048\n",
      "Epoch 64: Train Loss = 0.31960263089314167\n",
      "Epoch 65: Train Loss = 0.3194883153678473\n",
      "Recall = 1.0, Aging Rate = 0.5106856634016028, Precision = 0.984306887532694\n",
      "Validation: Test Loss = 0.31911788927266777\n",
      "Recall = 1.0, Aging Rate = 0.5106856634016028, precision = 0.984306887532694\n",
      "\n",
      "Epoch 66: Train Loss = 0.3192199027548915\n",
      "Epoch 67: Train Loss = 0.3194076145003124\n",
      "Epoch 68: Train Loss = 0.3193354235890712\n",
      "Epoch 69: Train Loss = 0.3200238612434849\n",
      "Epoch 70: Train Loss = 0.3198525637573996\n",
      "Recall = 1.0, Aging Rate = 0.5106856634016028, Precision = 0.984306887532694\n",
      "Validation: Test Loss = 0.31880434347069486\n",
      "Recall = 1.0, Aging Rate = 0.5102404274265361, precision = 0.9851657940663177\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.31914633443497803\n",
      "Epoch 72: Train Loss = 0.31897060261277993\n",
      "Epoch 73: Train Loss = 0.3190323948754016\n",
      "Epoch 74: Train Loss = 0.3191065955990038\n",
      "Epoch 75: Train Loss = 0.3189840612672634\n",
      "Recall = 1.0, Aging Rate = 0.5102404274265361, Precision = 0.9851657940663177\n",
      "Validation: Test Loss = 0.3184954771110232\n",
      "Recall = 1.0, Aging Rate = 0.5102404274265361, precision = 0.9851657940663177\n",
      "\n",
      "Epoch 76: Train Loss = 0.31881735004809747\n",
      "Epoch 77: Train Loss = 0.3190376641011514\n",
      "Epoch 78: Train Loss = 0.3192151134447232\n",
      "Epoch 79: Train Loss = 0.319138624550292\n",
      "Epoch 80: Train Loss = 0.31906723434757356\n",
      "Recall = 1.0, Aging Rate = 0.5100178094390027, Precision = 0.9855958096900916\n",
      "Validation: Test Loss = 0.31853140679620146\n",
      "Recall = 1.0, Aging Rate = 0.5100178094390027, precision = 0.9855958096900916\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.319191476832727\n",
      "Epoch 82: Train Loss = 0.3189216572741579\n",
      "Epoch 83: Train Loss = 0.31860408797289574\n",
      "Epoch 84: Train Loss = 0.3186173364719109\n",
      "Epoch 85: Train Loss = 0.318936424125865\n",
      "Recall = 1.0, Aging Rate = 0.5095725734639359, Precision = 0.9864569681083443\n",
      "Validation: Test Loss = 0.31830872793019294\n",
      "Recall = 1.0, Aging Rate = 0.5091273374888691, precision = 0.9873196327066025\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.3187607084919275\n",
      "Epoch 87: Train Loss = 0.31858885354583855\n",
      "Epoch 88: Train Loss = 0.3182091702336407\n",
      "Epoch 89: Train Loss = 0.31863840039776353\n",
      "Epoch 90: Train Loss = 0.31838985282400306\n",
      "Recall = 1.0, Aging Rate = 0.5089047195013358, Precision = 0.9877515310586177\n",
      "Validation: Test Loss = 0.3179503170008757\n",
      "Recall = 1.0, Aging Rate = 0.5084594835262689, precision = 0.98861646234676\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.3183412595933182\n",
      "Epoch 92: Train Loss = 0.31817962740110906\n",
      "Epoch 93: Train Loss = 0.31811008685312414\n",
      "Epoch 94: Train Loss = 0.3182437101964534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train Loss = 0.31794050678226\n",
      "Recall = 1.0, Aging Rate = 0.5080142475512022, Precision = 0.9894829097283085\n",
      "Validation: Test Loss = 0.3173197787304808\n",
      "Recall = 1.0, Aging Rate = 0.5075690115761353, precision = 0.9903508771929824\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.31788307553089207\n",
      "Epoch 97: Train Loss = 0.317966780992671\n",
      "Epoch 98: Train Loss = 0.3175710584580208\n",
      "Epoch 99: Train Loss = 0.3180626027040575\n",
      "Epoch 100: Train Loss = 0.31804952821557475\n",
      "Recall = 1.0, Aging Rate = 0.5077916295636687, Precision = 0.9899167032003507\n",
      "Validation: Test Loss = 0.31734974373161845\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, precision = 0.9907854322071084\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.3178978499194287\n",
      "Epoch 102: Train Loss = 0.31776759585735526\n",
      "Epoch 103: Train Loss = 0.3178320068497798\n",
      "Epoch 104: Train Loss = 0.3176187613078344\n",
      "Epoch 105: Train Loss = 0.3176856835771647\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, Precision = 0.9912203687445127\n",
      "Validation: Test Loss = 0.3172190932163891\n",
      "Recall = 1.0, Aging Rate = 0.5066785396260017, precision = 0.992091388400703\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.3174225834471141\n",
      "Epoch 107: Train Loss = 0.31749100236094435\n",
      "Epoch 108: Train Loss = 0.3174649568826942\n",
      "Epoch 109: Train Loss = 0.31758136353318644\n",
      "Epoch 110: Train Loss = 0.31761136494464143\n",
      "Recall = 1.0, Aging Rate = 0.5066785396260017, Precision = 0.992091388400703\n",
      "Validation: Test Loss = 0.31768680085906575\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, precision = 0.9912203687445127\n",
      "\n",
      "Epoch 111: Train Loss = 0.3179288058544203\n",
      "Epoch 112: Train Loss = 0.31763091479044137\n",
      "Epoch 113: Train Loss = 0.3175050052859798\n",
      "Epoch 114: Train Loss = 0.3174970744713438\n",
      "Epoch 115: Train Loss = 0.3175367056155778\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, Precision = 0.9912203687445127\n",
      "Validation: Test Loss = 0.31715311883606234\n",
      "Recall = 1.0, Aging Rate = 0.5066785396260017, precision = 0.992091388400703\n",
      "\n",
      "Epoch 116: Train Loss = 0.31744506503998543\n",
      "Epoch 117: Train Loss = 0.31794185040788997\n",
      "Epoch 118: Train Loss = 0.3173826065906328\n",
      "Epoch 119: Train Loss = 0.31754251677652395\n",
      "Epoch 120: Train Loss = 0.31785372576327164\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9907854322071084\n",
      "Validation: Test Loss = 0.31693053479079997\n",
      "Recall = 1.0, Aging Rate = 0.5069011576135352, precision = 0.9916556873078612\n",
      "\n",
      "Epoch 121: Train Loss = 0.31734014855148746\n",
      "Epoch 122: Train Loss = 0.317761380573733\n",
      "Epoch 123: Train Loss = 0.31748176382677107\n",
      "Epoch 124: Train Loss = 0.31760013793476544\n",
      "Epoch 125: Train Loss = 0.3176873325770812\n",
      "Recall = 1.0, Aging Rate = 0.5071237756010686, Precision = 0.9912203687445127\n",
      "Validation: Test Loss = 0.3172976122513904\n",
      "Recall = 1.0, Aging Rate = 0.5069011576135352, precision = 0.9916556873078612\n",
      "\n",
      "Epoch 126: Train Loss = 0.3174944310345399\n",
      "Epoch 127: Train Loss = 0.31738259199466007\n",
      "Epoch 128: Train Loss = 0.317440556476717\n",
      "Epoch 129: Train Loss = 0.3173767358781181\n",
      "Epoch 130: Train Loss = 0.31747173251897653\n",
      "Recall = 1.0, Aging Rate = 0.507346393588602, Precision = 0.9907854322071084\n",
      "Validation: Test Loss = 0.3169608276970983\n",
      "Recall = 1.0, Aging Rate = 0.5066785396260017, precision = 0.992091388400703\n",
      "\n",
      "Epoch 131: Train Loss = 0.3173882773648599\n",
      "Epoch 132: Train Loss = 0.31736528743406756\n",
      "Epoch 133: Train Loss = 0.31779969057013496\n",
      "Epoch 134: Train Loss = 0.3175563230540003\n",
      "Epoch 135: Train Loss = 0.3174610553261221\n",
      "Recall = 1.0, Aging Rate = 0.5066785396260017, Precision = 0.992091388400703\n",
      "Validation: Test Loss = 0.31675893776881725\n",
      "Recall = 1.0, Aging Rate = 0.5064559216384684, precision = 0.9925274725274725\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.3174187509162237\n",
      "Epoch 137: Train Loss = 0.31730042579762124\n",
      "Epoch 138: Train Loss = 0.31718349621323527\n",
      "Epoch 139: Train Loss = 0.3174897261687081\n",
      "Epoch 140: Train Loss = 0.31729151466438416\n",
      "Recall = 1.0, Aging Rate = 0.506233303650935, Precision = 0.9929639401934917\n",
      "Validation: Test Loss = 0.3169795441659348\n",
      "Recall = 1.0, Aging Rate = 0.5060106856634016, precision = 0.9934007919049714\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.3171405972164864\n",
      "Epoch 142: Train Loss = 0.31735029970548667\n",
      "Epoch 143: Train Loss = 0.3169924215026339\n",
      "Epoch 144: Train Loss = 0.31714577586759124\n",
      "Epoch 145: Train Loss = 0.31707849891612494\n",
      "Recall = 1.0, Aging Rate = 0.5060106856634016, Precision = 0.9934007919049714\n",
      "Validation: Test Loss = 0.3169871921902348\n",
      "Recall = 1.0, Aging Rate = 0.5057880676758683, precision = 0.9938380281690141\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.3172723760534798\n",
      "Epoch 147: Train Loss = 0.31711485976633597\n",
      "Epoch 148: Train Loss = 0.31708519979448907\n",
      "Epoch 149: Train Loss = 0.31733494317648353\n",
      "Epoch 150: Train Loss = 0.3170083253540742\n",
      "Recall = 1.0, Aging Rate = 0.5060106856634016, Precision = 0.9934007919049714\n",
      "Validation: Test Loss = 0.31680686534353375\n",
      "Recall = 1.0, Aging Rate = 0.5060106856634016, precision = 0.9934007919049714\n",
      "\n",
      "Validation: Test Loss = 0.33246050609924765\n",
      "Recall = 0.9918588873812755, Aging Rate = 0.5120160213618158, precision = 0.9530638852672751\n",
      "\u001b[32m[I 2022-05-26 16:22:56,867]\u001b[0m Trial 2 finished with value: 0.9741798982264723 and parameters: {'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'bad_weight': 0.8}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d647673820b420c9c16adae2cfea6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6623880440702634\n",
      "Epoch 2: Train Loss = 0.6135224968539957\n",
      "Epoch 3: Train Loss = 0.5585758477475955\n",
      "Epoch 4: Train Loss = 0.525979646459182\n",
      "Epoch 5: Train Loss = 0.5093644496488019\n",
      "Recall = 0.9151648351648352, Aging Rate = 0.6271148708815673, Precision = 0.739084132055378\n",
      "Validation: Test Loss = 0.49856988203281394\n",
      "Recall = 0.916043956043956, Aging Rate = 0.6079697239536954, precision = 0.7630904430611498\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.49528152536093284\n",
      "Epoch 7: Train Loss = 0.48441829205302395\n",
      "Epoch 8: Train Loss = 0.4754612958548012\n",
      "Epoch 9: Train Loss = 0.4658103729079901\n",
      "Epoch 10: Train Loss = 0.4563481250673347\n",
      "Recall = 0.9415384615384615, Aging Rate = 0.5874888691006234, Precision = 0.8116710875331565\n",
      "Validation: Test Loss = 0.4518023906068301\n",
      "Recall = 0.9261538461538461, Aging Rate = 0.5596616206589492, precision = 0.8381066030230708\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.44880752282703124\n",
      "Epoch 12: Train Loss = 0.44011189315537735\n",
      "Epoch 13: Train Loss = 0.4325886628029077\n",
      "Epoch 14: Train Loss = 0.42519622372392024\n",
      "Epoch 15: Train Loss = 0.41945688616242244\n",
      "Recall = 0.9613186813186814, Aging Rate = 0.5554318788958148, Precision = 0.8765531062124249\n",
      "Validation: Test Loss = 0.41343260742889804\n",
      "Recall = 0.9652747252747252, Aging Rate = 0.5547640249332146, precision = 0.8812199036918138\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.412188877485102\n",
      "Epoch 17: Train Loss = 0.40793340127062605\n",
      "Epoch 18: Train Loss = 0.4032228466717133\n",
      "Epoch 19: Train Loss = 0.3985501953192513\n",
      "Epoch 20: Train Loss = 0.3951699722322734\n",
      "Recall = 0.9749450549450549, Aging Rate = 0.5469723953695459, Precision = 0.9027269027269027\n",
      "Validation: Test Loss = 0.39145573230588\n",
      "Recall = 0.9767032967032967, Aging Rate = 0.5407390917186109, precision = 0.9147797447509263\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.39146089110646204\n",
      "Epoch 22: Train Loss = 0.38871807443809\n",
      "Epoch 23: Train Loss = 0.3847625910015055\n",
      "Epoch 24: Train Loss = 0.38187904752797563\n",
      "Epoch 25: Train Loss = 0.3791275360121434\n",
      "Recall = 0.9789010989010989, Aging Rate = 0.5329474621549422, Precision = 0.9302422723475355\n",
      "Validation: Test Loss = 0.3763447010081895\n",
      "Recall = 0.9872527472527473, Aging Rate = 0.5369545859305432, precision = 0.9311774461028193\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.37615235090149585\n",
      "Epoch 27: Train Loss = 0.3740627535569912\n",
      "Epoch 28: Train Loss = 0.3724188964916783\n",
      "Epoch 29: Train Loss = 0.37071050569507974\n",
      "Epoch 30: Train Loss = 0.3676716446717297\n",
      "Recall = 0.9872527472527473, Aging Rate = 0.5262689225289403, Precision = 0.9500846023688664\n",
      "Validation: Test Loss = 0.3657302142569558\n",
      "Recall = 0.985934065934066, Aging Rate = 0.5224844167408726, precision = 0.9556881124840222\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3667568111674444\n",
      "Epoch 32: Train Loss = 0.365478921525731\n",
      "Epoch 33: Train Loss = 0.366199360823695\n",
      "Epoch 34: Train Loss = 0.36285727786253524\n",
      "Epoch 35: Train Loss = 0.36057550654491993\n",
      "Recall = 0.9894505494505494, Aging Rate = 0.5235975066785397, Precision = 0.9570578231292517\n",
      "Validation: Test Loss = 0.3612062534581097\n",
      "Recall = 0.9956043956043956, Aging Rate = 0.5333926981300089, precision = 0.9453255425709516\n",
      "\n",
      "Epoch 36: Train Loss = 0.36090726493621444\n",
      "Epoch 37: Train Loss = 0.3588468451003145\n",
      "Epoch 38: Train Loss = 0.35811361646609763\n",
      "Epoch 39: Train Loss = 0.35627311985617965\n",
      "Epoch 40: Train Loss = 0.35572033775563977\n",
      "Recall = 0.9912087912087912, Aging Rate = 0.5229296527159395, Precision = 0.9599829714772243\n",
      "Validation: Test Loss = 0.3537074585950279\n",
      "Recall = 0.9938461538461538, Aging Rate = 0.5222617987533392, precision = 0.9637681159420289\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.35474111284407145\n",
      "Epoch 42: Train Loss = 0.3544620522036684\n",
      "Epoch 43: Train Loss = 0.35345414923114216\n",
      "Epoch 44: Train Loss = 0.3528489138755221\n",
      "Epoch 45: Train Loss = 0.3520075764012783\n",
      "Recall = 0.9925274725274725, Aging Rate = 0.5193677649154052, Precision = 0.9678525503643378\n",
      "Validation: Test Loss = 0.35060211710917133\n",
      "Recall = 0.9951648351648351, Aging Rate = 0.5229296527159395, precision = 0.9638143891017454\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3519552846838084\n",
      "Epoch 47: Train Loss = 0.3509858415685163\n",
      "Epoch 48: Train Loss = 0.35091649379352324\n",
      "Epoch 49: Train Loss = 0.3498425675043546\n",
      "Epoch 50: Train Loss = 0.34965009887836285\n",
      "Recall = 0.9929670329670329, Aging Rate = 0.5178094390026714, Precision = 0.971195184866724\n",
      "Validation: Test Loss = 0.3480289730235921\n",
      "Recall = 0.9938461538461538, Aging Rate = 0.5133570792520036, precision = 0.9804856895056374\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.35194306437818473\n",
      "Epoch 52: Train Loss = 0.3486448649253998\n",
      "Epoch 53: Train Loss = 0.348508003319573\n",
      "Epoch 54: Train Loss = 0.34696036253459517\n",
      "Epoch 55: Train Loss = 0.34751508189224284\n",
      "Recall = 0.9938461538461538, Aging Rate = 0.5164737310774711, Precision = 0.9745689655172414\n",
      "Validation: Test Loss = 0.34544227878004974\n",
      "Recall = 0.9956043956043956, Aging Rate = 0.5149154051647373, precision = 0.9792477302204928\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.3465957835064546\n",
      "Epoch 57: Train Loss = 0.34724265583794983\n",
      "Epoch 58: Train Loss = 0.3463171052539975\n",
      "Epoch 59: Train Loss = 0.34572480200978123\n",
      "Epoch 60: Train Loss = 0.344921318951825\n",
      "Recall = 0.9951648351648351, Aging Rate = 0.5146927871772039, Precision = 0.9792387543252595\n",
      "Validation: Test Loss = 0.343936923058672\n",
      "Recall = 0.9964835164835165, Aging Rate = 0.5166963490650045, precision = 0.976734166307626\n",
      "\n",
      "Epoch 61: Train Loss = 0.3457679597267803\n",
      "Epoch 62: Train Loss = 0.34480056838063605\n",
      "Epoch 63: Train Loss = 0.3442169897862024\n",
      "Epoch 64: Train Loss = 0.3441194684136052\n",
      "Epoch 65: Train Loss = 0.34418710150680476\n",
      "Recall = 0.996043956043956, Aging Rate = 0.5151380231522708, Precision = 0.9792566983578219\n",
      "Validation: Test Loss = 0.34303970134693496\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5171415850400712, precision = 0.9767541971588464\n",
      "\n",
      "Epoch 66: Train Loss = 0.3437773018465017\n",
      "Epoch 67: Train Loss = 0.34337304454240436\n",
      "Epoch 68: Train Loss = 0.3428438928173996\n",
      "Epoch 69: Train Loss = 0.34316067287461205\n",
      "Epoch 70: Train Loss = 0.34214847719998415\n",
      "Recall = 0.9956043956043956, Aging Rate = 0.5131344612644702, Precision = 0.982646420824295\n",
      "Validation: Test Loss = 0.3409359861672827\n",
      "Recall = 0.9964835164835165, Aging Rate = 0.5129118432769367, precision = 0.9839409722222222\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.34213051152144386\n",
      "Epoch 72: Train Loss = 0.34421689669043487\n",
      "Epoch 73: Train Loss = 0.3417916483944904\n",
      "Epoch 74: Train Loss = 0.34187270078612353\n",
      "Epoch 75: Train Loss = 0.341588742347668\n",
      "Recall = 0.9969230769230769, Aging Rate = 0.5142475512021372, Precision = 0.9818181818181818\n",
      "Validation: Test Loss = 0.3404479961202066\n",
      "Recall = 0.9964835164835165, Aging Rate = 0.51246660730187, precision = 0.9847958297132928\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.34096269284309494\n",
      "Epoch 77: Train Loss = 0.3410605571414356\n",
      "Epoch 78: Train Loss = 0.3410291062437416\n",
      "Epoch 79: Train Loss = 0.3409170961656001\n",
      "Epoch 80: Train Loss = 0.3408198705870237\n",
      "Recall = 0.9969230769230769, Aging Rate = 0.51246660730187, Precision = 0.9852302345786272\n",
      "Validation: Test Loss = 0.3395077224990033\n",
      "Recall = 0.996043956043956, Aging Rate = 0.5106856634016028, precision = 0.987794245858762\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.34045540333749563\n",
      "Epoch 82: Train Loss = 0.3402112887994795\n",
      "Epoch 83: Train Loss = 0.34053785131642145\n",
      "Epoch 84: Train Loss = 0.34121108020615176\n",
      "Epoch 85: Train Loss = 0.3399313611052234\n",
      "Recall = 0.996043956043956, Aging Rate = 0.5111308993766697, Precision = 0.9869337979094077\n",
      "Validation: Test Loss = 0.3396506364443211\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5142475512021372, precision = 0.9826839826839827\n",
      "\n",
      "Epoch 86: Train Loss = 0.3398891545944944\n",
      "Epoch 87: Train Loss = 0.33993645987655047\n",
      "Epoch 88: Train Loss = 0.3393635882507555\n",
      "Epoch 89: Train Loss = 0.3397945328843668\n",
      "Epoch 90: Train Loss = 0.33931835830370866\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5129118432769367, Precision = 0.9852430555555556\n",
      "Validation: Test Loss = 0.3388369099592802\n",
      "Recall = 0.9969230769230769, Aging Rate = 0.5097951914514692, precision = 0.9903930131004367\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.34020235080965266\n",
      "Epoch 92: Train Loss = 0.3394785973501333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Train Loss = 0.33896274791693753\n",
      "Epoch 94: Train Loss = 0.3402017065964951\n",
      "Epoch 95: Train Loss = 0.33896710335518354\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5122439893143366, Precision = 0.9860930030421556\n",
      "Validation: Test Loss = 0.3378582030720511\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5115761353517364, precision = 0.9878154917319408\n",
      "\n",
      "Epoch 96: Train Loss = 0.338825923729877\n",
      "Epoch 97: Train Loss = 0.3386825420765185\n",
      "Epoch 98: Train Loss = 0.33929367064898924\n",
      "Epoch 99: Train Loss = 0.3386058971644085\n",
      "Epoch 100: Train Loss = 0.3386805606599589\n",
      "Recall = 0.9969230769230769, Aging Rate = 0.5115761353517364, Precision = 0.9869451697127938\n",
      "Validation: Test Loss = 0.337716972307764\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5104630454140695, precision = 0.9895333624073267\n",
      "\n",
      "Epoch 101: Train Loss = 0.3383737251839251\n",
      "Epoch 102: Train Loss = 0.3389213918737291\n",
      "Epoch 103: Train Loss = 0.33929142899950593\n",
      "Epoch 104: Train Loss = 0.3384139348859779\n",
      "Epoch 105: Train Loss = 0.33853567559487674\n",
      "Recall = 0.9964835164835165, Aging Rate = 0.5111308993766697, Precision = 0.9873693379790941\n",
      "Validation: Test Loss = 0.3373767721631754\n",
      "Recall = 0.996043956043956, Aging Rate = 0.5093499554764025, precision = 0.9903846153846154\n",
      "\n",
      "Epoch 106: Train Loss = 0.33812079711992193\n",
      "Epoch 107: Train Loss = 0.3381373368515463\n",
      "Epoch 108: Train Loss = 0.3379925095069016\n",
      "Epoch 109: Train Loss = 0.3381870666308161\n",
      "Epoch 110: Train Loss = 0.33814434867720466\n",
      "Recall = 0.996043956043956, Aging Rate = 0.5104630454140695, Precision = 0.9882250327082425\n",
      "Validation: Test Loss = 0.33718318742296044\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5106856634016028, precision = 0.9891020052310375\n",
      "\n",
      "Epoch 111: Train Loss = 0.3381574912890726\n",
      "Epoch 112: Train Loss = 0.33815519168030866\n",
      "Epoch 113: Train Loss = 0.3380757096826446\n",
      "Epoch 114: Train Loss = 0.3380354300260756\n",
      "Epoch 115: Train Loss = 0.33828210103543654\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5109082813891362, Precision = 0.9886710239651416\n",
      "Validation: Test Loss = 0.33711795491611757\n",
      "Recall = 0.9986813186813187, Aging Rate = 0.5120213713268033, precision = 0.9878260869565217\n",
      "\n",
      "Epoch 116: Train Loss = 0.338376799626533\n",
      "Epoch 117: Train Loss = 0.33773135740844035\n",
      "Epoch 118: Train Loss = 0.3382407010450813\n",
      "Epoch 119: Train Loss = 0.33857437238455457\n",
      "Epoch 120: Train Loss = 0.33813761958880606\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.511353517364203, Precision = 0.9878101872006966\n",
      "Validation: Test Loss = 0.33694025152300683\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5097951914514692, precision = 0.990829694323144\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3376133713940903\n",
      "Epoch 122: Train Loss = 0.33773127543215015\n",
      "Epoch 123: Train Loss = 0.3376139533487899\n",
      "Epoch 124: Train Loss = 0.3373306951720271\n",
      "Epoch 125: Train Loss = 0.3373306715796276\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.511353517364203, Precision = 0.9882455376578145\n",
      "Validation: Test Loss = 0.33628471045757335\n",
      "Recall = 0.9982417582417582, Aging Rate = 0.5104630454140695, precision = 0.990405582206716\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.33724978743975226\n",
      "Epoch 127: Train Loss = 0.33833092496210404\n",
      "Epoch 128: Train Loss = 0.33745675132620684\n",
      "Epoch 129: Train Loss = 0.33731604548617333\n",
      "Epoch 130: Train Loss = 0.337039631865222\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5109082813891362, Precision = 0.9891067538126361\n",
      "Validation: Test Loss = 0.3364389401297429\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5097951914514692, precision = 0.9912663755458515\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.33828728223420207\n",
      "Epoch 132: Train Loss = 0.33712407556476065\n",
      "Epoch 133: Train Loss = 0.3385759376035879\n",
      "Epoch 134: Train Loss = 0.3377599631529882\n",
      "Epoch 135: Train Loss = 0.33758457736149494\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5109082813891362, Precision = 0.9886710239651416\n",
      "Validation: Test Loss = 0.3359443081189561\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5100178094390027, precision = 0.9908336970755128\n",
      "\n",
      "Epoch 136: Train Loss = 0.3370853829415696\n",
      "Epoch 137: Train Loss = 0.3368889921123389\n",
      "Epoch 138: Train Loss = 0.336837787343685\n",
      "Epoch 139: Train Loss = 0.33752638921606043\n",
      "Epoch 140: Train Loss = 0.3370737420164467\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5109082813891362, Precision = 0.9886710239651416\n",
      "Validation: Test Loss = 0.3373093974186922\n",
      "Recall = 0.9991208791208791, Aging Rate = 0.5131344612644702, precision = 0.986117136659436\n",
      "\n",
      "Epoch 141: Train Loss = 0.33749083616005343\n",
      "Epoch 142: Train Loss = 0.3375593936029334\n",
      "Epoch 143: Train Loss = 0.33675578350693863\n",
      "Epoch 144: Train Loss = 0.33664779499080283\n",
      "Epoch 145: Train Loss = 0.3367551648500023\n",
      "Recall = 0.9982417582417582, Aging Rate = 0.5115761353517364, Precision = 0.9882506527415144\n",
      "Validation: Test Loss = 0.33626234356250917\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5097951914514692, precision = 0.9912663755458515\n",
      "\n",
      "Epoch 146: Train Loss = 0.33688490277194383\n",
      "Epoch 147: Train Loss = 0.33670599709529486\n",
      "Epoch 148: Train Loss = 0.3366748091108759\n",
      "Epoch 149: Train Loss = 0.33667890439581255\n",
      "Epoch 150: Train Loss = 0.3364179670173784\n",
      "Recall = 0.9973626373626374, Aging Rate = 0.5095725734639359, Precision = 0.9912625600698995\n",
      "Validation: Test Loss = 0.3357361150923735\n",
      "Recall = 0.9978021978021978, Aging Rate = 0.5091273374888691, precision = 0.9925666812418015\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.35149418320929576\n",
      "Recall = 0.9805555555555555, Aging Rate = 0.4886515353805073, precision = 0.9644808743169399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330a5c34c68b4f19970f70bcde1ab114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6610896751377483\n",
      "Epoch 2: Train Loss = 0.6085659398739611\n",
      "Epoch 3: Train Loss = 0.5572098970200903\n",
      "Epoch 4: Train Loss = 0.5288527430112301\n",
      "Epoch 5: Train Loss = 0.5103423475210728\n",
      "Recall = 0.904635761589404, Aging Rate = 0.608860195903829, Precision = 0.7491773308957953\n",
      "Validation: Test Loss = 0.49710714716202214\n",
      "Recall = 0.9218543046357616, Aging Rate = 0.6137577916295637, precision = 0.7573449401523396\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.49256692995265033\n",
      "Epoch 7: Train Loss = 0.4810993415855024\n",
      "Epoch 8: Train Loss = 0.4698363181903547\n",
      "Epoch 9: Train Loss = 0.4593698485080196\n",
      "Epoch 10: Train Loss = 0.4496482873430023\n",
      "Recall = 0.9390728476821192, Aging Rate = 0.5732413178984862, Precision = 0.8260194174757282\n",
      "Validation: Test Loss = 0.44316603504751584\n",
      "Recall = 0.9629139072847682, Aging Rate = 0.5923864648263579, precision = 0.8196166854565953\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.43948318551187526\n",
      "Epoch 12: Train Loss = 0.43028862282089747\n",
      "Epoch 13: Train Loss = 0.4226161800689816\n",
      "Epoch 14: Train Loss = 0.4146333179193634\n",
      "Epoch 15: Train Loss = 0.4092918388936951\n",
      "Recall = 0.9668874172185431, Aging Rate = 0.5456366874443455, Precision = 0.8935128518971848\n",
      "Validation: Test Loss = 0.4036208429935248\n",
      "Recall = 0.9757174392935982, Aging Rate = 0.5483081032947462, precision = 0.8972797401542834\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.4024708121882309\n",
      "Epoch 17: Train Loss = 0.3968107170912164\n",
      "Epoch 18: Train Loss = 0.3922111705967282\n",
      "Epoch 19: Train Loss = 0.3875867011604411\n",
      "Epoch 20: Train Loss = 0.38353294261523047\n",
      "Recall = 0.9805739514348786, Aging Rate = 0.5333926981300089, Precision = 0.9269616026711185\n",
      "Validation: Test Loss = 0.3798635391019225\n",
      "Recall = 0.9832229580573951, Aging Rate = 0.5304986642920748, precision = 0.9345362987830466\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.38091594776933146\n",
      "Epoch 22: Train Loss = 0.37706319009843725\n",
      "Epoch 23: Train Loss = 0.3743568357886317\n",
      "Epoch 24: Train Loss = 0.37211498238630625\n",
      "Epoch 25: Train Loss = 0.3692179255101263\n",
      "Recall = 0.9849889624724062, Aging Rate = 0.5227070347284061, Precision = 0.9501703577512777\n",
      "Validation: Test Loss = 0.3694761954082088\n",
      "Recall = 0.9774834437086093, Aging Rate = 0.5117987533392698, precision = 0.9630274032187908\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.36789923378217465\n",
      "Epoch 27: Train Loss = 0.365178987689031\n",
      "Epoch 28: Train Loss = 0.363443448940559\n",
      "Epoch 29: Train Loss = 0.3615771914207819\n",
      "Epoch 30: Train Loss = 0.36122235661092233\n",
      "Recall = 0.9894039735099338, Aging Rate = 0.5195903829029386, Precision = 0.9601542416452442\n",
      "Validation: Test Loss = 0.35821718818782594\n",
      "Recall = 0.9876379690949227, Aging Rate = 0.5144701691896705, precision = 0.9679792297706621\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.35929284834585756\n",
      "Epoch 32: Train Loss = 0.35738482994366627\n",
      "Epoch 33: Train Loss = 0.3560902748857347\n",
      "Epoch 34: Train Loss = 0.35566160295122135\n",
      "Epoch 35: Train Loss = 0.35408595199045706\n",
      "Recall = 0.9911699779249448, Aging Rate = 0.5155832591273375, Precision = 0.9693436960276338\n",
      "Validation: Test Loss = 0.35305042221200117\n",
      "Recall = 0.9924944812362031, Aging Rate = 0.5166963490650045, precision = 0.968548039638087\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3537246565784818\n",
      "Epoch 37: Train Loss = 0.3525440669548055\n",
      "Epoch 38: Train Loss = 0.3517809111893655\n",
      "Epoch 39: Train Loss = 0.3517219395055796\n",
      "Epoch 40: Train Loss = 0.3503340694486086\n",
      "Recall = 0.9911699779249448, Aging Rate = 0.5138023152270703, Precision = 0.9727036395147314\n",
      "Validation: Test Loss = 0.34893051132174974\n",
      "Recall = 0.9942604856512142, Aging Rate = 0.5151380231522708, precision = 0.9732065687121867\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.3498299399976314\n",
      "Epoch 42: Train Loss = 0.34939814295921595\n",
      "Epoch 43: Train Loss = 0.3495189024472173\n",
      "Epoch 44: Train Loss = 0.34845449960560854\n",
      "Epoch 45: Train Loss = 0.34735585087022924\n",
      "Recall = 0.9947019867549669, Aging Rate = 0.5133570792520036, Precision = 0.9770164787510841\n",
      "Validation: Test Loss = 0.3461793496581134\n",
      "Recall = 0.9933774834437086, Aging Rate = 0.5115761353517364, precision = 0.97911227154047\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.3472683060837981\n",
      "Epoch 47: Train Loss = 0.34664350347336337\n",
      "Epoch 48: Train Loss = 0.34613796357373944\n",
      "Epoch 49: Train Loss = 0.34671740792525846\n",
      "Epoch 50: Train Loss = 0.34576891069420723\n",
      "Recall = 0.9960264900662251, Aging Rate = 0.5133570792520036, Precision = 0.9783174327840416\n",
      "Validation: Test Loss = 0.3451060252803198\n",
      "Recall = 0.9973509933774835, Aging Rate = 0.5151380231522708, precision = 0.9762316335350043\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.34595739326621416\n",
      "Epoch 52: Train Loss = 0.34473938479661304\n",
      "Epoch 53: Train Loss = 0.3442326086446417\n",
      "Epoch 54: Train Loss = 0.3440773970350451\n",
      "Epoch 55: Train Loss = 0.3440486527083712\n",
      "Recall = 0.9951434878587196, Aging Rate = 0.5120213713268033, Precision = 0.98\n",
      "Validation: Test Loss = 0.342495888790061\n",
      "Recall = 0.9960264900662251, Aging Rate = 0.51246660730187, precision = 0.9800173761946134\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.34376574640070234\n",
      "Epoch 57: Train Loss = 0.3434893903144215\n",
      "Epoch 58: Train Loss = 0.3437111024493526\n",
      "Epoch 59: Train Loss = 0.34277675208616554\n",
      "Epoch 60: Train Loss = 0.34265871862163527\n",
      "Recall = 0.9960264900662251, Aging Rate = 0.5117987533392698, Precision = 0.9812962157459765\n",
      "Validation: Test Loss = 0.341424312664798\n",
      "Recall = 0.9960264900662251, Aging Rate = 0.5104630454140695, precision = 0.9838639337112952\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.34250237048999177\n",
      "Epoch 62: Train Loss = 0.342230701648223\n",
      "Epoch 63: Train Loss = 0.34225746381526956\n",
      "Epoch 64: Train Loss = 0.34203382421580153\n",
      "Epoch 65: Train Loss = 0.3417683488698485\n",
      "Recall = 0.9960264900662251, Aging Rate = 0.5109082813891362, Precision = 0.9830065359477124\n",
      "Validation: Test Loss = 0.3409551919196614\n",
      "Recall = 0.9955849889624724, Aging Rate = 0.5095725734639359, precision = 0.9851463521188292\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.34185147243000413\n",
      "Epoch 67: Train Loss = 0.341242382914916\n",
      "Epoch 68: Train Loss = 0.3410530382251485\n",
      "Epoch 69: Train Loss = 0.34158336734092354\n",
      "Epoch 70: Train Loss = 0.3412554263537841\n",
      "Recall = 0.9969094922737307, Aging Rate = 0.511353517364203, Precision = 0.9830213321723987\n",
      "Validation: Test Loss = 0.3397806342011143\n",
      "Recall = 0.996467991169978, Aging Rate = 0.5106856634016028, precision = 0.9838709677419355\n",
      "\n",
      "Epoch 71: Train Loss = 0.3415243650055951\n",
      "Epoch 72: Train Loss = 0.34028368187821134\n",
      "Epoch 73: Train Loss = 0.33981461874735963\n",
      "Epoch 74: Train Loss = 0.3403185785665962\n",
      "Epoch 75: Train Loss = 0.3402693475449818\n",
      "Recall = 0.9973509933774835, Aging Rate = 0.5111308993766697, Precision = 0.9838850174216028\n",
      "Validation: Test Loss = 0.3389583937259837\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5109082813891362, precision = 0.9851851851851852\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.33978818790889276\n",
      "Epoch 77: Train Loss = 0.3392356761523049\n",
      "Epoch 78: Train Loss = 0.33955726444137385\n",
      "Epoch 79: Train Loss = 0.33907617391058087\n",
      "Epoch 80: Train Loss = 0.3392906242571871\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5106856634016028, Precision = 0.9856146469049695\n",
      "Validation: Test Loss = 0.33800344465040033\n",
      "Recall = 0.9977924944812362, Aging Rate = 0.5093499554764025, precision = 0.9877622377622378\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.3390168611321827\n",
      "Epoch 82: Train Loss = 0.3390312436851984\n",
      "Epoch 83: Train Loss = 0.3391243951031913\n",
      "Epoch 84: Train Loss = 0.3388105579656039\n",
      "Epoch 85: Train Loss = 0.33878557207535553\n",
      "Recall = 0.9977924944812362, Aging Rate = 0.5093499554764025, Precision = 0.9877622377622378\n",
      "Validation: Test Loss = 0.33851612118451807\n",
      "Recall = 0.9991169977924945, Aging Rate = 0.5115761353517364, precision = 0.984769364664926\n",
      "\n",
      "Epoch 86: Train Loss = 0.34037694706517674\n",
      "Epoch 87: Train Loss = 0.33906513410068895\n",
      "Epoch 88: Train Loss = 0.3389192328374511\n",
      "Epoch 89: Train Loss = 0.3381114924792615\n",
      "Epoch 90: Train Loss = 0.3381143649136075\n",
      "Recall = 0.9973509933774835, Aging Rate = 0.5091273374888691, Precision = 0.9877568867512024\n",
      "Validation: Test Loss = 0.33710255202499756\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5089047195013358, precision = 0.989063867016623\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.3386361979790702\n",
      "Epoch 92: Train Loss = 0.3382409654179537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Train Loss = 0.3388610804388805\n",
      "Epoch 94: Train Loss = 0.3384687720349296\n",
      "Epoch 95: Train Loss = 0.33824918488785294\n",
      "Recall = 0.9969094922737307, Aging Rate = 0.5086821015138023, Precision = 0.9881838074398249\n",
      "Validation: Test Loss = 0.3368641458657418\n",
      "Recall = 0.9991169977924945, Aging Rate = 0.5097951914514692, precision = 0.9882096069868995\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.33777353061381343\n",
      "Epoch 97: Train Loss = 0.3378894260602664\n",
      "Epoch 98: Train Loss = 0.33865521716838\n",
      "Epoch 99: Train Loss = 0.33799021393193374\n",
      "Epoch 100: Train Loss = 0.3377170528775332\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5093499554764025, Precision = 0.9881993006993007\n",
      "Validation: Test Loss = 0.3371729896945393\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5095725734639359, precision = 0.9877675840978594\n",
      "\n",
      "Epoch 101: Train Loss = 0.33816583349783297\n",
      "Epoch 102: Train Loss = 0.33773244648030687\n",
      "Epoch 103: Train Loss = 0.33777512672323257\n",
      "Epoch 104: Train Loss = 0.33740726161088036\n",
      "Epoch 105: Train Loss = 0.3372360786102971\n",
      "Recall = 0.9977924944812362, Aging Rate = 0.5091273374888691, Precision = 0.9881941407958024\n",
      "Validation: Test Loss = 0.3366980173708813\n",
      "Recall = 0.9991169977924945, Aging Rate = 0.5097951914514692, precision = 0.9882096069868995\n",
      "\n",
      "Epoch 106: Train Loss = 0.33731763795986835\n",
      "Epoch 107: Train Loss = 0.3379829426164194\n",
      "Epoch 108: Train Loss = 0.33739946255807035\n",
      "Epoch 109: Train Loss = 0.33758373493502847\n",
      "Epoch 110: Train Loss = 0.3378526716495558\n",
      "Recall = 0.9973509933774835, Aging Rate = 0.5084594835262689, Precision = 0.9890542907180385\n",
      "Validation: Test Loss = 0.33656665202452474\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5084594835262689, precision = 0.9899299474605955\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.337104563137727\n",
      "Epoch 112: Train Loss = 0.33726915549934705\n",
      "Epoch 113: Train Loss = 0.33712664673823073\n",
      "Epoch 114: Train Loss = 0.33760314802983354\n",
      "Epoch 115: Train Loss = 0.3375769836076752\n",
      "Recall = 0.9977924944812362, Aging Rate = 0.5091273374888691, Precision = 0.9881941407958024\n",
      "Validation: Test Loss = 0.3362693179163249\n",
      "Recall = 0.9991169977924945, Aging Rate = 0.5095725734639359, precision = 0.9886413280908694\n",
      "\n",
      "Epoch 116: Train Loss = 0.33742701869295416\n",
      "Epoch 117: Train Loss = 0.33745050541758004\n",
      "Epoch 118: Train Loss = 0.337799517734711\n",
      "Epoch 119: Train Loss = 0.336998943892952\n",
      "Epoch 120: Train Loss = 0.3371979603253617\n",
      "Recall = 0.9977924944812362, Aging Rate = 0.5091273374888691, Precision = 0.9881941407958024\n",
      "Validation: Test Loss = 0.33611645702793463\n",
      "Recall = 0.9991169977924945, Aging Rate = 0.5084594835262689, precision = 0.9908056042031523\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.33666050787069707\n",
      "Epoch 122: Train Loss = 0.3369662453315137\n",
      "Epoch 123: Train Loss = 0.3373155738239509\n",
      "Epoch 124: Train Loss = 0.3366156902200711\n",
      "Epoch 125: Train Loss = 0.33826342997334413\n",
      "Recall = 0.996467991169978, Aging Rate = 0.5075690115761353, Precision = 0.9899122807017544\n",
      "Validation: Test Loss = 0.3360583319105639\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5075690115761353, precision = 0.9916666666666667\n",
      "\n",
      "Epoch 126: Train Loss = 0.3371076618627258\n",
      "Epoch 127: Train Loss = 0.3366364554327084\n",
      "Epoch 128: Train Loss = 0.336946573224539\n",
      "Epoch 129: Train Loss = 0.3366538439864892\n",
      "Epoch 130: Train Loss = 0.33653160505069757\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5089047195013358, Precision = 0.989063867016623\n",
      "Validation: Test Loss = 0.3355202124730243\n",
      "Recall = 0.9986754966887417, Aging Rate = 0.5082368655387355, precision = 0.9908015768725361\n",
      "\n",
      "Epoch 131: Train Loss = 0.3367593632355823\n",
      "Epoch 132: Train Loss = 0.33695076396713697\n",
      "Epoch 133: Train Loss = 0.3365777185583157\n",
      "Epoch 134: Train Loss = 0.33647344386800526\n",
      "Epoch 135: Train Loss = 0.33661742923733173\n",
      "Recall = 0.9986754966887417, Aging Rate = 0.5086821015138023, Precision = 0.9899343544857768\n",
      "Validation: Test Loss = 0.3353746679618968\n",
      "Recall = 0.9986754966887417, Aging Rate = 0.5086821015138023, precision = 0.9899343544857768\n",
      "\n",
      "Epoch 136: Train Loss = 0.33638477773933884\n",
      "Epoch 137: Train Loss = 0.33687614651524583\n",
      "Epoch 138: Train Loss = 0.33788363488253376\n",
      "Epoch 139: Train Loss = 0.336554302372045\n",
      "Epoch 140: Train Loss = 0.3363974940691903\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5089047195013358, Precision = 0.989063867016623\n",
      "Validation: Test Loss = 0.33538438293515627\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5080142475512022, precision = 0.99079754601227\n",
      "\n",
      "Epoch 141: Train Loss = 0.33630502553041347\n",
      "Epoch 142: Train Loss = 0.3369658361400543\n",
      "Epoch 143: Train Loss = 0.3365578931405517\n",
      "Epoch 144: Train Loss = 0.33610389963495657\n",
      "Epoch 145: Train Loss = 0.33658299749702186\n",
      "Recall = 0.9973509933774835, Aging Rate = 0.5075690115761353, Precision = 0.9907894736842106\n",
      "Validation: Test Loss = 0.3352712461275388\n",
      "Recall = 0.9986754966887417, Aging Rate = 0.5080142475512022, precision = 0.9912357581069238\n",
      "\n",
      "Epoch 146: Train Loss = 0.33638456646926895\n",
      "Epoch 147: Train Loss = 0.3366734160447057\n",
      "Epoch 148: Train Loss = 0.336418195086086\n",
      "Epoch 149: Train Loss = 0.3364331277215046\n",
      "Epoch 150: Train Loss = 0.337397900550151\n",
      "Recall = 0.9991169977924945, Aging Rate = 0.5100178094390027, Precision = 0.9877782627673505\n",
      "Validation: Test Loss = 0.33530275933676706\n",
      "Recall = 0.9982339955849889, Aging Rate = 0.5082368655387355, precision = 0.9903635567236093\n",
      "\n",
      "Validation: Test Loss = 0.3582350885040451\n",
      "Recall = 0.9780821917808219, Aging Rate = 0.4993324432576769, precision = 0.9545454545454546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba7f84bae9b43eebcdf2a2081592cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6693130222581267\n",
      "Epoch 2: Train Loss = 0.6254179291393963\n",
      "Epoch 3: Train Loss = 0.570495310630102\n",
      "Epoch 4: Train Loss = 0.5343350851153648\n",
      "Epoch 5: Train Loss = 0.515923961771458\n",
      "Recall = 0.8999551368326604, Aging Rate = 0.608860195903829, Precision = 0.7334552102376599\n",
      "Validation: Test Loss = 0.5031928797439073\n",
      "Recall = 0.9084791386271871, Aging Rate = 0.6024042742653607, precision = 0.7483370288248337\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.49853006711201486\n",
      "Epoch 7: Train Loss = 0.48631972714077654\n",
      "Epoch 8: Train Loss = 0.4754738651103243\n",
      "Epoch 9: Train Loss = 0.4639279794608071\n",
      "Epoch 10: Train Loss = 0.4543774795394228\n",
      "Recall = 0.9331538806639749, Aging Rate = 0.5670080142475512, Precision = 0.8166470357283078\n",
      "Validation: Test Loss = 0.4468149855958492\n",
      "Recall = 0.9282189322566173, Aging Rate = 0.5485307212822796, precision = 0.8396915584415584\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.4445044609285951\n",
      "Epoch 12: Train Loss = 0.4357799186254333\n",
      "Epoch 13: Train Loss = 0.4273397589993074\n",
      "Epoch 14: Train Loss = 0.4196608202964837\n",
      "Epoch 15: Train Loss = 0.413270220704516\n",
      "Recall = 0.9632122027815164, Aging Rate = 0.5418521816562778, Precision = 0.8820870994248151\n",
      "Validation: Test Loss = 0.4075514905599219\n",
      "Recall = 0.9672498878420817, Aging Rate = 0.5365093499554764, precision = 0.8946058091286307\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.4073796884247159\n",
      "Epoch 17: Train Loss = 0.4028103680400475\n",
      "Epoch 18: Train Loss = 0.3972397090490652\n",
      "Epoch 19: Train Loss = 0.3932000758492936\n",
      "Epoch 20: Train Loss = 0.3895234611028024\n",
      "Recall = 0.9744279946164199, Aging Rate = 0.5251558325912734, Precision = 0.920729122509538\n",
      "Validation: Test Loss = 0.3854578014791596\n",
      "Recall = 0.9753252579632122, Aging Rate = 0.5204808548530722, precision = 0.9298545765611634\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3860866038637934\n",
      "Epoch 22: Train Loss = 0.3825006458181838\n",
      "Epoch 23: Train Loss = 0.38161004752000316\n",
      "Epoch 24: Train Loss = 0.3785768524409402\n",
      "Epoch 25: Train Loss = 0.37504522153445896\n",
      "Recall = 0.9793629430237775, Aging Rate = 0.5166963490650045, Precision = 0.9405428694528221\n",
      "Validation: Test Loss = 0.3729555183293452\n",
      "Recall = 0.9784656796769852, Aging Rate = 0.5093499554764025, precision = 0.9532342657342657\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.37325683042393765\n",
      "Epoch 27: Train Loss = 0.371866902414434\n",
      "Epoch 28: Train Loss = 0.3698567150696409\n",
      "Epoch 29: Train Loss = 0.3674895793385306\n",
      "Epoch 30: Train Loss = 0.3660775898878636\n",
      "Recall = 0.9851951547779273, Aging Rate = 0.5142475512021372, Precision = 0.9506493506493506\n",
      "Validation: Test Loss = 0.36371272372435165\n",
      "Recall = 0.9838492597577388, Aging Rate = 0.5104630454140695, precision = 0.9563890100305277\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.3648964856898052\n",
      "Epoch 32: Train Loss = 0.36340988810213143\n",
      "Epoch 33: Train Loss = 0.3625797853072838\n",
      "Epoch 34: Train Loss = 0.36062608092678305\n",
      "Epoch 35: Train Loss = 0.3598636299875944\n",
      "Recall = 0.9856437864513234, Aging Rate = 0.5091273374888691, Precision = 0.9606471359860078\n",
      "Validation: Test Loss = 0.3595035434936904\n",
      "Recall = 0.9928218932256617, Aging Rate = 0.5184772929652716, precision = 0.9501932159725204\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.35924325627295545\n",
      "Epoch 37: Train Loss = 0.3583603241776953\n",
      "Epoch 38: Train Loss = 0.3567570325580964\n",
      "Epoch 39: Train Loss = 0.3553747166402618\n",
      "Epoch 40: Train Loss = 0.3547991642385111\n",
      "Recall = 0.9896814715118888, Aging Rate = 0.5071237756010686, Precision = 0.9683933274802459\n",
      "Validation: Test Loss = 0.35415569632475863\n",
      "Recall = 0.9865410497981157, Aging Rate = 0.5008904719501336, precision = 0.9773333333333334\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.35416113937527394\n",
      "Epoch 42: Train Loss = 0.3531515001189146\n",
      "Epoch 43: Train Loss = 0.3521689074918402\n",
      "Epoch 44: Train Loss = 0.35211542179726746\n",
      "Epoch 45: Train Loss = 0.3511470999445962\n",
      "Recall = 0.990578734858681, Aging Rate = 0.5046749777382012, Precision = 0.9739744155271284\n",
      "Validation: Test Loss = 0.3490777831468217\n",
      "Recall = 0.9932705248990579, Aging Rate = 0.506233303650935, precision = 0.9736147757255936\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.35030050343524527\n",
      "Epoch 47: Train Loss = 0.3500826008757203\n",
      "Epoch 48: Train Loss = 0.34913226431326894\n",
      "Epoch 49: Train Loss = 0.34826312180300006\n",
      "Epoch 50: Train Loss = 0.34833597147029527\n",
      "Recall = 0.9919246298788694, Aging Rate = 0.5042297417631345, Precision = 0.976158940397351\n",
      "Validation: Test Loss = 0.3473387186748049\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.5089047195013358, precision = 0.9711286089238845\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.3478211859242787\n",
      "Epoch 52: Train Loss = 0.3472121879922845\n",
      "Epoch 53: Train Loss = 0.3475987202656237\n",
      "Epoch 54: Train Loss = 0.34736790048258703\n",
      "Epoch 55: Train Loss = 0.34595889312182804\n",
      "Recall = 0.9941677882458502, Aging Rate = 0.505120213713268, Precision = 0.9766416923754958\n",
      "Validation: Test Loss = 0.3446859772037632\n",
      "Recall = 0.9950650515926425, Aging Rate = 0.5048975957257347, precision = 0.9779541446208113\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.3457329217759181\n",
      "Epoch 57: Train Loss = 0.34612948310131064\n",
      "Epoch 58: Train Loss = 0.3459876569274909\n",
      "Epoch 59: Train Loss = 0.3446170832839059\n",
      "Epoch 60: Train Loss = 0.34451048080133523\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.5042297417631345, Precision = 0.9801324503311258\n",
      "Validation: Test Loss = 0.34308745469881397\n",
      "Recall = 0.9946164199192463, Aging Rate = 0.5026714158504008, precision = 0.9818423383525243\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.34404097328946087\n",
      "Epoch 62: Train Loss = 0.34389157045237845\n",
      "Epoch 63: Train Loss = 0.3438211209468298\n",
      "Epoch 64: Train Loss = 0.3437731022932438\n",
      "Epoch 65: Train Loss = 0.3437890837156444\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.5044523597506678, Precision = 0.9796999117387467\n",
      "Validation: Test Loss = 0.34247267105594237\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.5017809439002672, precision = 0.9849157054125999\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.34384007721422827\n",
      "Epoch 67: Train Loss = 0.3437115964832204\n",
      "Epoch 68: Train Loss = 0.3427264927650071\n",
      "Epoch 69: Train Loss = 0.3426718585000756\n",
      "Epoch 70: Train Loss = 0.3422873210981289\n",
      "Recall = 0.9964109466128309, Aging Rate = 0.505120213713268, Precision = 0.9788453063023358\n",
      "Validation: Test Loss = 0.3411745575794872\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5037845057880677, precision = 0.9823243482103402\n",
      "\n",
      "Epoch 71: Train Loss = 0.3424183207755628\n",
      "Epoch 72: Train Loss = 0.3424349660758768\n",
      "Epoch 73: Train Loss = 0.3423505015704426\n",
      "Epoch 74: Train Loss = 0.3418586091971886\n",
      "Epoch 75: Train Loss = 0.3417223545184437\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5033392698130009, Precision = 0.9831932773109243\n",
      "Validation: Test Loss = 0.3405536377631652\n",
      "Recall = 0.9964109466128309, Aging Rate = 0.5020035618878005, precision = 0.9849223946784922\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.3412804475397482\n",
      "Epoch 77: Train Loss = 0.3415044238019605\n",
      "Epoch 78: Train Loss = 0.3413159095965001\n",
      "Epoch 79: Train Loss = 0.34126994618855727\n",
      "Epoch 80: Train Loss = 0.341750437355636\n",
      "Recall = 0.9964109466128309, Aging Rate = 0.5020035618878005, Precision = 0.9849223946784922\n",
      "Validation: Test Loss = 0.34056875396284375\n",
      "Recall = 0.9955136832660386, Aging Rate = 0.5002226179875334, precision = 0.9875389408099688\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.34143335218951837\n",
      "Epoch 82: Train Loss = 0.34140023482981685\n",
      "Epoch 83: Train Loss = 0.34118937392481075\n",
      "Epoch 84: Train Loss = 0.3405727349215072\n",
      "Epoch 85: Train Loss = 0.3407297044169765\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.5022261798753339, Precision = 0.9840425531914894\n",
      "Validation: Test Loss = 0.34067777053968035\n",
      "Recall = 0.9986541049798116, Aging Rate = 0.505120213713268, precision = 0.9810489202291759\n",
      "\n",
      "Epoch 86: Train Loss = 0.34044560604295027\n",
      "Epoch 87: Train Loss = 0.34095655162739946\n",
      "Epoch 88: Train Loss = 0.3408557857293055\n",
      "Epoch 89: Train Loss = 0.34054461280787934\n",
      "Epoch 90: Train Loss = 0.3403227294223816\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5033392698130009, Precision = 0.9831932773109243\n",
      "Validation: Test Loss = 0.3389305080671132\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5017809439002672, precision = 0.9866903283052352\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.34048410703428916\n",
      "Epoch 92: Train Loss = 0.33980918448096603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Train Loss = 0.3400475075227919\n",
      "Epoch 94: Train Loss = 0.3401956354680066\n",
      "Epoch 95: Train Loss = 0.3399554009641374\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5028940338379341, Precision = 0.984506418769367\n",
      "Validation: Test Loss = 0.3388814223417722\n",
      "Recall = 0.9982054733064154, Aging Rate = 0.5026714158504008, precision = 0.9853852967227635\n",
      "\n",
      "Epoch 96: Train Loss = 0.339766413796086\n",
      "Epoch 97: Train Loss = 0.3399601058830455\n",
      "Epoch 98: Train Loss = 0.33982785418961375\n",
      "Epoch 99: Train Loss = 0.340779394972887\n",
      "Epoch 100: Train Loss = 0.3397062584585307\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5020035618878005, Precision = 0.985809312638581\n",
      "Validation: Test Loss = 0.338356706492092\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5017809439002672, precision = 0.9866903283052352\n",
      "\n",
      "Epoch 101: Train Loss = 0.33931913778172573\n",
      "Epoch 102: Train Loss = 0.33942430431250475\n",
      "Epoch 103: Train Loss = 0.3392739999219974\n",
      "Epoch 104: Train Loss = 0.3394682533966041\n",
      "Epoch 105: Train Loss = 0.3391634991470449\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5024487978628673, Precision = 0.9853788214443953\n",
      "Validation: Test Loss = 0.3381348784385997\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5017809439002672, precision = 0.9866903283052352\n",
      "\n",
      "Epoch 106: Train Loss = 0.3392120519601545\n",
      "Epoch 107: Train Loss = 0.33907032649854624\n",
      "Epoch 108: Train Loss = 0.33989704777805907\n",
      "Epoch 109: Train Loss = 0.3388063788201696\n",
      "Epoch 110: Train Loss = 0.33947410362168073\n",
      "Recall = 0.996859578286227, Aging Rate = 0.5015583259127337, Precision = 0.9862405681313804\n",
      "Validation: Test Loss = 0.3392718100218718\n",
      "Recall = 0.9986541049798116, Aging Rate = 0.5044523597506678, precision = 0.9823477493380406\n",
      "\n",
      "Epoch 111: Train Loss = 0.340362678971125\n",
      "Epoch 112: Train Loss = 0.3393900783223333\n",
      "Epoch 113: Train Loss = 0.3390783180324286\n",
      "Epoch 114: Train Loss = 0.338802165440545\n",
      "Epoch 115: Train Loss = 0.33856487157402143\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.501113089937667, Precision = 0.9875610839626833\n",
      "Validation: Test Loss = 0.3381003700827024\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5006678539626002, precision = 0.9884393063583815\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.33936202932125953\n",
      "Epoch 117: Train Loss = 0.3390931793068099\n",
      "Epoch 118: Train Loss = 0.33928494382307983\n",
      "Epoch 119: Train Loss = 0.33906397621544576\n",
      "Epoch 120: Train Loss = 0.33940552261293944\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.49977738201246663, Precision = 0.9888641425389755\n",
      "Validation: Test Loss = 0.33845381721363255\n",
      "Recall = 0.9959623149394348, Aging Rate = 0.4995547640249332, precision = 0.9893048128342246\n",
      "\n",
      "Epoch 121: Train Loss = 0.3391281811127361\n",
      "Epoch 122: Train Loss = 0.33911915472439114\n",
      "Epoch 123: Train Loss = 0.3386172680640369\n",
      "Epoch 124: Train Loss = 0.3383536024063905\n",
      "Epoch 125: Train Loss = 0.3384363175712307\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5008904719501336, Precision = 0.9884444444444445\n",
      "Validation: Test Loss = 0.33798090666399827\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5, precision = 0.9897595725734639\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.33858856269534104\n",
      "Epoch 127: Train Loss = 0.3389036327945475\n",
      "Epoch 128: Train Loss = 0.3387876598302106\n",
      "Epoch 129: Train Loss = 0.33855307999192236\n",
      "Epoch 130: Train Loss = 0.3387693488640331\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.501113089937667, Precision = 0.9875610839626833\n",
      "Validation: Test Loss = 0.3374370366255301\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5004452359750667, precision = 0.9893238434163701\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.33857032879589927\n",
      "Epoch 132: Train Loss = 0.33939741545241536\n",
      "Epoch 133: Train Loss = 0.3400711842604439\n",
      "Epoch 134: Train Loss = 0.33879626407648766\n",
      "Epoch 135: Train Loss = 0.33849245524788585\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5008904719501336, Precision = 0.988\n",
      "Validation: Test Loss = 0.3374357825926342\n",
      "Recall = 0.9986541049798116, Aging Rate = 0.5028940338379341, precision = 0.9853917662682603\n",
      "\n",
      "Epoch 136: Train Loss = 0.3379519238391309\n",
      "Epoch 137: Train Loss = 0.3381460313009345\n",
      "Epoch 138: Train Loss = 0.3379887501681796\n",
      "Epoch 139: Train Loss = 0.33826746690092824\n",
      "Epoch 140: Train Loss = 0.33828579198539205\n",
      "Recall = 0.9973082099596231, Aging Rate = 0.5004452359750667, Precision = 0.9888790035587188\n",
      "Validation: Test Loss = 0.33708811997836546\n",
      "Recall = 0.9986541049798116, Aging Rate = 0.5035618878005342, precision = 0.9840848806366048\n",
      "\n",
      "Epoch 141: Train Loss = 0.3381150870427211\n",
      "Epoch 142: Train Loss = 0.338317338659205\n",
      "Epoch 143: Train Loss = 0.3382272540142997\n",
      "Epoch 144: Train Loss = 0.3379319461986409\n",
      "Epoch 145: Train Loss = 0.3380461029462908\n",
      "Recall = 0.9977568416330193, Aging Rate = 0.5013357079252003, Precision = 0.9875666074600356\n",
      "Validation: Test Loss = 0.33692885788019916\n",
      "Recall = 0.9982054733064154, Aging Rate = 0.5006678539626002, precision = 0.9893285904846598\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.33798508042219166\n",
      "Epoch 147: Train Loss = 0.3379506486818818\n",
      "Epoch 148: Train Loss = 0.33788674711544175\n",
      "Epoch 149: Train Loss = 0.33819103336504075\n",
      "Epoch 150: Train Loss = 0.3378616986482778\n",
      "Recall = 0.9982054733064154, Aging Rate = 0.5008904719501336, Precision = 0.9888888888888889\n",
      "Validation: Test Loss = 0.33678348631807875\n",
      "Recall = 0.9982054733064154, Aging Rate = 0.5013357079252003, precision = 0.9880106571936057\n",
      "\n",
      "Validation: Test Loss = 0.35436115985878003\n",
      "Recall = 0.9882506527415144, Aging Rate = 0.527369826435247, precision = 0.9582278481012658\n",
      "\u001b[32m[I 2022-05-26 16:23:30,070]\u001b[0m Trial 3 finished with value: 0.9705433344641538 and parameters: {'batch_size': 128, 'learning_rate': 0.001, 'weight_decay': 0.001, 'bad_weight': 0.6}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eddc0bf2f3429e94387409385fac9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6749714134212912\n",
      "Epoch 2: Train Loss = 0.6557347845097047\n",
      "Epoch 3: Train Loss = 0.6376454450143626\n",
      "Epoch 4: Train Loss = 0.6226679959789728\n",
      "Epoch 5: Train Loss = 0.612231655515737\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5044523597506678\n",
      "Validation: Test Loss = 0.6075165402857406\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5044523597506678\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6060224240206229\n",
      "Epoch 7: Train Loss = 0.6012508120795816\n",
      "Epoch 8: Train Loss = 0.5980278843232594\n",
      "Epoch 9: Train Loss = 0.5944435240324332\n",
      "Epoch 10: Train Loss = 0.5914725435172885\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5044523597506678\n",
      "Validation: Test Loss = 0.5890757842246486\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5044523597506678\n",
      "\n",
      "Epoch 11: Train Loss = 0.5870462264318288\n",
      "Epoch 12: Train Loss = 0.5820550125619713\n",
      "Epoch 13: Train Loss = 0.5759476304372718\n",
      "Epoch 14: Train Loss = 0.5691896043817379\n",
      "Epoch 15: Train Loss = 0.561925818690846\n",
      "Recall = 0.999558693733451, Aging Rate = 0.9666073018699911, Precision = 0.5216490096729618\n",
      "Validation: Test Loss = 0.5580802316232547\n",
      "Recall = 0.998676081200353, Aging Rate = 0.9432324131789849, precision = 0.5341043190936984\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5546645311139464\n",
      "Epoch 17: Train Loss = 0.5470449378844997\n",
      "Epoch 18: Train Loss = 0.5401738574222487\n",
      "Epoch 19: Train Loss = 0.5338719064703183\n",
      "Epoch 20: Train Loss = 0.5276842689206318\n",
      "Recall = 0.9876434245366285, Aging Rate = 0.815227070347284, Precision = 0.6111414527580558\n",
      "Validation: Test Loss = 0.5244117448983825\n",
      "Recall = 0.9872021182700794, Aging Rate = 0.7980854853072128, precision = 0.6239888423988842\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5220473959630659\n",
      "Epoch 22: Train Loss = 0.5172905912382307\n",
      "Epoch 23: Train Loss = 0.5127553243458749\n",
      "Epoch 24: Train Loss = 0.5091701243885478\n",
      "Epoch 25: Train Loss = 0.5052985643448834\n",
      "Recall = 0.96954986760812, Aging Rate = 0.7330810329474622, Precision = 0.6671727907682964\n",
      "Validation: Test Loss = 0.5035292224382993\n",
      "Recall = 0.9752868490732568, Aging Rate = 0.7477738201246661, precision = 0.6579339089014588\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.502296887941904\n",
      "Epoch 27: Train Loss = 0.4991515386030277\n",
      "Epoch 28: Train Loss = 0.49627891421742665\n",
      "Epoch 29: Train Loss = 0.4940459996806864\n",
      "Epoch 30: Train Loss = 0.49131720170100035\n",
      "Recall = 0.9651368049426302, Aging Rate = 0.7094835262689225, Precision = 0.6862252902416065\n",
      "Validation: Test Loss = 0.4901025085340943\n",
      "Recall = 0.9646954986760812, Aging Rate = 0.7054764024933214, precision = 0.6898075102556012\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4896753292102848\n",
      "Epoch 32: Train Loss = 0.48714822748359143\n",
      "Epoch 33: Train Loss = 0.4853303341173425\n",
      "Epoch 34: Train Loss = 0.4839574622980527\n",
      "Epoch 35: Train Loss = 0.48180458605130233\n",
      "Recall = 0.9660194174757282, Aging Rate = 0.696126447016919, Precision = 0.7000319795330988\n",
      "Validation: Test Loss = 0.48071967247120523\n",
      "Recall = 0.9669020300088261, Aging Rate = 0.695013357079252, precision = 0.7017937219730942\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.480575968936842\n",
      "Epoch 37: Train Loss = 0.4791974214603088\n",
      "Epoch 38: Train Loss = 0.4773481286272659\n",
      "Epoch 39: Train Loss = 0.4761419567484359\n",
      "Epoch 40: Train Loss = 0.47484350552117305\n",
      "Recall = 0.9624889673433362, Aging Rate = 0.6843276936776491, Precision = 0.7094990240728692\n",
      "Validation: Test Loss = 0.4738762861155869\n",
      "Recall = 0.9638128861429832, Aging Rate = 0.6818788958147818, precision = 0.713026444662096\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.47365911980346176\n",
      "Epoch 42: Train Loss = 0.47212870393814194\n",
      "Epoch 43: Train Loss = 0.4708612059920575\n",
      "Epoch 44: Train Loss = 0.46990233765896794\n",
      "Epoch 45: Train Loss = 0.46890800217380507\n",
      "Recall = 0.9633715798764343, Aging Rate = 0.6754229741763135, Precision = 0.7195121951219512\n",
      "Validation: Test Loss = 0.46848437402360904\n",
      "Recall = 0.9633715798764343, Aging Rate = 0.6749777382012466, precision = 0.7199868073878628\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.46785892783693195\n",
      "Epoch 47: Train Loss = 0.46716680119638454\n",
      "Epoch 48: Train Loss = 0.465841022805242\n",
      "Epoch 49: Train Loss = 0.46467023289108106\n",
      "Epoch 50: Train Loss = 0.4639894300980538\n",
      "Recall = 0.9660194174757282, Aging Rate = 0.6731967943009796, Precision = 0.7238756613756614\n",
      "Validation: Test Loss = 0.4632485451861778\n",
      "Recall = 0.9677846425419241, Aging Rate = 0.6703027604630454, precision = 0.7283294586516108\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.4631393008325531\n",
      "Epoch 52: Train Loss = 0.4621754434102365\n",
      "Epoch 53: Train Loss = 0.4613652721591433\n",
      "Epoch 54: Train Loss = 0.4603526358719076\n",
      "Epoch 55: Train Loss = 0.45943173283142164\n",
      "Recall = 0.9651368049426302, Aging Rate = 0.6620658949243099, Precision = 0.7353732347007398\n",
      "Validation: Test Loss = 0.45888323517962426\n",
      "Recall = 0.9682259488084731, Aging Rate = 0.6662956366874443, precision = 0.7330437687938524\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.4590822964614773\n",
      "Epoch 57: Train Loss = 0.4578332742833709\n",
      "Epoch 58: Train Loss = 0.4573836301387896\n",
      "Epoch 59: Train Loss = 0.45633666133732004\n",
      "Epoch 60: Train Loss = 0.45554882083635506\n",
      "Recall = 0.969991173874669, Aging Rate = 0.6642920747996438, Precision = 0.7365951742627346\n",
      "Validation: Test Loss = 0.45517659171394015\n",
      "Recall = 0.96954986760812, Aging Rate = 0.6627337488869101, precision = 0.7379912663755459\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.4548672618923289\n",
      "Epoch 62: Train Loss = 0.45424831194847903\n",
      "Epoch 63: Train Loss = 0.45341000565968764\n",
      "Epoch 64: Train Loss = 0.4527630902037277\n",
      "Epoch 65: Train Loss = 0.4517789196989309\n",
      "Recall = 0.9664607237422771, Aging Rate = 0.6556099732858415, Precision = 0.7436332767402377\n",
      "Validation: Test Loss = 0.4513267593413512\n",
      "Recall = 0.971756398940865, Aging Rate = 0.6618432769367765, precision = 0.7406659939455096\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4511908326208432\n",
      "Epoch 67: Train Loss = 0.45036292399876054\n",
      "Epoch 68: Train Loss = 0.44956354899058254\n",
      "Epoch 69: Train Loss = 0.44905084124762146\n",
      "Epoch 70: Train Loss = 0.44847946005640343\n",
      "Recall = 0.9664607237422771, Aging Rate = 0.6493766696349065, Precision = 0.7507713404182379\n",
      "Validation: Test Loss = 0.4477814351567602\n",
      "Recall = 0.972197705207414, Aging Rate = 0.6567230632235085, precision = 0.7467796610169491\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.4474201481573728\n",
      "Epoch 72: Train Loss = 0.44724728845106315\n",
      "Epoch 73: Train Loss = 0.44618243467351315\n",
      "Epoch 74: Train Loss = 0.44571970975091174\n",
      "Epoch 75: Train Loss = 0.44480634022268567\n",
      "Recall = 0.970432480141218, Aging Rate = 0.6500445235975066, Precision = 0.7530821917808219\n",
      "Validation: Test Loss = 0.4444568015482419\n",
      "Recall = 0.9691085613415711, Aging Rate = 0.6469278717720391, precision = 0.755677907777013\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.44439019404769686\n",
      "Epoch 77: Train Loss = 0.4435862351344933\n",
      "Epoch 78: Train Loss = 0.44285695611740583\n",
      "Epoch 79: Train Loss = 0.4422380538095031\n",
      "Epoch 80: Train Loss = 0.4414717649787213\n",
      "Recall = 0.970432480141218, Aging Rate = 0.6444790739091718, Precision = 0.7595854922279792\n",
      "Validation: Test Loss = 0.4407994845883717\n",
      "Recall = 0.9735216240070609, Aging Rate = 0.6464826357969724, precision = 0.759641873278237\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.4407014113553167\n",
      "Epoch 82: Train Loss = 0.440317810059443\n",
      "Epoch 83: Train Loss = 0.4397088831700285\n",
      "Epoch 84: Train Loss = 0.4389855190779838\n",
      "Epoch 85: Train Loss = 0.43829673321673834\n",
      "Recall = 0.9770520741394528, Aging Rate = 0.6478183437221727, Precision = 0.7608247422680412\n",
      "Validation: Test Loss = 0.43747691081659346\n",
      "Recall = 0.969991173874669, Aging Rate = 0.6389136242208371, precision = 0.7658536585365854\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.4375948091224168\n",
      "Epoch 87: Train Loss = 0.4367749914991357\n",
      "Epoch 88: Train Loss = 0.4363034440010441\n",
      "Epoch 89: Train Loss = 0.4353809611720903\n",
      "Epoch 90: Train Loss = 0.43489670323561264\n",
      "Recall = 0.972197705207414, Aging Rate = 0.6393588601959038, Precision = 0.7670612813370473\n",
      "Validation: Test Loss = 0.4342007482837378\n",
      "Recall = 0.9735216240070609, Aging Rate = 0.6393588601959038, precision = 0.7681058495821727\n",
      "Model in epoch 90 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss = 0.43431124494527984\n",
      "Epoch 92: Train Loss = 0.43353948034246587\n",
      "Epoch 93: Train Loss = 0.4326028615528202\n",
      "Epoch 94: Train Loss = 0.4320342084815857\n",
      "Epoch 95: Train Loss = 0.4313878156941381\n",
      "Recall = 0.9748455428067079, Aging Rate = 0.6375779162956366, Precision = 0.7712988826815642\n",
      "Validation: Test Loss = 0.4305946635860263\n",
      "Recall = 0.9748455428067079, Aging Rate = 0.6362422083704363, precision = 0.7729181245626312\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.43080448363895196\n",
      "Epoch 97: Train Loss = 0.4299896054520526\n",
      "Epoch 98: Train Loss = 0.42916375762208275\n",
      "Epoch 99: Train Loss = 0.4286979710482532\n",
      "Epoch 100: Train Loss = 0.4279212864615083\n",
      "Recall = 0.9752868490732568, Aging Rate = 0.6300089047195013, Precision = 0.7809187279151943\n",
      "Validation: Test Loss = 0.42720656257490974\n",
      "Recall = 0.9752868490732568, Aging Rate = 0.6282279608192342, precision = 0.7831325301204819\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.42708698570569076\n",
      "Epoch 102: Train Loss = 0.4265494021791915\n",
      "Epoch 103: Train Loss = 0.4256651960147457\n",
      "Epoch 104: Train Loss = 0.42503294242564205\n",
      "Epoch 105: Train Loss = 0.4242234475519226\n",
      "Recall = 0.9757281553398058, Aging Rate = 0.6233303650934996, Precision = 0.7896428571428571\n",
      "Validation: Test Loss = 0.4237527132883522\n",
      "Recall = 0.9752868490732568, Aging Rate = 0.6208815672306323, precision = 0.7923987092147723\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.4236019817931463\n",
      "Epoch 107: Train Loss = 0.4228858747125627\n",
      "Epoch 108: Train Loss = 0.42219610343739483\n",
      "Epoch 109: Train Loss = 0.42170796674060057\n",
      "Epoch 110: Train Loss = 0.4210940385013421\n",
      "Recall = 0.9761694616063548, Aging Rate = 0.6188780053428317, Precision = 0.7956834532374101\n",
      "Validation: Test Loss = 0.4201642259464451\n",
      "Recall = 0.9774933804060018, Aging Rate = 0.6199910952804987, precision = 0.7953321364452424\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.4201636293426965\n",
      "Epoch 112: Train Loss = 0.41944778156620205\n",
      "Epoch 113: Train Loss = 0.418657981834556\n",
      "Epoch 114: Train Loss = 0.4180636248573701\n",
      "Epoch 115: Train Loss = 0.41743168269324704\n",
      "Recall = 0.9779346866725508, Aging Rate = 0.6155387355298308, Precision = 0.8014466546112116\n",
      "Validation: Test Loss = 0.4168319292134296\n",
      "Recall = 0.9766107678729038, Aging Rate = 0.6093054318788959, precision = 0.8085495067592254\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.41653754582706565\n",
      "Epoch 117: Train Loss = 0.41611507784439217\n",
      "Epoch 118: Train Loss = 0.41539641259402327\n",
      "Epoch 119: Train Loss = 0.4146015453264316\n",
      "Epoch 120: Train Loss = 0.4138495880689137\n",
      "Recall = 0.9788172992056487, Aging Rate = 0.6095280498664292, Precision = 0.8100803506208911\n",
      "Validation: Test Loss = 0.4134668047578868\n",
      "Recall = 0.9770520741394528, Aging Rate = 0.6012911843276937, precision = 0.8196964087375046\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.4131222902295426\n",
      "Epoch 122: Train Loss = 0.41258688251142084\n",
      "Epoch 123: Train Loss = 0.41201359975688284\n",
      "Epoch 124: Train Loss = 0.4111285607462362\n",
      "Epoch 125: Train Loss = 0.4108805815131136\n",
      "Recall = 0.9788172992056487, Aging Rate = 0.6019590382902938, Precision = 0.8202662721893491\n",
      "Validation: Test Loss = 0.409933510388207\n",
      "Recall = 0.9783759929390997, Aging Rate = 0.597506678539626, precision = 0.8260059612518629\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.4099035446970779\n",
      "Epoch 127: Train Loss = 0.4091137814288254\n",
      "Epoch 128: Train Loss = 0.4086312613950069\n",
      "Epoch 129: Train Loss = 0.4081091286609561\n",
      "Epoch 130: Train Loss = 0.40759987743009124\n",
      "Recall = 0.9801412180052956, Aging Rate = 0.5957257346393589, Precision = 0.8299701046337817\n",
      "Validation: Test Loss = 0.40694251770646256\n",
      "Recall = 0.9796999117387467, Aging Rate = 0.5921638468388246, precision = 0.8345864661654135\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.40698956249021356\n",
      "Epoch 132: Train Loss = 0.40621685047608236\n",
      "Epoch 133: Train Loss = 0.40555681302838315\n",
      "Epoch 134: Train Loss = 0.40494722324296606\n",
      "Epoch 135: Train Loss = 0.4044612583207532\n",
      "Recall = 0.9819064430714917, Aging Rate = 0.5934995547640249, Precision = 0.8345836459114778\n",
      "Validation: Test Loss = 0.40440787148072266\n",
      "Recall = 0.9783759929390997, Aging Rate = 0.5841495992876224, precision = 0.8448932926829268\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.40406387306597225\n",
      "Epoch 137: Train Loss = 0.40303741059766107\n",
      "Epoch 138: Train Loss = 0.40275839239173133\n",
      "Epoch 139: Train Loss = 0.4020853079755924\n",
      "Epoch 140: Train Loss = 0.4017060446824119\n",
      "Recall = 0.9805825242718447, Aging Rate = 0.5841495992876224, Precision = 0.8467987804878049\n",
      "Validation: Test Loss = 0.4010124806304755\n",
      "Recall = 0.9827890556045896, Aging Rate = 0.5863757791629564, precision = 0.8454821564160971\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.40103720344822\n",
      "Epoch 142: Train Loss = 0.4007210270538992\n",
      "Epoch 143: Train Loss = 0.40001360173747674\n",
      "Epoch 144: Train Loss = 0.39921856207817874\n",
      "Epoch 145: Train Loss = 0.3993015218449828\n",
      "Recall = 0.9836716681376876, Aging Rate = 0.5819234194122885, Precision = 0.8527161438408569\n",
      "Validation: Test Loss = 0.3984974887311618\n",
      "Recall = 0.9810238305383936, Aging Rate = 0.5770258236865539, precision = 0.8576388888888888\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.39845583795545786\n",
      "Epoch 147: Train Loss = 0.39784822767585487\n",
      "Epoch 148: Train Loss = 0.3975179102255209\n",
      "Epoch 149: Train Loss = 0.3968525876026544\n",
      "Epoch 150: Train Loss = 0.3963757324515977\n",
      "Recall = 0.9832303618711385, Aging Rate = 0.578806767586821, Precision = 0.8569230769230769\n",
      "Validation: Test Loss = 0.3958386826334738\n",
      "Recall = 0.9854368932038835, Aging Rate = 0.5803650934995548, precision = 0.8565400843881856\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.4182326942085424\n",
      "Recall = 0.9780521262002744, Aging Rate = 0.5867823765020027, precision = 0.8111490329920364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9b9622cd93420a8128b265b212506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6803869248922659\n",
      "Epoch 2: Train Loss = 0.6579728031519366\n",
      "Epoch 3: Train Loss = 0.6401532091206986\n",
      "Epoch 4: Train Loss = 0.6274631838658402\n",
      "Epoch 5: Train Loss = 0.6174941266209338\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49755120213713266\n",
      "Validation: Test Loss = 0.6137774969357417\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49755120213713266\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6120748715218114\n",
      "Epoch 7: Train Loss = 0.6078302584794198\n",
      "Epoch 8: Train Loss = 0.6052023289677084\n",
      "Epoch 9: Train Loss = 0.6019734921247324\n",
      "Epoch 10: Train Loss = 0.5988469546327395\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49755120213713266\n",
      "Validation: Test Loss = 0.5972195376483649\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49755120213713266\n",
      "\n",
      "Epoch 11: Train Loss = 0.5951544757093156\n",
      "Epoch 12: Train Loss = 0.5914158487256255\n",
      "Epoch 13: Train Loss = 0.5860950710618485\n",
      "Epoch 14: Train Loss = 0.5803870790681983\n",
      "Epoch 15: Train Loss = 0.5737039546614127\n",
      "Recall = 1.0, Aging Rate = 0.9846393588601959, Precision = 0.505313135880624\n",
      "Validation: Test Loss = 0.5696461851429111\n",
      "Recall = 0.9991051454138703, Aging Rate = 0.9768477292965272, precision = 0.5088878760255242\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5660377592777208\n",
      "Epoch 17: Train Loss = 0.558570960591334\n",
      "Epoch 18: Train Loss = 0.5509768956384803\n",
      "Epoch 19: Train Loss = 0.543702094128168\n",
      "Epoch 20: Train Loss = 0.5370802623505903\n",
      "Recall = 0.9870246085011186, Aging Rate = 0.8308103294746215, Precision = 0.5911039657020365\n",
      "Validation: Test Loss = 0.5337503105269302\n",
      "Recall = 0.9861297539149888, Aging Rate = 0.8199020480854853, precision = 0.5984251968503937\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.530906831538794\n",
      "Epoch 22: Train Loss = 0.5253386761286593\n",
      "Epoch 23: Train Loss = 0.5203120454761033\n",
      "Epoch 24: Train Loss = 0.5160223872451408\n",
      "Epoch 25: Train Loss = 0.5117001811097163\n",
      "Recall = 0.9691275167785235, Aging Rate = 0.7379786286731967, Precision = 0.653393665158371\n",
      "Validation: Test Loss = 0.5097010609090912\n",
      "Recall = 0.9695749440715884, Aging Rate = 0.7382012466607302, precision = 0.6534981905910736\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5078942565544312\n",
      "Epoch 27: Train Loss = 0.5049529251835756\n",
      "Epoch 28: Train Loss = 0.5022720617581348\n",
      "Epoch 29: Train Loss = 0.49914983926027456\n",
      "Epoch 30: Train Loss = 0.4963408859308978\n",
      "Recall = 0.9655480984340045, Aging Rate = 0.7110418521816563, Precision = 0.6756418284283031\n",
      "Validation: Test Loss = 0.4953310523092587\n",
      "Recall = 0.96331096196868, Aging Rate = 0.7043633125556545, precision = 0.6804677623261695\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4941105814194531\n",
      "Epoch 32: Train Loss = 0.4921628625819647\n",
      "Epoch 33: Train Loss = 0.4898173820197529\n",
      "Epoch 34: Train Loss = 0.48803344010669847\n",
      "Epoch 35: Train Loss = 0.48625955301422363\n",
      "Recall = 0.9615212527964205, Aging Rate = 0.6892252894033838, Precision = 0.6941214470284238\n",
      "Validation: Test Loss = 0.48516186174811365\n",
      "Recall = 0.963758389261745, Aging Rate = 0.6921193232413179, precision = 0.6928272756513348\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4844569968837558\n",
      "Epoch 37: Train Loss = 0.4830656316486301\n",
      "Epoch 38: Train Loss = 0.4813393169179731\n",
      "Epoch 39: Train Loss = 0.47989948485328593\n",
      "Epoch 40: Train Loss = 0.4786643265404026\n",
      "Recall = 0.963758389261745, Aging Rate = 0.6872217275155833, Precision = 0.6977648202137998\n",
      "Validation: Test Loss = 0.4779897163408947\n",
      "Recall = 0.9574944071588367, Aging Rate = 0.6734194122885129, precision = 0.7074380165289256\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.4771626782491604\n",
      "Epoch 42: Train Loss = 0.4760694806053505\n",
      "Epoch 43: Train Loss = 0.47483062632680473\n",
      "Epoch 44: Train Loss = 0.473838253893814\n",
      "Epoch 45: Train Loss = 0.47219840183071227\n",
      "Recall = 0.9642058165548099, Aging Rate = 0.678539626001781, Precision = 0.7070209973753281\n",
      "Validation: Test Loss = 0.4712917647612275\n",
      "Recall = 0.963758389261745, Aging Rate = 0.6783170080142475, precision = 0.7069248441089596\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.47160073836149113\n",
      "Epoch 47: Train Loss = 0.47019607640756417\n",
      "Epoch 48: Train Loss = 0.46930934705059124\n",
      "Epoch 49: Train Loss = 0.46828328169888933\n",
      "Epoch 50: Train Loss = 0.46692348097120134\n",
      "Recall = 0.96331096196868, Aging Rate = 0.6685218165627783, Precision = 0.716949716949717\n",
      "Validation: Test Loss = 0.466347656102448\n",
      "Recall = 0.96331096196868, Aging Rate = 0.6665182546749777, precision = 0.7191048764195057\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.46609793060715454\n",
      "Epoch 52: Train Loss = 0.4653684722719082\n",
      "Epoch 53: Train Loss = 0.4643664386584519\n",
      "Epoch 54: Train Loss = 0.46297852568932124\n",
      "Epoch 55: Train Loss = 0.4625176835145042\n",
      "Recall = 0.9619686800894854, Aging Rate = 0.6600623330365093, Precision = 0.7251264755480608\n",
      "Validation: Test Loss = 0.4618072515604014\n",
      "Recall = 0.9624161073825503, Aging Rate = 0.6607301869991096, precision = 0.7247304582210242\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.46172911531141797\n",
      "Epoch 57: Train Loss = 0.4608367485651881\n",
      "Epoch 58: Train Loss = 0.4599809182985277\n",
      "Epoch 59: Train Loss = 0.4587050281989182\n",
      "Epoch 60: Train Loss = 0.4578823245006911\n",
      "Recall = 0.96331096196868, Aging Rate = 0.6567230632235085, Precision = 0.7298305084745763\n",
      "Validation: Test Loss = 0.4574815358100782\n",
      "Recall = 0.9624161073825503, Aging Rate = 0.6509349955476402, precision = 0.7356361149110807\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.457066118796808\n",
      "Epoch 62: Train Loss = 0.4567077819884938\n",
      "Epoch 63: Train Loss = 0.45583661477797177\n",
      "Epoch 64: Train Loss = 0.4548292382689108\n",
      "Epoch 65: Train Loss = 0.4540415214887179\n",
      "Recall = 0.9659955257270694, Aging Rate = 0.652493321460374, Precision = 0.7366086659843057\n",
      "Validation: Test Loss = 0.4536040419196828\n",
      "Recall = 0.9642058165548099, Aging Rate = 0.6455921638468388, precision = 0.743103448275862\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.4534080104167612\n",
      "Epoch 67: Train Loss = 0.4526421855786817\n",
      "Epoch 68: Train Loss = 0.4517854953173113\n",
      "Epoch 69: Train Loss = 0.45100330661580906\n",
      "Epoch 70: Train Loss = 0.450356867731626\n",
      "Recall = 0.96331096196868, Aging Rate = 0.641139804096171, Precision = 0.7475694444444444\n",
      "Validation: Test Loss = 0.44936799546595885\n",
      "Recall = 0.9668903803131991, Aging Rate = 0.646260017809439, precision = 0.7444023424044093\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.44962830253721664\n",
      "Epoch 72: Train Loss = 0.4487425747293505\n",
      "Epoch 73: Train Loss = 0.44797981148942284\n",
      "Epoch 74: Train Loss = 0.44706825252526167\n",
      "Epoch 75: Train Loss = 0.4467397913376561\n",
      "Recall = 0.9677852348993289, Aging Rate = 0.6375779162956366, Precision = 0.7552374301675978\n",
      "Validation: Test Loss = 0.4457052933352395\n",
      "Recall = 0.9691275167785235, Aging Rate = 0.6406945681211041, precision = 0.752605976372481\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.44577072562114106\n",
      "Epoch 77: Train Loss = 0.444850634753651\n",
      "Epoch 78: Train Loss = 0.4444384655566058\n",
      "Epoch 79: Train Loss = 0.44346080708482494\n",
      "Epoch 80: Train Loss = 0.4428472865243523\n",
      "Recall = 0.9651006711409396, Aging Rate = 0.628673196794301, Precision = 0.7638101983002833\n",
      "Validation: Test Loss = 0.44206211981981436\n",
      "Recall = 0.9677852348993289, Aging Rate = 0.6320124666073018, precision = 0.7618879887284254\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.4421046041317105\n",
      "Epoch 82: Train Loss = 0.4411095785224215\n",
      "Epoch 83: Train Loss = 0.4406664065505815\n",
      "Epoch 84: Train Loss = 0.43971606008940683\n",
      "Epoch 85: Train Loss = 0.438973357793166\n",
      "Recall = 0.9704697986577181, Aging Rate = 0.6313446126447017, Precision = 0.7648095909732017\n",
      "Validation: Test Loss = 0.43822296156165647\n",
      "Recall = 0.9677852348993289, Aging Rate = 0.6264470169189671, precision = 0.7686567164179104\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.4384870226374717\n",
      "Epoch 87: Train Loss = 0.43749873423724966\n",
      "Epoch 88: Train Loss = 0.43713598028953227\n",
      "Epoch 89: Train Loss = 0.43626326922317327\n",
      "Epoch 90: Train Loss = 0.435686764129017\n",
      "Recall = 0.9700223713646532, Aging Rate = 0.6231077471059662, Precision = 0.7745623436941765\n",
      "Validation: Test Loss = 0.43487647998470974\n",
      "Recall = 0.9749440715883669, Aging Rate = 0.6271148708815673, precision = 0.7735179268725595\n",
      "Model in epoch 90 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss = 0.4348950540233911\n",
      "Epoch 92: Train Loss = 0.433993803637429\n",
      "Epoch 93: Train Loss = 0.43352203250249793\n",
      "Epoch 94: Train Loss = 0.43266155999999756\n",
      "Epoch 95: Train Loss = 0.43164140346956803\n",
      "Recall = 0.9740492170022371, Aging Rate = 0.6197684772929652, Precision = 0.7819683908045977\n",
      "Validation: Test Loss = 0.43134033289851614\n",
      "Recall = 0.974496644295302, Aging Rate = 0.6195458593054319, precision = 0.782608695652174\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.43131262844201185\n",
      "Epoch 97: Train Loss = 0.4303451474765954\n",
      "Epoch 98: Train Loss = 0.42994506950369926\n",
      "Epoch 99: Train Loss = 0.42899871005188217\n",
      "Epoch 100: Train Loss = 0.4285442621073124\n",
      "Recall = 0.974496644295302, Aging Rate = 0.6153161175422974, Precision = 0.7879884225759769\n",
      "Validation: Test Loss = 0.4276965745091969\n",
      "Recall = 0.9753914988814317, Aging Rate = 0.6164292074799644, precision = 0.7872878295413507\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4277958906291323\n",
      "Epoch 102: Train Loss = 0.4270602232732629\n",
      "Epoch 103: Train Loss = 0.42632285354929317\n",
      "Epoch 104: Train Loss = 0.4255587905778061\n",
      "Epoch 105: Train Loss = 0.42516866273255827\n",
      "Recall = 0.9740492170022371, Aging Rate = 0.6090828138913624, Precision = 0.7956871345029239\n",
      "Validation: Test Loss = 0.42461050801370576\n",
      "Recall = 0.9740492170022371, Aging Rate = 0.6066340160284951, precision = 0.7988990825688074\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.42437244162215576\n",
      "Epoch 107: Train Loss = 0.42381426352320883\n",
      "Epoch 108: Train Loss = 0.4232690535744491\n",
      "Epoch 109: Train Loss = 0.4227096886106187\n",
      "Epoch 110: Train Loss = 0.42164421004582386\n",
      "Recall = 0.9762863534675615, Aging Rate = 0.6030721282279609, Precision = 0.8054632705795497\n",
      "Validation: Test Loss = 0.42117629187614497\n",
      "Recall = 0.9758389261744966, Aging Rate = 0.6024042742653607, precision = 0.8059866962305987\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.4213223745805922\n",
      "Epoch 112: Train Loss = 0.4206594233786752\n",
      "Epoch 113: Train Loss = 0.4198112254522787\n",
      "Epoch 114: Train Loss = 0.4192242839457409\n",
      "Epoch 115: Train Loss = 0.4186203182009003\n",
      "Recall = 0.9767337807606264, Aging Rate = 0.5999554764024934, Precision = 0.8100185528756957\n",
      "Validation: Test Loss = 0.4179870030068969\n",
      "Recall = 0.9762863534675615, Aging Rate = 0.5988423864648263, precision = 0.8111524163568773\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.41827193839361065\n",
      "Epoch 117: Train Loss = 0.41731627729994214\n",
      "Epoch 118: Train Loss = 0.4166866564527751\n",
      "Epoch 119: Train Loss = 0.41612821541401496\n",
      "Epoch 120: Train Loss = 0.41559450035103707\n",
      "Recall = 0.9771812080536912, Aging Rate = 0.597506678539626, Precision = 0.8137108792846498\n",
      "Validation: Test Loss = 0.41485068746050446\n",
      "Recall = 0.9762863534675615, Aging Rate = 0.5941674087266251, precision = 0.8175346571749719\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.41501320018793786\n",
      "Epoch 122: Train Loss = 0.4145083636815062\n",
      "Epoch 123: Train Loss = 0.4137963445613348\n",
      "Epoch 124: Train Loss = 0.41317721649354205\n",
      "Epoch 125: Train Loss = 0.41279031040407355\n",
      "Recall = 0.9762863534675615, Aging Rate = 0.5919412288512912, Precision = 0.8206092515983452\n",
      "Validation: Test Loss = 0.412002067691178\n",
      "Recall = 0.978076062639821, Aging Rate = 0.5937221727515584, precision = 0.8196475440569929\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.41197316145535995\n",
      "Epoch 127: Train Loss = 0.41158735433648125\n",
      "Epoch 128: Train Loss = 0.4107434194690929\n",
      "Epoch 129: Train Loss = 0.41018292047780003\n",
      "Epoch 130: Train Loss = 0.40977343104826164\n",
      "Recall = 0.9767337807606264, Aging Rate = 0.585040071237756, Precision = 0.8306697108066972\n",
      "Validation: Test Loss = 0.40914099232914825\n",
      "Recall = 0.9776286353467561, Aging Rate = 0.5859305431878896, precision = 0.8301671732522796\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.409248830002541\n",
      "Epoch 132: Train Loss = 0.40873060567190045\n",
      "Epoch 133: Train Loss = 0.40816149216407344\n",
      "Epoch 134: Train Loss = 0.4076272916125062\n",
      "Epoch 135: Train Loss = 0.40692049063218033\n",
      "Recall = 0.9767337807606264, Aging Rate = 0.582146037399822, Precision = 0.8347992351816443\n",
      "Validation: Test Loss = 0.40648954923622116\n",
      "Recall = 0.9794183445190157, Aging Rate = 0.5852626892252895, precision = 0.8326359832635983\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.40636746475235863\n",
      "Epoch 137: Train Loss = 0.405968118754223\n",
      "Epoch 138: Train Loss = 0.40549688321187466\n",
      "Epoch 139: Train Loss = 0.4050669906559738\n",
      "Epoch 140: Train Loss = 0.4041407783171585\n",
      "Recall = 0.978523489932886, Aging Rate = 0.5799198575244879, Precision = 0.8395393474088292\n",
      "Validation: Test Loss = 0.4038976727854643\n",
      "Recall = 0.9758389261744966, Aging Rate = 0.5754674977738201, precision = 0.8437137330754352\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.4038871066016166\n",
      "Epoch 142: Train Loss = 0.40321134130336933\n",
      "Epoch 143: Train Loss = 0.4027221906376225\n",
      "Epoch 144: Train Loss = 0.4022799261106727\n",
      "Epoch 145: Train Loss = 0.4017133446060327\n",
      "Recall = 0.9821029082774049, Aging Rate = 0.5785841495992876, Precision = 0.8445555983070412\n",
      "Validation: Test Loss = 0.40134293822868533\n",
      "Recall = 0.978076062639821, Aging Rate = 0.5730186999109528, precision = 0.8492618492618492\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.40126239778415923\n",
      "Epoch 147: Train Loss = 0.40084234339152713\n",
      "Epoch 148: Train Loss = 0.4003736638428373\n",
      "Epoch 149: Train Loss = 0.399800828096597\n",
      "Epoch 150: Train Loss = 0.39946337231332874\n",
      "Recall = 0.9816554809843401, Aging Rate = 0.5741317898486198, Precision = 0.8507173322993409\n",
      "Validation: Test Loss = 0.3987384633835362\n",
      "Recall = 0.9816554809843401, Aging Rate = 0.5730186999109528, precision = 0.8523698523698524\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.41009873513863465\n",
      "Recall = 0.968421052631579, Aging Rate = 0.5861148197596796, precision = 0.8382687927107062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed42c57b80e4d08ac840d3eed4a1aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.7024943866266912\n",
      "Epoch 2: Train Loss = 0.6781946677558675\n",
      "Epoch 3: Train Loss = 0.6559206090754732\n",
      "Epoch 4: Train Loss = 0.6356035729974694\n",
      "Epoch 5: Train Loss = 0.619000422561795\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5046749777382012\n",
      "Validation: Test Loss = 0.6130999793246296\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5046749777382012\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6088447212005235\n",
      "Epoch 7: Train Loss = 0.602587153469996\n",
      "Epoch 8: Train Loss = 0.5985842911876532\n",
      "Epoch 9: Train Loss = 0.5945047438410065\n",
      "Epoch 10: Train Loss = 0.5903576624680075\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5046749777382012\n",
      "Validation: Test Loss = 0.5877702145415126\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5046749777382012\n",
      "\n",
      "Epoch 11: Train Loss = 0.5854565184878538\n",
      "Epoch 12: Train Loss = 0.5795157008264495\n",
      "Epoch 13: Train Loss = 0.5729488984135146\n",
      "Epoch 14: Train Loss = 0.5654603809197886\n",
      "Epoch 15: Train Loss = 0.5574016196857259\n",
      "Recall = 0.9964711071901191, Aging Rate = 0.9343276936776491, Precision = 0.538241601143674\n",
      "Validation: Test Loss = 0.5529439767237125\n",
      "Recall = 0.9955888839876489, Aging Rate = 0.9185218165627783, precision = 0.5470189045079981\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5499308092829805\n",
      "Epoch 17: Train Loss = 0.5426033685914343\n",
      "Epoch 18: Train Loss = 0.5357542199846473\n",
      "Epoch 19: Train Loss = 0.529845360921943\n",
      "Epoch 20: Train Loss = 0.5245066839780324\n",
      "Recall = 0.9823555359505955, Aging Rate = 0.794746215494212, Precision = 0.6238095238095238\n",
      "Validation: Test Loss = 0.5216350052522318\n",
      "Recall = 0.9814733127481253, Aging Rate = 0.7887355298308103, precision = 0.6279988710132656\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5197506373945561\n",
      "Epoch 22: Train Loss = 0.5152883476852202\n",
      "Epoch 23: Train Loss = 0.5110800498209144\n",
      "Epoch 24: Train Loss = 0.5076007858675076\n",
      "Epoch 25: Train Loss = 0.5047838418269306\n",
      "Recall = 0.9735333039258932, Aging Rate = 0.7410952804986642, Precision = 0.6629618504055271\n",
      "Validation: Test Loss = 0.5027517202913178\n",
      "Recall = 0.9744155271283634, Aging Rate = 0.7408726625111309, precision = 0.6637620192307693\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5019119748661588\n",
      "Epoch 27: Train Loss = 0.49890482935221836\n",
      "Epoch 28: Train Loss = 0.49659105283494304\n",
      "Epoch 29: Train Loss = 0.4942036335848318\n",
      "Epoch 30: Train Loss = 0.4920681402127019\n",
      "Recall = 0.9677988531098368, Aging Rate = 0.715939447907391, Precision = 0.6822139303482587\n",
      "Validation: Test Loss = 0.4915689350979093\n",
      "Recall = 0.9704455227172475, Aging Rate = 0.7190560997328584, precision = 0.6811145510835913\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.49023133293391336\n",
      "Epoch 32: Train Loss = 0.4883007254859537\n",
      "Epoch 33: Train Loss = 0.48689304683639445\n",
      "Epoch 34: Train Loss = 0.48536131502471647\n",
      "Epoch 35: Train Loss = 0.48374696570852455\n",
      "Recall = 0.9651521835024262, Aging Rate = 0.7050311665182547, Precision = 0.6908746447742343\n",
      "Validation: Test Loss = 0.4827617263539179\n",
      "Recall = 0.9673577415086017, Aging Rate = 0.7094835262689225, precision = 0.6881079385001568\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4820919674311168\n",
      "Epoch 37: Train Loss = 0.4807784761155384\n",
      "Epoch 38: Train Loss = 0.4793861082697806\n",
      "Epoch 39: Train Loss = 0.47802895811765406\n",
      "Epoch 40: Train Loss = 0.47696472485470964\n",
      "Recall = 0.9642699602999559, Aging Rate = 0.6932324131789849, Precision = 0.7019910083493899\n",
      "Validation: Test Loss = 0.4764369328639811\n",
      "Recall = 0.9655932951036612, Aging Rate = 0.6970169189670525, precision = 0.699137655701054\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.47591565625007304\n",
      "Epoch 42: Train Loss = 0.4750363633554107\n",
      "Epoch 43: Train Loss = 0.47395902179440325\n",
      "Epoch 44: Train Loss = 0.47288423107865657\n",
      "Epoch 45: Train Loss = 0.4716757483919709\n",
      "Recall = 0.964711071901191, Aging Rate = 0.6890026714158504, Precision = 0.7066235864297253\n",
      "Validation: Test Loss = 0.47121917930757584\n",
      "Recall = 0.9638288486987208, Aging Rate = 0.6856634016028496, precision = 0.7094155844155844\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.47085345295636866\n",
      "Epoch 47: Train Loss = 0.47011281620256623\n",
      "Epoch 48: Train Loss = 0.4692507933423865\n",
      "Epoch 49: Train Loss = 0.4682118819424433\n",
      "Epoch 50: Train Loss = 0.46750059185129983\n",
      "Recall = 0.9620644022937803, Aging Rate = 0.6789848619768477, Precision = 0.7150819672131148\n",
      "Validation: Test Loss = 0.4666820265624742\n",
      "Recall = 0.9660344067048964, Aging Rate = 0.6843276936776491, precision = 0.7124268054651919\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.4664854553800975\n",
      "Epoch 52: Train Loss = 0.46581613409763345\n",
      "Epoch 53: Train Loss = 0.46478057648067694\n",
      "Epoch 54: Train Loss = 0.46403490095826416\n",
      "Epoch 55: Train Loss = 0.4635939298998747\n",
      "Recall = 0.9669166299073666, Aging Rate = 0.6816562778272485, Precision = 0.7158719790986283\n",
      "Validation: Test Loss = 0.46281668493393163\n",
      "Recall = 0.9642699602999559, Aging Rate = 0.6754229741763135, precision = 0.7205009887936717\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.46267285624573723\n",
      "Epoch 57: Train Loss = 0.4620772847509342\n",
      "Epoch 58: Train Loss = 0.46121562028822893\n",
      "Epoch 59: Train Loss = 0.4603609712869062\n",
      "Epoch 60: Train Loss = 0.45990097496197463\n",
      "Recall = 0.9620644022937803, Aging Rate = 0.6703027604630454, Precision = 0.7243440717369645\n",
      "Validation: Test Loss = 0.45935531635955307\n",
      "Recall = 0.9682399647110719, Aging Rate = 0.6776491540516474, precision = 0.7210906701708278\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.4593134517926567\n",
      "Epoch 62: Train Loss = 0.45841049651747184\n",
      "Epoch 63: Train Loss = 0.4579889998835108\n",
      "Epoch 64: Train Loss = 0.4571831091004403\n",
      "Epoch 65: Train Loss = 0.4569133558341253\n",
      "Recall = 0.9633877370974857, Aging Rate = 0.6654051647373108, Precision = 0.7306791569086651\n",
      "Validation: Test Loss = 0.4558355362682818\n",
      "Recall = 0.9655932951036612, Aging Rate = 0.6680765805877115, precision = 0.7294235254915028\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.45594435675486006\n",
      "Epoch 67: Train Loss = 0.4549740442503592\n",
      "Epoch 68: Train Loss = 0.45479166356453593\n",
      "Epoch 69: Train Loss = 0.45390020455511576\n",
      "Epoch 70: Train Loss = 0.4532057124646986\n",
      "Recall = 0.964711071901191, Aging Rate = 0.6627337488869101, Precision = 0.7346321800470272\n",
      "Validation: Test Loss = 0.45289578330591124\n",
      "Recall = 0.9669166299073666, Aging Rate = 0.6647373107747105, precision = 0.7340924313462827\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.452636495007008\n",
      "Epoch 72: Train Loss = 0.4522307066649915\n",
      "Epoch 73: Train Loss = 0.4516368063825215\n",
      "Epoch 74: Train Loss = 0.4510226850252754\n",
      "Epoch 75: Train Loss = 0.4504208791128568\n",
      "Recall = 0.9677988531098368, Aging Rate = 0.6611754229741763, Precision = 0.7387205387205387\n",
      "Validation: Test Loss = 0.45017454606660856\n",
      "Recall = 0.9691221879135421, Aging Rate = 0.6640694568121104, precision = 0.736506872276232\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.44980991422758504\n",
      "Epoch 77: Train Loss = 0.4490258968099355\n",
      "Epoch 78: Train Loss = 0.4488034244636712\n",
      "Epoch 79: Train Loss = 0.447906204529989\n",
      "Epoch 80: Train Loss = 0.4476164608583425\n",
      "Recall = 0.968681076312307, Aging Rate = 0.6580587711487088, Precision = 0.7428958051420839\n",
      "Validation: Test Loss = 0.44695130391303495\n",
      "Recall = 0.9677988531098368, Aging Rate = 0.6547195013357079, precision = 0.7460047602856171\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.4470783844921914\n",
      "Epoch 82: Train Loss = 0.4462617574954606\n",
      "Epoch 83: Train Loss = 0.44543614412988813\n",
      "Epoch 84: Train Loss = 0.44510122448975553\n",
      "Epoch 85: Train Loss = 0.4450831933947195\n",
      "Recall = 0.9722099691221879, Aging Rate = 0.6565004452359751, Precision = 0.747371990505256\n",
      "Validation: Test Loss = 0.4438573290541675\n",
      "Recall = 0.9704455227172475, Aging Rate = 0.6511576135351737, precision = 0.7521367521367521\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.44402234220441067\n",
      "Epoch 87: Train Loss = 0.44337524912873233\n",
      "Epoch 88: Train Loss = 0.4428270974545636\n",
      "Epoch 89: Train Loss = 0.4422121314151098\n",
      "Epoch 90: Train Loss = 0.44203214489341525\n",
      "Recall = 0.9704455227172475, Aging Rate = 0.6482635796972396, Precision = 0.7554945054945055\n",
      "Validation: Test Loss = 0.4414308193211882\n",
      "Recall = 0.9717688575209528, Aging Rate = 0.6516028495102404, precision = 0.7526477622138709\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.4415769405855415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.44063536768392697\n",
      "Epoch 93: Train Loss = 0.44005811302022857\n",
      "Epoch 94: Train Loss = 0.4394932973629008\n",
      "Epoch 95: Train Loss = 0.4389997558039634\n",
      "Recall = 0.9691221879135421, Aging Rate = 0.6438112199465716, Precision = 0.7596818810511756\n",
      "Validation: Test Loss = 0.43853934551070867\n",
      "Recall = 0.9713277459197177, Aging Rate = 0.6453695458593054, precision = 0.7595722662987237\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.4386438023587581\n",
      "Epoch 97: Train Loss = 0.43831421054057107\n",
      "Epoch 98: Train Loss = 0.43735554718801406\n",
      "Epoch 99: Train Loss = 0.43689739409558814\n",
      "Epoch 100: Train Loss = 0.43643535045991494\n",
      "Recall = 0.9695632995147773, Aging Rate = 0.6364648263579697, Precision = 0.7688002798181183\n",
      "Validation: Test Loss = 0.43604543839621945\n",
      "Recall = 0.9761799735333039, Aging Rate = 0.649154051647373, precision = 0.7589163237311386\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4360342303345698\n",
      "Epoch 102: Train Loss = 0.4353675544792695\n",
      "Epoch 103: Train Loss = 0.434831096408416\n",
      "Epoch 104: Train Loss = 0.43429212995331307\n",
      "Epoch 105: Train Loss = 0.4337739253405047\n",
      "Recall = 0.9748566387295986, Aging Rate = 0.6391362422083704, Precision = 0.7697666318355973\n",
      "Validation: Test Loss = 0.4330926520263947\n",
      "Recall = 0.9744155271283634, Aging Rate = 0.6389136242208371, precision = 0.7696864111498258\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.4329729883509455\n",
      "Epoch 107: Train Loss = 0.43253058351689117\n",
      "Epoch 108: Train Loss = 0.4321400495851454\n",
      "Epoch 109: Train Loss = 0.43150021078750056\n",
      "Epoch 110: Train Loss = 0.4310093922266871\n",
      "Recall = 0.9739744155271284, Aging Rate = 0.6322350845948352, Precision = 0.7774647887323943\n",
      "Validation: Test Loss = 0.4304614772195808\n",
      "Recall = 0.9748566387295986, Aging Rate = 0.6320124666073018, precision = 0.7784431137724551\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.43021121576017074\n",
      "Epoch 112: Train Loss = 0.42993246574860433\n",
      "Epoch 113: Train Loss = 0.4293468870813042\n",
      "Epoch 114: Train Loss = 0.4290483144650158\n",
      "Epoch 115: Train Loss = 0.4285861500543032\n",
      "Recall = 0.9744155271283634, Aging Rate = 0.6262243989314337, Precision = 0.7852826164237469\n",
      "Validation: Test Loss = 0.42785666008560336\n",
      "Recall = 0.9779444199382443, Aging Rate = 0.6315672306322351, precision = 0.7814592879802609\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.4278220250632013\n",
      "Epoch 117: Train Loss = 0.42730545474818427\n",
      "Epoch 118: Train Loss = 0.426790689158843\n",
      "Epoch 119: Train Loss = 0.4263565703788615\n",
      "Epoch 120: Train Loss = 0.42577817415087116\n",
      "Recall = 0.9797088663431849, Aging Rate = 0.6268922528940338, Precision = 0.7887073863636364\n",
      "Validation: Test Loss = 0.42529506425398966\n",
      "Recall = 0.9744155271283634, Aging Rate = 0.6191006233303651, precision = 0.7943185904350953\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.42530598196193564\n",
      "Epoch 122: Train Loss = 0.42473721183100027\n",
      "Epoch 123: Train Loss = 0.42398825853081124\n",
      "Epoch 124: Train Loss = 0.42373190394280114\n",
      "Epoch 125: Train Loss = 0.42308696255233813\n",
      "Recall = 0.9779444199382443, Aging Rate = 0.6208815672306323, Precision = 0.794908569379706\n",
      "Validation: Test Loss = 0.422460595704781\n",
      "Recall = 0.9735333039258932, Aging Rate = 0.6139804096170971, precision = 0.8002175489485134\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.4224112582121804\n",
      "Epoch 127: Train Loss = 0.4217691821703822\n",
      "Epoch 128: Train Loss = 0.4216857458052631\n",
      "Epoch 129: Train Loss = 0.42099458387889505\n",
      "Epoch 130: Train Loss = 0.42042881223843337\n",
      "Recall = 0.976621085134539, Aging Rate = 0.6144256455921638, Precision = 0.8021739130434783\n",
      "Validation: Test Loss = 0.4199691368125956\n",
      "Recall = 0.9801499779444199, Aging Rate = 0.6184327693677649, precision = 0.7998560115190785\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.4197630225975176\n",
      "Epoch 132: Train Loss = 0.41943726301830153\n",
      "Epoch 133: Train Loss = 0.4189724288184199\n",
      "Epoch 134: Train Loss = 0.41830901736567727\n",
      "Epoch 135: Train Loss = 0.4178932260585914\n",
      "Recall = 0.9792677547419497, Aging Rate = 0.6133125556544969, Precision = 0.8058076225045372\n",
      "Validation: Test Loss = 0.41722940820089327\n",
      "Recall = 0.9805910895456551, Aging Rate = 0.6146482635796973, precision = 0.8051430641072075\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.41747685255796274\n",
      "Epoch 137: Train Loss = 0.4167252396592474\n",
      "Epoch 138: Train Loss = 0.41661062608419097\n",
      "Epoch 139: Train Loss = 0.41566850344516926\n",
      "Epoch 140: Train Loss = 0.41519300343729193\n",
      "Recall = 0.9792677547419497, Aging Rate = 0.6090828138913624, Precision = 0.8114035087719298\n",
      "Validation: Test Loss = 0.41480538302092074\n",
      "Recall = 0.9814733127481253, Aging Rate = 0.6121994657168299, precision = 0.8090909090909091\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.4145707250543715\n",
      "Epoch 142: Train Loss = 0.41410827758794155\n",
      "Epoch 143: Train Loss = 0.4134261698884191\n",
      "Epoch 144: Train Loss = 0.41326420140499953\n",
      "Epoch 145: Train Loss = 0.41251547129050176\n",
      "Recall = 0.9805910895456551, Aging Rate = 0.6084149599287623, Precision = 0.8133918770581778\n",
      "Validation: Test Loss = 0.41191691200009223\n",
      "Recall = 0.9788266431407145, Aging Rate = 0.6017364203027604, precision = 0.8209396966333703\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.4119988565771896\n",
      "Epoch 147: Train Loss = 0.4114392997161257\n",
      "Epoch 148: Train Loss = 0.41118371539740084\n",
      "Epoch 149: Train Loss = 0.41050513989672743\n",
      "Epoch 150: Train Loss = 0.4101902069996641\n",
      "Recall = 0.9788266431407145, Aging Rate = 0.5995102404274265, Precision = 0.8239881173412551\n",
      "Validation: Test Loss = 0.40935961648702407\n",
      "Recall = 0.9810322011468902, Aging Rate = 0.6012911843276937, precision = 0.8233987412069604\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.4275152513754861\n",
      "Recall = 0.9615384615384616, Aging Rate = 0.5867823765020027, precision = 0.7963594994311718\n",
      "\u001b[32m[I 2022-05-26 16:24:06,903]\u001b[0m Trial 4 finished with value: 0.8855537897160835 and parameters: {'batch_size': 96, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'bad_weight': 0.7}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583b5bede9b84ebca47857e6359e0649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6869553726163169\n",
      "Epoch 2: Train Loss = 0.6787519933173196\n",
      "Epoch 3: Train Loss = 0.6713459888209431\n",
      "Epoch 4: Train Loss = 0.6649514111895064\n",
      "Epoch 5: Train Loss = 0.6598106830754878\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.5060106856634016\n",
      "Validation: Test Loss = 0.6570724665534783\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5060106856634016\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.654798172142287\n",
      "Epoch 7: Train Loss = 0.6501216299811964\n",
      "Epoch 8: Train Loss = 0.6453036097469652\n",
      "Epoch 9: Train Loss = 0.6400730023931841\n",
      "Epoch 10: Train Loss = 0.6343695175828195\n",
      "Recall = 1.0, Aging Rate = 0.9908726625111309, Precision = 0.510671759155246\n",
      "Validation: Test Loss = 0.6313539121900408\n",
      "Recall = 0.9991201055873296, Aging Rate = 0.9819679430097952, precision = 0.5148492405350261\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.6282894362961726\n",
      "Epoch 12: Train Loss = 0.6218262575400906\n",
      "Epoch 13: Train Loss = 0.6152690987659158\n",
      "Epoch 14: Train Loss = 0.6084156554932055\n",
      "Epoch 15: Train Loss = 0.6017165442502403\n",
      "Recall = 0.9678838539375275, Aging Rate = 0.7954140694568121, Precision = 0.6157290792051497\n",
      "Validation: Test Loss = 0.5980882520985624\n",
      "Recall = 0.968763748350198, Aging Rate = 0.796527159394479, precision = 0.6154276131917272\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5952341623955928\n",
      "Epoch 17: Train Loss = 0.5889896505768556\n",
      "Epoch 18: Train Loss = 0.5834216171359761\n",
      "Epoch 19: Train Loss = 0.5781080953690587\n",
      "Epoch 20: Train Loss = 0.573294462365756\n",
      "Recall = 0.9441267047954246, Aging Rate = 0.709706144256456, Precision = 0.6731493099121706\n",
      "Validation: Test Loss = 0.5706088284456825\n",
      "Recall = 0.9388473383194017, Aging Rate = 0.6985752448797863, precision = 0.680050987890376\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5690595237994343\n",
      "Epoch 22: Train Loss = 0.564908148981266\n",
      "Epoch 23: Train Loss = 0.5613553989496172\n",
      "Epoch 24: Train Loss = 0.5582076123221474\n",
      "Epoch 25: Train Loss = 0.555116283925431\n",
      "Recall = 0.9234491860976682, Aging Rate = 0.6687444345503116, Precision = 0.6987350199733688\n",
      "Validation: Test Loss = 0.5533549304212296\n",
      "Recall = 0.9225692916849978, Aging Rate = 0.6656277827248441, precision = 0.7013377926421405\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5525138360628143\n",
      "Epoch 27: Train Loss = 0.5499197975504324\n",
      "Epoch 28: Train Loss = 0.5476746562327642\n",
      "Epoch 29: Train Loss = 0.5455179333368371\n",
      "Epoch 30: Train Loss = 0.5435792598889964\n",
      "Recall = 0.9212494500659921, Aging Rate = 0.6613980409617097, Precision = 0.7048131942107034\n",
      "Validation: Test Loss = 0.5424438305763082\n",
      "Recall = 0.913770347558293, Aging Rate = 0.6527159394479074, precision = 0.7083901773533424\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.5418865134009058\n",
      "Epoch 32: Train Loss = 0.5402695866747829\n",
      "Epoch 33: Train Loss = 0.5388878378502938\n",
      "Epoch 34: Train Loss = 0.5373151702532679\n",
      "Epoch 35: Train Loss = 0.5360978585157878\n",
      "Recall = 0.9106907171139463, Aging Rate = 0.6420302760463046, Precision = 0.717753120665742\n",
      "Validation: Test Loss = 0.5350853407478502\n",
      "Recall = 0.9142102947646282, Aging Rate = 0.6471504897595726, precision = 0.7148262813897489\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.5348255403956874\n",
      "Epoch 37: Train Loss = 0.5335973901400477\n",
      "Epoch 38: Train Loss = 0.5326000704684644\n",
      "Epoch 39: Train Loss = 0.5315486983539585\n",
      "Epoch 40: Train Loss = 0.530756193829347\n",
      "Recall = 0.9146502419709635, Aging Rate = 0.6431433659839715, Precision = 0.719626168224299\n",
      "Validation: Test Loss = 0.5297916777836671\n",
      "Recall = 0.9115706115266168, Aging Rate = 0.6364648263579697, precision = 0.7247289261979714\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.5299469210797936\n",
      "Epoch 42: Train Loss = 0.5287872851691072\n",
      "Epoch 43: Train Loss = 0.5280271679401822\n",
      "Epoch 44: Train Loss = 0.5269851390527809\n",
      "Epoch 45: Train Loss = 0.5264743798456336\n",
      "Recall = 0.913770347558293, Aging Rate = 0.6375779162956366, Precision = 0.7252094972067039\n",
      "Validation: Test Loss = 0.5258988514078162\n",
      "Recall = 0.9111306643202816, Aging Rate = 0.6333481745325023, precision = 0.7279437609841828\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.5257915514235186\n",
      "Epoch 47: Train Loss = 0.5252984705665551\n",
      "Epoch 48: Train Loss = 0.5243375552411814\n",
      "Epoch 49: Train Loss = 0.5235978272591758\n",
      "Epoch 50: Train Loss = 0.5230159580123711\n",
      "Recall = 0.9115706115266168, Aging Rate = 0.6326803205699021, Precision = 0.729064039408867\n",
      "Validation: Test Loss = 0.5223363164483917\n",
      "Recall = 0.9168499780026397, Aging Rate = 0.6378005342831701, precision = 0.7273996509598604\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.5224145698632285\n",
      "Epoch 52: Train Loss = 0.521835618059442\n",
      "Epoch 53: Train Loss = 0.5217537828990634\n",
      "Epoch 54: Train Loss = 0.5208695674197332\n",
      "Epoch 55: Train Loss = 0.5202850609725858\n",
      "Recall = 0.9133304003519578, Aging Rate = 0.6331255565449688, Precision = 0.729957805907173\n",
      "Validation: Test Loss = 0.5198116051970267\n",
      "Recall = 0.9115706115266168, Aging Rate = 0.6280053428317008, precision = 0.7344913151364765\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.5197119578546642\n",
      "Epoch 57: Train Loss = 0.5192844668139546\n",
      "Epoch 58: Train Loss = 0.5186933225430449\n",
      "Epoch 59: Train Loss = 0.518081359054399\n",
      "Epoch 60: Train Loss = 0.5179228778407707\n",
      "Recall = 0.9128904531456226, Aging Rate = 0.628673196794301, Precision = 0.7347733711048159\n",
      "Validation: Test Loss = 0.5172081346078738\n",
      "Recall = 0.9208095028596568, Aging Rate = 0.6398040961709706, precision = 0.7282533054975644\n",
      "\n",
      "Epoch 61: Train Loss = 0.5173919793759089\n",
      "Epoch 62: Train Loss = 0.5169644730492352\n",
      "Epoch 63: Train Loss = 0.5165440967225222\n",
      "Epoch 64: Train Loss = 0.5159894518839495\n",
      "Epoch 65: Train Loss = 0.5155126796114795\n",
      "Recall = 0.923009238891333, Aging Rate = 0.6395814781834372, Precision = 0.7302471284371737\n",
      "Validation: Test Loss = 0.5150640801670078\n",
      "Recall = 0.9124505059392873, Aging Rate = 0.6257791629563668, precision = 0.7378157239416577\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.5153546940505451\n",
      "Epoch 67: Train Loss = 0.5149623616826609\n",
      "Epoch 68: Train Loss = 0.5144951099916324\n",
      "Epoch 69: Train Loss = 0.513966967352564\n",
      "Epoch 70: Train Loss = 0.5135876542738476\n",
      "Recall = 0.9186097668279807, Aging Rate = 0.6313446126447017, Precision = 0.7362482369534555\n",
      "Validation: Test Loss = 0.5131923306869378\n",
      "Recall = 0.9133304003519578, Aging Rate = 0.6255565449688335, precision = 0.7387900355871886\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.5133082927175643\n",
      "Epoch 72: Train Loss = 0.512954079961522\n",
      "Epoch 73: Train Loss = 0.5125271877006877\n",
      "Epoch 74: Train Loss = 0.512109311504865\n",
      "Epoch 75: Train Loss = 0.511970489235298\n",
      "Recall = 0.912010558732952, Aging Rate = 0.6188780053428317, Precision = 0.74568345323741\n",
      "Validation: Test Loss = 0.5115546219495398\n",
      "Recall = 0.9278486581610207, Aging Rate = 0.6409171861086376, precision = 0.7325460229246266\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.5115271641649737\n",
      "Epoch 77: Train Loss = 0.5112787057858753\n",
      "Epoch 78: Train Loss = 0.5109004078968656\n",
      "Epoch 79: Train Loss = 0.5105630084009757\n",
      "Epoch 80: Train Loss = 0.5103208582420383\n",
      "Recall = 0.9106907171139463, Aging Rate = 0.6137577916295637, Precision = 0.750816104461371\n",
      "Validation: Test Loss = 0.5099513055964018\n",
      "Recall = 0.9225692916849978, Aging Rate = 0.6300089047195013, precision = 0.7409893992932862\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.5099855473502236\n",
      "Epoch 82: Train Loss = 0.5097167192031948\n",
      "Epoch 83: Train Loss = 0.5093370424459157\n",
      "Epoch 84: Train Loss = 0.509202685349771\n",
      "Epoch 85: Train Loss = 0.5088626112878482\n",
      "Recall = 0.9181698196216455, Aging Rate = 0.6208815672306323, Precision = 0.7482968806023664\n",
      "Validation: Test Loss = 0.5084060650527849\n",
      "Recall = 0.9287285525736911, Aging Rate = 0.6366874443455031, precision = 0.7381118881118881\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.5084121506445555\n",
      "Epoch 87: Train Loss = 0.5081486485414174\n",
      "Epoch 88: Train Loss = 0.507771673472142\n",
      "Epoch 89: Train Loss = 0.5075886830196568\n",
      "Epoch 90: Train Loss = 0.5074299432715452\n",
      "Recall = 0.9225692916849978, Aging Rate = 0.6251113089937667, Precision = 0.7467948717948718\n",
      "Validation: Test Loss = 0.5069606538235026\n",
      "Recall = 0.923009238891333, Aging Rate = 0.6262243989314337, precision = 0.7458229648062566\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.5067736243999971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.5069690832896415\n",
      "Epoch 93: Train Loss = 0.5064589224642977\n",
      "Epoch 94: Train Loss = 0.5062254818814839\n",
      "Epoch 95: Train Loss = 0.5058225986687923\n",
      "Recall = 0.9238891333040036, Aging Rate = 0.6246660730186999, Precision = 0.7483962936564504\n",
      "Validation: Test Loss = 0.505362847609278\n",
      "Recall = 0.9164100307963045, Aging Rate = 0.6133125556544969, precision = 0.7560798548094374\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.5056242765800717\n",
      "Epoch 97: Train Loss = 0.505696366062572\n",
      "Epoch 98: Train Loss = 0.5052768607704319\n",
      "Epoch 99: Train Loss = 0.5049256269462599\n",
      "Epoch 100: Train Loss = 0.5046618830276618\n",
      "Recall = 0.9199296084469863, Aging Rate = 0.6166518254674977, Precision = 0.7548736462093862\n",
      "Validation: Test Loss = 0.50421135630442\n",
      "Recall = 0.9269687637483502, Aging Rate = 0.6248886910062333, precision = 0.7506234413965087\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.5042566030766427\n",
      "Epoch 102: Train Loss = 0.5041065746030952\n",
      "Epoch 103: Train Loss = 0.5038570209687455\n",
      "Epoch 104: Train Loss = 0.5040068359695581\n",
      "Epoch 105: Train Loss = 0.5035121972711619\n",
      "Recall = 0.9238891333040036, Aging Rate = 0.6179875333926982, Precision = 0.7564841498559077\n",
      "Validation: Test Loss = 0.5029608331295601\n",
      "Recall = 0.9256489221293445, Aging Rate = 0.6202137132680321, precision = 0.7552045944005743\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.50335549236087\n",
      "Epoch 107: Train Loss = 0.5031543098330391\n",
      "Epoch 108: Train Loss = 0.5027840594999088\n",
      "Epoch 109: Train Loss = 0.5026121787587554\n",
      "Epoch 110: Train Loss = 0.5023927850481239\n",
      "Recall = 0.9265288165420149, Aging Rate = 0.6168744434550312, Precision = 0.7600144352219416\n",
      "Validation: Test Loss = 0.5018173058120035\n",
      "Recall = 0.9309282886053674, Aging Rate = 0.6222172751558326, precision = 0.7570661896243291\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.5021123379760837\n",
      "Epoch 112: Train Loss = 0.5019415981092309\n",
      "Epoch 113: Train Loss = 0.5015596561631451\n",
      "Epoch 114: Train Loss = 0.5016388156428894\n",
      "Epoch 115: Train Loss = 0.5012762248516083\n",
      "Recall = 0.9274087109546855, Aging Rate = 0.615093499554764, Precision = 0.7629388346000724\n",
      "Validation: Test Loss = 0.5007549069443666\n",
      "Recall = 0.9243290805103388, Aging Rate = 0.6086375779162957, precision = 0.768471104608632\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.5009146107166661\n",
      "Epoch 117: Train Loss = 0.5008483365085225\n",
      "Epoch 118: Train Loss = 0.5005297323793358\n",
      "Epoch 119: Train Loss = 0.5002639240063628\n",
      "Epoch 120: Train Loss = 0.50015168108265\n",
      "Recall = 0.9287285525736911, Aging Rate = 0.6146482635796973, Precision = 0.7645780514306411\n",
      "Validation: Test Loss = 0.4995726777418108\n",
      "Recall = 0.9300483941926969, Aging Rate = 0.6148708815672307, precision = 0.7653874004344677\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.5000460391477719\n",
      "Epoch 122: Train Loss = 0.4996756139018126\n",
      "Epoch 123: Train Loss = 0.49953114421157463\n",
      "Epoch 124: Train Loss = 0.49940127495241715\n",
      "Epoch 125: Train Loss = 0.49890690574875285\n",
      "Recall = 0.9326880774307084, Aging Rate = 0.6195458593054319, Precision = 0.7617678763923823\n",
      "Validation: Test Loss = 0.49881557885388234\n",
      "Recall = 0.924769027716674, Aging Rate = 0.6064113980409617, precision = 0.7716593245227606\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.4987323106874447\n",
      "Epoch 127: Train Loss = 0.49873804040286335\n",
      "Epoch 128: Train Loss = 0.49876455467721764\n",
      "Epoch 129: Train Loss = 0.49826876050854835\n",
      "Epoch 130: Train Loss = 0.4980090084292479\n",
      "Recall = 0.9300483941926969, Aging Rate = 0.6137577916295637, Precision = 0.7667754805948495\n",
      "Validation: Test Loss = 0.49764810904581847\n",
      "Recall = 0.9304883413990321, Aging Rate = 0.6121994657168299, precision = 0.769090909090909\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.49791678650933296\n",
      "Epoch 132: Train Loss = 0.4980573928579091\n",
      "Epoch 133: Train Loss = 0.4975432387336704\n",
      "Epoch 134: Train Loss = 0.49719235088712704\n",
      "Epoch 135: Train Loss = 0.4971896932789606\n",
      "Recall = 0.9326880774307084, Aging Rate = 0.6135351736420303, Precision = 0.7692307692307693\n",
      "Validation: Test Loss = 0.4966014461228599\n",
      "Recall = 0.934007919049714, Aging Rate = 0.6173196794300979, precision = 0.7655968265416516\n",
      "\n",
      "Epoch 136: Train Loss = 0.49683478631191985\n",
      "Epoch 137: Train Loss = 0.49661833530860827\n",
      "Epoch 138: Train Loss = 0.4964276137143931\n",
      "Epoch 139: Train Loss = 0.4963158655676158\n",
      "Epoch 140: Train Loss = 0.4963013220873669\n",
      "Recall = 0.9296084469863617, Aging Rate = 0.6101959038290294, Precision = 0.7708865377599416\n",
      "Validation: Test Loss = 0.4957344400383804\n",
      "Recall = 0.932248130224373, Aging Rate = 0.6113089937666963, precision = 0.7716678805535324\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.4958564507993119\n",
      "Epoch 142: Train Loss = 0.4957949415127932\n",
      "Epoch 143: Train Loss = 0.4956695951209574\n",
      "Epoch 144: Train Loss = 0.4953759350313847\n",
      "Epoch 145: Train Loss = 0.4952324401132782\n",
      "Recall = 0.9313682358117026, Aging Rate = 0.6108637577916296, Precision = 0.7715014577259475\n",
      "Validation: Test Loss = 0.49506306709292097\n",
      "Recall = 0.9318081830180378, Aging Rate = 0.6093054318788959, precision = 0.773839970770917\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.49502885232837945\n",
      "Epoch 147: Train Loss = 0.49492714642310714\n",
      "Epoch 148: Train Loss = 0.49494538743476835\n",
      "Epoch 149: Train Loss = 0.4946866760748577\n",
      "Epoch 150: Train Loss = 0.49443910881969827\n",
      "Recall = 0.9300483941926969, Aging Rate = 0.6084149599287623, Precision = 0.7735089645078668\n",
      "Validation: Test Loss = 0.4940351605521497\n",
      "Recall = 0.9296084469863617, Aging Rate = 0.6055209260908282, precision = 0.7768382352941177\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.5084395042567132\n",
      "Recall = 0.9155124653739612, Aging Rate = 0.589452603471295, precision = 0.7485843714609286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67a4198cbdc488ca76e108cd995f560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6842999364368001\n",
      "Epoch 2: Train Loss = 0.6785637715089989\n",
      "Epoch 3: Train Loss = 0.6734377240773725\n",
      "Epoch 4: Train Loss = 0.6685551537643664\n",
      "Epoch 5: Train Loss = 0.6641161938596812\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49755120213713266\n",
      "Validation: Test Loss = 0.6617902917513758\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49755120213713266\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.65972987313623\n",
      "Epoch 7: Train Loss = 0.6550912575751463\n",
      "Epoch 8: Train Loss = 0.6500247349085175\n",
      "Epoch 9: Train Loss = 0.6443746843618255\n",
      "Epoch 10: Train Loss = 0.6383279956883866\n",
      "Recall = 0.9982102908277405, Aging Rate = 0.9697239536954586, Precision = 0.5121671258034894\n",
      "Validation: Test Loss = 0.6347525600121686\n",
      "Recall = 0.9959731543624161, Aging Rate = 0.9539180765805877, precision = 0.5194865810968494\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.6316925166763584\n",
      "Epoch 12: Train Loss = 0.6248544888207664\n",
      "Epoch 13: Train Loss = 0.6178789823477754\n",
      "Epoch 14: Train Loss = 0.6108313452847176\n",
      "Epoch 15: Train Loss = 0.603718643250045\n",
      "Recall = 0.9530201342281879, Aging Rate = 0.7729296527159395, Precision = 0.613479262672811\n",
      "Validation: Test Loss = 0.6001044069139\n",
      "Recall = 0.9476510067114094, Aging Rate = 0.7575690115761353, precision = 0.6223920070526007\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5971013719443222\n",
      "Epoch 17: Train Loss = 0.5907876048890587\n",
      "Epoch 18: Train Loss = 0.584629553190216\n",
      "Epoch 19: Train Loss = 0.5790849252142019\n",
      "Epoch 20: Train Loss = 0.5740946583310516\n",
      "Recall = 0.9181208053691275, Aging Rate = 0.677426536064114, Precision = 0.6743345382845876\n",
      "Validation: Test Loss = 0.5715205987860662\n",
      "Recall = 0.9118568232662192, Aging Rate = 0.6627337488869101, precision = 0.6845817937520994\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5695782198719116\n",
      "Epoch 22: Train Loss = 0.5654619416273394\n",
      "Epoch 23: Train Loss = 0.5616547340064418\n",
      "Epoch 24: Train Loss = 0.5584668568596708\n",
      "Epoch 25: Train Loss = 0.5553784672426307\n",
      "Recall = 0.9033557046979865, Aging Rate = 0.6418076580587712, Precision = 0.700312174817898\n",
      "Validation: Test Loss = 0.5536756844045005\n",
      "Recall = 0.9109619686800895, Aging Rate = 0.649154051647373, precision = 0.6982167352537723\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5524143801134607\n",
      "Epoch 27: Train Loss = 0.5500289666153763\n",
      "Epoch 28: Train Loss = 0.5477042890826401\n",
      "Epoch 29: Train Loss = 0.5454916633990654\n",
      "Epoch 30: Train Loss = 0.5437264340006658\n",
      "Recall = 0.9029082774049217, Aging Rate = 0.6364648263579697, Precision = 0.7058412032179083\n",
      "Validation: Test Loss = 0.542428058911304\n",
      "Recall = 0.9033557046979865, Aging Rate = 0.6364648263579697, precision = 0.7061909758656874\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.5418562448566128\n",
      "Epoch 32: Train Loss = 0.5404322657432284\n",
      "Epoch 33: Train Loss = 0.5386726254346853\n",
      "Epoch 34: Train Loss = 0.5372353064514969\n",
      "Epoch 35: Train Loss = 0.5359252248612877\n",
      "Recall = 0.8988814317673378, Aging Rate = 0.6235529830810329, Precision = 0.7172438414851838\n",
      "Validation: Test Loss = 0.5349803831252474\n",
      "Recall = 0.9024608501118568, Aging Rate = 0.628673196794301, precision = 0.7142351274787535\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.5346638475903844\n",
      "Epoch 37: Train Loss = 0.5334359454662801\n",
      "Epoch 38: Train Loss = 0.5323299467298248\n",
      "Epoch 39: Train Loss = 0.5311012574316452\n",
      "Epoch 40: Train Loss = 0.530050162852925\n",
      "Recall = 0.8953020134228188, Aging Rate = 0.6104185218165628, Precision = 0.7297592997811816\n",
      "Validation: Test Loss = 0.5294891772053651\n",
      "Recall = 0.90917225950783, Aging Rate = 0.6320124666073018, precision = 0.7157449806269813\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.5292528839060375\n",
      "Epoch 42: Train Loss = 0.5283792751342827\n",
      "Epoch 43: Train Loss = 0.5273939493980561\n",
      "Epoch 44: Train Loss = 0.5265629343283782\n",
      "Epoch 45: Train Loss = 0.5257620853392652\n",
      "Recall = 0.9029082774049217, Aging Rate = 0.6179875333926982, Precision = 0.7269452449567724\n",
      "Validation: Test Loss = 0.5249033104810774\n",
      "Recall = 0.9002237136465324, Aging Rate = 0.6130899376669635, precision = 0.7305737109658679\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.5251463665456934\n",
      "Epoch 47: Train Loss = 0.5241912409649931\n",
      "Epoch 48: Train Loss = 0.5233742494935556\n",
      "Epoch 49: Train Loss = 0.5227567694172303\n",
      "Epoch 50: Train Loss = 0.5218793884622976\n",
      "Recall = 0.9020134228187919, Aging Rate = 0.609973285841496, Precision = 0.7357664233576642\n",
      "Validation: Test Loss = 0.5212251762142165\n",
      "Recall = 0.905592841163311, Aging Rate = 0.6137577916295637, precision = 0.7341313021400072\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.5212725967779185\n",
      "Epoch 52: Train Loss = 0.5208530397364208\n",
      "Epoch 53: Train Loss = 0.5200398693314007\n",
      "Epoch 54: Train Loss = 0.519276265761519\n",
      "Epoch 55: Train Loss = 0.5187134390841184\n",
      "Recall = 0.9073825503355705, Aging Rate = 0.6108637577916296, Precision = 0.739067055393586\n",
      "Validation: Test Loss = 0.5180986542470308\n",
      "Recall = 0.9154362416107382, Aging Rate = 0.6193232413178985, precision = 0.7354421279654925\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.5181745182480646\n",
      "Epoch 57: Train Loss = 0.5174858606986881\n",
      "Epoch 58: Train Loss = 0.5172837362529333\n",
      "Epoch 59: Train Loss = 0.5164392789027995\n",
      "Epoch 60: Train Loss = 0.5158274590385247\n",
      "Recall = 0.90917225950783, Aging Rate = 0.608860195903829, Precision = 0.7429616087751371\n",
      "Validation: Test Loss = 0.515197378126299\n",
      "Recall = 0.9154362416107382, Aging Rate = 0.6164292074799644, precision = 0.7388949079089924\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.5153208143564599\n",
      "Epoch 62: Train Loss = 0.5149364865261428\n",
      "Epoch 63: Train Loss = 0.5142270607387818\n",
      "Epoch 64: Train Loss = 0.5137139737659124\n",
      "Epoch 65: Train Loss = 0.513358232784993\n",
      "Recall = 0.9078299776286354, Aging Rate = 0.6028495102404274, Precision = 0.7492614475627769\n",
      "Validation: Test Loss = 0.5128078650706386\n",
      "Recall = 0.9194630872483222, Aging Rate = 0.6182101513802315, precision = 0.7400072020165647\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.5128278161199201\n",
      "Epoch 67: Train Loss = 0.5122584728980213\n",
      "Epoch 68: Train Loss = 0.5119374696420753\n",
      "Epoch 69: Train Loss = 0.5115706841274764\n",
      "Epoch 70: Train Loss = 0.5109049791962784\n",
      "Recall = 0.9181208053691275, Aging Rate = 0.6106411398040962, Precision = 0.7480860371855632\n",
      "Validation: Test Loss = 0.5104670570773517\n",
      "Recall = 0.9190156599552572, Aging Rate = 0.6121994657168299, precision = 0.7469090909090909\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.5103328042760664\n",
      "Epoch 72: Train Loss = 0.5101352794724496\n",
      "Epoch 73: Train Loss = 0.5095301290653905\n",
      "Epoch 74: Train Loss = 0.5092387775013941\n",
      "Epoch 75: Train Loss = 0.50865830060423\n",
      "Recall = 0.9190156599552572, Aging Rate = 0.6073018699910953, Precision = 0.7529325513196481\n",
      "Validation: Test Loss = 0.5081820855794585\n",
      "Recall = 0.9199105145413871, Aging Rate = 0.6097506678539626, precision = 0.7506389193136181\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.5082577233959922\n",
      "Epoch 77: Train Loss = 0.5080722177124193\n",
      "Epoch 78: Train Loss = 0.5075074963450751\n",
      "Epoch 79: Train Loss = 0.5070450662080453\n",
      "Epoch 80: Train Loss = 0.5067753228669918\n",
      "Recall = 0.9212527964205817, Aging Rate = 0.608860195903829, Precision = 0.7528336380255941\n",
      "Validation: Test Loss = 0.5064353927213596\n",
      "Recall = 0.9145413870246085, Aging Rate = 0.597506678539626, precision = 0.7615499254843517\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.5065544161112947\n",
      "Epoch 82: Train Loss = 0.5060881523287731\n",
      "Epoch 83: Train Loss = 0.5059509593943666\n",
      "Epoch 84: Train Loss = 0.505258924283837\n",
      "Epoch 85: Train Loss = 0.5047684639347311\n",
      "Recall = 0.9221476510067114, Aging Rate = 0.6070792520035619, Precision = 0.7557755775577558\n",
      "Validation: Test Loss = 0.5044917130937348\n",
      "Recall = 0.9181208053691275, Aging Rate = 0.5995102404274265, precision = 0.7619754920163386\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.5045737444239861\n",
      "Epoch 87: Train Loss = 0.5041963991477674\n",
      "Epoch 88: Train Loss = 0.5038487260243983\n",
      "Epoch 89: Train Loss = 0.5036339847402497\n",
      "Epoch 90: Train Loss = 0.503314037569272\n",
      "Recall = 0.9225950782997763, Aging Rate = 0.6048530721282279, Precision = 0.7589252852410747\n",
      "Validation: Test Loss = 0.5027422673555749\n",
      "Recall = 0.9230425055928412, Aging Rate = 0.6030721282279609, precision = 0.7615356220007383\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.5030859008179323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.502580275492061\n",
      "Epoch 93: Train Loss = 0.502188009071435\n",
      "Epoch 94: Train Loss = 0.5018720206467042\n",
      "Epoch 95: Train Loss = 0.5016840488806645\n",
      "Recall = 0.9257270693512304, Aging Rate = 0.6035173642030276, Precision = 0.7631870158613058\n",
      "Validation: Test Loss = 0.5010390806070737\n",
      "Recall = 0.9261744966442953, Aging Rate = 0.6041852181656278, precision = 0.7627118644067796\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.5012337762334999\n",
      "Epoch 97: Train Loss = 0.5009450913006348\n",
      "Epoch 98: Train Loss = 0.5007314641403494\n",
      "Epoch 99: Train Loss = 0.5003272762069294\n",
      "Epoch 100: Train Loss = 0.4999405751245317\n",
      "Recall = 0.9266219239373602, Aging Rate = 0.5997328584149599, Precision = 0.7687453600593912\n",
      "Validation: Test Loss = 0.49956218562588983\n",
      "Recall = 0.9319910514541387, Aging Rate = 0.6084149599287623, precision = 0.7621661178192463\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.4996951960710575\n",
      "Epoch 102: Train Loss = 0.49948765712981763\n",
      "Epoch 103: Train Loss = 0.49926043623595606\n",
      "Epoch 104: Train Loss = 0.49890014616804046\n",
      "Epoch 105: Train Loss = 0.4987397046622058\n",
      "Recall = 0.9239373601789709, Aging Rate = 0.5934995547640249, Precision = 0.7745686421605401\n",
      "Validation: Test Loss = 0.49817615585569175\n",
      "Recall = 0.930648769574944, Aging Rate = 0.6052983081032948, precision = 0.7649871276204487\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.49841396424352113\n",
      "Epoch 107: Train Loss = 0.4980037874763604\n",
      "Epoch 108: Train Loss = 0.4978016642513598\n",
      "Epoch 109: Train Loss = 0.49746422802669493\n",
      "Epoch 110: Train Loss = 0.49718041013418723\n",
      "Recall = 0.930648769574944, Aging Rate = 0.5990650044523598, Precision = 0.7729468599033816\n",
      "Validation: Test Loss = 0.4967722585980423\n",
      "Recall = 0.9230425055928412, Aging Rate = 0.5894924309884239, precision = 0.7790785498489426\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.4969887157582005\n",
      "Epoch 112: Train Loss = 0.4965170142593282\n",
      "Epoch 113: Train Loss = 0.49637639724242294\n",
      "Epoch 114: Train Loss = 0.4963522507267559\n",
      "Epoch 115: Train Loss = 0.49586183618246604\n",
      "Recall = 0.9266219239373602, Aging Rate = 0.5919412288512912, Precision = 0.7788642346746897\n",
      "Validation: Test Loss = 0.4954638610911603\n",
      "Recall = 0.9319910514541387, Aging Rate = 0.5995102404274265, precision = 0.7734868176754549\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.4955238790673437\n",
      "Epoch 117: Train Loss = 0.49541191548400976\n",
      "Epoch 118: Train Loss = 0.49537838588096367\n",
      "Epoch 119: Train Loss = 0.49479980120569705\n",
      "Epoch 120: Train Loss = 0.49471071041384024\n",
      "Recall = 0.9266219239373602, Aging Rate = 0.590160284951024, Precision = 0.7812146359864203\n",
      "Validation: Test Loss = 0.4941231659344022\n",
      "Recall = 0.934675615212528, Aging Rate = 0.5992876224398932, precision = 0.7760029717682021\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.4942884410690858\n",
      "Epoch 122: Train Loss = 0.4940797834128858\n",
      "Epoch 123: Train Loss = 0.4938999084574987\n",
      "Epoch 124: Train Loss = 0.4936336343137685\n",
      "Epoch 125: Train Loss = 0.49347671985838526\n",
      "Recall = 0.9288590604026845, Aging Rate = 0.5897150489759573, Precision = 0.7836919592298981\n",
      "Validation: Test Loss = 0.4930719580909342\n",
      "Recall = 0.9364653243847875, Aging Rate = 0.600845948352627, precision = 0.7754723971841423\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.49313546129347274\n",
      "Epoch 127: Train Loss = 0.4930209652823417\n",
      "Epoch 128: Train Loss = 0.49262275325859217\n",
      "Epoch 129: Train Loss = 0.4923620750250184\n",
      "Epoch 130: Train Loss = 0.4924199863246585\n",
      "Recall = 0.9328859060402684, Aging Rate = 0.5926090828138914, Precision = 0.7832456799398948\n",
      "Validation: Test Loss = 0.4917753422674279\n",
      "Recall = 0.9297539149888143, Aging Rate = 0.5859305431878896, precision = 0.7895136778115501\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.4919990186752853\n",
      "Epoch 132: Train Loss = 0.4917830989919172\n",
      "Epoch 133: Train Loss = 0.49154143697538233\n",
      "Epoch 134: Train Loss = 0.4912803331497834\n",
      "Epoch 135: Train Loss = 0.4912790252444793\n",
      "Recall = 0.9293064876957494, Aging Rate = 0.5870436331255565, Precision = 0.7876374668183542\n",
      "Validation: Test Loss = 0.4908754379751421\n",
      "Recall = 0.941834451901566, Aging Rate = 0.6075244879786287, precision = 0.7713448149505313\n",
      "\n",
      "Epoch 136: Train Loss = 0.49094125402897254\n",
      "Epoch 137: Train Loss = 0.4906740841967446\n",
      "Epoch 138: Train Loss = 0.4903394328571279\n",
      "Epoch 139: Train Loss = 0.49021916453687614\n",
      "Epoch 140: Train Loss = 0.49003199223206706\n",
      "Recall = 0.9360178970917226, Aging Rate = 0.5941674087266251, Precision = 0.7838141626077183\n",
      "Validation: Test Loss = 0.4896138216487446\n",
      "Recall = 0.930648769574944, Aging Rate = 0.5819234194122885, precision = 0.7957153787299158\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.4900436647906435\n",
      "Epoch 142: Train Loss = 0.48969641134129605\n",
      "Epoch 143: Train Loss = 0.4895304045503095\n",
      "Epoch 144: Train Loss = 0.48923304308129545\n",
      "Epoch 145: Train Loss = 0.4890641009075134\n",
      "Recall = 0.9337807606263983, Aging Rate = 0.5857079252003562, Precision = 0.7932345115925503\n",
      "Validation: Test Loss = 0.4885572986420201\n",
      "Recall = 0.9337807606263983, Aging Rate = 0.583926981300089, precision = 0.7956538314906596\n",
      "Model in epoch 145 is saved.\n",
      "\n",
      "Epoch 146: Train Loss = 0.4887267584951457\n",
      "Epoch 147: Train Loss = 0.48851592110292463\n",
      "Epoch 148: Train Loss = 0.48841568196021545\n",
      "Epoch 149: Train Loss = 0.4880065929114765\n",
      "Epoch 150: Train Loss = 0.4880614529628363\n",
      "Recall = 0.934675615212528, Aging Rate = 0.583926981300089, Precision = 0.7964163171940526\n",
      "Validation: Test Loss = 0.4874824196073697\n",
      "Recall = 0.9351230425055929, Aging Rate = 0.5845948352626892, precision = 0.7958872810357959\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.4975648393459091\n",
      "Recall = 0.9078947368421053, Aging Rate = 0.5934579439252337, precision = 0.7761529808773904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c734ccab918c4942b768365d9d14fe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6928209667745066\n",
      "Epoch 2: Train Loss = 0.6811577983446028\n",
      "Epoch 3: Train Loss = 0.6729976670187918\n",
      "Epoch 4: Train Loss = 0.6658533841291921\n",
      "Epoch 5: Train Loss = 0.660986628632193\n",
      "Recall = 1.0, Aging Rate = 0.9997773820124666, Precision = 0.5016700066800267\n",
      "Validation: Test Loss = 0.6576948805993302\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5015583259127337\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.6554881823137203\n",
      "Epoch 7: Train Loss = 0.651090390748035\n",
      "Epoch 8: Train Loss = 0.6461546067563105\n",
      "Epoch 9: Train Loss = 0.6410586263914779\n",
      "Epoch 10: Train Loss = 0.6353375531686595\n",
      "Recall = 0.9977807367953839, Aging Rate = 0.973953695458593, Precision = 0.5138285714285714\n",
      "Validation: Test Loss = 0.6322560982840038\n",
      "Recall = 0.9973368841544608, Aging Rate = 0.959706144256456, precision = 0.5212247738343772\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.6291754321657115\n",
      "Epoch 12: Train Loss = 0.6228579893137659\n",
      "Epoch 13: Train Loss = 0.6157802994614293\n",
      "Epoch 14: Train Loss = 0.6088097910740922\n",
      "Epoch 15: Train Loss = 0.6020251474312556\n",
      "Recall = 0.9644917887261429, Aging Rate = 0.7827248441674087, Precision = 0.6180318543799772\n",
      "Validation: Test Loss = 0.5984993821154294\n",
      "Recall = 0.9631602308033733, Aging Rate = 0.7778272484416741, precision = 0.6210646823125358\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5955034802242357\n",
      "Epoch 17: Train Loss = 0.5893694529656524\n",
      "Epoch 18: Train Loss = 0.5837324764297567\n",
      "Epoch 19: Train Loss = 0.5785859634806191\n",
      "Epoch 20: Train Loss = 0.5740040257798702\n",
      "Recall = 0.9236573457612073, Aging Rate = 0.6832146037399822, Precision = 0.6780710329097426\n",
      "Validation: Test Loss = 0.5713571241044192\n",
      "Recall = 0.9325343985796716, Aging Rate = 0.6970169189670525, precision = 0.6710316192909613\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.5695437754994509\n",
      "Epoch 22: Train Loss = 0.565512585406418\n",
      "Epoch 23: Train Loss = 0.5622260786546519\n",
      "Epoch 24: Train Loss = 0.5589070185846446\n",
      "Epoch 25: Train Loss = 0.556190000828737\n",
      "Recall = 0.9081225033288948, Aging Rate = 0.6527159394479074, Precision = 0.6978171896316507\n",
      "Validation: Test Loss = 0.5543873957619535\n",
      "Recall = 0.9103417665335108, Aging Rate = 0.6549421193232413, precision = 0.6971447994561523\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.5534601765133288\n",
      "Epoch 27: Train Loss = 0.5510446446767792\n",
      "Epoch 28: Train Loss = 0.5490648751267342\n",
      "Epoch 29: Train Loss = 0.5468452313810401\n",
      "Epoch 30: Train Loss = 0.5451886285550872\n",
      "Recall = 0.9103417665335108, Aging Rate = 0.6489314336598397, Precision = 0.7036020583190394\n",
      "Validation: Test Loss = 0.5441289771271941\n",
      "Recall = 0.9116733244562805, Aging Rate = 0.6495992876224399, precision = 0.7039067854694997\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.5433590958294652\n",
      "Epoch 32: Train Loss = 0.5418659727274044\n",
      "Epoch 33: Train Loss = 0.5404960082666426\n",
      "Epoch 34: Train Loss = 0.5391046534344647\n",
      "Epoch 35: Train Loss = 0.5380043469415853\n",
      "Recall = 0.9041278295605859, Aging Rate = 0.6351291184327693, Precision = 0.7139852786540484\n",
      "Validation: Test Loss = 0.5370411886663599\n",
      "Recall = 0.9112294718153573, Aging Rate = 0.6433659839715049, precision = 0.7103806228373702\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.5365371522580739\n",
      "Epoch 37: Train Loss = 0.5357939388374081\n",
      "Epoch 38: Train Loss = 0.5344701262318653\n",
      "Epoch 39: Train Loss = 0.5336055407434941\n",
      "Epoch 40: Train Loss = 0.5327361739860086\n",
      "Recall = 0.9130048823790502, Aging Rate = 0.6413624220837043, Precision = 0.7139881985421729\n",
      "Validation: Test Loss = 0.5317781942822736\n",
      "Recall = 0.9081225033288948, Aging Rate = 0.633793410507569, precision = 0.7186512118018967\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.5317347845857099\n",
      "Epoch 42: Train Loss = 0.5307483077049255\n",
      "Epoch 43: Train Loss = 0.5300651683089782\n",
      "Epoch 44: Train Loss = 0.5293491369682237\n",
      "Epoch 45: Train Loss = 0.5286301481118716\n",
      "Recall = 0.9130048823790502, Aging Rate = 0.6357969723953696, Precision = 0.7202380952380952\n",
      "Validation: Test Loss = 0.5278949259331687\n",
      "Recall = 0.9125610297381269, Aging Rate = 0.6371326803205699, precision = 0.7183787561146052\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.5280550388173131\n",
      "Epoch 47: Train Loss = 0.5275051015249237\n",
      "Epoch 48: Train Loss = 0.5267126650971593\n",
      "Epoch 49: Train Loss = 0.5260382413545678\n",
      "Epoch 50: Train Loss = 0.5254665268178721\n",
      "Recall = 0.9081225033288948, Aging Rate = 0.627560106856634, Precision = 0.725789286981199\n",
      "Validation: Test Loss = 0.5249154923329477\n",
      "Recall = 0.9072347980470484, Aging Rate = 0.6253339269813001, precision = 0.7276610893556426\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.524708580248927\n",
      "Epoch 52: Train Loss = 0.5245246241268895\n",
      "Epoch 53: Train Loss = 0.5237556542547707\n",
      "Epoch 54: Train Loss = 0.5231813883951277\n",
      "Epoch 55: Train Loss = 0.5228491097502908\n",
      "Recall = 0.9130048823790502, Aging Rate = 0.6291184327693677, Precision = 0.7278839348903043\n",
      "Validation: Test Loss = 0.5222217692186656\n",
      "Recall = 0.918774966711052, Aging Rate = 0.6364648263579697, precision = 0.7240293809024134\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.5224016368760663\n",
      "Epoch 57: Train Loss = 0.5218680136350257\n",
      "Epoch 58: Train Loss = 0.521438509358536\n",
      "Epoch 59: Train Loss = 0.5209171056535131\n",
      "Epoch 60: Train Loss = 0.5204121435951783\n",
      "Recall = 0.9059032401242787, Aging Rate = 0.6146482635796973, Precision = 0.739224918507787\n",
      "Validation: Test Loss = 0.5201969075606322\n",
      "Recall = 0.9196626719928983, Aging Rate = 0.6355743544078362, precision = 0.7257443082311734\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.5201745385955936\n",
      "Epoch 62: Train Loss = 0.5199272383140435\n",
      "Epoch 63: Train Loss = 0.5192434554745444\n",
      "Epoch 64: Train Loss = 0.5189255462728859\n",
      "Epoch 65: Train Loss = 0.5186249383941678\n",
      "Recall = 0.9143364403018198, Aging Rate = 0.6235529830810329, Precision = 0.73545162441985\n",
      "Validation: Test Loss = 0.5181588065592392\n",
      "Recall = 0.908566355969818, Aging Rate = 0.6124220837043634, precision = 0.7440930570701563\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.5181183352177416\n",
      "Epoch 67: Train Loss = 0.5178322758084414\n",
      "Epoch 68: Train Loss = 0.5173169131907415\n",
      "Epoch 69: Train Loss = 0.5171775141679487\n",
      "Epoch 70: Train Loss = 0.5166666297538941\n",
      "Recall = 0.9103417665335108, Aging Rate = 0.6175422974176313, Precision = 0.7393655371304975\n",
      "Validation: Test Loss = 0.5161994524227118\n",
      "Recall = 0.9116733244562805, Aging Rate = 0.6153161175422974, precision = 0.7431259044862518\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.5162809381289665\n",
      "Epoch 72: Train Loss = 0.5158899371791714\n",
      "Epoch 73: Train Loss = 0.5155534234199796\n",
      "Epoch 74: Train Loss = 0.5154485576723902\n",
      "Epoch 75: Train Loss = 0.5149396179726584\n",
      "Recall = 0.9169995561473591, Aging Rate = 0.6235529830810329, Precision = 0.7375937165298108\n",
      "Validation: Test Loss = 0.5145654553615505\n",
      "Recall = 0.9138925876608965, Aging Rate = 0.6155387355298308, precision = 0.7446654611211573\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.5145901009533306\n",
      "Epoch 77: Train Loss = 0.5143792725309981\n",
      "Epoch 78: Train Loss = 0.5140895761237649\n",
      "Epoch 79: Train Loss = 0.5138212491758997\n",
      "Epoch 80: Train Loss = 0.5133400366758515\n",
      "Recall = 0.9169995561473591, Aging Rate = 0.6166518254674977, Precision = 0.7458483754512636\n",
      "Validation: Test Loss = 0.5130112842853221\n",
      "Recall = 0.9130048823790502, Aging Rate = 0.6106411398040962, precision = 0.7499088589135983\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.513168529060836\n",
      "Epoch 82: Train Loss = 0.5127495616700961\n",
      "Epoch 83: Train Loss = 0.5125152584387591\n",
      "Epoch 84: Train Loss = 0.5121105927924227\n",
      "Epoch 85: Train Loss = 0.5118811784423257\n",
      "Recall = 0.914780292942743, Aging Rate = 0.6130899376669635, Precision = 0.7483660130718954\n",
      "Validation: Test Loss = 0.5114496306235091\n",
      "Recall = 0.9156679982245894, Aging Rate = 0.6146482635796973, precision = 0.7471930459978269\n",
      "\n",
      "Epoch 86: Train Loss = 0.5116638115019216\n",
      "Epoch 87: Train Loss = 0.5114275323208803\n",
      "Epoch 88: Train Loss = 0.5112139448881786\n",
      "Epoch 89: Train Loss = 0.5107524503264593\n",
      "Epoch 90: Train Loss = 0.5105320477952728\n",
      "Recall = 0.9138925876608965, Aging Rate = 0.6106411398040962, Precision = 0.7506379876048123\n",
      "Validation: Test Loss = 0.5102098412738776\n",
      "Recall = 0.9227696404793608, Aging Rate = 0.6228851291184327, precision = 0.7430307362401716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: Train Loss = 0.5102599683555236\n",
      "Epoch 92: Train Loss = 0.5098873426738002\n",
      "Epoch 93: Train Loss = 0.5097633498328558\n",
      "Epoch 94: Train Loss = 0.5094116917170273\n",
      "Epoch 95: Train Loss = 0.5092869253956837\n",
      "Recall = 0.914780292942743, Aging Rate = 0.6117542297417632, Precision = 0.75\n",
      "Validation: Test Loss = 0.508705368661923\n",
      "Recall = 0.9165557035064359, Aging Rate = 0.6104185218165628, precision = 0.7530999270605397\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.5089000146885801\n",
      "Epoch 97: Train Loss = 0.5088408936481866\n",
      "Epoch 98: Train Loss = 0.5085602091660164\n",
      "Epoch 99: Train Loss = 0.5081568704262867\n",
      "Epoch 100: Train Loss = 0.5080562177929407\n",
      "Recall = 0.9143364403018198, Aging Rate = 0.6073018699910953, Precision = 0.7551319648093842\n",
      "Validation: Test Loss = 0.5074896900545564\n",
      "Recall = 0.9218819351975144, Aging Rate = 0.6157613535173642, precision = 0.7509038322487346\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.5078414000789714\n",
      "Epoch 102: Train Loss = 0.5075774742414775\n",
      "Epoch 103: Train Loss = 0.5073597211977889\n",
      "Epoch 104: Train Loss = 0.5068893940026274\n",
      "Epoch 105: Train Loss = 0.5068265366055343\n",
      "Recall = 0.918774966711052, Aging Rate = 0.6106411398040962, Precision = 0.7546481954064892\n",
      "Validation: Test Loss = 0.5063327633687034\n",
      "Recall = 0.914780292942743, Aging Rate = 0.6030721282279609, precision = 0.760797342192691\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.506438303397154\n",
      "Epoch 107: Train Loss = 0.5062650072712613\n",
      "Epoch 108: Train Loss = 0.5060444082517446\n",
      "Epoch 109: Train Loss = 0.5059960148001185\n",
      "Epoch 110: Train Loss = 0.5056185689709172\n",
      "Recall = 0.9223257878384377, Aging Rate = 0.6124220837043634, Precision = 0.7553616866593966\n",
      "Validation: Test Loss = 0.505074835497677\n",
      "Recall = 0.9232134931202841, Aging Rate = 0.6124220837043634, precision = 0.7560886950199928\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.5053901093301238\n",
      "Epoch 112: Train Loss = 0.5051502604518527\n",
      "Epoch 113: Train Loss = 0.5051392913395447\n",
      "Epoch 114: Train Loss = 0.5046991297207235\n",
      "Epoch 115: Train Loss = 0.5045821284303045\n",
      "Recall = 0.920994229915668, Aging Rate = 0.6073018699910953, Precision = 0.7606304985337243\n",
      "Validation: Test Loss = 0.5040176369627989\n",
      "Recall = 0.9236573457612073, Aging Rate = 0.6115316117542298, precision = 0.7575536949399345\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.5044355547969509\n",
      "Epoch 117: Train Loss = 0.5041021487591846\n",
      "Epoch 118: Train Loss = 0.503866486112029\n",
      "Epoch 119: Train Loss = 0.5036874000556959\n",
      "Epoch 120: Train Loss = 0.5037257662863999\n",
      "Recall = 0.9169995561473591, Aging Rate = 0.6021816562778273, Precision = 0.7637707948243992\n",
      "Validation: Test Loss = 0.5033012406696513\n",
      "Recall = 0.9316466932978251, Aging Rate = 0.6242208370436332, precision = 0.7485734664764622\n",
      "\n",
      "Epoch 121: Train Loss = 0.5031677680363744\n",
      "Epoch 122: Train Loss = 0.5031191895078148\n",
      "Epoch 123: Train Loss = 0.5028380190062927\n",
      "Epoch 124: Train Loss = 0.5025708299604571\n",
      "Epoch 125: Train Loss = 0.5024450723337257\n",
      "Recall = 0.9249889036839769, Aging Rate = 0.6086375779162957, Precision = 0.7622531089978054\n",
      "Validation: Test Loss = 0.5021002237699123\n",
      "Recall = 0.9174434087882823, Aging Rate = 0.5972840605520926, precision = 0.7704062616474097\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.5024823656542855\n",
      "Epoch 127: Train Loss = 0.5020047764319558\n",
      "Epoch 128: Train Loss = 0.5017760512771504\n",
      "Epoch 129: Train Loss = 0.5017405097121547\n",
      "Epoch 130: Train Loss = 0.5015031088173443\n",
      "Recall = 0.9227696404793608, Aging Rate = 0.6021816562778273, Precision = 0.7685767097966728\n",
      "Validation: Test Loss = 0.5009048672945289\n",
      "Recall = 0.9232134931202841, Aging Rate = 0.6024042742653607, precision = 0.7686622320768662\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.5013425094096234\n",
      "Epoch 132: Train Loss = 0.5010092048802125\n",
      "Epoch 133: Train Loss = 0.500813088641035\n",
      "Epoch 134: Train Loss = 0.5006343060482429\n",
      "Epoch 135: Train Loss = 0.500607557866792\n",
      "Recall = 0.9241011984021305, Aging Rate = 0.6039626001780944, Precision = 0.767416144489495\n",
      "Validation: Test Loss = 0.5000332599756235\n",
      "Recall = 0.9245450510430537, Aging Rate = 0.6024042742653607, precision = 0.7697708795269771\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.5003589814195437\n",
      "Epoch 137: Train Loss = 0.5001321121293949\n",
      "Epoch 138: Train Loss = 0.5000741810588463\n",
      "Epoch 139: Train Loss = 0.4996210723611147\n",
      "Epoch 140: Train Loss = 0.49958287629291404\n",
      "Recall = 0.9245450510430537, Aging Rate = 0.602626892252894, Precision = 0.7694865164388622\n",
      "Validation: Test Loss = 0.49918568781838285\n",
      "Recall = 0.9214380825565912, Aging Rate = 0.5943900267141585, precision = 0.7775280898876404\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.49917795630511913\n",
      "Epoch 142: Train Loss = 0.4991545155445381\n",
      "Epoch 143: Train Loss = 0.4990099363845051\n",
      "Epoch 144: Train Loss = 0.49883642787394095\n",
      "Epoch 145: Train Loss = 0.49846344191053565\n",
      "Recall = 0.9249889036839769, Aging Rate = 0.6004007123775601, Precision = 0.7727104189840563\n",
      "Validation: Test Loss = 0.49818979998518076\n",
      "Recall = 0.9276520195295161, Aging Rate = 0.6039626001780944, precision = 0.7703649096940656\n",
      "\n",
      "Epoch 146: Train Loss = 0.4984276925254271\n",
      "Epoch 147: Train Loss = 0.4981305256870742\n",
      "Epoch 148: Train Loss = 0.49792727766988115\n",
      "Epoch 149: Train Loss = 0.4978279193808538\n",
      "Epoch 150: Train Loss = 0.4975371265241533\n",
      "Recall = 0.9276520195295161, Aging Rate = 0.6019590382902938, Precision = 0.7729289940828402\n",
      "Validation: Test Loss = 0.49727805432526\n",
      "Recall = 0.927208166888593, Aging Rate = 0.6004007123775601, precision = 0.7745643307378569\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Validation: Test Loss = 0.5014733685670453\n",
      "Recall = 0.9285714285714286, Aging Rate = 0.6041388518024032, precision = 0.7613259668508288\n",
      "\u001b[32m[I 2022-05-26 16:24:43,746]\u001b[0m Trial 5 finished with value: 0.8324065271932305 and parameters: {'batch_size': 96, 'learning_rate': 0.0001, 'weight_decay': 0.01, 'bad_weight': 0.6}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b448cf1ebd44069f016db2c091c268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6984740365114153\n",
      "Epoch 2: Train Loss = 0.6733888325580825\n",
      "Epoch 3: Train Loss = 0.6474849264853147\n",
      "Epoch 4: Train Loss = 0.619707719247891\n",
      "Epoch 5: Train Loss = 0.592602059337993\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.58008298333585\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5707458765349639\n",
      "Epoch 7: Train Loss = 0.5549128683464716\n",
      "Epoch 8: Train Loss = 0.5442185551326187\n",
      "Epoch 9: Train Loss = 0.5374139036223597\n",
      "Epoch 10: Train Loss = 0.5327769848032924\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5309830752739605\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 11: Train Loss = 0.5298747212252019\n",
      "Epoch 12: Train Loss = 0.5276772756716659\n",
      "Epoch 13: Train Loss = 0.5257326779573599\n",
      "Epoch 14: Train Loss = 0.5246610534000057\n",
      "Epoch 15: Train Loss = 0.5236386742479336\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5227543770046184\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 16: Train Loss = 0.5221760490061658\n",
      "Epoch 17: Train Loss = 0.5228328050934409\n",
      "Epoch 18: Train Loss = 0.5212019791267647\n",
      "Epoch 19: Train Loss = 0.5206987540953306\n",
      "Epoch 20: Train Loss = 0.5203978790837744\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5211415211748885\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 21: Train Loss = 0.5198997748503171\n",
      "Epoch 22: Train Loss = 0.5209261743436407\n",
      "Epoch 23: Train Loss = 0.5198021669103329\n",
      "Epoch 24: Train Loss = 0.5194864652566579\n",
      "Epoch 25: Train Loss = 0.5191213334392248\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5197373701330815\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 26: Train Loss = 0.5204655109296817\n",
      "Epoch 27: Train Loss = 0.5196068557850081\n",
      "Epoch 28: Train Loss = 0.5192660163633546\n",
      "Epoch 29: Train Loss = 0.5185914079419015\n",
      "Epoch 30: Train Loss = 0.5183667604772512\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5191319209066546\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 31: Train Loss = 0.5184103402301656\n",
      "Epoch 32: Train Loss = 0.518088388474839\n",
      "Epoch 33: Train Loss = 0.5182930367765741\n",
      "Epoch 34: Train Loss = 0.5181251960414281\n",
      "Epoch 35: Train Loss = 0.5178101453946726\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5177443086676372\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 36: Train Loss = 0.5174323648400108\n",
      "Epoch 37: Train Loss = 0.518020660932427\n",
      "Epoch 38: Train Loss = 0.5180207001027102\n",
      "Epoch 39: Train Loss = 0.5179505665601627\n",
      "Epoch 40: Train Loss = 0.5174013210638018\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5172788022568686\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 41: Train Loss = 0.5172441238180825\n",
      "Epoch 42: Train Loss = 0.5172240336824504\n",
      "Epoch 43: Train Loss = 0.5168874010589647\n",
      "Epoch 44: Train Loss = 0.5166855047880701\n",
      "Epoch 45: Train Loss = 0.5170303682397756\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5168698623789705\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 46: Train Loss = 0.516619828377891\n",
      "Epoch 47: Train Loss = 0.5165961817344809\n",
      "Epoch 48: Train Loss = 0.5165258958630549\n",
      "Epoch 49: Train Loss = 0.5161696511618494\n",
      "Epoch 50: Train Loss = 0.5160543923917246\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5161548942619418\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Epoch 51: Train Loss = 0.5158769259575957\n",
      "Epoch 52: Train Loss = 0.5159222080786103\n",
      "Epoch 53: Train Loss = 0.515578068109463\n",
      "Epoch 54: Train Loss = 0.5147455524411035\n",
      "Epoch 55: Train Loss = 0.5140689003180948\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4991095280498664\n",
      "Validation: Test Loss = 0.5145863442153455\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4991095280498664\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5125480790800342\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5026702269692924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dac9151b98422ab8ec66a9d4094798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6398336659131683\n",
      "Epoch 2: Train Loss = 0.6156218774278357\n",
      "Epoch 3: Train Loss = 0.5955474984401693\n",
      "Epoch 4: Train Loss = 0.5766888673774706\n",
      "Epoch 5: Train Loss = 0.5612911688145632\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5547019261286287\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5503083598581894\n",
      "Epoch 7: Train Loss = 0.5418097498898833\n",
      "Epoch 8: Train Loss = 0.5359380059972578\n",
      "Epoch 9: Train Loss = 0.5321767762210469\n",
      "Epoch 10: Train Loss = 0.5297930629680121\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5286490860414208\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 11: Train Loss = 0.5271114468415296\n",
      "Epoch 12: Train Loss = 0.5260294751195321\n",
      "Epoch 13: Train Loss = 0.5247166905674888\n",
      "Epoch 14: Train Loss = 0.523997646140288\n",
      "Epoch 15: Train Loss = 0.5235926678216787\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5232296696966074\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 16: Train Loss = 0.5227922322170498\n",
      "Epoch 17: Train Loss = 0.5220173992753985\n",
      "Epoch 18: Train Loss = 0.5220952932577736\n",
      "Epoch 19: Train Loss = 0.521775875310227\n",
      "Epoch 20: Train Loss = 0.5208211573552364\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5208587907088833\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 21: Train Loss = 0.5210307647315711\n",
      "Epoch 22: Train Loss = 0.5208354781805566\n",
      "Epoch 23: Train Loss = 0.5215042129437624\n",
      "Epoch 24: Train Loss = 0.5203064800582607\n",
      "Epoch 25: Train Loss = 0.5204730151173056\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5202019762377175\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 26: Train Loss = 0.5199900665722674\n",
      "Epoch 27: Train Loss = 0.520185554176171\n",
      "Epoch 28: Train Loss = 0.5193874791914718\n",
      "Epoch 29: Train Loss = 0.5197644082967022\n",
      "Epoch 30: Train Loss = 0.5195426618745045\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5194645930058385\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 31: Train Loss = 0.5200708846056556\n",
      "Epoch 32: Train Loss = 0.5193538101570795\n",
      "Epoch 33: Train Loss = 0.5191592441428907\n",
      "Epoch 34: Train Loss = 0.519244828345196\n",
      "Epoch 35: Train Loss = 0.5190158867984612\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5190011702895908\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 36: Train Loss = 0.5189354314351867\n",
      "Epoch 37: Train Loss = 0.5185004665930146\n",
      "Epoch 38: Train Loss = 0.5184092423741773\n",
      "Epoch 39: Train Loss = 0.5185724407142545\n",
      "Epoch 40: Train Loss = 0.5187502563105878\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5185121039354896\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 41: Train Loss = 0.5185702384738973\n",
      "Epoch 42: Train Loss = 0.5179621146548994\n",
      "Epoch 43: Train Loss = 0.5178696010543742\n",
      "Epoch 44: Train Loss = 0.5177569285844547\n",
      "Epoch 45: Train Loss = 0.5168174065762297\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.518291958028041\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 46: Train Loss = 0.5174138400295645\n",
      "Epoch 47: Train Loss = 0.5182814529615115\n",
      "Epoch 48: Train Loss = 0.5161353536514119\n",
      "Epoch 49: Train Loss = 0.5157654754625084\n",
      "Epoch 50: Train Loss = 0.515732036752565\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5157479760324116\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Epoch 51: Train Loss = 0.5150126865795438\n",
      "Epoch 52: Train Loss = 0.514634986780418\n",
      "Epoch 53: Train Loss = 0.5134745024191091\n",
      "Epoch 54: Train Loss = 0.5129951306304439\n",
      "Epoch 55: Train Loss = 0.5112558032982284\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.49621549421193234\n",
      "Validation: Test Loss = 0.5105685886801722\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.49621549421193234\n",
      "\n",
      "Training Finished at epoch 55.\n",
      "Validation: Test Loss = 0.5037740072675636\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.5113484646194927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51ddc995a9a4e7bb640123a264edee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6815277566255891\n",
      "Epoch 2: Train Loss = 0.6503381365022803\n",
      "Epoch 3: Train Loss = 0.6219070805361094\n",
      "Epoch 4: Train Loss = 0.5967367693768584\n",
      "Epoch 5: Train Loss = 0.5757771560048165\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5655864800392042\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.55859627016718\n",
      "Epoch 7: Train Loss = 0.5475539681323384\n",
      "Epoch 8: Train Loss = 0.5395449131116842\n",
      "Epoch 9: Train Loss = 0.5344213996205284\n",
      "Epoch 10: Train Loss = 0.5310958346031865\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5286980300000599\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 11: Train Loss = 0.5280796326066592\n",
      "Epoch 12: Train Loss = 0.5255616994065465\n",
      "Epoch 13: Train Loss = 0.5252544198414096\n",
      "Epoch 14: Train Loss = 0.5233016844742657\n",
      "Epoch 15: Train Loss = 0.523188332309706\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5221964594835908\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 16: Train Loss = 0.5217838925330213\n",
      "Epoch 17: Train Loss = 0.5218361538377917\n",
      "Epoch 18: Train Loss = 0.5214108318382358\n",
      "Epoch 19: Train Loss = 0.520363000361705\n",
      "Epoch 20: Train Loss = 0.520428618520684\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5202673249550409\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 21: Train Loss = 0.5204839579037864\n",
      "Epoch 22: Train Loss = 0.5199428813752593\n",
      "Epoch 23: Train Loss = 0.519181050313761\n",
      "Epoch 24: Train Loss = 0.5198009557630585\n",
      "Epoch 25: Train Loss = 0.5187806136249328\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.519034294443054\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 26: Train Loss = 0.5187837970861875\n",
      "Epoch 27: Train Loss = 0.5188580564590617\n",
      "Epoch 28: Train Loss = 0.5192910729511869\n",
      "Epoch 29: Train Loss = 0.5184428356687405\n",
      "Epoch 30: Train Loss = 0.518124229336889\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5189632569055735\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 31: Train Loss = 0.5183700917028256\n",
      "Epoch 32: Train Loss = 0.5181166178821349\n",
      "Epoch 33: Train Loss = 0.5178929873056318\n",
      "Epoch 34: Train Loss = 0.517320494299368\n",
      "Epoch 35: Train Loss = 0.5176987359540758\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5175156901165087\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 36: Train Loss = 0.5181196621243485\n",
      "Epoch 37: Train Loss = 0.5176832840204876\n",
      "Epoch 38: Train Loss = 0.5169852324500216\n",
      "Epoch 39: Train Loss = 0.5164135303331716\n",
      "Epoch 40: Train Loss = 0.5166317451138212\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.516668846829067\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 41: Train Loss = 0.515994904196273\n",
      "Epoch 42: Train Loss = 0.5158110916986066\n",
      "Epoch 43: Train Loss = 0.516606482298164\n",
      "Epoch 44: Train Loss = 0.5151973591780726\n",
      "Epoch 45: Train Loss = 0.5146563491010284\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.514617845004728\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 46: Train Loss = 0.5152328128487748\n",
      "Epoch 47: Train Loss = 0.5138668464637716\n",
      "Epoch 48: Train Loss = 0.5130179113080644\n",
      "Epoch 49: Train Loss = 0.5125153922970976\n",
      "Epoch 50: Train Loss = 0.5115315845420291\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.511260171103244\n",
      "Recall = 1.0, Aging Rate = 1.0, precision = 0.4977738201246661\n",
      "\n",
      "Epoch 51: Train Loss = 0.5101629398491164\n",
      "Epoch 52: Train Loss = 0.508754987830258\n",
      "Epoch 53: Train Loss = 0.5078168414261123\n",
      "Epoch 54: Train Loss = 0.5058986684199644\n",
      "Epoch 55: Train Loss = 0.5039195439057592\n",
      "Recall = 1.0, Aging Rate = 1.0, Precision = 0.4977738201246661\n",
      "Validation: Test Loss = 0.5031096802050794\n",
      "Recall = 1.0, Aging Rate = 0.9995547640249333, precision = 0.4979955456570156\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.5027029876558248\n",
      "Epoch 57: Train Loss = 0.5004506869074073\n",
      "Epoch 58: Train Loss = 0.49810042295303075\n",
      "Epoch 59: Train Loss = 0.496081040782368\n",
      "Epoch 60: Train Loss = 0.4938052167132404\n",
      "Recall = 0.9991055456171736, Aging Rate = 0.961487088156723, Precision = 0.5172493632785367\n",
      "Validation: Test Loss = 0.49226559275298487\n",
      "Recall = 0.9991055456171736, Aging Rate = 0.9585930543187889, precision = 0.5188109614491407\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.49147088137038136\n",
      "Epoch 62: Train Loss = 0.4897473589554495\n",
      "Epoch 63: Train Loss = 0.4874081828184459\n",
      "Epoch 64: Train Loss = 0.48497945165804\n",
      "Epoch 65: Train Loss = 0.4842851095611457\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.8998219056099733, Precision = 0.5507174666006928\n",
      "Validation: Test Loss = 0.4823432096071999\n",
      "Recall = 0.9968694096601073, Aging Rate = 0.8995992876224399, precision = 0.5515961395694136\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.481489788923323\n",
      "Epoch 67: Train Loss = 0.4803772757388817\n",
      "Epoch 68: Train Loss = 0.47892467790380294\n",
      "Epoch 69: Train Loss = 0.4777746558030163\n",
      "Epoch 70: Train Loss = 0.47692126255956807\n",
      "Recall = 0.9964221824686941, Aging Rate = 0.8635351736420303, Precision = 0.5743748388759989\n",
      "Validation: Test Loss = 0.47558901634793777\n",
      "Recall = 0.9964221824686941, Aging Rate = 0.8657613535173642, precision = 0.5728979172023656\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.47556741738680314\n",
      "Epoch 72: Train Loss = 0.47430829784749134\n",
      "Epoch 73: Train Loss = 0.4730713187799004\n",
      "Epoch 74: Train Loss = 0.47255227136909167\n",
      "Epoch 75: Train Loss = 0.4715513564440574\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8403829029385574, Precision = 0.5894039735099338\n",
      "Validation: Test Loss = 0.47052447347904247\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8388245770258237, precision = 0.5904989384288747\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.4703325941322748\n",
      "Epoch 77: Train Loss = 0.46987199066795626\n",
      "Epoch 78: Train Loss = 0.4690528388864105\n",
      "Epoch 79: Train Loss = 0.4684580528672423\n",
      "Epoch 80: Train Loss = 0.46780574642858225\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8317008014247551, Precision = 0.5952890792291221\n",
      "Validation: Test Loss = 0.46742700078820715\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8323686553873553, precision = 0.5948114469109388\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.4673139533079424\n",
      "Epoch 82: Train Loss = 0.4669106911307239\n",
      "Epoch 83: Train Loss = 0.4662921750587963\n",
      "Epoch 84: Train Loss = 0.4658964255083276\n",
      "Epoch 85: Train Loss = 0.46508661822771663\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8290293855743545, Precision = 0.5972073039742213\n",
      "Validation: Test Loss = 0.4648722805354814\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8270258236865539, precision = 0.5986541049798115\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.46532023669563016\n",
      "Epoch 87: Train Loss = 0.46475447484773924\n",
      "Epoch 88: Train Loss = 0.464514374096056\n",
      "Epoch 89: Train Loss = 0.4636956746304343\n",
      "Epoch 90: Train Loss = 0.46372230654832836\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8219056099732859, Precision = 0.6023835319609967\n",
      "Validation: Test Loss = 0.4638447260761091\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.8234639358860196, precision = 0.6017842660178426\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.4636041173049624\n",
      "Epoch 92: Train Loss = 0.4637102643931857\n",
      "Epoch 93: Train Loss = 0.46278924498299034\n",
      "Epoch 94: Train Loss = 0.4625767297439032\n",
      "Epoch 95: Train Loss = 0.46233360659088923\n",
      "Recall = 0.9941860465116279, Aging Rate = 0.8116651825467498, Precision = 0.6097092704333517\n",
      "Validation: Test Loss = 0.4615902306241641\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.8205699020480854, precision = 0.603906673901248\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.4618568088948886\n",
      "Epoch 97: Train Loss = 0.46152461060751576\n",
      "Epoch 98: Train Loss = 0.4611536168086136\n",
      "Epoch 99: Train Loss = 0.46095576932784815\n",
      "Epoch 100: Train Loss = 0.46099415543561306\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.8176758682101514, Precision = 0.6060441056357201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Test Loss = 0.46068942252271217\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.8147818343722173, precision = 0.6081967213114754\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.460535534555532\n",
      "Epoch 102: Train Loss = 0.46061499041313586\n",
      "Epoch 103: Train Loss = 0.46096437694446807\n",
      "Epoch 104: Train Loss = 0.45991995084105275\n",
      "Epoch 105: Train Loss = 0.46024274252826153\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8143365983971504, Precision = 0.6082558775287042\n",
      "Validation: Test Loss = 0.45975115231818\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8145592163846839, precision = 0.6080896419786826\n",
      "\n",
      "Epoch 106: Train Loss = 0.45930332611526425\n",
      "Epoch 107: Train Loss = 0.46003903948825486\n",
      "Epoch 108: Train Loss = 0.4605770198129058\n",
      "Epoch 109: Train Loss = 0.45931411219407486\n",
      "Epoch 110: Train Loss = 0.4589356744756469\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8081032947462155, Precision = 0.61267217630854\n",
      "Validation: Test Loss = 0.45878845102958987\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8078806767586821, precision = 0.6128410030311381\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.45858510211441844\n",
      "Epoch 112: Train Loss = 0.4586272920653529\n",
      "Epoch 113: Train Loss = 0.4588854213378733\n",
      "Epoch 114: Train Loss = 0.4584993367097469\n",
      "Epoch 115: Train Loss = 0.4582508687238447\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8067675868210151, Precision = 0.6139624724061811\n",
      "Validation: Test Loss = 0.45823284065415576\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8098842386464826, precision = 0.6115997800989554\n",
      "\n",
      "Epoch 116: Train Loss = 0.45881567695049547\n",
      "Epoch 117: Train Loss = 0.4582009036309573\n",
      "Epoch 118: Train Loss = 0.4579429086413855\n",
      "Epoch 119: Train Loss = 0.45778205566181207\n",
      "Epoch 120: Train Loss = 0.4580110822897985\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8065449688334817, Precision = 0.6141319348606128\n",
      "Validation: Test Loss = 0.4575039337282189\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8130008904719501, precision = 0.6092552026286966\n",
      "\n",
      "Epoch 121: Train Loss = 0.45794965646570535\n",
      "Epoch 122: Train Loss = 0.45757697019742627\n",
      "Epoch 123: Train Loss = 0.4576589402245923\n",
      "Epoch 124: Train Loss = 0.45735710617801706\n",
      "Epoch 125: Train Loss = 0.45741246740200214\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8141139804096171, Precision = 0.6084222039923435\n",
      "Validation: Test Loss = 0.4571560638776764\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.810106856634016, precision = 0.6117065127782357\n",
      "\n",
      "Epoch 126: Train Loss = 0.4572160744231828\n",
      "Epoch 127: Train Loss = 0.45759237545044\n",
      "Epoch 128: Train Loss = 0.45716235492766594\n",
      "Epoch 129: Train Loss = 0.4575082282614517\n",
      "Epoch 130: Train Loss = 0.4571641955859727\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.8045414069456812, Precision = 0.6153846153846154\n",
      "Validation: Test Loss = 0.4568768760665442\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8087711487088157, precision = 0.6124415083952656\n",
      "\n",
      "Epoch 131: Train Loss = 0.457112471678378\n",
      "Epoch 132: Train Loss = 0.45752834780133417\n",
      "Epoch 133: Train Loss = 0.4571490793862942\n",
      "Epoch 134: Train Loss = 0.4570698315080318\n",
      "Epoch 135: Train Loss = 0.45687099511031054\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8016473731077471, Precision = 0.6178839211330186\n",
      "Validation: Test Loss = 0.45692502263392704\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8032056990204809, precision = 0.6166851441241685\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.4567652287381309\n",
      "Epoch 137: Train Loss = 0.4570202961736562\n",
      "Epoch 138: Train Loss = 0.45724462210229755\n",
      "Epoch 139: Train Loss = 0.4568407275744665\n",
      "Epoch 140: Train Loss = 0.4567558733565618\n",
      "Recall = 0.9946332737030411, Aging Rate = 0.800979519145147, Precision = 0.6181211784324625\n",
      "Validation: Test Loss = 0.45618860844194303\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.803873552983081, precision = 0.6161728053170866\n",
      "\n",
      "Epoch 141: Train Loss = 0.4564613867103257\n",
      "Epoch 142: Train Loss = 0.45658103000448946\n",
      "Epoch 143: Train Loss = 0.45654804650312647\n",
      "Epoch 144: Train Loss = 0.4561126835889723\n",
      "Epoch 145: Train Loss = 0.4562125872100769\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.7978628673196795, Precision = 0.6208147321428571\n",
      "Validation: Test Loss = 0.45653145077604323\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8052092609082814, precision = 0.6151506773569256\n",
      "\n",
      "Epoch 146: Train Loss = 0.45630606395903167\n",
      "Epoch 147: Train Loss = 0.4565583415999544\n",
      "Epoch 148: Train Loss = 0.4559014206843406\n",
      "Epoch 149: Train Loss = 0.4561553863096959\n",
      "Epoch 150: Train Loss = 0.4560813129160517\n",
      "Recall = 0.9950805008944544, Aging Rate = 0.8018699910952805, Precision = 0.6177123820099945\n",
      "Validation: Test Loss = 0.45601408261862164\n",
      "Recall = 0.9955277280858676, Aging Rate = 0.8063223508459484, precision = 0.6145775814467145\n",
      "\n",
      "Validation: Test Loss = 0.45523900012467033\n",
      "Recall = 0.9920948616600791, Aging Rate = 0.8137516688918558, precision = 0.6177194421657096\n",
      "\u001b[32m[I 2022-05-26 16:25:02,261]\u001b[0m Trial 6 finished with value: 0.7023631852090277 and parameters: {'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 0.01, 'bad_weight': 0.8}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc98f301bf7343febea7b2077a9e6868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5365293149903536\n",
      "Epoch 2: Train Loss = 0.4666229281060311\n",
      "Epoch 3: Train Loss = 0.434268922967138\n",
      "Epoch 4: Train Loss = 0.41295535630237173\n",
      "Epoch 5: Train Loss = 0.4088295969916367\n",
      "Recall = 0.9583892617449664, Aging Rate = 0.575912733748887, Precision = 0.8279860842674913\n",
      "Validation: Test Loss = 0.3833647213498079\n",
      "Recall = 0.9834451901565996, Aging Rate = 0.5618878005342832, precision = 0.8708399366085579\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3786786193573783\n",
      "Epoch 7: Train Loss = 0.37057298273246203\n",
      "Epoch 8: Train Loss = 0.36737264545496723\n",
      "Epoch 9: Train Loss = 0.3635953586757555\n",
      "Epoch 10: Train Loss = 0.35744113430632934\n",
      "Recall = 0.9888143176733781, Aging Rate = 0.526046304541407, Precision = 0.935251798561151\n",
      "Validation: Test Loss = 0.3547760407678378\n",
      "Recall = 0.9955257270693513, Aging Rate = 0.5269367764915405, precision = 0.940008449514153\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.3591869896209781\n",
      "Epoch 12: Train Loss = 0.35572337791098085\n",
      "Epoch 13: Train Loss = 0.3552400477741408\n",
      "Epoch 14: Train Loss = 0.3536136342230803\n",
      "Epoch 15: Train Loss = 0.3541888493251716\n",
      "Recall = 0.9906040268456375, Aging Rate = 0.5211487088156723, Precision = 0.9457496796240923\n",
      "Validation: Test Loss = 0.34844588362947704\n",
      "Recall = 0.9968680089485459, Aging Rate = 0.5289403383793411, precision = 0.9377104377104377\n",
      "\n",
      "Epoch 16: Train Loss = 0.35688536905435614\n",
      "Epoch 17: Train Loss = 0.35051813506697926\n",
      "Epoch 18: Train Loss = 0.3497511707345822\n",
      "Epoch 19: Train Loss = 0.3531908318068656\n",
      "Epoch 20: Train Loss = 0.34824307598604864\n",
      "Recall = 0.9932885906040269, Aging Rate = 0.5182546749777382, Precision = 0.9536082474226805\n",
      "Validation: Test Loss = 0.34819002135035615\n",
      "Recall = 0.9995525727069351, Aging Rate = 0.530053428317008, precision = 0.9382612347753045\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.3489344970838151\n",
      "Epoch 22: Train Loss = 0.3485157233418255\n",
      "Epoch 23: Train Loss = 0.3488246707489527\n",
      "Epoch 24: Train Loss = 0.3500379176672717\n",
      "Epoch 25: Train Loss = 0.35303715224363713\n",
      "Recall = 0.9883668903803132, Aging Rate = 0.5204808548530722, Precision = 0.9448246364414029\n",
      "Validation: Test Loss = 0.348001261375254\n",
      "Recall = 0.9986577181208054, Aging Rate = 0.5347284060552092, precision = 0.929225645295587\n",
      "\n",
      "Epoch 26: Train Loss = 0.3521450151053265\n",
      "Epoch 27: Train Loss = 0.3489092122149701\n",
      "Epoch 28: Train Loss = 0.34907631319968274\n",
      "Epoch 29: Train Loss = 0.34898321016070466\n",
      "Epoch 30: Train Loss = 0.3510473155858574\n",
      "Recall = 0.9906040268456375, Aging Rate = 0.5200356188780053, Precision = 0.9477739726027398\n",
      "Validation: Test Loss = 0.3438326392392441\n",
      "Recall = 0.9955257270693513, Aging Rate = 0.5153606411398041, precision = 0.9611231101511879\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.34671568658239377\n",
      "Epoch 32: Train Loss = 0.34753993583489823\n",
      "Epoch 33: Train Loss = 0.3503413453286817\n",
      "Epoch 34: Train Loss = 0.3533250667479881\n",
      "Epoch 35: Train Loss = 0.34796701752491965\n",
      "Recall = 0.9910514541387024, Aging Rate = 0.5173642030276047, Precision = 0.9530981067125646\n",
      "Validation: Test Loss = 0.34743158239290745\n",
      "Recall = 0.9897091722595078, Aging Rate = 0.5008904719501336, precision = 0.9831111111111112\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.3467849426946784\n",
      "Epoch 37: Train Loss = 0.34921528753274694\n",
      "Epoch 38: Train Loss = 0.3512010439261728\n",
      "Epoch 39: Train Loss = 0.3462014001390282\n",
      "Epoch 40: Train Loss = 0.3497495546419495\n",
      "Recall = 0.992841163310962, Aging Rate = 0.5169189670525378, Precision = 0.9556416881998278\n",
      "Validation: Test Loss = 0.34343899949681833\n",
      "Recall = 0.9977628635346756, Aging Rate = 0.5182546749777382, precision = 0.9579037800687286\n",
      "\n",
      "Epoch 41: Train Loss = 0.3506477814589031\n",
      "Epoch 42: Train Loss = 0.347786728123523\n",
      "Epoch 43: Train Loss = 0.34664035477706184\n",
      "Epoch 44: Train Loss = 0.3478015467245878\n",
      "Epoch 45: Train Loss = 0.34913659756563653\n",
      "Recall = 0.9968680089485459, Aging Rate = 0.5218165627782725, Precision = 0.9505119453924915\n",
      "Validation: Test Loss = 0.3455982536263266\n",
      "Recall = 0.9870246085011186, Aging Rate = 0.5042297417631345, precision = 0.9739514348785872\n",
      "\n",
      "Epoch 46: Train Loss = 0.34988320784067745\n",
      "Epoch 47: Train Loss = 0.34673624323185065\n",
      "Epoch 48: Train Loss = 0.3485924355546386\n",
      "Epoch 49: Train Loss = 0.34791422137063416\n",
      "Epoch 50: Train Loss = 0.3483192260416087\n",
      "Recall = 0.9906040268456375, Aging Rate = 0.5142475512021372, Precision = 0.9584415584415584\n",
      "Validation: Test Loss = 0.3452623617882614\n",
      "Recall = 0.9968680089485459, Aging Rate = 0.5138023152270703, precision = 0.9653379549393414\n",
      "\n",
      "Epoch 51: Train Loss = 0.3474136189966465\n",
      "Epoch 52: Train Loss = 0.3472229462060563\n",
      "Epoch 53: Train Loss = 0.34891030152567987\n",
      "Epoch 54: Train Loss = 0.3469623970804953\n",
      "Epoch 55: Train Loss = 0.34903249226822347\n",
      "Recall = 0.9919463087248322, Aging Rate = 0.5195903829029386, Precision = 0.9498714652956298\n",
      "Validation: Test Loss = 0.35179074704381685\n",
      "Recall = 0.9856823266219239, Aging Rate = 0.5044523597506678, precision = 0.972197705207414\n",
      "\n",
      "Epoch 56: Train Loss = 0.35021465212664005\n",
      "Epoch 57: Train Loss = 0.3483914447117361\n",
      "Epoch 58: Train Loss = 0.3482646151833097\n",
      "Epoch 59: Train Loss = 0.34877257254754235\n",
      "Epoch 60: Train Loss = 0.34831869005095395\n",
      "Recall = 0.9932885906040269, Aging Rate = 0.5178094390026714, Precision = 0.9544282029234737\n",
      "Validation: Test Loss = 0.3433035819634517\n",
      "Recall = 1.0, Aging Rate = 0.5204808548530722, precision = 0.955945252352438\n",
      "\n",
      "Epoch 61: Train Loss = 0.3462648183930058\n",
      "Epoch 62: Train Loss = 0.3463738356067576\n",
      "Epoch 63: Train Loss = 0.34866104793145203\n",
      "Epoch 64: Train Loss = 0.34954461487190913\n",
      "Epoch 65: Train Loss = 0.34716108390930395\n",
      "Recall = 0.9950782997762864, Aging Rate = 0.5202582368655387, Precision = 0.9516474112109542\n",
      "Validation: Test Loss = 0.34381429924035006\n",
      "Recall = 0.9941834451901566, Aging Rate = 0.5055654496883348, precision = 0.9784236019374725\n",
      "\n",
      "Epoch 66: Train Loss = 0.34746259424374343\n",
      "Epoch 67: Train Loss = 0.3480056627244686\n",
      "Epoch 68: Train Loss = 0.3462688028016265\n",
      "Epoch 69: Train Loss = 0.346761521333472\n",
      "Epoch 70: Train Loss = 0.3465174468680354\n",
      "Recall = 0.992393736017897, Aging Rate = 0.5149154051647373, Precision = 0.9589277993947255\n",
      "Validation: Test Loss = 0.3473208173512351\n",
      "Recall = 1.0, Aging Rate = 0.5320569902048086, precision = 0.9351464435146444\n",
      "\n",
      "Epoch 71: Train Loss = 0.3478509844517135\n",
      "Epoch 72: Train Loss = 0.3471339829830007\n",
      "Epoch 73: Train Loss = 0.35442353483830197\n",
      "Epoch 74: Train Loss = 0.3479526895873799\n",
      "Epoch 75: Train Loss = 0.34792886375850157\n",
      "Recall = 0.9959731543624161, Aging Rate = 0.5200356188780053, Precision = 0.9529109589041096\n",
      "Validation: Test Loss = 0.3457162004173174\n",
      "Recall = 0.9995525727069351, Aging Rate = 0.5276046304541407, precision = 0.9426160337552743\n",
      "\n",
      "Epoch 76: Train Loss = 0.3500482179124973\n",
      "Epoch 77: Train Loss = 0.3461309159216027\n",
      "Epoch 78: Train Loss = 0.3455285816615114\n",
      "Epoch 79: Train Loss = 0.3461796408079611\n",
      "Epoch 80: Train Loss = 0.34504928195041307\n",
      "Recall = 0.9955257270693513, Aging Rate = 0.513579697239537, Precision = 0.964456003467707\n",
      "Validation: Test Loss = 0.3511068284086956\n",
      "Recall = 0.9995525727069351, Aging Rate = 0.5371772039180766, precision = 0.9258184832159138\n",
      "\n",
      "Epoch 81: Train Loss = 0.34870782935077976\n",
      "Epoch 82: Train Loss = 0.34611790942381454\n",
      "Epoch 83: Train Loss = 0.3454686374613353\n",
      "Epoch 84: Train Loss = 0.3493973154206416\n",
      "Epoch 85: Train Loss = 0.34479438056398054\n",
      "Recall = 0.9941834451901566, Aging Rate = 0.51246660730187, Precision = 0.9652476107732406\n",
      "Validation: Test Loss = 0.34003936371310733\n",
      "Recall = 0.9950782997762864, Aging Rate = 0.5066785396260017, precision = 0.9771528998242531\n",
      "\n",
      "Training Finished at epoch 85.\n",
      "Validation: Test Loss = 0.35383320224778514\n",
      "Recall = 0.9842105263157894, Aging Rate = 0.5226969292389854, precision = 0.9553001277139208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b889d3b9a4c444f4ac6417be99b235ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5542595663828608\n",
      "Epoch 2: Train Loss = 0.47494742921814787\n",
      "Epoch 3: Train Loss = 0.44285054299200843\n",
      "Epoch 4: Train Loss = 0.4192658521878539\n",
      "Epoch 5: Train Loss = 0.40855626821624097\n",
      "Recall = 0.9626168224299065, Aging Rate = 0.5785841495992876, Precision = 0.8322431704501732\n",
      "Validation: Test Loss = 0.3839609706285057\n",
      "Recall = 0.9835336003560302, Aging Rate = 0.5707925200356189, precision = 0.8619344773790951\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3870640507735849\n",
      "Epoch 7: Train Loss = 0.3685580148881605\n",
      "Epoch 8: Train Loss = 0.36791648232077023\n",
      "Epoch 9: Train Loss = 0.36074045937398874\n",
      "Epoch 10: Train Loss = 0.35614015415430283\n",
      "Recall = 0.9875389408099688, Aging Rate = 0.5287177203918076, Precision = 0.9343157894736842\n",
      "Validation: Test Loss = 0.3712687246362121\n",
      "Recall = 0.9991099243435692, Aging Rate = 0.5814781834372217, precision = 0.8594946401225115\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.35495646121453095\n",
      "Epoch 12: Train Loss = 0.3510603165637141\n",
      "Epoch 13: Train Loss = 0.35005931894586007\n",
      "Epoch 14: Train Loss = 0.35087524392938146\n",
      "Epoch 15: Train Loss = 0.3520876167612424\n",
      "Recall = 0.9897641299510458, Aging Rate = 0.5231522707034728, Precision = 0.9463829787234043\n",
      "Validation: Test Loss = 0.35162888978170903\n",
      "Recall = 0.9786381842456608, Aging Rate = 0.5020035618878005, precision = 0.9751662971175167\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.3555332216851327\n",
      "Epoch 17: Train Loss = 0.34853769718061467\n",
      "Epoch 18: Train Loss = 0.3486925991698237\n",
      "Epoch 19: Train Loss = 0.3497016223434455\n",
      "Epoch 20: Train Loss = 0.35045501210279795\n",
      "Recall = 0.9902091677792613, Aging Rate = 0.5207034728406055, Precision = 0.9512612227447628\n",
      "Validation: Test Loss = 0.34392464566952613\n",
      "Recall = 0.9977748108589231, Aging Rate = 0.5227070347284061, precision = 0.954855195911414\n",
      "\n",
      "Epoch 21: Train Loss = 0.3485821916498675\n",
      "Epoch 22: Train Loss = 0.3500497551338012\n",
      "Epoch 23: Train Loss = 0.34499042496443433\n",
      "Epoch 24: Train Loss = 0.34717034224092375\n",
      "Epoch 25: Train Loss = 0.34699463576264183\n",
      "Recall = 0.9924343569203382, Aging Rate = 0.518699910952805, Precision = 0.9570815450643777\n",
      "Validation: Test Loss = 0.34161981868616514\n",
      "Recall = 0.9951045838896306, Aging Rate = 0.5117987533392698, precision = 0.9725967812092214\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.34840013510079015\n",
      "Epoch 27: Train Loss = 0.3469402299379517\n",
      "Epoch 28: Train Loss = 0.3465260355364289\n",
      "Epoch 29: Train Loss = 0.34630634393526416\n",
      "Epoch 30: Train Loss = 0.34808561282824535\n",
      "Recall = 0.9928793947485536, Aging Rate = 0.5211487088156723, Precision = 0.9530115335326783\n",
      "Validation: Test Loss = 0.34125396938379177\n",
      "Recall = 0.9933244325767691, Aging Rate = 0.5089047195013358, precision = 0.9763779527559056\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.346960960597941\n",
      "Epoch 32: Train Loss = 0.34714523857444496\n",
      "Epoch 33: Train Loss = 0.34574044820249666\n",
      "Epoch 34: Train Loss = 0.3460913388886201\n",
      "Epoch 35: Train Loss = 0.34745039792859117\n",
      "Recall = 0.9933244325767691, Aging Rate = 0.5213713268032057, Precision = 0.9530315969257045\n",
      "Validation: Test Loss = 0.34031555587334605\n",
      "Recall = 0.9982198486871384, Aging Rate = 0.5180320569902048, precision = 0.9639020197679415\n",
      "\n",
      "Epoch 36: Train Loss = 0.35000286843770334\n",
      "Epoch 37: Train Loss = 0.3474675791664837\n",
      "Epoch 38: Train Loss = 0.3442681834353789\n",
      "Epoch 39: Train Loss = 0.34556242367144896\n",
      "Epoch 40: Train Loss = 0.3474629527739086\n",
      "Recall = 0.9942145082331998, Aging Rate = 0.5209260908281389, Precision = 0.9547008547008548\n",
      "Validation: Test Loss = 0.34005994777645476\n",
      "Recall = 0.9924343569203382, Aging Rate = 0.5111308993766697, precision = 0.9712543554006968\n",
      "\n",
      "Epoch 41: Train Loss = 0.34951668547819686\n",
      "Epoch 42: Train Loss = 0.3432083013911175\n",
      "Epoch 43: Train Loss = 0.34725756705709576\n",
      "Epoch 44: Train Loss = 0.34682935752299676\n",
      "Epoch 45: Train Loss = 0.3455741603461526\n",
      "Recall = 0.9942145082331998, Aging Rate = 0.5182546749777382, Precision = 0.9596219931271478\n",
      "Validation: Test Loss = 0.34036299456259234\n",
      "Recall = 0.9977748108589231, Aging Rate = 0.5162511130899377, precision = 0.9667960327727468\n",
      "\n",
      "Epoch 46: Train Loss = 0.34481421150167607\n",
      "Epoch 47: Train Loss = 0.34489466728956064\n",
      "Epoch 48: Train Loss = 0.3452921821565789\n",
      "Epoch 49: Train Loss = 0.3450036030047086\n",
      "Epoch 50: Train Loss = 0.34476516022283055\n",
      "Recall = 0.9951045838896306, Aging Rate = 0.5180320569902048, Precision = 0.9608938547486033\n",
      "Validation: Test Loss = 0.340474958888994\n",
      "Recall = 0.995549621717846, Aging Rate = 0.5089047195013358, precision = 0.9785651793525809\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.34776913968453105\n",
      "Epoch 52: Train Loss = 0.34366445151802055\n",
      "Epoch 53: Train Loss = 0.34401948491272283\n",
      "Epoch 54: Train Loss = 0.3432145152416807\n",
      "Epoch 55: Train Loss = 0.34507019978384834\n",
      "Recall = 0.9946595460614153, Aging Rate = 0.5155832591273375, Precision = 0.9650259067357513\n",
      "Validation: Test Loss = 0.34210441360384464\n",
      "Recall = 0.9991099243435692, Aging Rate = 0.5231522707034728, precision = 0.9553191489361702\n",
      "\n",
      "Epoch 56: Train Loss = 0.3462765681244705\n",
      "Epoch 57: Train Loss = 0.3453606559778045\n",
      "Epoch 58: Train Loss = 0.346421605093821\n",
      "Epoch 59: Train Loss = 0.3507507016164112\n",
      "Epoch 60: Train Loss = 0.34285353828729104\n",
      "Recall = 0.9942145082331998, Aging Rate = 0.5144701691896705, Precision = 0.9666810904370402\n",
      "Validation: Test Loss = 0.33863820412916895\n",
      "Recall = 0.9973297730307076, Aging Rate = 0.5149154051647373, precision = 0.9688715953307393\n",
      "\n",
      "Epoch 61: Train Loss = 0.34452283759257246\n",
      "Epoch 62: Train Loss = 0.3437838522801947\n",
      "Epoch 63: Train Loss = 0.344331520243829\n",
      "Epoch 64: Train Loss = 0.34342935718496465\n",
      "Epoch 65: Train Loss = 0.34559507881650303\n",
      "Recall = 0.9946595460614153, Aging Rate = 0.517586821015138, Precision = 0.9612903225806452\n",
      "Validation: Test Loss = 0.336363501083819\n",
      "Recall = 0.9973297730307076, Aging Rate = 0.5115761353517364, precision = 0.9751958224543081\n",
      "\n",
      "Epoch 66: Train Loss = 0.3439088651938621\n",
      "Epoch 67: Train Loss = 0.34558543125009916\n",
      "Epoch 68: Train Loss = 0.34653986503477086\n",
      "Epoch 69: Train Loss = 0.34511036936024525\n",
      "Epoch 70: Train Loss = 0.3449020162025734\n",
      "Recall = 0.9959946595460614, Aging Rate = 0.5195903829029386, Precision = 0.9588688946015425\n",
      "Validation: Test Loss = 0.34361290257633104\n",
      "Recall = 0.9888740542946151, Aging Rate = 0.5040071237756011, precision = 0.9814487632508834\n",
      "\n",
      "Epoch 71: Train Loss = 0.34528053589729146\n",
      "Epoch 72: Train Loss = 0.3436881246995629\n",
      "Epoch 73: Train Loss = 0.3426750282411159\n",
      "Epoch 74: Train Loss = 0.3431040131663597\n",
      "Epoch 75: Train Loss = 0.3480104175350651\n",
      "Recall = 0.9942145082331998, Aging Rate = 0.5184772929652716, Precision = 0.9592099613568055\n",
      "Validation: Test Loss = 0.33826037126784864\n",
      "Recall = 0.9968847352024922, Aging Rate = 0.5126892252894034, precision = 0.9726443768996961\n",
      "\n",
      "Epoch 76: Train Loss = 0.3440719069324215\n",
      "Epoch 77: Train Loss = 0.3434889312578118\n",
      "Epoch 78: Train Loss = 0.3443470265805881\n",
      "Epoch 79: Train Loss = 0.3439991735817594\n",
      "Epoch 80: Train Loss = 0.34988043436702615\n",
      "Recall = 0.9928793947485536, Aging Rate = 0.5227070347284061, Precision = 0.9501703577512777\n",
      "Validation: Test Loss = 0.34205203356747105\n",
      "Recall = 0.9902091677792613, Aging Rate = 0.5071237756010686, precision = 0.9767339771729587\n",
      "\n",
      "Epoch 81: Train Loss = 0.3448974938129381\n",
      "Epoch 82: Train Loss = 0.3431315717162135\n",
      "Epoch 83: Train Loss = 0.3430035709804015\n",
      "Epoch 84: Train Loss = 0.34618286447448593\n",
      "Epoch 85: Train Loss = 0.3435975945324953\n",
      "Recall = 0.9964396973742768, Aging Rate = 0.5153606411398041, Precision = 0.967170626349892\n",
      "Validation: Test Loss = 0.3391819308617236\n",
      "Recall = 0.9991099243435692, Aging Rate = 0.5155832591273375, precision = 0.9693436960276338\n",
      "\n",
      "Epoch 86: Train Loss = 0.3431571173487448\n",
      "Epoch 87: Train Loss = 0.3435159679883734\n",
      "Epoch 88: Train Loss = 0.34538567275843873\n",
      "Epoch 89: Train Loss = 0.3431926271210583\n",
      "Epoch 90: Train Loss = 0.3447522754023782\n",
      "Recall = 0.9964396973742768, Aging Rate = 0.517586821015138, Precision = 0.963010752688172\n",
      "Validation: Test Loss = 0.3415605817537486\n",
      "Recall = 0.9995549621717846, Aging Rate = 0.5218165627782725, precision = 0.9581911262798635\n",
      "\n",
      "Epoch 91: Train Loss = 0.3437850368627988\n",
      "Epoch 92: Train Loss = 0.34785680280449344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Train Loss = 0.3434881824047992\n",
      "Epoch 94: Train Loss = 0.34224984278024995\n",
      "Epoch 95: Train Loss = 0.34221105549554154\n",
      "Recall = 0.9951045838896306, Aging Rate = 0.5144701691896705, Precision = 0.9675465166594548\n",
      "Validation: Test Loss = 0.34006651195690024\n",
      "Recall = 0.9977748108589231, Aging Rate = 0.5111308993766697, precision = 0.9764808362369338\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.34245107490784976\n",
      "Epoch 97: Train Loss = 0.3441767345181768\n",
      "Epoch 98: Train Loss = 0.34370545054373736\n",
      "Epoch 99: Train Loss = 0.3441559337444424\n",
      "Epoch 100: Train Loss = 0.34395020656148345\n",
      "Recall = 0.9937694704049844, Aging Rate = 0.5138023152270703, Precision = 0.9675043327556326\n",
      "Validation: Test Loss = 0.34066531874193\n",
      "Recall = 0.9951045838896306, Aging Rate = 0.5138023152270703, precision = 0.9688041594454073\n",
      "\n",
      "Epoch 101: Train Loss = 0.3501671292584386\n",
      "Epoch 102: Train Loss = 0.3479961857451781\n",
      "Epoch 103: Train Loss = 0.34342490002605813\n",
      "Epoch 104: Train Loss = 0.34350747459187425\n",
      "Epoch 105: Train Loss = 0.3450659498081394\n",
      "Recall = 0.9951045838896306, Aging Rate = 0.5164737310774711, Precision = 0.9637931034482758\n",
      "Validation: Test Loss = 0.33967036592673744\n",
      "Recall = 0.9986648865153538, Aging Rate = 0.5182546749777382, precision = 0.9639175257731959\n",
      "\n",
      "Epoch 106: Train Loss = 0.3439049316323026\n",
      "Epoch 107: Train Loss = 0.34300680385565824\n",
      "Epoch 108: Train Loss = 0.34169516837819813\n",
      "Epoch 109: Train Loss = 0.34474246786411383\n",
      "Epoch 110: Train Loss = 0.34675821338183943\n",
      "Recall = 0.9919893190921228, Aging Rate = 0.5169189670525378, Precision = 0.9599483204134367\n",
      "Validation: Test Loss = 0.34654879232973046\n",
      "Recall = 0.9986648865153538, Aging Rate = 0.530053428317008, precision = 0.9424611507769844\n",
      "\n",
      "Epoch 111: Train Loss = 0.3433120229206017\n",
      "Epoch 112: Train Loss = 0.34275218263228663\n",
      "Epoch 113: Train Loss = 0.34410810345851833\n",
      "Epoch 114: Train Loss = 0.34343087309614845\n",
      "Epoch 115: Train Loss = 0.34620572150231255\n",
      "Recall = 0.9919893190921228, Aging Rate = 0.5164737310774711, Precision = 0.9607758620689655\n",
      "Validation: Test Loss = 0.3413520308604542\n",
      "Recall = 0.9995549621717846, Aging Rate = 0.5229296527159395, precision = 0.9561515538527032\n",
      "\n",
      "Epoch 116: Train Loss = 0.3437529747388454\n",
      "Epoch 117: Train Loss = 0.34576352687680284\n",
      "Epoch 118: Train Loss = 0.3425617618477992\n",
      "Epoch 119: Train Loss = 0.3442622513932409\n",
      "Epoch 120: Train Loss = 0.34747188988797706\n",
      "Recall = 0.9928793947485536, Aging Rate = 0.5202582368655387, Precision = 0.9546427043217801\n",
      "Validation: Test Loss = 0.3403728940820227\n",
      "Recall = 0.9991099243435692, Aging Rate = 0.5191451469278717, precision = 0.9626929674099486\n",
      "\n",
      "Epoch 121: Train Loss = 0.34811036388256245\n",
      "Epoch 122: Train Loss = 0.3433475948982969\n",
      "Epoch 123: Train Loss = 0.3430954767250102\n",
      "Epoch 124: Train Loss = 0.34373626192552326\n",
      "Epoch 125: Train Loss = 0.34432687699953995\n",
      "Recall = 0.9964396973742768, Aging Rate = 0.517586821015138, Precision = 0.963010752688172\n",
      "Validation: Test Loss = 0.3437758926451472\n",
      "Recall = 0.991099243435692, Aging Rate = 0.5048975957257347, precision = 0.9819223985890653\n",
      "\n",
      "Epoch 126: Train Loss = 0.34422809344684024\n",
      "Epoch 127: Train Loss = 0.3439915225055742\n",
      "Epoch 128: Train Loss = 0.342808468365181\n",
      "Epoch 129: Train Loss = 0.3453000982956279\n",
      "Epoch 130: Train Loss = 0.3459808287987832\n",
      "Recall = 0.9937694704049844, Aging Rate = 0.5178094390026714, Precision = 0.9600171969045572\n",
      "Validation: Test Loss = 0.33908647412926834\n",
      "Recall = 0.9973297730307076, Aging Rate = 0.5142475512021372, precision = 0.9701298701298702\n",
      "\n",
      "Epoch 131: Train Loss = 0.3433248976565639\n",
      "Epoch 132: Train Loss = 0.3434534658837934\n",
      "Epoch 133: Train Loss = 0.34458877237800606\n",
      "Epoch 134: Train Loss = 0.34246486637704837\n",
      "Epoch 135: Train Loss = 0.3432745551118231\n",
      "Recall = 0.995549621717846, Aging Rate = 0.5142475512021372, Precision = 0.9683982683982684\n",
      "Validation: Test Loss = 0.3407457418082977\n",
      "Recall = 0.9982198486871384, Aging Rate = 0.5178094390026714, precision = 0.9643164230438521\n",
      "\n",
      "Epoch 136: Train Loss = 0.34435993836378265\n",
      "Epoch 137: Train Loss = 0.3431282193662859\n",
      "Epoch 138: Train Loss = 0.3434788423091943\n",
      "Epoch 139: Train Loss = 0.34493976274135385\n",
      "Epoch 140: Train Loss = 0.34423423338870546\n",
      "Recall = 0.9946595460614153, Aging Rate = 0.5166963490650045, Precision = 0.9629470056010341\n",
      "Validation: Test Loss = 0.3432052739740798\n",
      "Recall = 0.9977748108589231, Aging Rate = 0.5184772929652716, precision = 0.9626449119793903\n",
      "\n",
      "Epoch 141: Train Loss = 0.3424849034364162\n",
      "Epoch 142: Train Loss = 0.34352878102317835\n",
      "Epoch 143: Train Loss = 0.34499026626640417\n",
      "Epoch 144: Train Loss = 0.3434770314266718\n",
      "Epoch 145: Train Loss = 0.34454428499443446\n",
      "Recall = 0.9942145082331998, Aging Rate = 0.5160284951024042, Precision = 0.9637618636755824\n",
      "Validation: Test Loss = 0.34229466836577743\n",
      "Recall = 0.995549621717846, Aging Rate = 0.5122439893143366, precision = 0.9721860060843112\n",
      "\n",
      "Training Finished at epoch 145.\n",
      "Validation: Test Loss = 0.368201115619993\n",
      "Recall = 0.9705882352941176, Aging Rate = 0.5240320427236315, precision = 0.9248407643312102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f833dd8e873e4d5798d9e12a71876b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5638780082747645\n",
      "Epoch 2: Train Loss = 0.4799644634167424\n",
      "Epoch 3: Train Loss = 0.44633764134701726\n",
      "Epoch 4: Train Loss = 0.4241545933861873\n",
      "Epoch 5: Train Loss = 0.41165101883778693\n",
      "Recall = 0.9642857142857143, Aging Rate = 0.5948352626892253, Precision = 0.8184880239520959\n",
      "Validation: Test Loss = 0.3869753769223221\n",
      "Recall = 0.9700176366843033, Aging Rate = 0.5547640249332146, precision = 0.8828250401284109\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3892845412979992\n",
      "Epoch 7: Train Loss = 0.37882327603529525\n",
      "Epoch 8: Train Loss = 0.37609679629414183\n",
      "Epoch 9: Train Loss = 0.3636430881167774\n",
      "Epoch 10: Train Loss = 0.36446490108913754\n",
      "Recall = 0.9841269841269841, Aging Rate = 0.5445235975066786, Precision = 0.9125102207686018\n",
      "Validation: Test Loss = 0.37504753650880135\n",
      "Recall = 0.9581128747795414, Aging Rate = 0.5155832591273375, precision = 0.9382556131260794\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.3605929729615803\n",
      "Epoch 12: Train Loss = 0.3571214040580437\n",
      "Epoch 13: Train Loss = 0.35377737879753113\n",
      "Epoch 14: Train Loss = 0.3540325173817887\n",
      "Epoch 15: Train Loss = 0.35303812964304365\n",
      "Recall = 0.9898589065255732, Aging Rate = 0.5291629563668745, Precision = 0.9444678165755154\n",
      "Validation: Test Loss = 0.3473563937342602\n",
      "Recall = 0.9982363315696648, Aging Rate = 0.5356188780053428, precision = 0.940980881130507\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.35276083240844475\n",
      "Epoch 17: Train Loss = 0.3508782783524861\n",
      "Epoch 18: Train Loss = 0.3499184407141202\n",
      "Epoch 19: Train Loss = 0.3501444493991821\n",
      "Epoch 20: Train Loss = 0.3544846118451863\n",
      "Recall = 0.9920634920634921, Aging Rate = 0.5345057880676759, Precision = 0.9371095376926281\n",
      "Validation: Test Loss = 0.34618955041188165\n",
      "Recall = 0.9876543209876543, Aging Rate = 0.5115761353517364, precision = 0.9747606614447345\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.35127459459822835\n",
      "Epoch 22: Train Loss = 0.3528389170672993\n",
      "Epoch 23: Train Loss = 0.35136236718798575\n",
      "Epoch 24: Train Loss = 0.35008904751877856\n",
      "Epoch 25: Train Loss = 0.34876945346779625\n",
      "Recall = 0.9925044091710759, Aging Rate = 0.5264915405164737, Precision = 0.9517970401691332\n",
      "Validation: Test Loss = 0.3455951908074207\n",
      "Recall = 0.9982363315696648, Aging Rate = 0.5280498664292075, precision = 0.954468802698145\n",
      "\n",
      "Epoch 26: Train Loss = 0.34945986054353384\n",
      "Epoch 27: Train Loss = 0.3485178172535272\n",
      "Epoch 28: Train Loss = 0.3496421713809933\n",
      "Epoch 29: Train Loss = 0.3487216676597816\n",
      "Epoch 30: Train Loss = 0.3491981823796793\n",
      "Recall = 0.9925044091710759, Aging Rate = 0.5262689225289403, Precision = 0.9521996615905245\n",
      "Validation: Test Loss = 0.35592335374994355\n",
      "Recall = 0.9986772486772487, Aging Rate = 0.554986642920748, precision = 0.9085439229843562\n",
      "\n",
      "Epoch 31: Train Loss = 0.34879552675800884\n",
      "Epoch 32: Train Loss = 0.3473666593472234\n",
      "Epoch 33: Train Loss = 0.34999362913392423\n",
      "Epoch 34: Train Loss = 0.35531544831323497\n",
      "Epoch 35: Train Loss = 0.34695949975338136\n",
      "Recall = 0.9933862433862434, Aging Rate = 0.5244879786286732, Precision = 0.9562818336162988\n",
      "Validation: Test Loss = 0.34472823294272725\n",
      "Recall = 0.9995590828924162, Aging Rate = 0.5347284060552092, precision = 0.943796835970025\n",
      "\n",
      "Epoch 36: Train Loss = 0.3472313560339774\n",
      "Epoch 37: Train Loss = 0.3468344607792682\n",
      "Epoch 38: Train Loss = 0.3519283572107368\n",
      "Epoch 39: Train Loss = 0.3462957690734473\n",
      "Epoch 40: Train Loss = 0.3495506778531911\n",
      "Recall = 0.9902998236331569, Aging Rate = 0.5247105966162066, Precision = 0.9529062367416207\n",
      "Validation: Test Loss = 0.34213959018990914\n",
      "Recall = 0.9977954144620811, Aging Rate = 0.5251558325912734, precision = 0.9593047901653243\n",
      "\n",
      "Epoch 41: Train Loss = 0.3449937027156725\n",
      "Epoch 42: Train Loss = 0.3470395598577158\n",
      "Epoch 43: Train Loss = 0.34582544700014517\n",
      "Epoch 44: Train Loss = 0.349295383702403\n",
      "Epoch 45: Train Loss = 0.3454125247039863\n",
      "Recall = 0.9947089947089947, Aging Rate = 0.5247105966162066, Precision = 0.9571489181162495\n",
      "Validation: Test Loss = 0.3430517390171333\n",
      "Recall = 0.9964726631393298, Aging Rate = 0.5182546749777382, precision = 0.9707903780068728\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.34740638138456\n",
      "Epoch 47: Train Loss = 0.35148673488004656\n",
      "Epoch 48: Train Loss = 0.3462566506724642\n",
      "Epoch 49: Train Loss = 0.3494239580334029\n",
      "Epoch 50: Train Loss = 0.34573097658390883\n",
      "Recall = 0.9942680776014109, Aging Rate = 0.5233748886910062, Precision = 0.9591663122075712\n",
      "Validation: Test Loss = 0.341903592233454\n",
      "Recall = 0.9977954144620811, Aging Rate = 0.5253784505788067, precision = 0.9588983050847457\n",
      "\n",
      "Epoch 51: Train Loss = 0.34571706293314985\n",
      "Epoch 52: Train Loss = 0.3465999825554879\n",
      "Epoch 53: Train Loss = 0.3479025375598897\n",
      "Epoch 54: Train Loss = 0.34744536815640764\n",
      "Epoch 55: Train Loss = 0.3472299854721433\n",
      "Recall = 0.9955908289241623, Aging Rate = 0.5258236865538736, Precision = 0.955969517358171\n",
      "Validation: Test Loss = 0.3426604144878931\n",
      "Recall = 0.9995590828924162, Aging Rate = 0.5276046304541407, precision = 0.9565400843881856\n",
      "\n",
      "Epoch 56: Train Loss = 0.34665012157929337\n",
      "Epoch 57: Train Loss = 0.34547662631486636\n",
      "Epoch 58: Train Loss = 0.3525201197405108\n",
      "Epoch 59: Train Loss = 0.34737322750838867\n",
      "Epoch 60: Train Loss = 0.3440018552865498\n",
      "Recall = 0.9969135802469136, Aging Rate = 0.5242653606411398, Precision = 0.9600849256900212\n",
      "Validation: Test Loss = 0.3449204687762239\n",
      "Recall = 0.9938271604938271, Aging Rate = 0.5182546749777382, precision = 0.968213058419244\n",
      "\n",
      "Epoch 61: Train Loss = 0.3481244699613175\n",
      "Epoch 62: Train Loss = 0.34455275490468673\n",
      "Epoch 63: Train Loss = 0.3456828107233039\n",
      "Epoch 64: Train Loss = 0.34743008288228927\n",
      "Epoch 65: Train Loss = 0.34604294209318937\n",
      "Recall = 0.996031746031746, Aging Rate = 0.526046304541407, Precision = 0.9559881506559458\n",
      "Validation: Test Loss = 0.36135051108213373\n",
      "Recall = 0.9753086419753086, Aging Rate = 0.5017809439002672, precision = 0.9813664596273292\n",
      "\n",
      "Epoch 66: Train Loss = 0.35225251983131345\n",
      "Epoch 67: Train Loss = 0.3518103763447844\n",
      "Epoch 68: Train Loss = 0.3505550169403385\n",
      "Epoch 69: Train Loss = 0.34544934745038713\n",
      "Epoch 70: Train Loss = 0.3472703927240516\n",
      "Recall = 0.9920634920634921, Aging Rate = 0.5211487088156723, Precision = 0.9611277231952157\n",
      "Validation: Test Loss = 0.3401761791691224\n",
      "Recall = 0.9991181657848325, Aging Rate = 0.523820124666073, precision = 0.9630259243518912\n",
      "\n",
      "Epoch 71: Train Loss = 0.3443444947366298\n",
      "Epoch 72: Train Loss = 0.34743036928181126\n",
      "Epoch 73: Train Loss = 0.34994264219025895\n",
      "Epoch 74: Train Loss = 0.3473630280815271\n",
      "Epoch 75: Train Loss = 0.3466855447046054\n",
      "Recall = 0.9955908289241623, Aging Rate = 0.5258236865538736, Precision = 0.955969517358171\n",
      "Validation: Test Loss = 0.34979923184917955\n",
      "Recall = 0.9995590828924162, Aging Rate = 0.5434105075690115, precision = 0.9287177386317084\n",
      "\n",
      "Epoch 76: Train Loss = 0.34874867501263096\n",
      "Epoch 77: Train Loss = 0.3451526937002809\n",
      "Epoch 78: Train Loss = 0.3467835637733327\n",
      "Epoch 79: Train Loss = 0.3460538903465254\n",
      "Epoch 80: Train Loss = 0.344913262426057\n",
      "Recall = 0.9942680776014109, Aging Rate = 0.5231522707034728, Precision = 0.9595744680851064\n",
      "Validation: Test Loss = 0.3413238996707852\n",
      "Recall = 0.9986772486772487, Aging Rate = 0.5195903829029386, precision = 0.9704370179948586\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.34758785401511594\n",
      "Epoch 82: Train Loss = 0.3492363684960804\n",
      "Epoch 83: Train Loss = 0.34735947850020144\n",
      "Epoch 84: Train Loss = 0.3450379084765858\n",
      "Epoch 85: Train Loss = 0.34434002990820317\n",
      "Recall = 0.9955908289241623, Aging Rate = 0.5224844167408726, Precision = 0.962079250106519\n",
      "Validation: Test Loss = 0.34385523712221044\n",
      "Recall = 0.9969135802469136, Aging Rate = 0.5224844167408726, precision = 0.9633574776310183\n",
      "\n",
      "Epoch 86: Train Loss = 0.3526571537380439\n",
      "Epoch 87: Train Loss = 0.3464957191173455\n",
      "Epoch 88: Train Loss = 0.34520226852339714\n",
      "Epoch 89: Train Loss = 0.3452879086946231\n",
      "Epoch 90: Train Loss = 0.3466734727206022\n",
      "Recall = 0.9920634920634921, Aging Rate = 0.5235975066785397, Precision = 0.9566326530612245\n",
      "Validation: Test Loss = 0.3423503579460715\n",
      "Recall = 0.9925044091710759, Aging Rate = 0.5122439893143366, precision = 0.9782703172533681\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.34535625348745025\n",
      "Epoch 92: Train Loss = 0.35126623487430075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Train Loss = 0.346200076549688\n",
      "Epoch 94: Train Loss = 0.34491460894434345\n",
      "Epoch 95: Train Loss = 0.3476584913575639\n",
      "Recall = 0.9933862433862434, Aging Rate = 0.5253784505788067, Precision = 0.9546610169491525\n",
      "Validation: Test Loss = 0.3402209941128589\n",
      "Recall = 0.9973544973544973, Aging Rate = 0.5173642030276047, precision = 0.9733218588640276\n",
      "\n",
      "Epoch 96: Train Loss = 0.3445361222099855\n",
      "Epoch 97: Train Loss = 0.34727753931034494\n",
      "Epoch 98: Train Loss = 0.3437383737770023\n",
      "Epoch 99: Train Loss = 0.34459435815059597\n",
      "Epoch 100: Train Loss = 0.3475092710762924\n",
      "Recall = 0.9916225749559083, Aging Rate = 0.523820124666073, Precision = 0.9558011049723757\n",
      "Validation: Test Loss = 0.3417300856070973\n",
      "Recall = 0.9964726631393298, Aging Rate = 0.5229296527159395, precision = 0.9621115368241805\n",
      "\n",
      "Epoch 101: Train Loss = 0.34696159207279514\n",
      "Epoch 102: Train Loss = 0.3484883372367543\n",
      "Epoch 103: Train Loss = 0.3458051652591991\n",
      "Epoch 104: Train Loss = 0.3477672893663442\n",
      "Epoch 105: Train Loss = 0.34497361868381077\n",
      "Recall = 0.9951499118165785, Aging Rate = 0.5244879786286732, Precision = 0.9579796264855688\n",
      "Validation: Test Loss = 0.3402277195920715\n",
      "Recall = 0.9986772486772487, Aging Rate = 0.5229296527159395, precision = 0.9642401021711366\n",
      "\n",
      "Epoch 106: Train Loss = 0.3450793135622624\n",
      "Epoch 107: Train Loss = 0.34458111710879596\n",
      "Epoch 108: Train Loss = 0.34839640797936056\n",
      "Epoch 109: Train Loss = 0.3469312085653137\n",
      "Epoch 110: Train Loss = 0.34445087642194117\n",
      "Recall = 0.9947089947089947, Aging Rate = 0.5220391807658059, Precision = 0.962046908315565\n",
      "Validation: Test Loss = 0.34200591432761634\n",
      "Recall = 0.9986772486772487, Aging Rate = 0.5258236865538736, precision = 0.9589331075359865\n",
      "\n",
      "Epoch 111: Train Loss = 0.34824719143996574\n",
      "Epoch 112: Train Loss = 0.3448455866860791\n",
      "Epoch 113: Train Loss = 0.34911107772816957\n",
      "Epoch 114: Train Loss = 0.34615704762224414\n",
      "Epoch 115: Train Loss = 0.346314571340065\n",
      "Recall = 0.9938271604938271, Aging Rate = 0.5233748886910062, Precision = 0.9587409612930667\n",
      "Validation: Test Loss = 0.3481516252907917\n",
      "Recall = 1.0, Aging Rate = 0.5422974176313446, precision = 0.9310344827586207\n",
      "\n",
      "Epoch 116: Train Loss = 0.35103609489205684\n",
      "Epoch 117: Train Loss = 0.34654341058230037\n",
      "Epoch 118: Train Loss = 0.3444532234012709\n",
      "Epoch 119: Train Loss = 0.34690708033441964\n",
      "Epoch 120: Train Loss = 0.34579432992561526\n",
      "Recall = 0.9933862433862434, Aging Rate = 0.5240427426536064, Precision = 0.9570943075615973\n",
      "Validation: Test Loss = 0.34292092054206563\n",
      "Recall = 0.9907407407407407, Aging Rate = 0.5093499554764025, precision = 0.9820804195804196\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.3450946902071697\n",
      "Epoch 122: Train Loss = 0.34523043819335775\n",
      "Epoch 123: Train Loss = 0.34532572510830545\n",
      "Epoch 124: Train Loss = 0.34913586872237556\n",
      "Epoch 125: Train Loss = 0.34729356483912105\n",
      "Recall = 0.9942680776014109, Aging Rate = 0.5235975066785397, Precision = 0.9587585034013606\n",
      "Validation: Test Loss = 0.3420928563534523\n",
      "Recall = 0.996031746031746, Aging Rate = 0.5191451469278717, precision = 0.9686963979416809\n",
      "\n",
      "Epoch 126: Train Loss = 0.3462178539401808\n",
      "Epoch 127: Train Loss = 0.3462995879106615\n",
      "Epoch 128: Train Loss = 0.3449458206007763\n",
      "Epoch 129: Train Loss = 0.3467714273812828\n",
      "Epoch 130: Train Loss = 0.34379995049691475\n",
      "Recall = 0.9955908289241623, Aging Rate = 0.5209260908281389, Precision = 0.964957264957265\n",
      "Validation: Test Loss = 0.3455985421223611\n",
      "Recall = 0.9977954144620811, Aging Rate = 0.5338379341050757, precision = 0.9437030859049208\n",
      "\n",
      "Epoch 131: Train Loss = 0.3442613340496274\n",
      "Epoch 132: Train Loss = 0.34555206700615443\n",
      "Epoch 133: Train Loss = 0.3462621884641112\n",
      "Epoch 134: Train Loss = 0.34562988516908616\n",
      "Epoch 135: Train Loss = 0.34927047233122965\n",
      "Recall = 0.9925044091710759, Aging Rate = 0.5273820124666073, Precision = 0.9501899535669058\n",
      "Validation: Test Loss = 0.354251959657202\n",
      "Recall = 0.9770723104056437, Aging Rate = 0.5024487978628673, precision = 0.9818342933097032\n",
      "\n",
      "Epoch 136: Train Loss = 0.34815726270658676\n",
      "Epoch 137: Train Loss = 0.3456635379886797\n",
      "Epoch 138: Train Loss = 0.34346984545779036\n",
      "Epoch 139: Train Loss = 0.3465266581210088\n",
      "Epoch 140: Train Loss = 0.3467129624431301\n",
      "Recall = 0.9942680776014109, Aging Rate = 0.5251558325912734, Precision = 0.9559135226791013\n",
      "Validation: Test Loss = 0.340980542390769\n",
      "Recall = 0.9947089947089947, Aging Rate = 0.5169189670525378, precision = 0.9715762273901809\n",
      "\n",
      "Epoch 141: Train Loss = 0.3449892173467315\n",
      "Epoch 142: Train Loss = 0.34806383176244804\n",
      "Epoch 143: Train Loss = 0.3442186634665818\n",
      "Epoch 144: Train Loss = 0.3443452561421789\n",
      "Epoch 145: Train Loss = 0.3459881646550349\n",
      "Recall = 0.996031746031746, Aging Rate = 0.52493321460374, Precision = 0.9580152671755725\n",
      "Validation: Test Loss = 0.3410729017251321\n",
      "Recall = 0.9991181657848325, Aging Rate = 0.5218165627782725, precision = 0.9667235494880546\n",
      "\n",
      "Epoch 146: Train Loss = 0.3456537552635689\n",
      "Epoch 147: Train Loss = 0.34640659642665383\n",
      "Epoch 148: Train Loss = 0.34542650682312614\n",
      "Epoch 149: Train Loss = 0.34890462650535153\n",
      "Epoch 150: Train Loss = 0.3467108459209398\n",
      "Recall = 0.9942680776014109, Aging Rate = 0.526046304541407, Precision = 0.954295387219636\n",
      "Validation: Test Loss = 0.34009353411802734\n",
      "Recall = 0.9955908289241623, Aging Rate = 0.5153606411398041, precision = 0.975377969762419\n",
      "\n",
      "Validation: Test Loss = 0.35646776335739166\n",
      "Recall = 0.9738651994497937, Aging Rate = 0.4939919893190921, precision = 0.9567567567567568\n",
      "\u001b[32m[I 2022-05-26 16:25:44,383]\u001b[0m Trial 7 finished with value: 0.9606458192864071 and parameters: {'batch_size': 64, 'learning_rate': 0.01, 'weight_decay': 0.001, 'bad_weight': 0.7}. Best is trial 1 with value: 0.97844849003899.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d545c6dcf6d84262916d1882d508a71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6603423383017257\n",
      "Epoch 2: Train Loss = 0.5957662996073865\n",
      "Epoch 3: Train Loss = 0.5541844836909752\n",
      "Epoch 4: Train Loss = 0.53875514830422\n",
      "Epoch 5: Train Loss = 0.5315851984232107\n",
      "Recall = 0.8897389738973898, Aging Rate = 0.6077471059661621, Precision = 0.7241758241758242\n",
      "Validation: Test Loss = 0.5243144019947239\n",
      "Recall = 0.9014401440144014, Aging Rate = 0.6101959038290294, precision = 0.7307551988325429\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.5258701373876893\n",
      "Epoch 7: Train Loss = 0.5226336032921782\n",
      "Epoch 8: Train Loss = 0.5190778477639889\n",
      "Epoch 9: Train Loss = 0.5163509452438524\n",
      "Epoch 10: Train Loss = 0.5140318157199442\n",
      "Recall = 0.9081908190819082, Aging Rate = 0.6095280498664292, Precision = 0.7370343316289262\n",
      "Validation: Test Loss = 0.5093112504599037\n",
      "Recall = 0.9117911791179117, Aging Rate = 0.6073018699910953, precision = 0.7426686217008798\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.5110344598044908\n",
      "Epoch 12: Train Loss = 0.5086431206068819\n",
      "Epoch 13: Train Loss = 0.5076040501904509\n",
      "Epoch 14: Train Loss = 0.5062884562586634\n",
      "Epoch 15: Train Loss = 0.5023040855610253\n",
      "Recall = 0.9162916291629163, Aging Rate = 0.5981745325022262, Precision = 0.757722366951991\n",
      "Validation: Test Loss = 0.498001250563619\n",
      "Recall = 0.9338433843384338, Aging Rate = 0.6106411398040962, precision = 0.7564710171345242\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.5013492748552629\n",
      "Epoch 17: Train Loss = 0.499066973385382\n",
      "Epoch 18: Train Loss = 0.4966810623769344\n",
      "Epoch 19: Train Loss = 0.49590916946225155\n",
      "Epoch 20: Train Loss = 0.49319817206738575\n",
      "Recall = 0.9198919891989199, Aging Rate = 0.585040071237756, Precision = 0.7777777777777778\n",
      "Validation: Test Loss = 0.4898482009118302\n",
      "Recall = 0.9446444644464447, Aging Rate = 0.6057435440783615, precision = 0.7714075707460493\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.4923868542362937\n",
      "Epoch 22: Train Loss = 0.4900814150447625\n",
      "Epoch 23: Train Loss = 0.48924122282361093\n",
      "Epoch 24: Train Loss = 0.4882059605760651\n",
      "Epoch 25: Train Loss = 0.48633504471180167\n",
      "Recall = 0.9338433843384338, Aging Rate = 0.5852626892252895, Precision = 0.7892734880182579\n",
      "Validation: Test Loss = 0.4836058316491909\n",
      "Recall = 0.9486948694869487, Aging Rate = 0.5959483526268923, precision = 0.787448636533433\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.4865071541585778\n",
      "Epoch 27: Train Loss = 0.4854467028979202\n",
      "Epoch 28: Train Loss = 0.4863001164961158\n",
      "Epoch 29: Train Loss = 0.48307263110751886\n",
      "Epoch 30: Train Loss = 0.48398111621291107\n",
      "Recall = 0.9387938793879388, Aging Rate = 0.5801424755120214, Precision = 0.8004604758250192\n",
      "Validation: Test Loss = 0.47936677394015176\n",
      "Recall = 0.9414941494149415, Aging Rate = 0.5745770258236865, precision = 0.8105385509492444\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.4817998259861556\n",
      "Epoch 32: Train Loss = 0.48179509760753875\n",
      "Epoch 33: Train Loss = 0.48082462422888084\n",
      "Epoch 34: Train Loss = 0.4802401702741587\n",
      "Epoch 35: Train Loss = 0.4796441620678532\n",
      "Recall = 0.941944194419442, Aging Rate = 0.5796972395369546, Precision = 0.803763440860215\n",
      "Validation: Test Loss = 0.48197994880769685\n",
      "Recall = 0.9014401440144014, Aging Rate = 0.5227070347284061, precision = 0.8530664395229983\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.4792873915931739\n",
      "Epoch 37: Train Loss = 0.47905339487726306\n",
      "Epoch 38: Train Loss = 0.47825874108240635\n",
      "Epoch 39: Train Loss = 0.4780782067605882\n",
      "Epoch 40: Train Loss = 0.47714391555089775\n",
      "Recall = 0.945994599459946, Aging Rate = 0.5768032056990204, Precision = 0.811269780007719\n",
      "Validation: Test Loss = 0.4748111355548869\n",
      "Recall = 0.959045904590459, Aging Rate = 0.5917186108637578, precision = 0.8017306245297215\n",
      "\n",
      "Epoch 41: Train Loss = 0.4771105218496263\n",
      "Epoch 42: Train Loss = 0.47713843616967955\n",
      "Epoch 43: Train Loss = 0.4758228742428794\n",
      "Epoch 44: Train Loss = 0.47534075578407636\n",
      "Epoch 45: Train Loss = 0.47525389799027173\n",
      "Recall = 0.9473447344734474, Aging Rate = 0.5739091718610864, Precision = 0.8165244375484872\n",
      "Validation: Test Loss = 0.4729831722147849\n",
      "Recall = 0.9446444644464447, Aging Rate = 0.5656723063223509, precision = 0.8260527351436442\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.4749783426390518\n",
      "Epoch 47: Train Loss = 0.4748688695001899\n",
      "Epoch 48: Train Loss = 0.47418407237328064\n",
      "Epoch 49: Train Loss = 0.47378926314844794\n",
      "Epoch 50: Train Loss = 0.47382311035985936\n",
      "Recall = 0.9464446444644464, Aging Rate = 0.5719056099732859, Precision = 0.8186064616582328\n",
      "Validation: Test Loss = 0.4713926537431783\n",
      "Recall = 0.963996399639964, Aging Rate = 0.5906055209260909, precision = 0.8073878627968337\n",
      "\n",
      "Epoch 51: Train Loss = 0.47349289476287654\n",
      "Epoch 52: Train Loss = 0.473414309621388\n",
      "Epoch 53: Train Loss = 0.4726148489746576\n",
      "Epoch 54: Train Loss = 0.472118517488427\n",
      "Epoch 55: Train Loss = 0.47159112763001465\n",
      "Recall = 0.9527452745274527, Aging Rate = 0.5727960819234195, Precision = 0.8227749708511465\n",
      "Validation: Test Loss = 0.4697848028990167\n",
      "Recall = 0.9405940594059405, Aging Rate = 0.5520926090828139, precision = 0.842741935483871\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.47392568698124704\n",
      "Epoch 57: Train Loss = 0.47212468749799585\n",
      "Epoch 58: Train Loss = 0.4714696701095238\n",
      "Epoch 59: Train Loss = 0.4710542894800327\n",
      "Epoch 60: Train Loss = 0.4704460002488572\n",
      "Recall = 0.9522952295229523, Aging Rate = 0.5703472840605521, Precision = 0.8259172521467604\n",
      "Validation: Test Loss = 0.4681647367914765\n",
      "Recall = 0.9518451845184518, Aging Rate = 0.563446126447017, precision = 0.8356380877123667\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.47062309719257234\n",
      "Epoch 62: Train Loss = 0.4701046221561975\n",
      "Epoch 63: Train Loss = 0.47073679339747715\n",
      "Epoch 64: Train Loss = 0.4700682733808366\n",
      "Epoch 65: Train Loss = 0.4696889876363963\n",
      "Recall = 0.9545454545454546, Aging Rate = 0.5703472840605521, Precision = 0.8278688524590164\n",
      "Validation: Test Loss = 0.46713138688173234\n",
      "Recall = 0.9558955895589559, Aging Rate = 0.565227070347284, precision = 0.8365498227648681\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.46909772519118426\n",
      "Epoch 67: Train Loss = 0.46910518373322085\n",
      "Epoch 68: Train Loss = 0.4684051937910455\n",
      "Epoch 69: Train Loss = 0.4692314081126202\n",
      "Epoch 70: Train Loss = 0.4688136029859154\n",
      "Recall = 0.950945094509451, Aging Rate = 0.5674532502226179, Precision = 0.82895253040408\n",
      "Validation: Test Loss = 0.46601010823186123\n",
      "Recall = 0.959045904590459, Aging Rate = 0.5716829919857525, precision = 0.8298286604361371\n",
      "\n",
      "Epoch 71: Train Loss = 0.46835472248328763\n",
      "Epoch 72: Train Loss = 0.46787107383578563\n",
      "Epoch 73: Train Loss = 0.46954028084144356\n",
      "Epoch 74: Train Loss = 0.467978084156371\n",
      "Epoch 75: Train Loss = 0.46814359322468935\n",
      "Recall = 0.9554455445544554, Aging Rate = 0.566340160284951, Precision = 0.8345125786163522\n",
      "Validation: Test Loss = 0.4651299397115291\n",
      "Recall = 0.9594959495949595, Aging Rate = 0.569679430097952, precision = 0.833137944509574\n",
      "\n",
      "Epoch 76: Train Loss = 0.4679621084927451\n",
      "Epoch 77: Train Loss = 0.46738147003143254\n",
      "Epoch 78: Train Loss = 0.46780806641863165\n",
      "Epoch 79: Train Loss = 0.46753697785966014\n",
      "Epoch 80: Train Loss = 0.4670809005799298\n",
      "Recall = 0.954995499549955, Aging Rate = 0.5670080142475512, Precision = 0.8331370239497448\n",
      "Validation: Test Loss = 0.46506186109616726\n",
      "Recall = 0.9486948694869487, Aging Rate = 0.5505342831700801, precision = 0.8524059846340477\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.46662699265981084\n",
      "Epoch 82: Train Loss = 0.4662524580637048\n",
      "Epoch 83: Train Loss = 0.4660888269928451\n",
      "Epoch 84: Train Loss = 0.46676580654332817\n",
      "Epoch 85: Train Loss = 0.46614445253662623\n",
      "Recall = 0.9563456345634563, Aging Rate = 0.566340160284951, Precision = 0.8352987421383647\n",
      "Validation: Test Loss = 0.46401445298245836\n",
      "Recall = 0.9617461746174617, Aging Rate = 0.568566340160285, precision = 0.83672670321065\n",
      "\n",
      "Epoch 86: Train Loss = 0.4670458327736689\n"
     ]
    }
   ],
   "source": [
    "training_month = range(2, 5)\n",
    "table_setC = full_stackingcv1(training_month, times = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-26T06:26:07.515Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_chart(table_setC, title = 'StackingCV Scheme 1 Classifier')\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-26T06:26:11.197Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20220601'\n",
    "TPE_multi = False\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'StackingCV1'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'StackingCV1')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
