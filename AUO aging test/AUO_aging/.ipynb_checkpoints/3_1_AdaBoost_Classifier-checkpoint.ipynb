{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from Dataset_Construction import Balance_Ratio\n",
    "from Sampling import label_divide\n",
    "from Aging_Score import score1\n",
    "\n",
    "#os.chdir('C:/Users/Darui Yen/OneDrive/桌面/data_after_mid') \n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_set(num_set):\n",
    "    \n",
    "    data_dict = {}\n",
    "    for i in range(num_set):\n",
    "        data_dict[f'set{i}'] = pd.read_csv(f'dataset_{i}.csv').iloc[:, 1:]\n",
    "        print('Dimension of dataset', i, ':', data_dict[f'set{i}'].shape, ' balance ratio:', \\\n",
    "              Balance_Ratio(data_dict[f'set{i}']))\n",
    "    \n",
    "    print('\\n', num_set, 'datasets are loaded.')\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def train_set(data_dict, num_set, label = 'GB'):\n",
    "    \n",
    "    trainset_x = {}\n",
    "    trainset_y = {}\n",
    "    \n",
    "    for i in range(num_set):\n",
    "        X, Y = label_divide(data_dict[f'set{i}'], None, label, train_only = True)\n",
    "        trainset_x[f'set{i}'] = X\n",
    "        trainset_y[f'set{i}'] = Y\n",
    "        \n",
    "    print('\\nLabels of ', num_set, 'datasets are divided.')\n",
    "    return trainset_x, trainset_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoostC(train_x, test_x, train_y, test_y, n_estimator = 100, LR = 0.7):\n",
    "    \n",
    "    clf = AdaBoostClassifier(n_estimators = n_estimator, learning_rate = LR)\n",
    "    clf.fit(train_x, train_y)\n",
    "    predict_y = clf.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall & Precision for Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_matrix(predict, train_y):\n",
    "    \n",
    "    #confusion matrix\n",
    "#     cf = np.zeros([2,2])\n",
    "#     for i in range(len(predict)):\n",
    "#         if predict['predict'].values[i] == 0:\n",
    "#             row_index = 1\n",
    "#         else:\n",
    "#             row_index = 0\n",
    "#         if predict['truth'].values[i] == 0:\n",
    "#             col_index = 1\n",
    "#         else:\n",
    "#             col_index = 0\n",
    "#         cf[row_index][col_index] += 1\n",
    "#     cf = cf.astype(int)\n",
    "    \n",
    "#     TP, FP, FN, TN = cf[0,0], cf[0,1], cf[1,0], cf[1,1]\n",
    "\n",
    "    mask_FP = predict['predict'] > predict['truth']\n",
    "    mask_FN = predict['predict'] < predict['truth']\n",
    "    mask_TP = (predict['predict'] == predict['truth']) * (predict['predict'] == 1)\n",
    "    mask_TN = (predict['predict'] == predict['truth']) * (predict['predict'] == 0)\n",
    "    TP = mask_TP.sum()\n",
    "    FP = mask_FP.sum()\n",
    "    FN = mask_FN.sum()\n",
    "    TN = mask_TN.sum()\n",
    "    \n",
    "    #balance ratio, train OK & NG\n",
    "    train_OK = sum(train_y < 0.5)\n",
    "    train_NG = len(train_y) - train_OK\n",
    "    br = train_OK / train_NG\n",
    "    \n",
    "    #precision, recall, aging rate, efficiency, score\n",
    "    num_pd = TP + FP\n",
    "    if num_pd != 0:\n",
    "        precision = TP / num_pd\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    ar = (TP + FP) / (TP + FP + FN + TN)\n",
    "    eff = recall / ar\n",
    "    score = score1(recall, ar)\n",
    "    \n",
    "    table = pd.Series({'Balance Ratio': br, 'Train_OK': train_OK, 'Train_NG': train_NG, 'TP': TP, 'FP': FP, 'FN': FN, \\\n",
    "                       'TN': TN, 'Precision': precision, 'Recall': recall, 'Aging Rate': ar, 'Efficiency': eff, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    print('Precision:', precision, '\\nRecall:', recall, '\\nAging Rate:', ar)\n",
    "    return  table\n",
    "\n",
    "\n",
    "\n",
    "def print_badC(predict, test_x, Bad_Types, threshold = 1):\n",
    "    \n",
    "    Bad = []\n",
    "    Bad_miss = []\n",
    "    TP = predict[(predict['truth'] == 1) & (predict['predict'] >= threshold)].index\n",
    "    FN = predict[(predict['truth'] == 1) & (predict['predict'] < threshold)].index\n",
    "    for j in range(len(TP)):\n",
    "        Index = TP[j]\n",
    "        Key = test_x.values[Index]\n",
    "        Key = pd.DataFrame(Key).T.apply(lambda x:'_'.join(x.astype(str)), axis = 1)\n",
    "        Bad.append(Bad_Types[Key[0]])\n",
    "        Bad.sort()\n",
    "    print('Types of Bad found:', Bad) \n",
    "    \n",
    "    for j in range(len(FN)):\n",
    "        Index = FN[j]\n",
    "        Key = test_x.values[Index]\n",
    "        Key = pd.DataFrame(Key).T.apply(lambda x:'_'.join(x.astype(str)),axis=1)\n",
    "        Bad_miss.append(Bad_Types[Key[0]])\n",
    "        Bad_miss.sort()\n",
    "    print('Types of Bad not found:', Bad_miss)\n",
    "    \n",
    "    bad_table = pd.Series({'Bad_Found': set(Bad), 'Bad_Missed': set(Bad_miss)})\n",
    "    bad_table = pd.DataFrame(bad_table).T\n",
    "    bad_table['Detect Ratio'] = len(Bad) / (len(Bad) + len(Bad_miss))\n",
    "    \n",
    "    return bad_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runall_AdaBoostC(num_set, trainset_x, test_x, trainset_y, test_y, record_bad = True):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    bad_set = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(num_set)):\n",
    "        print('\\n', f'Dataset {i}:')\n",
    "\n",
    "        result = AdaBoostC(trainset_x[f'set{i}'], test_x, trainset_y[f'set{i}'], test_y)\n",
    "        table = cf_matrix(result, trainset_y[f'set{i}'])\n",
    "        table_set = pd.concat([table_set, table]).rename(index = {0: f'dataset {i}'})\n",
    "        \n",
    "        if record_bad:\n",
    "            bad_table = print_badC(result, test_x, Bad_Types) \n",
    "            bad_set = pd.concat([bad_set, bad_table]).rename(index = {0: f'dataset {i}'})\n",
    "\n",
    "    if record_bad:\n",
    "        return table_set, bad_set\n",
    "    else:\n",
    "        return table_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_plot(bad_set):\n",
    "    \n",
    "    # record all bad types\n",
    "    bad_list = []\n",
    "    [bad_list.append(x) for x in bad_set.loc['dataset 1'][0]]\n",
    "    [bad_list.append(x) for x in bad_set.loc['dataset 1'][1]]\n",
    "    bad_list.sort()\n",
    "    \n",
    "    bad_array = np.empty([len(bad_set), len(bad_list)])\n",
    "    for j in range(len(bad_set)):\n",
    "        for i in range(len(bad_list)):\n",
    "            if bad_list[i] in bad_set.iloc[j, 0]:\n",
    "                bad_array[j, i] = 1\n",
    "            else:\n",
    "                bad_array[j ,i] = 0\n",
    "                          \n",
    "    bad_df = pd.DataFrame(bad_array)\n",
    "    bad_df.columns = bad_list\n",
    "    \n",
    "    plt.pcolor(bad_df, cmap = 'Reds')\n",
    "    plt.title(\"Bad Types Detection across All Datasets\")\n",
    "    plt.yticks(np.arange(0.5, len(bad_df.index), 1), bad_df.index)\n",
    "    plt.xticks(np.arange(0.5, len(bad_df.columns), 1), bad_df.columns.astype(int))\n",
    "    plt.xlabel(\"ID of Bad Types\", size = 12)\n",
    "    plt.ylabel(\"Dataset\", size = 12)\n",
    "    \n",
    "    plt.savefig('Bad Types Detection across All Datasets.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def line_chart(table_set, title):\n",
    "    \n",
    "    plt.style.use('seaborn-dark-palette')\n",
    "    \n",
    "    x = list(range(len(table_set)))\n",
    "    fig, ax1 = plt.subplots(figsize = (15,8))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    plt.title(title, fontsize = 16)\n",
    "    plt.xticks(range(1,13,1))\n",
    "    ax1.plot(x, table_set['Aging Rate'], 'b--', linewidth = 1, label = 'Aging Rate')\n",
    "    ax1.plot(x, table_set['Aging Rate'], 'b.', markersize = 15)\n",
    "    ax1.plot(x, table_set['Recall'], 'r-', linewidth = 1, label = 'Recall')\n",
    "    ax1.plot(x, table_set['Recall'], 'r.', markersize = 15)\n",
    "    ax2.plot(x, table_set['Precision'], 'g--', linewidth = 1, label = 'Precision')\n",
    "    ax2.plot(x, table_set['Precision'], 'g.', markersize = 15)\n",
    "    ax1.set_xlabel('\\nDataset', fontsize = 12)\n",
    "    ax1.set_ylabel('Recall & Aging Rate', color = 'b')\n",
    "    ax2.set_ylabel('Precision', color = 'g')\n",
    "    \n",
    "    ax1.legend(loc = 'upper left', frameon = False)\n",
    "    ax2.legend(loc = 'upper right', frameon = False)\n",
    "    \n",
    "    plt.savefig(f'{title}.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bad types: 62\n",
      "\n",
      "testing data: (55903, 83) \n",
      "Balance Ratio: 3104.72222\n",
      "Dimension of dataset 0 : (80395, 112)  balance ratio: 122.11639\n",
      "Dimension of dataset 1 : (119611, 112)  balance ratio: 2.0001\n",
      "Dimension of dataset 2 : (119939, 112)  balance ratio: 1.98378\n",
      "Dimension of dataset 3 : (119939, 112)  balance ratio: 1.98378\n",
      "Dimension of dataset 4 : (119613, 112)  balance ratio: 2.0\n",
      "Dimension of dataset 5 : (4902, 112)  balance ratio: 1.9945\n",
      "Dimension of dataset 6 : (5224, 112)  balance ratio: 1.66667\n",
      "Dimension of dataset 7 : (5224, 112)  balance ratio: 1.66667\n",
      "Dimension of dataset 8 : (4897, 112)  balance ratio: 2.00061\n",
      "\n",
      " 9 datasets are loaded.\n",
      "\n",
      "Labels of  9 datasets are divided.\n",
      "\n",
      " Dimension of run test: (55903, 112)\n"
     ]
    }
   ],
   "source": [
    "###bad types###\n",
    "bad = pd.read_csv('original_data/Bad_Types.csv').iloc[:, 1:]\n",
    "Bad_Types = {bad.cb[i]:i for i in range (len(bad))}\n",
    "print('Total bad types:', len(bad))\n",
    "\n",
    "###single dataset###\n",
    "test = pd.read_csv('original_data/TestingSet_0.csv').iloc[:, 2:]\n",
    "#train = pd.read_csv('data_from_newpy/Train_sample.csv').iloc[:, 1:]\n",
    "#print('\\ntraining data:', train.shape, '\\nBalance Ratio:', Balance_Ratio(train))\n",
    "print('\\ntesting data:', test.shape, '\\nBalance Ratio:', Balance_Ratio(test))\n",
    "\n",
    "#train_x, train_y, test_x, test_y = label_divide(train, test, 'GB')\n",
    "\n",
    "###multiple dataset###\n",
    "data_dict = multiple_set(num_set = 9)\n",
    "trainset_x, trainset_y = train_set(data_dict, num_set = 9, label = 'GB')\n",
    "test_x, test_y = label_divide(test, None, 'GB', train_only = True)\n",
    "\n",
    "\n",
    "#####for runhist dataset#####\n",
    "# bad = pd.read_csv('run_bad_types.csv').iloc[:, 1:]\n",
    "# Bad_Types = {bad.cb[i]:i for i in range (len(bad))}\n",
    "# print('Total bad types:', len(bad))\n",
    "\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of run test:', run_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec937d28145e418d88738af1506f7364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset 0:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-1e6e313002f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#table_set, bad_set = runall_AdaBoostC(9, trainset_x, test_x, trainset_y, test_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtable_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunall_AdaBoostC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_bad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mline_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AdaBoost Classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-c53f70f32060>\u001b[0m in \u001b[0;36mrunall_AdaBoostC\u001b[0;34m(num_set, trainset_x, test_x, trainset_y, test_y, record_bad)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Dataset {i}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'set{i}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'set{i}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'set{i}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtable_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'dataset {i}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-59b5c254a0d5>\u001b[0m in \u001b[0;36mAdaBoostC\u001b[0;34m(train_x, test_x, train_y, test_y, n_estimator, LR)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mAdaBoostC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_jobs'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#table_set, bad_set = runall_AdaBoostC(9, trainset_x, test_x, trainset_y, test_y)\n",
    "table_set = runall_AdaBoostC(9, trainset_x, run_test_x, trainset_y, run_test_y, record_bad = False)\n",
    "line_chart(table_set, title = 'AdaBoost Classifier')\n",
    "\n",
    "end = time.time()\n",
    "print(\"\\nRun Time：%f seconds\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balance Ratio</th>\n",
       "      <th>Train_OK</th>\n",
       "      <th>Train_NG</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Aging Rate</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset 0</th>\n",
       "      <td>122.116386</td>\n",
       "      <td>79742.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>55885.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 1</th>\n",
       "      <td>2.000100</td>\n",
       "      <td>79742.0</td>\n",
       "      <td>39869.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>55110.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 2</th>\n",
       "      <td>1.983780</td>\n",
       "      <td>79742.0</td>\n",
       "      <td>40197.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9034.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>46851.0</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.161619</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 3</th>\n",
       "      <td>1.983780</td>\n",
       "      <td>79742.0</td>\n",
       "      <td>40197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>55882.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 4</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>79742.0</td>\n",
       "      <td>39871.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9210.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>46675.0</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.164768</td>\n",
       "      <td>0.337175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 5</th>\n",
       "      <td>1.994502</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>32913.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22972.0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.589002</td>\n",
       "      <td>1.320500</td>\n",
       "      <td>0.654720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 6</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35978.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19907.0</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.643847</td>\n",
       "      <td>1.294303</td>\n",
       "      <td>0.692024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 7</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23705.0</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.575872</td>\n",
       "      <td>1.254136</td>\n",
       "      <td>0.586998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 8</th>\n",
       "      <td>2.000613</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35212.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20673.0</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.630145</td>\n",
       "      <td>1.322447</td>\n",
       "      <td>0.702184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Balance Ratio  Train_OK  Train_NG    TP       FP    FN       TN  \\\n",
       "dataset 0     122.116386   79742.0     653.0   0.0      0.0  18.0  55885.0   \n",
       "dataset 1       2.000100   79742.0   39869.0   0.0    775.0  18.0  55110.0   \n",
       "dataset 2       1.983780   79742.0   40197.0   1.0   9034.0  17.0  46851.0   \n",
       "dataset 3       1.983780   79742.0   40197.0   0.0      3.0  18.0  55882.0   \n",
       "dataset 4       2.000000   79742.0   39871.0   1.0   9210.0  17.0  46675.0   \n",
       "dataset 5       1.994502    3265.0    1637.0  14.0  32913.0   4.0  22972.0   \n",
       "dataset 6       1.666667    3265.0    1959.0  15.0  35978.0   3.0  19907.0   \n",
       "dataset 7       1.666667    3265.0    1959.0  13.0  32180.0   5.0  23705.0   \n",
       "dataset 8       2.000613    3265.0    1632.0  15.0  35212.0   3.0  20673.0   \n",
       "\n",
       "           Precision    Recall  Aging Rate  Efficiency     Score  \n",
       "dataset 0   0.000000  0.000000    0.000000         NaN       NaN  \n",
       "dataset 1   0.000000  0.000000    0.013863    0.000000  0.000000  \n",
       "dataset 2   0.000111  0.055556    0.161619    0.343743  0.000000  \n",
       "dataset 3   0.000000  0.000000    0.000054    0.000000  0.000000  \n",
       "dataset 4   0.000109  0.055556    0.164768    0.337175  0.000000  \n",
       "dataset 5   0.000425  0.777778    0.589002    1.320500  0.654720  \n",
       "dataset 6   0.000417  0.833333    0.643847    1.294303  0.692024  \n",
       "dataset 7   0.000404  0.722222    0.575872    1.254136  0.586998  \n",
       "dataset 8   0.000426  0.833333    0.630145    1.322447  0.702184  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bad_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-8fedef8d2a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbad_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbad_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bad_set' is not defined"
     ]
    }
   ],
   "source": [
    "bad_plot(bad_set)\n",
    "bad_set"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
