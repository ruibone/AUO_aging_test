{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T07:49:55.253155Z",
     "start_time": "2021-09-13T07:49:53.331298Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import optuna\n",
    "\n",
    "from Dataset_Construction import Balance_Ratio \n",
    "from Sampling import label_divide\n",
    "from AdaClassifier import train_set, multiple_set, print_badC, bad_plot, line_chart, cf_matrix, runall_AdaBoostC\n",
    "from AdaRegressor import AUC, PR_curve, multiple_curve, PR_matrix, best_threshold, runall_AdaBoostR\n",
    "from Aging_Score import score1\n",
    "from XGBoost import optuna_history, runall_XGBoostC, runall_XGBoostR\n",
    "from CatBoost import runall_CatBoostC, runall_CatBoostR\n",
    "from Light_GBM import runall_LightGBMC, runall_LightGBMR\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T07:49:57.050129Z",
     "start_time": "2021-09-13T07:49:57.036131Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_hyper(num_set, date, model_list, iter_list, filename, mode, sampler) :\n",
    "    \n",
    "    allset_dict = {}\n",
    "    for j in range(num_set) :\n",
    "\n",
    "        oneset_dict = {}\n",
    "        for i, model in enumerate(model_list) :\n",
    "\n",
    "            with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[i]}.data', 'rb') as f :\n",
    "                temp_dict = pickle.load(f)\n",
    "                oneset_dict[model] = temp_dict[f'set{j}']\n",
    "        allset_dict[f'set{j}'] = oneset_dict\n",
    "        \n",
    "    return allset_dict\n",
    "\n",
    "\n",
    "def tableau_hyper(num_set, date, model_list, iter_list, filename, mode, sampler_list) :\n",
    "    \n",
    "    model_dict = {}\n",
    "    for j, model in enumerate(model_list) :\n",
    "\n",
    "        sampler_dict = {}\n",
    "        for i, sampler in enumerate(sampler_list) :\n",
    "\n",
    "            with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[j]}.data', 'rb') as f :\n",
    "                temp_dict = pickle.load(f)\n",
    "                sampler_dict[sampler] = temp_dict\n",
    "                \n",
    "        model_dict[model] = sampler_dict\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T07:49:58.455882Z",
     "start_time": "2021-09-13T07:49:58.434938Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_level(num_set, hyper_info, hyper_dict, x_dict, y_dict, x_test) :\n",
    "\n",
    "    available_model = ['CatBoost', 'LightGBM', 'XGBoost', 'AdaBoost']\n",
    "    second_train = {}\n",
    "    second_test = {}\n",
    "    \n",
    "    for i in range(num_set) :\n",
    "        feature_table = pd.DataFrame()\n",
    "        test_table = pd.DataFrame()\n",
    "        \n",
    "        if available_model[0] in hyper_info['model_list'] :\n",
    "            \n",
    "            if hyper_info['mode'] == 'C' :\n",
    "                clf1 = CatBoostClassifier(**hyper_dict[f'set{i}']['CatBoost'], verbose = 0)\n",
    "                clf1.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = clf1.predict_proba(x_dict[f'set{i}'])[:, 0]\n",
    "                predict_test = clf1.predict_proba(x_test)[:, 0]\n",
    "                \n",
    "            elif hyper_info['mode'] == 'R' :\n",
    "                reg1 = CatBoostRegressor(**hyper_dict[f'set{i}']['CatBoost'], verbose = 0)\n",
    "                reg1.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = reg1.predict(x_dict[f'set{i}'])\n",
    "                predict_test = reg1.predict(x_test)\n",
    "                \n",
    "            feature1 = pd.DataFrame(predict_y).rename(columns = {0: 'CatBoost'})\n",
    "            feature_table = pd.concat([feature_table, feature1], axis = 1)\n",
    "            new_test = pd.DataFrame(predict_test).rename(columns = {0: 'CatBoost'})\n",
    "            test_table = pd.concat([test_table, new_test], axis = 1)\n",
    "\n",
    "        if available_model[1] in hyper_info['model_list'] :\n",
    "            \n",
    "            if hyper_info['mode'] == 'C' :\n",
    "                clf2 = LGBMClassifier(**hyper_dict[f'set{i}']['LightGBM'])\n",
    "                clf2.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = clf2.predict_proba(x_dict[f'set{i}'])[:, 0]\n",
    "                predict_test = clf2.predict_proba(x_test)[:, 0]\n",
    "            \n",
    "            elif hyper_info['mode'] == 'R' :\n",
    "                reg2 = LGBMRegressor(**hyper_dict[f'set{i}']['LightGBM'])\n",
    "                reg2.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = reg2.predict(x_dict[f'set{i}'])\n",
    "                predict_test = reg2.predict(x_test)\n",
    "                \n",
    "            feature2 = pd.DataFrame(predict_y).rename(columns = {0: 'LightGBM'})\n",
    "            feature_table = pd.concat([feature_table, feature2], axis = 1)\n",
    "            new_test = pd.DataFrame(predict_test).rename(columns = {0: 'LightGBM'})\n",
    "            test_table = pd.concat([test_table, new_test], axis = 1)\n",
    "\n",
    "        if available_model[2] in hyper_info['model_list'] :\n",
    "            \n",
    "            if hyper_info['mode'] == 'C' :\n",
    "                clf3 = XGBClassifier(**hyper_dict[f'set{i}']['XGBoost'], n_jobs = -1)\n",
    "                clf3.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = clf3.predict_proba(x_dict[f'set{i}'])[:, 0]\n",
    "                predict_test = clf3.predict_proba(x_test)[:, 0]\n",
    "            \n",
    "            elif hyper_info['mode'] == 'R' :\n",
    "                reg3 = XGBRegressor(**hyper_dict[f'set{i}']['XGBoost'], n_jobs = -1)\n",
    "                reg3.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = reg3.predict(x_dict[f'set{i}'])\n",
    "                predict_test = reg3.predict(x_test)\n",
    "                \n",
    "            feature3 = pd.DataFrame(predict_y).rename(columns = {0: 'XGBoost'})\n",
    "            feature_table = pd.concat([feature_table, feature3], axis = 1)\n",
    "            new_test = pd.DataFrame(predict_test).rename(columns = {0: 'XGBoost'})\n",
    "            test_table = pd.concat([test_table, new_test], axis = 1)\n",
    "\n",
    "\n",
    "        if available_model[3] in hyper_info['model_list'] :\n",
    "            \n",
    "            if hyper_info['mode'] == 'C' :\n",
    "                clf4 = AdaBoostClassifier(**hyper_dict[f'set{i}']['AdaBoost'])\n",
    "                clf4.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = clf4.predict_proba(x_dict[f'set{i}'])[:, 0]\n",
    "                predict_test = clf4.predict_proba(x_test)[:, 0]\n",
    "                \n",
    "            elif hyper_info['mode'] == 'R' :\n",
    "                reg4 = AdaBoostRegressor(**hyper_dict[f'set{i}']['AdaBoost'])\n",
    "                reg4.fit(x_dict[f'set{i}'], y_dict[f'set{i}'])\n",
    "                predict_y = reg4.predict(x_dict[f'set{i}'])\n",
    "                predict_test = reg4.predict(x_test)\n",
    "                \n",
    "            feature4 = pd.DataFrame(predict_y).rename(columns = {0: 'AdaBoost'})\n",
    "            feature_table = pd.concat([feature_table, feature4], axis = 1)\n",
    "            new_test = pd.DataFrame(predict_test).rename(columns = {0: 'AdaBoost'})\n",
    "            test_table = pd.concat([test_table, new_test], axis = 1)\n",
    "            \n",
    "        second_train[f'set{i}'] = feature_table\n",
    "        second_test[f'set{i}'] = test_table \n",
    "\n",
    "    return second_train, second_test\n",
    "\n",
    "\n",
    "def second_level(num_set, hyper_info, x_train, y_train, x_test, y_test, meta_param, meta_learner) :\n",
    "    \n",
    "    result_dict = {}\n",
    "    judge = list(meta_param.keys())[0]\n",
    "    \n",
    "    for i in range(num_set) :\n",
    "        print('Dataset: ', i)\n",
    "        \n",
    "        if isinstance(meta_param[judge], dict) :\n",
    "            best_param = meta_param[f'set{i}']\n",
    "        else :\n",
    "            best_param = meta_param\n",
    "        \n",
    "        if hyper_info['mode'] == 'C' :\n",
    "    \n",
    "            if meta_learner == 'Logistic Regression' :\n",
    "                clf = LogisticRegression(**best_param)\n",
    "                \n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                clf = ExtraTreesClassifier(**best_param)\n",
    "                \n",
    "            clf.fit(x_train[f'set{i}'], y_train[f'set{i}'])\n",
    "            predict_y = clf.predict(x_test[f'set{i}'])\n",
    "            \n",
    "            if meta_learner == 'Logistic Regression' :\n",
    "                print(f'Coefficient of {meta_learner} :', clf.coef_)\n",
    "                \n",
    "        elif hyper_info['mode'] == 'R' :\n",
    "            \n",
    "            if meta_learner == 'Ridge Regression' :\n",
    "                reg = RidgeCV(**best_param)\n",
    "            \n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                reg = ExtraTreesRegressor(**best_param)\n",
    "            \n",
    "            reg.fit(x_train[f'set{i}'], y_train[f'set{i}'])\n",
    "            predict_y = reg.predict(x_test[f'set{i}'])\n",
    "            \n",
    "            if meta_learner == 'Ridge Regression' :\n",
    "                print(f'Coefficient of {meta_learner} :', reg.coef_)\n",
    "                \n",
    "        result = pd.DataFrame({'truth': y_test, 'predict': predict_y})\n",
    "        result_dict[f'set{i}'] = result\n",
    "                \n",
    "            \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def stack_table(result, train_y, hyper_info, thres_target, threshold) :\n",
    "    \n",
    "    num_set = len(result)\n",
    "    table_set = pd.DataFrame()\n",
    "    \n",
    "    for i in range(num_set) : \n",
    "        \n",
    "        if hyper_info['mode'] == 'C' :  \n",
    "            table = cf_matrix(result[f'set{i}'], train_y[f'set{i}'])\n",
    "            table_set = pd.concat([table_set, table], axis = 0).rename(index = {0: f'dataset {i}'})\n",
    "            \n",
    "        elif hyper_info['mode'] == 'R' :\n",
    "            pr_matrix = PR_matrix(result[f'set{i}'], train_y[f'set{i}']) \n",
    "            best_data, best_thres = best_threshold(pr_matrix, target = thres_target, threshold = threshold)\n",
    "            table_set = pd.concat([table_set, best_data]).rename(index = {best_data.index.values[0]: f'dataset {i}'})\n",
    "        \n",
    "    return table_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runhist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T07:50:01.198272Z",
     "start_time": "2021-09-13T07:49:59.931572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bad types: 62\n",
      "\n",
      "training data: (77138, 83) \n",
      "Balance Ratio: 18.17902\n",
      "\n",
      "testing data: (55903, 83) \n",
      "Balance Ratio: 3104.72222\n",
      "Dimension of dataset 0 : (80518, 141)  balance ratio: 1101.9863\n",
      "Dimension of dataset 1 : (1634, 141)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (1484, 141)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1752, 141)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1608, 141)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (1618, 141)  balance ratio: 1.00496\n",
      "Dimension of dataset 6 : (1555, 141)  balance ratio: 1.09005\n",
      "Dimension of dataset 7 : (1622, 141)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (1622, 141)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (803, 141)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      " Dimension of run test: (47725, 141)\n"
     ]
    }
   ],
   "source": [
    "###bad types###\n",
    "bad = pd.read_csv('event/Bad_Types.csv').iloc[:, 1:]\n",
    "Bad_Types = {bad.cb[i]:i for i in range (len(bad))}\n",
    "print('Total bad types:', len(bad))\n",
    "\n",
    "###single dataset###\n",
    "test = pd.read_csv('event/TestingSet_0.csv').iloc[:, 2:]\n",
    "train = pd.read_csv('event/TrainingSet_new.csv').iloc[:, 2:]\n",
    "print('\\ntraining data:', train.shape, '\\nBalance Ratio:', Balance_Ratio(train))\n",
    "print('\\ntesting data:', test.shape, '\\nBalance Ratio:', Balance_Ratio(test))\n",
    "\n",
    "train_x, train_y, test_x, test_y = label_divide(train, test, 'GB')\n",
    "\n",
    "###multiple dataset###\n",
    "data_dict = multiple_set(num_set = 10)\n",
    "trainset_x, trainset_y = train_set(data_dict, num_set = 10, label = 'GB')\n",
    "test_x, test_y = label_divide(test, None, 'GB', train_only = True)\n",
    "\n",
    "\n",
    "#####for runhist dataset#####\n",
    "# bad = pd.read_csv('run_bad_types.csv').iloc[:, 1:]\n",
    "# Bad_Types = {bad.cb[i]:i for i in range (len(bad))}\n",
    "# print('Total bad types:', len(bad))\n",
    "\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of run test:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T04:09:33.627701Z",
     "start_time": "2021-09-13T04:09:06.493366Z"
    }
   },
   "outputs": [],
   "source": [
    "# give details of hyperparameters \n",
    "hyper_infoC = {\n",
    "    'num_set': 10,\n",
    "    'date': '20210916',\n",
    "    'model_list': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'iter_list': [250, 250, 250],\n",
    "    'filename': 'runhist_array_m8m3_one-sided_joinnewevent_eqp+rework',\n",
    "    'mode': 'C',\n",
    "    'sampler': 'univariate-TPE',\n",
    "}\n",
    "\n",
    "best_param = {'n_estimators': 100}\n",
    "#best_param = {'C': 1}\n",
    "\n",
    "\n",
    "# loading all the hyperparameters from previous optimization sections\n",
    "all_hyperC = load_hyper(**hyper_infoC)\n",
    "\n",
    "# go through the first level of stacking model and output the probability from boosting models as new training data\n",
    "# and transform the testing data by the same stacking model at the same time\n",
    "second_train, second_test = first_level(10, hyper_infoC, all_hyperC, trainset_x, trainset_y, run_test_x)\n",
    "\n",
    "# go through the second level of stacking model and output the categorical results\n",
    "result_dictC = second_level(10, hyper_infoC, second_train, trainset_y, second_test, run_test_y, best_param, \n",
    "                           meta_learner = 'Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T04:09:54.924920Z",
     "start_time": "2021-09-13T04:09:54.879666Z"
    }
   },
   "outputs": [],
   "source": [
    "table_setC = stack_table(result_dictC, trainset_y, hyper_infoC, thres_target = None, threshold = None)\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T06:54:12.299928Z",
     "start_time": "2021-09-13T06:53:26.803592Z"
    }
   },
   "outputs": [],
   "source": [
    "hyper_infoR = {\n",
    "    'num_set': 10,\n",
    "    'date': '20210916',\n",
    "    'model_list': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'iter_list': [250, 250, 250],\n",
    "    'filename': 'runhist_array_m8m3_one-sided_joinnewevent_eqp+rework',\n",
    "    'mode': 'R',\n",
    "    'sampler': 'multivariate-TPE'\n",
    "}\n",
    "\n",
    "best_param = {'n_estimators': 100}\n",
    "#best_param = {'alphas': 0.5}\n",
    "\n",
    "\n",
    "all_hyperR = load_hyper(**hyper_infoR)\n",
    "\n",
    "second_train, second_test = first_level(10, hyper_infoR, all_hyperR, trainset_x, trainset_y, run_test_x)\n",
    "\n",
    "result_dictR = second_level(10, hyper_infoR, second_train, trainset_y, second_test, run_test_y, best_param, \n",
    "                           meta_learner = 'Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T06:56:10.690590Z",
     "start_time": "2021-09-13T06:56:10.511455Z"
    }
   },
   "outputs": [],
   "source": [
    "table_setR = stack_table(result_dictR, trainset_y, hyper_infoR, thres_target = 'Recall', threshold = 0.7)\n",
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Opmization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:09:40.294537Z",
     "start_time": "2021-09-11T16:09:40.226466Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def objective_creator(train_x, train_y, valid_x, valid_y, hyper_info, meta_learner) :    \n",
    "    \n",
    "    def second_objective(trial) :\n",
    "\n",
    "        # hyperparameter setting\n",
    "        if meta_learner == 'Extra Trees' :\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100)\n",
    "            }\n",
    "\n",
    "        elif hyper_info['mode'] == 'C' :\n",
    "\n",
    "            param = {\n",
    "                'solver': trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                'C': trial.suggest_categorical('C', [100, 10 ,1 ,0.1, 0.01]),\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "\n",
    "            if param['solver'] in ['liblinear', 'saga'] :\n",
    "                subparam = {'penalty': trial.suggest_categorical('penalty', ['l1', 'l2'])}\n",
    "                param.update(subparam)\n",
    "\n",
    "        elif hyper_info['mode'] == 'R' :\n",
    "            param = {\n",
    "                'cv': trial.suggest_categorical('cv', ['none', 3, 5]),\n",
    "                'alpha': trial.suggest_float('alpha', 0.1, 1, step = 0.1)\n",
    "            }\n",
    "\n",
    "        # optimization function\n",
    "        result_list = []\n",
    "        \n",
    "        # always name the dataset 'set0' if only one dataset need to be run by second_level function\n",
    "        train_set_x = dict(set0 = train_x)\n",
    "        train_set_y = dict(set0 = train_y)\n",
    "        valid_set_x = dict(set0 = valid_x)\n",
    "\n",
    "        result_dict = second_level(1, hyper_info, train_set_x, train_set_y, valid_set_x, valid_y, param, \n",
    "                                   meta_learner = meta_learner)\n",
    "        result = result_dict['set0']\n",
    "        #the key of this dictionary will always be 'set0'\n",
    "\n",
    "        if hyper_info['mode'] == 'C':\n",
    "\n",
    "            table = cf_matrix(result, valid_y)\n",
    "            recall = table['Recall']\n",
    "            aging = table['Aging Rate']\n",
    "            effi = table['Efficiency']\n",
    "\n",
    "            #result_list.append(effi)\n",
    "            result_list.append(recall - 0.1*aging)\n",
    "\n",
    "        elif hyper_info['mode'] == 'R':\n",
    "\n",
    "            pr_matrix = PR_matrix(result, valid_y)\n",
    "            #best_data, _ = best_threshold(pr_matrix, target = 'Recall', threshold = 0.8)\n",
    "            #aging = best_data['Aging Rate']\n",
    "            #result_list.append((-1)*aging)\n",
    "\n",
    "            auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "            result_list.append((-1)*auc)\n",
    "\n",
    "        return np.mean(result_list)\n",
    "    \n",
    "    return second_objective\n",
    "\n",
    "        \n",
    "def second_optuna(num_set, creator, hyper_info, trainset_x, trainset_y, n_iter, meta_learner, return_addition = True) :\n",
    "    \n",
    "    best_param = {}\n",
    "    all_score = {}\n",
    "    TPE_multi = True if hyper_info['sampler'] == 'multivariate-TPE' else False\n",
    "    all_hyper = load_hyper(**hyper_info)\n",
    "    \n",
    "    for i in tqdm(range(num_set)) :\n",
    "        \n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(trainset_x[f'set{i}'], trainset_y[f'set{i}'], \n",
    "                                                              test_size = 0.25)\n",
    "        # always name the dataset 'set0' if only one dataset need to be run by second_level function\n",
    "        train_set_x = dict(set0 = train_x)\n",
    "        train_set_y = dict(set0 = train_y)\n",
    "    \n",
    "        second_train, second_test = first_level(1, hyper_info, all_hyper, train_set_x, train_set_y, valid_x)\n",
    "        \n",
    "        # start to optimize\n",
    "        #the key of these dictionaries will always be 'set0'\n",
    "        objective = creator(second_train['set0'], train_y, second_test['set0'], valid_y, hyper_info, meta_learner)\n",
    "        \n",
    "        ##### optimize one dataset in each loop #####\n",
    "        print(f'Dataset{i} :')\n",
    "        \n",
    "        study = optuna.create_study(sampler = optuna.samplers.TPESampler(multivariate = TPE_multi), \n",
    "                                       direction = 'maximize')\n",
    "        study.optimize(objective, n_trials = n_iter, show_progress_bar = True, gc_after_trial = True)\n",
    "        best_param[f'set{i}'] = study.best_trial.params\n",
    "        \n",
    "        ##### return score and entire params for score plot or feature importance\n",
    "        if return_addition :\n",
    "            collect_score = []\n",
    "            [collect_score.append(x.values) for x in study.trials]\n",
    "            all_score[f'set{i}'] = collect_score \n",
    "        \n",
    "        print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
    "        \n",
    "    ##### store the best hyperparameters #####\n",
    "    multi_mode = hyper_info['sampler']\n",
    "    mode = hyper_info['mode']\n",
    "    learner_name = meta_learner.replace(' ', '')\n",
    "    filename = hyper_info['filename'] + mode + f'_{meta_learner}'\n",
    "    with open(f'{filename}_{multi_mode}_{n_iter}.data', 'wb') as f:\n",
    "        pickle.dump(best_param, f)\n",
    "\n",
    "    if return_addition :\n",
    "        return best_param, all_score\n",
    "    else :\n",
    "        return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T14:21:02.991223Z",
     "start_time": "2021-09-11T14:19:49.484493Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hyper_info = {\n",
    "    'num_set': 10,\n",
    "    'date': '20210916',\n",
    "    'model_list': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'iter_list': [250, 250, 250],\n",
    "    'filename': 'runhist_array_m8m3_joinnewevent_eqp+rework',\n",
    "    'mode': 'R',\n",
    "    'sampler': 'multivariate-TPE'\n",
    "}\n",
    "\n",
    "best_param, all_score = second_optuna(\n",
    "    num_set = 10,\n",
    "    creator = objective_creator, \n",
    "    hyper_info = hyper_info, \n",
    "    trainset_x = trainset_x,\n",
    "    trainset_y = trainset_y,\n",
    "    n_iter = 20, \n",
    "    meta_learner = 'Extra Trees', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T14:01:15.341758Z",
     "start_time": "2021-09-11T14:01:14.568771Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##### optimization history plot #####\n",
    "optuna_history(best_param, all_score, 4, 3, model = 'Stacking Classifier')\n",
    "            \n",
    "##### best hyperparameter table #####\n",
    "param_table = pd.DataFrame(best_param).T\n",
    "param_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T07:53:08.099099Z",
     "start_time": "2021-09-13T07:52:02.395018Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 4.1906757464641176e-05\n",
      "\n",
      " Dataset 1:\n",
      "Precision: 0.0006370921771786875 \n",
      "Recall: 0.5588235294117647 \n",
      "Aging Rate: 0.6248926139339969\n",
      "\n",
      " Dataset 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007584695769425249 \n",
      "Recall: 0.7941176470588235 \n",
      "Aging Rate: 0.7458983761131482\n",
      "\n",
      " Dataset 3:\n",
      "Precision: 0.0008131805968038467 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.5926453640649555\n",
      "\n",
      " Dataset 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007052186177715092 \n",
      "Recall: 0.6470588235294118 \n",
      "Aging Rate: 0.653661602933473\n",
      "\n",
      " Dataset 5:\n",
      "Precision: 0.0006040759228054558 \n",
      "Recall: 0.5588235294117647 \n",
      "Aging Rate: 0.6590466212676794\n",
      "\n",
      " Dataset 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007614454773311336 \n",
      "Recall: 0.7058823529411765 \n",
      "Aging Rate: 0.6604295442640126\n",
      "\n",
      " Dataset 7:\n",
      "Precision: 0.000676328502415459 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.6506024096385542\n",
      "\n",
      " Dataset 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007656381594058648 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.5473441592456784\n",
      "\n",
      " Dataset 9:\n",
      "Precision: 0.0007172929985966006 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.6718700890518596\n",
      "\n",
      " Dataset 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0022210581456259823\n",
      "\n",
      " Dataset 1:\n",
      "Precision: 0.0006810829218457347 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.6152959664745941\n",
      "\n",
      " Dataset 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007140307033202428 \n",
      "Recall: 0.7647058823529411 \n",
      "Aging Rate: 0.7629753797799895\n",
      "\n",
      " Dataset 3:\n",
      "Precision: 0.0007634912372028457 \n",
      "Recall: 0.6470588235294118 \n",
      "Aging Rate: 0.6037716081718177\n",
      "\n",
      " Dataset 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0008231864174241125 \n",
      "Recall: 0.7058823529411765 \n",
      "Aging Rate: 0.6108957569408067\n",
      "\n",
      " Dataset 5:\n",
      "Precision: 0.0006610009442870633 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.665688842325825\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset 6:\n",
      "Precision: 0.0006529972574115189 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.6417600838135149\n",
      "\n",
      " Dataset 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007427501130271912 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.6488423258250393\n",
      "\n",
      " Dataset 8:\n",
      "Precision: 0.0006394270733422853 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.6553797799895233\n",
      "\n",
      " Dataset 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007075775119001673 \n",
      "Recall: 0.6470588235294118 \n",
      "Aging Rate: 0.6514824515453117\n",
      "\n",
      " Dataset 0:\n",
      "[15:52:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 6.286013619696175e-05\n",
      "\n",
      " Dataset 1:\n",
      "[15:52:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006890441972635102 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.6385961236249346\n",
      "\n",
      " Dataset 2:\n",
      "[15:52:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.000731858357259472 \n",
      "Recall: 0.7647058823529411 \n",
      "Aging Rate: 0.7443897328444211\n",
      "\n",
      " Dataset 3:\n",
      "[15:52:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0007877375521055569 \n",
      "Recall: 0.7058823529411765 \n",
      "Aging Rate: 0.6383865898376113\n",
      "\n",
      " Dataset 4:\n",
      "[15:52:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006387939570091667 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.6560293347302253\n",
      "\n",
      " Dataset 5:\n",
      "[15:52:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006944663657312415 \n",
      "Recall: 0.6470588235294118 \n",
      "Aging Rate: 0.6637820848611838\n",
      "\n",
      " Dataset 6:\n",
      "[15:52:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006975269499048827 \n",
      "Recall: 0.6470588235294118 \n",
      "Aging Rate: 0.6608695652173913\n",
      "\n",
      " Dataset 7:\n",
      "[15:52:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0008229863670519197 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.5855840754321634\n",
      "\n",
      " Dataset 8:\n",
      "[15:52:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0007637643620907219 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.6309900471451021\n",
      "\n",
      " Dataset 9:\n",
      "[15:52:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0007360414987206897 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.5978208486118387\n",
      "\n",
      " Dataset 0:\n",
      "[15:52:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 4.1906757464641176e-05\n",
      "\n",
      " Dataset 1:\n",
      "[15:52:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006270834020924783 \n",
      "Recall: 0.5588235294117647 \n",
      "Aging Rate: 0.6348664222105814\n",
      "\n",
      " Dataset 2:\n",
      "[15:52:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006738355279782126 \n",
      "Recall: 0.7058823529411765 \n",
      "Aging Rate: 0.7462964903090623\n",
      "\n",
      " Dataset 3:\n",
      "[15:52:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0008297320656871219 \n",
      "Recall: 0.7058823529411765 \n",
      "Aging Rate: 0.606076479832373\n",
      "\n",
      " Dataset 4:\n",
      "[15:52:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0008401460561605326 \n",
      "Recall: 0.7647058823529411 \n",
      "Aging Rate: 0.6484442116291252\n",
      "\n",
      " Dataset 5:\n",
      "[15:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0006503125112901478 \n",
      "Recall: 0.5294117647058824 \n",
      "Aging Rate: 0.5799685699319015\n",
      "\n",
      " Dataset 6:\n",
      "[15:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.000705543114819473 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.6830591932949188\n",
      "\n",
      " Dataset 7:\n",
      "[15:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0007161528270132846 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.585165007857517\n",
      "\n",
      " Dataset 8:\n",
      "[15:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0007756382153576367 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.6213305395495023\n",
      "\n",
      " Dataset 9:\n",
      "[15:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision: 0.0007076389626012808 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.5922053431115767\n",
      "\n",
      " Dataset 0:\n",
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 4.1906757464641176e-05\n",
      "\n",
      " Dataset 1:\n",
      "Precision: 0.0005635496111507683 \n",
      "Recall: 0.4411764705882353 \n",
      "Aging Rate: 0.5577160817181771\n",
      "\n",
      " Dataset 2:\n",
      "Precision: 0.0007561551025346319 \n",
      "Recall: 0.7352941176470589 \n",
      "Aging Rate: 0.6927606076479832\n",
      "\n",
      " Dataset 3:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0007829738771442808 \n",
      "Recall: 0.6470588235294118 \n",
      "Aging Rate: 0.5887480356207438\n",
      "\n",
      " Dataset 4:\n",
      "Precision: 0.0008110300081103001 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.5167103195390257\n",
      "\n",
      " Dataset 5:\n",
      "Precision: 0.0006466678641997485 \n",
      "Recall: 0.5294117647058824 \n",
      "Aging Rate: 0.5832372970141435\n",
      "\n",
      " Dataset 6:\n",
      "Precision: 0.0006537609414157557 \n",
      "Recall: 0.5294117647058824 \n",
      "Aging Rate: 0.5769093766369827\n",
      "\n",
      " Dataset 7:\n",
      "Precision: 0.0008170490910328862 \n",
      "Recall: 0.7058823529411765 \n",
      "Aging Rate: 0.6154845468831849\n",
      "\n",
      " Dataset 8:\n",
      "Precision: 0.0007540394973070018 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.5835515976951283\n",
      "\n",
      " Dataset 9:\n",
      "Precision: 0.0008249852681202121 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.5333682556312206\n",
      "\n",
      " Dataset 0:\n",
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.00020953378732320587\n",
      "\n",
      " Dataset 1:\n",
      "Precision: 0.0006217101172959755 \n",
      "Recall: 0.4411764705882353 \n",
      "Aging Rate: 0.5055421686746988\n",
      "\n",
      " Dataset 2:\n",
      "Precision: 0.0006132147784761613 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.683394447354636\n",
      "\n",
      " Dataset 3:\n",
      "Precision: 0.0008159789974101536 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.5906128863279204\n",
      "\n",
      " Dataset 4:\n",
      "Precision: 0.0008032435740514076 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.5478051335777894\n",
      "\n",
      " Dataset 5:\n",
      "Precision: 0.0006607201850016518 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.6342587742273441\n",
      "\n",
      " Dataset 6:\n",
      "Precision: 0.000646324454876348 \n",
      "Recall: 0.5588235294117647 \n",
      "Aging Rate: 0.6159664745940283\n",
      "\n",
      " Dataset 7:\n",
      "Precision: 0.0007721439864690958 \n",
      "Recall: 0.6176470588235294 \n",
      "Aging Rate: 0.569869041382923\n",
      "\n",
      " Dataset 8:\n",
      "Precision: 0.0007164093563061933 \n",
      "Recall: 0.5882352941176471 \n",
      "Aging Rate: 0.5849554740701938\n",
      "\n",
      " Dataset 9:\n",
      "Precision: 0.00090192541468962 \n",
      "Recall: 0.6764705882352942 \n",
      "Aging Rate: 0.5343321110529072\n"
     ]
    }
   ],
   "source": [
    "def tableau_classifier(hyper, num_set, trainset_x, trainset_y, test_x, test_y) :\n",
    "    \n",
    "    temp_models = pd.DataFrame()\n",
    "    for model in hyper.keys() :\n",
    "    \n",
    "        if model == 'XGBoost' :\n",
    "\n",
    "            temp_uni = runall_XGBoostC(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                       hyper['XGBoost']['univariate-TPE'], record_bad = False)\n",
    "            temp_multi = runall_XGBoostC(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                         hyper['XGBoost']['multivariate-TPE'], record_bad = False)\n",
    "            \n",
    "        elif model == 'LightGBM' :\n",
    "            \n",
    "            temp_uni = runall_LightGBMC(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                        hyper['LightGBM']['univariate-TPE'], record_bad = False)\n",
    "            temp_multi = runall_LightGBMC(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                          hyper['LightGBM']['multivariate-TPE'], record_bad = False)\n",
    "            \n",
    "        elif model == 'CatBoost' :\n",
    "            \n",
    "            temp_uni = runall_CatBoostC(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                        hyper['CatBoost']['univariate-TPE'], cat_feature = [], record_bad = False)\n",
    "            temp_multi = runall_CatBoostC(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                          hyper['CatBoost']['multivariate-TPE'], cat_feature = [], record_bad = False)\n",
    "            \n",
    "        temp_uni['Sampler'] = 'univariate-TPE'\n",
    "        temp_uni['Model'] = model\n",
    "        temp_multi['Sampler'] = 'multivaraite-TPE'\n",
    "        temp_multi['Model'] = model\n",
    "        \n",
    "        temp_samplers = pd.concat([temp_uni, temp_multi], axis = 0)\n",
    "        temp_models = pd.concat([temp_models, temp_samplers], axis = 0)\n",
    "        final_models = temp_models.reset_index().rename(columns = {'index': 'dataset'})\n",
    "        \n",
    "    return final_models\n",
    "\n",
    "\n",
    "\n",
    "hyper_info = {\n",
    "    'num_set': 10,\n",
    "    'date': '20210914',\n",
    "    'model_list': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'iter_list': [250, 250, 250],\n",
    "    'filename': 'runhist_array_m8m3_joinnewevent_eqp+rework',\n",
    "    'mode': 'C',\n",
    "    'sampler_list': ['univariate-TPE', 'multivariate-TPE']\n",
    "}\n",
    "\n",
    "output_hyper = tableau_hyper(**hyper_info)\n",
    "        \n",
    "table_C = tableau_classifier(output_hyper, 10, trainset_x, trainset_y, run_test_x, run_test_y)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T08:02:20.986832Z",
     "start_time": "2021-09-13T07:53:08.630364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\aging\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 8.60987633180383e-05 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00067991] ,   Aging Rate: [0.73963332]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.2354142175051204 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00072734] ,   Aging Rate: [0.69139864]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.6546842580783218 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00071835] ,   Aging Rate: [0.70005238]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.2452390864930486 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00073329] ,   Aging Rate: [0.68578313]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.37404649794061257 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00074377] ,   Aging Rate: [0.67612362]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.1640264323601627 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00068292] ,   Aging Rate: [0.73636459]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.5505206546869672 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00087114] ,   Aging Rate: [0.57726558]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.22228451542768662 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00070786] ,   Aging Rate: [0.71042431]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.06550840593694722 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.0007053] ,   Aging Rate: [0.71300157]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.2135722211156239 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00078642] ,   Aging Rate: [0.63945521]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 1.1880340089441421e-07 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00062246] ,   Aging Rate: [0.80789942]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.10804295247355201 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00066847] ,   Aging Rate: [0.75228916]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.3583469485620142 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00066836] ,   Aging Rate: [0.75241488]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.09881832713626662 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075401] ,   Aging Rate: [0.66694605]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.5063399517272242 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00080515] ,   Aging Rate: [0.62457831]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.37793794750361254 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00078557] ,   Aging Rate: [0.64014667]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.19439797824546523 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00076094] ,   Aging Rate: [0.66086957]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.17357826033062895 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00072771] ,   Aging Rate: [0.69104243]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.14900988223809553 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00069697] ,   Aging Rate: [0.7215296]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.25124413518879707 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075379] ,   Aging Rate: [0.66713463]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 0.0027317816857248545 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.0006781] ,   Aging Rate: [0.74160293]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.19529813528060913 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00068505] ,   Aging Rate: [0.73408067]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.18385499715805054 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.0006375] ,   Aging Rate: [0.78883185]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.36179596185684204 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00072842] ,   Aging Rate: [0.69037192]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.3106038272380829 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.0007284] ,   Aging Rate: [0.69039288]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.3350037634372711 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00080421] ,   Aging Rate: [0.62531168]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.3526906967163086 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075641] ,   Aging Rate: [0.66482975]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.325070858001709 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00070688] ,   Aging Rate: [0.71140911]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.36835378408432007 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00078963] ,   Aging Rate: [0.63685699]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.4988952875137329 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00088509] ,   Aging Rate: [0.56817182]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: -0.00113894313108176 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075134] ,   Aging Rate: [0.66931378]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.2101210057735443 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00066322] ,   Aging Rate: [0.75823992]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.38852301239967346 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00074536] ,   Aging Rate: [0.67467784]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.4736872911453247 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00082228] ,   Aging Rate: [0.61156627]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.4145931005477905 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00101437] ,   Aging Rate: [0.49575694]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.40148240327835083 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00079077] ,   Aging Rate: [0.63593504]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.3888906240463257 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00076857] ,   Aging Rate: [0.65431116]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.39102908968925476 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.0007908] ,   Aging Rate: [0.63591409]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.42225542664527893 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00084078] ,   Aging Rate: [0.5981142]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.3571353554725647 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075693] ,   Aging Rate: [0.66436878]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 0.0003698144721331456 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075717] ,   Aging Rate: [0.66415925]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.3761366446002115 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075401] ,   Aging Rate: [0.66694605]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.22180349758737872 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00061789] ,   Aging Rate: [0.81387114]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.3223894803842625 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00075209] ,   Aging Rate: [0.66864327]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.4331247679993924 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.0008115] ,   Aging Rate: [0.61969618]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.3553697001764127 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00074564] ,   Aging Rate: [0.6744264]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.39593951390589993 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00076761] ,   Aging Rate: [0.65512834]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.3391315701942753 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00074136] ,   Aging Rate: [0.67832373]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.407448180514626 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00076492] ,   Aging Rate: [0.65743321]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.5087717529224755 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00085525] ,   Aging Rate: [0.58799371]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: -0.0007243723941524481 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00067904] ,   Aging Rate: [0.74057622]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.43848529362982536 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00078194] ,   Aging Rate: [0.64312205]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.39532144689509197 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00065735] ,   Aging Rate: [0.76500786]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.31148592229473937 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00072982] ,   Aging Rate: [0.68905186]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.4601018175005041 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00082006] ,   Aging Rate: [0.61322158]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.40659657131760324 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00066737] ,   Aging Rate: [0.75352541]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.3012698279726932 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00066657] ,   Aging Rate: [0.7544264]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.46097943522828094 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00080521] ,   Aging Rate: [0.62453641]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.45288865668708767 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00077889] ,   Aging Rate: [0.64563646]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.14049801283966598 \n",
      "\n",
      "Recall: [0.70588235] ,   Precision: [0.00060874] ,   Aging Rate: [0.82610791]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 4.761245483358789e-05 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00069551] ,   Aging Rate: [0.84354112]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.04029081432490746 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00073842] ,   Aging Rate: [0.79453117]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.10990798423813072 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.0006882] ,   Aging Rate: [0.85250917]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.027707191547884622 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00068879] ,   Aging Rate: [0.8517758]\n",
      "\n",
      " Dataset 4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.03683226577238504 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00074182] ,   Aging Rate: [0.79088528]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.090878813575646 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00076411] ,   Aging Rate: [0.76781561]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.12229180193182403 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00077095] ,   Aging Rate: [0.76100576]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.13986249223211952 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00075417] ,   Aging Rate: [0.77793609]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.006200695344266687 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072376] ,   Aging Rate: [0.81062336]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.020660312907986127 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00075372] ,   Aging Rate: [0.77839707]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 6.857744625792842e-08 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00067797] ,   Aging Rate: [0.86537454]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.015317000415591496 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00070605] ,   Aging Rate: [0.83094814]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.15794573887766933 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00073143] ,   Aging Rate: [0.80211629]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.009104885991922855 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00069634] ,   Aging Rate: [0.84253536]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.2993916806573719 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00079159] ,   Aging Rate: [0.74116291]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.07928481072513198 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072858] ,   Aging Rate: [0.8052593]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.0017000125840016876 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.0006941] ,   Aging Rate: [0.8452593]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.03319432596609258 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00075868] ,   Aging Rate: [0.7733054]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.028542402851911227 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.0007229] ,   Aging Rate: [0.81158722]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.02970058325360315 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00077378] ,   Aging Rate: [0.75821896]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 0.0026297506410628557 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00068938] ,   Aging Rate: [0.85104243]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.06701523065567017 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00070679] ,   Aging Rate: [0.83008905]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.10648676753044128 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00067141] ,   Aging Rate: [0.87381875]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.22224067151546478 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072786] ,   Aging Rate: [0.80605553]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.17017386853694916 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.000711] ,   Aging Rate: [0.82516501]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.15228508412837982 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00078086] ,   Aging Rate: [0.75134625]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.23050054907798767 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00075811] ,   Aging Rate: [0.77389209]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.19552433490753174 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00070479] ,   Aging Rate: [0.83243583]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.29957544803619385 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00084467] ,   Aging Rate: [0.69458355]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.21100342273712158 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00069056] ,   Aging Rate: [0.84959665]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: -0.0022720657289028168 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00073136] ,   Aging Rate: [0.8022001]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.10785581916570663 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00069782] ,   Aging Rate: [0.84075432]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.21995726227760315 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00071066] ,   Aging Rate: [0.82556312]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.2701001465320587 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072464] ,   Aging Rate: [0.80963855]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.08721035718917847 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00074156] ,   Aging Rate: [0.79115767]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.25359076261520386 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00078822] ,   Aging Rate: [0.74432687]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.23400789499282837 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072659] ,   Aging Rate: [0.8074594]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.2949516475200653 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00083042] ,   Aging Rate: [0.70650602]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.20837661623954773 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072133] ,   Aging Rate: [0.8133473]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.18667861819267273 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00072884] ,   Aging Rate: [0.80496595]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: 0.00031472429228330016 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.0007718] ,   Aging Rate: [0.76016763]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.26109397640698534 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00074553] ,   Aging Rate: [0.78694605]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.17808970197592916 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.0006903] ,   Aging Rate: [0.84991095]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.1301460567878272 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00065657] ,   Aging Rate: [0.89357779]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.30448950254029306 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00076978] ,   Aging Rate: [0.7621582]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.13849852550935254 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00066545] ,   Aging Rate: [0.88165532]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.27702293953264634 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00074328] ,   Aging Rate: [0.78933473]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.2748912818494415 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00077853] ,   Aging Rate: [0.75358827]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.29542056401948646 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00076545] ,   Aging Rate: [0.76647459]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.40716652127722835 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00085109] ,   Aging Rate: [0.68934521]\n",
      "\n",
      " Dataset 0:\n",
      "Best Threshold: -0.0008689819472005313 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00073555] ,   Aging Rate: [0.79763227]\n",
      "\n",
      " Dataset 1:\n",
      "Best Threshold: 0.25879413178967237 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00074039] ,   Aging Rate: [0.79241488]\n",
      "\n",
      " Dataset 2:\n",
      "Best Threshold: 0.35130761670905664 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00071811] ,   Aging Rate: [0.81699319]\n",
      "\n",
      " Dataset 3:\n",
      "Best Threshold: 0.17165502842037078 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00067818] ,   Aging Rate: [0.86510215]\n",
      "\n",
      " Dataset 4:\n",
      "Best Threshold: 0.37109285348482324 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00079298] ,   Aging Rate: [0.7398638]\n",
      "\n",
      " Dataset 5:\n",
      "Best Threshold: 0.30488617138079843 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00068766] ,   Aging Rate: [0.85317968]\n",
      "\n",
      " Dataset 6:\n",
      "Best Threshold: 0.1831326416741771 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00067621] ,   Aging Rate: [0.86761655]\n",
      "\n",
      " Dataset 7:\n",
      "Best Threshold: 0.26921487015951373 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00073795] ,   Aging Rate: [0.79503405]\n",
      "\n",
      " Dataset 8:\n",
      "Best Threshold: 0.41859653408197633 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00085454] ,   Aging Rate: [0.68655841]\n",
      "\n",
      " Dataset 9:\n",
      "Best Threshold: 0.10704126386584806 \n",
      "\n",
      "Recall: [0.82352941] ,   Precision: [0.00066058] ,   Aging Rate: [0.88815086]\n"
     ]
    }
   ],
   "source": [
    "def tableau_regressor(hyper, num_set, trainset_x, trainset_y, test_x, test_y, thres_target, threshold) :\n",
    "    \n",
    "    temp_models = pd.DataFrame()\n",
    "    for model in hyper.keys() :\n",
    "    \n",
    "        if model == 'XGBoost' :\n",
    "\n",
    "            _, temp_uni = runall_XGBoostR(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                          hyper['XGBoost']['univariate-TPE'], thres_target = thres_target, \n",
    "                                          threshold = threshold, record_bad = False)\n",
    "            _, temp_multi = runall_XGBoostR(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                            hyper['XGBoost']['multivariate-TPE'], thres_target = thres_target, \n",
    "                                            threshold = threshold, record_bad = False)\n",
    "            \n",
    "        elif model == 'LightGBM' :\n",
    "            \n",
    "            _, temp_uni = runall_LightGBMR(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                           hyper['LightGBM']['univariate-TPE'], thres_target = thres_target, \n",
    "                                           threshold = threshold, record_bad = False)\n",
    "            _, temp_multi = runall_LightGBMR(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                             hyper['LightGBM']['multivariate-TPE'], thres_target = thres_target, \n",
    "                                             threshold = threshold, record_bad = False)\n",
    "            \n",
    "        elif model == 'CatBoost' :\n",
    "            \n",
    "            _, temp_uni = runall_CatBoostR(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                           hyper['CatBoost']['univariate-TPE'], cat_feature = [], \n",
    "                                           thres_target = thres_target, threshold = threshold, record_bad = False)\n",
    "            _, temp_multi = runall_CatBoostR(num_set, trainset_x, test_x, trainset_y, test_y, \n",
    "                                             hyper['CatBoost']['multivariate-TPE'], cat_feature = [], \n",
    "                                             thres_target = thres_target, threshold = threshold, record_bad = False)\n",
    "            \n",
    "        temp_uni['Sampler'] = 'univariate-TPE'\n",
    "        temp_uni['Model'] = model\n",
    "        temp_multi['Sampler'] = 'multivaraite-TPE'\n",
    "        temp_multi['Model'] = model\n",
    "        \n",
    "        temp_samplers = pd.concat([temp_uni, temp_multi], axis = 0)\n",
    "        temp_models = pd.concat([temp_models, temp_samplers], axis = 0)\n",
    "        final_models = temp_models.reset_index().rename(columns = {'index': 'Dataset'})\n",
    "        \n",
    "    return final_models\n",
    "\n",
    "\n",
    "\n",
    "hyper_info = {\n",
    "    'num_set': 10,\n",
    "    'date': '20210914',\n",
    "    'model_list': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'iter_list': [250, 250, 250],\n",
    "    'filename': 'runhist_array_m8m3_joinnewevent_eqp+rework',\n",
    "    'mode': 'R',\n",
    "    'sampler_list': ['univariate-TPE', 'multivariate-TPE']\n",
    "}\n",
    "\n",
    "output_hyper = tableau_hyper(**hyper_info)\n",
    "        \n",
    "table_7 = tableau_regressor(output_hyper, 10, trainset_x, trainset_y, run_test_x, run_test_y, 'Recall', threshold = 0.7)    \n",
    "table_8 = tableau_regressor(output_hyper, 10, trainset_x, trainset_y, run_test_x, run_test_y, 'Recall', threshold = 0.8)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T08:02:21.690276Z",
     "start_time": "2021-09-13T08:02:21.596903Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f'{hyper_info[\"filename\"]}_result_table.xlsx') as writer :  \n",
    "    table_C.to_excel(writer, sheet_name = 'classifier')\n",
    "    table_7.to_excel(writer, sheet_name = 'regressor_7')\n",
    "    table_8.to_excel(writer, sheet_name = 'regressor_8')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
