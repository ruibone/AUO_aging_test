{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:26:00.580174Z",
     "start_time": "2021-12-24T08:25:58.834761Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import shap\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio, train_col\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, AUC, PR_curve, multiple_curve, \\\n",
    "    best_threshold, all_optuna\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:08:29.719550Z",
     "start_time": "2021-12-24T08:08:29.698765Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### GPU ??? #####\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:08:32.429049Z",
     "start_time": "2021-12-24T08:08:32.411324Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunhistSet(Dataset):\n",
    "    \n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.x = torch.tensor(train_x.values.astype(np.float32))\n",
    "        self.y = torch.tensor(train_y.values.astype(np.float32)).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.x[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:08:34.412498Z",
     "start_time": "2021-12-24T08:08:34.386690Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkC(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super(NeuralNetworkC, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class NeuralNetworkR(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkR, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(114, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T16:16:08.969799Z",
     "start_time": "2021-12-15T16:16:08.939879Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.cls = classes\n",
    "        self.dim = dim                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            pred = pred * self.weight.unsqueeze(0)   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:08:59.397563Z",
     "start_time": "2021-12-24T08:08:59.378863Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainingC(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop):\n",
    "    \n",
    "    network.train()\n",
    "    best_model = network\n",
    "    best_objective = 0\n",
    "    stop_trigger = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        \n",
    "        total_loss = 0\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0  \n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)         \n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            TP += torch.dot((predicted == y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "            TN += torch.dot((predicted == y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "            FN += torch.dot((predicted != y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "            FP += torch.dot((predicted != y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "            total_loss += loss.item()*len(y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss.append(total_loss)\n",
    "        recall = TP / (TP + FN)\n",
    "        precision = TP / (TP + FP) if FP != 0 else 0\n",
    "        aging = (TP + FP) / (TP + TN + FP + FN)\n",
    "        f1 = 2*(recall*precision) / (recall + precision) if (recall + precision) != 0 else 0\n",
    "            \n",
    "        print(f'Epoch {i+1}: Train Loss = {total_loss / (TP + TN + FP + FN)}, Recall = {recall}, Aging Rate = {aging}, Precision = {precision}, f1 = {f1}')\n",
    "\n",
    "        if ((i+1) % 5 == 0):\n",
    "            five_loss, valid_objective, _ = testingC(network, validloader, criterion)\n",
    "            valid_loss.append(five_loss)\n",
    "            \n",
    "            if valid_objective > best_objective:\n",
    "                best_objective = valid_objective\n",
    "                best_model = network\n",
    "                torch.save(best_model, f'{filename}_NeuralNetworkC_{epoch}.ckpt')\n",
    "                print(f'Model in epoch {i+1} is saved.\\n')\n",
    "                stop_trigger = 0\n",
    "            else:\n",
    "                stop_trigger += 1\n",
    "                print('')\n",
    "                \n",
    "            if stop_trigger == early_stop:\n",
    "                print(f'Training Finished at epoch {i+1}.')\n",
    "                return network, train_loss, valid_loss\n",
    "            \n",
    "    return network, train_loss, valid_loss\n",
    "\n",
    "\n",
    "def PR_matrix(predict):\n",
    "    \n",
    "    Y_new = predict.sort_values(['predict', 'truth'], ascending = [False, True]).reset_index(drop = True)\n",
    "    Y_new.loc[Y_new['truth'] != 1, 'truth'] = 0\n",
    "    \n",
    "    matrix = pd.DataFrame(Y_new.groupby('predict').sum()).rename(columns = {'truth': 'Bad_Count'})\n",
    "    matrix = matrix.sort_index(ascending = False)\n",
    "    matrix['All_Count'] = Y_new.groupby('predict').count()\n",
    "    matrix['Class_Prob'] = matrix.index\n",
    "    \n",
    "    matrix['TP'] = matrix['Bad_Count'].cumsum()\n",
    "    matrix['FP'] = matrix['All_Count'].cumsum() - matrix['TP']\n",
    "    matrix['FN'] = matrix['TP'].values[-1] - matrix['TP']\n",
    "    matrix['TN'] = matrix['FP'].values[-1] - matrix['FP']\n",
    "    \n",
    "    matrix['Precision'] = matrix['TP'] / (matrix['TP'] + matrix['FP'])\n",
    "    matrix['Recall'] = matrix['TP'] / (matrix['TP'] + matrix['FN'])\n",
    "    matrix['Aging Rate'] = (matrix['TP'] + matrix['FP']) / (matrix['TP'] + matrix['FP'] + matrix['FN'] + matrix['TN'])\n",
    "    matrix['Efficiency'] = matrix['Recall'] / matrix['Aging Rate']\n",
    "    matrix['Score'] = score1(matrix['Recall'], matrix['Aging Rate'])            \n",
    "    matrix = matrix.drop(columns = ['Bad_Count', 'All_Count']).reset_index(drop = True)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def trainingR(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop):\n",
    "    \n",
    "    network.train()\n",
    "    best_model = network\n",
    "    best_objective = 1\n",
    "    stop_trigger = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        \n",
    "        total_loss = 0\n",
    "        predict_vector = torch.tensor([0])\n",
    "        y_vector = torch.tensor([0])\n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.type(torch.FloatTensor).to(device)\n",
    "            y = y.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()*len(y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            predict_vector = torch.cat((predict_vector, output.data[:,0]), axis = 0)\n",
    "            y_vector = torch.cat((y_vector, y[:,0]), axis = 0)       \n",
    "        result_df = pd.DataFrame(dict(predict = predict_vector, truth = y_vector))\n",
    "        pr_matrix = PR_matrix(result_df.iloc[1:, :])\n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = 'Recall', threshold = 0.7)\n",
    "        auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "        train_loss.append(total_loss)\n",
    "        \n",
    "        recall = best_data[\"Recall\"].values\n",
    "        aging = best_data[\"Aging Rate\"].values\n",
    "        print(f'Epoch {i+1}: Train Loss = {total_loss}, AUC = {auc}, Recall(0.7) = {recall}, Aging Rate = {aging}')\n",
    "        \n",
    "        if ((i+1) % 5 == 0):\n",
    "            five_loss, valid_auc, _ = testingR(network, validloader, criterion)\n",
    "            valid_loss.append(five_loss)\n",
    "            \n",
    "            if valid_auc < best_objective:\n",
    "                best_objective = valid_auc\n",
    "                best_model = network\n",
    "                torch.save(best_model, f'{filename}_NeuralNetworkR_{epoch}.ckpt')\n",
    "                print(f'Model in epoch {i+1} is saved.\\n')\n",
    "                stop_trigger = 0\n",
    "            else:\n",
    "                stop_trigger += 1\n",
    "                print('')\n",
    "                \n",
    "            if stop_trigger == early_stop:\n",
    "                print(f'Training Finished at epoch {i+1}.')\n",
    "                return network, train_loss, valid_loss\n",
    "      \n",
    "    return network, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:09:04.957459Z",
     "start_time": "2021-12-24T08:09:04.930764Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingC(network, dataloader, criterion):\n",
    "    \n",
    "    network.eval()\n",
    "    total_loss = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = network(x)\n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        TP += torch.dot((predicted == y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "        TN += torch.dot((predicted == y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "        FN += torch.dot((predicted != y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "        FP += torch.dot((predicted != y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "        total_loss += loss.item()*len(y)\n",
    "        \n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    aging = (TP + FP) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    beta = 0.3\n",
    "    fscore = ((1+beta**2)*recall*precision) / (recall+(beta**2)*precision) if (recall + precision) != 0 else 0\n",
    "    if aging != 0:\n",
    "        efficiency = recall / aging\n",
    "        score = score1(recall, aging)\n",
    "    else:\n",
    "        efficiency = 0\n",
    "        score = 0\n",
    "        \n",
    "    print(f'Test Loss = {total_loss / (TP + TN + FP + FN)}, Recall = {recall}, Aging Rate = {aging}, precision = {precision}')\n",
    "    \n",
    "    valid_objective = fscore\n",
    "    table = pd.Series({'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN, 'Precision': precision, 'Recall': recall, \n",
    "                       'Aging Rate': aging, 'Efficiency': efficiency, 'fscore': fscore, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    return total_loss, valid_objective, table\n",
    "\n",
    "\n",
    "def testingR(network, dataloader, criterion):\n",
    "    \n",
    "    network.eval()   \n",
    "    total_loss = 0\n",
    "    predict_vector = torch.tensor([0])\n",
    "    y_vector = torch.tensor([0])\n",
    "    for x, y in dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y = y.unsqueeze(1)\n",
    "        output = network(x)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.item()*len(y)\n",
    "        \n",
    "        predict_vector = torch.cat((predict_vector, output.data[:,0]), axis = 0)\n",
    "        y_vector = torch.cat((y_vector, y[:,0]), axis = 0)\n",
    "    result_df = pd.DataFrame(dict(predict = predict_vector, truth = y_vector))\n",
    "    pr_matrix = PR_matrix(result_df.iloc[1:, :])\n",
    "    best_data, best_thres = best_threshold(pr_matrix, target = 'Recall', threshold = 0.7)\n",
    "    auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "    recall = best_data['Recall'].values[0]\n",
    "    aging = best_data['Aging Rate'].values[0]\n",
    "    precision = best_data['Precision'].values[0]\n",
    "    efficiency = best_data['Efficiency'].values[0]\n",
    "    score = best_data['Score'].values[0]\n",
    "    TP = best_data['TP'].values[0]\n",
    "    FP = best_data['FP'].values[0]\n",
    "    TN = best_data['TN'].values[0]\n",
    "    FN = best_data['FN'].values[0]\n",
    "        \n",
    "    print(f'\\nTest Loss = {total_loss}, Recall = {recall}, Aging Rate = {aging}, Efficiency = {efficiency}')\n",
    "    \n",
    "    valid_objective = auc\n",
    "    table = pd.Series({'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN, 'Precision': precision, 'Recall': recall,\n",
    "                       'Aging Rate': aging,'Efficiency': efficiency, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    return total_loss, valid_objective, table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:09:11.213755Z",
     "start_time": "2021-12-24T08:09:11.194767Z"
    }
   },
   "outputs": [],
   "source": [
    "def runall_nn(train_x, train_y, test_x, test_y, n_epoch, config, filename, early_stop, mode):\n",
    "    \n",
    "    set_name = list(train_x.keys())\n",
    "    result_table = pd.DataFrame()\n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    judge = list(config.keys())[0]\n",
    "    for num, i in enumerate(tqdm(set_name)):\n",
    "        print(f'\\nStarting training Dataset {num}:')\n",
    "        \n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[i]\n",
    "        else :\n",
    "            best_config = config\n",
    "        \n",
    "        # data preparation\n",
    "        train_ratio = 0.75\n",
    "        train_data = RunhistSet(train_x[i], train_y[i])\n",
    "        test_data = RunhistSet(test_x, test_y)\n",
    "        train_size = int(len(train_data)*train_ratio)\n",
    "        valid_size = len(train_data) - train_size\n",
    "        train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "        train_loader = DataLoader(train_data, batch_size = best_config['batch_size'], shuffle = True)\n",
    "        valid_loader = DataLoader(valid_data, batch_size = best_config['batch_size'], shuffle = False)\n",
    "        test_loader = DataLoader(test_data, batch_size = best_config['batch_size'], shuffle = False)\n",
    "        \n",
    "        modelC = NeuralNetworkC(dim = train_x[i].shape[1]).to(device)\n",
    "        optimizerC = torch.optim.Adam(modelC.parameters(), lr = best_config['learning_rate'], \n",
    "                                      weight_decay = best_config['weight_decay'])\n",
    "        criterionC = nn.CrossEntropyLoss(\n",
    "            weight = torch.tensor([1-best_config['bad_weight'], best_config['bad_weight']])).to(device)\n",
    "        \n",
    "        # training\n",
    "        if mode == 'C':\n",
    "            done_model, train_loss, valid_loss = trainingC(network = modelC, \n",
    "                                                           trainloader = train_loader, \n",
    "                                                           validloader = valid_loader, \n",
    "                                                           optimizer = optimizerC, \n",
    "                                                           criterion = criterionC, \n",
    "                                                           epoch = n_epoch, \n",
    "                                                           filename = filename, \n",
    "                                                           early_stop = early_stop)\n",
    "        elif mode == 'R':\n",
    "            pass\n",
    "        \n",
    "        train_dict[i] = train_loss\n",
    "        valid_dict[i] = valid_loss\n",
    "        \n",
    "        # testing\n",
    "        if mode == 'C':\n",
    "            _, _, table = testingC(done_model, test_loader, criterionC)\n",
    "        elif mode == 'R':\n",
    "            pass\n",
    "        result_table = pd.concat([result_table, table], axis = 0).rename({0: f'dataset {num}'})\n",
    "    loss_dict = dict(train = train_dict, valid = valid_dict)\n",
    "        \n",
    "    return result_table, loss_dict\n",
    "\n",
    "\n",
    "def loss_plot(train_loss, valid_loss, num_row, num_col):\n",
    "    \n",
    "    fig , axes = plt.subplots(num_row, num_col, sharex = False, sharey = False, figsize = (num_row*8 + 1, num_col*6))\n",
    "    plt.suptitle('Training & Validation Loss Curve', y = 0.94, fontsize = 30)\n",
    "    for row in range(num_row):\n",
    "        for col in range(num_col):\n",
    "            \n",
    "            index = num_col*row + col\n",
    "            if index < len(train_loss):\n",
    "                \n",
    "                train = train_loss[f'set{index}']\n",
    "                valid = valid_loss[f'set{index}']\n",
    "                axes[row, col].plot(range(len(train)), train, 'b-', linewidth = 5, label = 'train')\n",
    "                axes[row, col].plot(range(4, len(train)+1, 5), valid, 'r-', linewidth = 5, label = 'valid')\n",
    "                axes[row, col].set_xlabel('Epoch')\n",
    "                axes[row, col].set_ylabel('Total Loss')\n",
    "                axes[row, col].set_title(f'dataset {index}')\n",
    "                axes[row, col].legend(loc = 'upper right', fancybox = True, prop = dict(size = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:09:17.391049Z",
     "start_time": "2021-12-24T08:09:17.379081Z"
    }
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork_creator(train_data, mode, num_valid = 3, label = 'GB') :\n",
    "    \n",
    "    def objective(trial) :\n",
    "\n",
    "        param = {\n",
    "            'batch_size': trial.suggest_int('batch_size', 32, 96, step = 32),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', [1e-2, 1e-3, 1e-4]),\n",
    "            'weight_decay': trial.suggest_categorical('weight_decay', [1e-2, 1e-3, 1e-4]),\n",
    "            'bad_weight': trial.suggest_categorical('bad_weight', [0.5, 0.6, 0.7])\n",
    "        }\n",
    "\n",
    "        result_list = []\n",
    "        for i in range(num_valid):\n",
    "            \n",
    "            train_x, train_y = label_divide(train_data, None, label, train_only = True)\n",
    "            train_set = RunhistSet(train_x, train_y)\n",
    "            train_ratio = 0.75\n",
    "            train_size = int(len(train_data)*train_ratio)\n",
    "            valid_size = len(train_data) - train_size\n",
    "            training_data, validing_data = random_split(train_set, [train_size, valid_size])\n",
    "            train_loader = DataLoader(training_data, batch_size = param['batch_size'], shuffle = True)\n",
    "            valid_loader = DataLoader(validing_data, batch_size = param['batch_size'], shuffle = False)\n",
    "\n",
    "            if mode == 'C':\n",
    "                model = NeuralNetworkC(dim = train_x.shape[1]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = param['learning_rate'], \n",
    "                                             weight_decay = param['weight_decay'])\n",
    "                criterion = nn.CrossEntropyLoss(\n",
    "                    weight = torch.tensor([1-param['bad_weight'], param['bad_weight']])).to(device)\n",
    "\n",
    "                done_modelC, train_lossC, valid_lossC = trainingC(network = model, \n",
    "                                                                  trainloader = train_loader, \n",
    "                                                                  validloader = train_loader, \n",
    "                                                                  optimizer = optimizer, \n",
    "                                                                  criterion = criterion, \n",
    "                                                                  epoch = 100, \n",
    "                                                                  filename = 'tamama',\n",
    "                                                                  early_stop = 10)\n",
    "                _, valid_objective, _ = testingC(done_modelC, valid_loader, criterion)\n",
    "                result_list.append(valid_objective)\n",
    "\n",
    "            elif mode == 'R':\n",
    "                pass\n",
    "\n",
    "        return np.mean(result_list)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:14:29.638835Z",
     "start_time": "2021-12-24T08:14:27.170946Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### training data ### \n",
    "training_month = range(2, 5)\n",
    "\n",
    "data_dict, trainset_x, trainset_y = multiple_month(training_month, num_set = 10, filename = 'dataset')\n",
    "\n",
    "print('\\nCombined training data:\\n')\n",
    "run_train = multiple_set(num_set = 10)\n",
    "run_train_x, run_train_y = train_set(run_train, num_set = 10)\n",
    "\n",
    "### testing data ###\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of testing data:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:37:33.045099Z",
     "start_time": "2021-12-24T08:37:33.019168Z"
    }
   },
   "outputs": [],
   "source": [
    "##### data preparation #####\n",
    "target = 'set4'\n",
    "print(best_paramC[target])\n",
    "\n",
    "train_data = RunhistSet(run_train_x[target], run_train_y[target])\n",
    "test_data = RunhistSet(run_test_x, run_test_y)\n",
    "train_ratio = 0.75\n",
    "train_size = int(len(train_data)*train_ratio)\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = 64, shuffle = False)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:38:52.903379Z",
     "start_time": "2021-12-24T08:38:37.979839Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### model preparation #####\n",
    "# hyperparameter: learning rate, weight decay, weight\n",
    "modelC = NeuralNetworkC(dim = len(train_data[0][0])).to(device)\n",
    "optimizerC = torch.optim.Adam(modelC.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "criterionC = nn.CrossEntropyLoss(weight = torch.tensor([0.5, 0.5])).to(device)\n",
    "\n",
    "### label smoothing ###\n",
    "#criterionC = LabelSmoothingLoss(classes = 2, smoothing = 0.2)\n",
    "\n",
    "##### training #####\n",
    "done_modelC, train_lossC, valid_lossC = trainingC(network = modelC, \n",
    "                                                  trainloader = train_loader, \n",
    "                                                  validloader = valid_loader, \n",
    "                                                  optimizer = optimizerC, \n",
    "                                                  criterion = criterionC, \n",
    "                                                  epoch = 200, \n",
    "                                                  filename = 'tamama',\n",
    "                                                  early_stop = 20)\n",
    "\n",
    "##### testing #####\n",
    "_, _, result_tableC = testingC(done_modelC, test_loader, criterionC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:56:10.839153Z",
     "start_time": "2021-12-24T08:56:10.459077Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "sample_x, _ = batch\n",
    "background = sample_x[:32].to(device)\n",
    "tobetest = sample_x[16:].to(device)\n",
    "\n",
    "e = shap.DeepExplainer(modelC, background)\n",
    "shap_values = e.shap_values(tobetest)\n",
    "values = abs(shap_values[1]).mean(axis = 0)\n",
    "shap.summary_plot(shap_values, run_train_x[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T12:54:51.808548Z",
     "start_time": "2021-12-24T12:54:50.388473Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (24, 8))\n",
    "colname = run_train[target].columns.to_list()[:-1]\n",
    "plt.bar(colname, values, color = 'green')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('20211228_NeuralNetwork_ShapValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T15:08:00.678632Z",
     "start_time": "2021-11-26T15:08:00.628936Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### model preparation #####\n",
    "# hyperparameter: learning rate, weight decay, weight\n",
    "modelR = NeuralNetworkR().to(device)\n",
    "optimizerR = torch.optim.Adam(modelR.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "criterionR = nn.MSELoss().to(device)\n",
    "\n",
    "##### training #####\n",
    "done_modelR, train_lossR, valid_lossR = trainingR(network = modelR, \n",
    "                                                  trainloader = train_loader, \n",
    "                                                  validloader = valid_loader, \n",
    "                                                  optimizer = optimizerR, \n",
    "                                                  criterion = criterionR, \n",
    "                                                  epoch = 150, \n",
    "                                                  filename = 'tamama',\n",
    "                                                  early_stop = 10)\n",
    "\n",
    "##### testing #####\n",
    "_, _, result_tableR = testingR(done_modelR, test_loader, criterionR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search for best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:06:21.084181Z",
     "start_time": "2021-12-15T16:16:41.039090Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_paramC, all_scoreC = all_optuna(num_set = 10, \n",
    "                                     all_data = run_train, \n",
    "                                     mode = 'C', \n",
    "                                     TPE_multi = True, \n",
    "                                     n_iter = 25, \n",
    "                                     filename = 'runhist_array_m2m4_m5_3criteria_NeuralNetwork', \n",
    "                                     creator = NeuralNetwork_creator\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:17:11.579623Z",
     "start_time": "2021-12-15T23:06:22.116440Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_setC, loss_dictC = runall_nn(train_x = run_train_x, \n",
    "                                   train_y = run_train_y, \n",
    "                                   test_x = run_test_x, \n",
    "                                   test_y = run_test_y, \n",
    "                                   n_epoch = 200, \n",
    "                                   config = best_paramC,\n",
    "                                   filename = 'runhist_array_m2m4_m5_3criteria_NeuralNetworkC', \n",
    "                                   early_stop = 10,\n",
    "                                   mode = 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:17:13.484164Z",
     "start_time": "2021-12-15T23:17:12.602765Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_plot(loss_dictC['train'], loss_dictC['valid'], num_row = 4, num_col = 3)\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:36:13.481632Z",
     "start_time": "2021-11-07T06:36:13.285186Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "runall_modelR = NeuralNetworkR().to(device)\n",
    "runall_optimizerR = torch.optim.Adam(runall_modelR.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "runall_criterionR = nn.MSELoss().to(device)\n",
    "\n",
    "table_setR, loss_dictR = runall_nn(train_x = run_train_x, \n",
    "                                   train_y = run_train_y, \n",
    "                                   test_x = run_test_x, \n",
    "                                   test_y = run_test_y, \n",
    "                                   n_epoch = 150, \n",
    "                                   batch_size = 64,\n",
    "                                   model = runall_modelR,\n",
    "                                   optimizer = runall_optimizerR, \n",
    "                                   criterion = runall_criterionR, \n",
    "                                   filename = 'runhist_array_4criteria_NeuralNetworkR', \n",
    "                                   train_ratio = 0.75, \n",
    "                                   early_stop = 10,\n",
    "                                   mode = 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:17:21.986069Z",
     "start_time": "2021-10-31T16:17:20.989691Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_plot(loss_dictR['train'], loss_dictR['valid'], num_row = 4, num_col = 3)\n",
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:17:14.742243Z",
     "start_time": "2021-12-15T23:17:14.476183Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20211221'\n",
    "TPE_multi = True\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'NeuralNetwork'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'NeuralNetwork')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
