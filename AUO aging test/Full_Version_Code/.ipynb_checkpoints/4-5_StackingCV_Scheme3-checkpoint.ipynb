{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:40.022718Z",
     "start_time": "2021-12-05T06:29:36.608935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import random\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor,\\\n",
    "    AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio, train_col\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, cf_matrix, AUC, PR_curve, \\\n",
    "     multiple_curve, PR_matrix, best_threshold, all_optuna, optuna_history, AdaBoost_creator \n",
    "from library.XGBoost import XGBoost_creator\n",
    "from library.LightGBM import LightGBM_creator\n",
    "from library.CatBoost import CatBoost_creator\n",
    "from library.Random_Forest import RandomForest_creator\n",
    "from library.Extra_Trees import ExtraTrees_creator\n",
    "from library.Neural_Network import RunhistSet, NeuralNetworkC, trainingC, NeuralNetwork_creator\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:40.093416Z",
     "start_time": "2021-12-05T06:29:40.081442Z"
    }
   },
   "outputs": [],
   "source": [
    "def month_param(num_set, date, month_list, model_list, iter_list, filename, mode, TPE_multi):\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    month_dict = {}\n",
    "    for month in month_list:\n",
    "        \n",
    "        model_dict = {}\n",
    "        for i, model in enumerate(model_list):\n",
    "                \n",
    "            with open(f'hyperparameter/{date}/{filename}_m{month}_{model}{mode}_{sampler}_{iter_list[i]}.data', 'rb') as f:\n",
    "                model_param = pickle.load(f)\n",
    "            model_dict[model] = model_param\n",
    "        \n",
    "        month_dict[f'm{month}'] = model_dict\n",
    "\n",
    "    return month_dict\n",
    "\n",
    "\n",
    "def all_param(num_set, date, model_list, iter_list, filename, mode, TPE_multi):\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    model_dict = {}\n",
    "    for i, model in enumerate(model_list):\n",
    "\n",
    "        with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[i]}.data', 'rb') as f:\n",
    "                set_dict = pickle.load(f)\n",
    "                \n",
    "        model_dict[model] = set_dict\n",
    "        \n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def optimize_base(num_set, train_data, mode, TPE_multi, base_list, iter_list, filename):\n",
    "    \n",
    "    best_param = {}\n",
    "    month_list = list(train_data.keys())\n",
    "    \n",
    "    for i in tqdm(month_list):\n",
    "        \n",
    "        best_param[f'{i}'] = {}\n",
    "        if 'NeuralNetwork' in base_list:\n",
    "            print('\\nStarting for NeuralNetwork:\\n')\n",
    "            model_index = base_list.index('NeuralNetwork')\n",
    "            best_param[f'{i}'][f'NeuralNetwork'], _ = all_optuna(num_set = num_set, \n",
    "                                                                 all_data = train_data[f'{i}'], \n",
    "                                                                 mode = mode, \n",
    "                                                                 TPE_multi = TPE_multi, \n",
    "                                                                 n_iter = iter_list[model_index],\n",
    "                                                                 filename = f'{filename}_{i}_NeuralNetwork',\n",
    "                                                                 creator = NeuralNetwork_creator)\n",
    "        \n",
    "        if 'XGBoost' in base_list:\n",
    "            print('\\nStarting for XGBoost:\\n')\n",
    "            model_index = base_list.index('XGBoost')\n",
    "            best_param[f'{i}'][f'XGBoost'], _ = all_optuna(num_set = num_set, \n",
    "                                                           all_data = train_data[f'{i}'], \n",
    "                                                           mode = mode, \n",
    "                                                           TPE_multi = TPE_multi, \n",
    "                                                           n_iter = iter_list[model_index],\n",
    "                                                           filename = f'{filename}_{i}_XGBoost',\n",
    "                                                           creator = XGBoost_creator)\n",
    "\n",
    "        if 'LightGBM' in base_list:\n",
    "            print('\\nStarting for LightGBM:\\n')\n",
    "            model_index = base_list.index('LightGBM')\n",
    "            best_param[f'{i}'][f'LightGBM'], _ = all_optuna(num_set = num_set, \n",
    "                                                            all_data = train_data[f'{i}'], \n",
    "                                                            mode = mode, \n",
    "                                                            TPE_multi = TPE_multi, \n",
    "                                                            n_iter = iter_list[model_index],\n",
    "                                                            filename = f'{filename}_{i}_LightGBM',\n",
    "                                                            creator = LightGBM_creator)\n",
    "        \n",
    "        if 'AdaBoost' in base_list:\n",
    "            print('\\nStarting for AdaBoost:\\n')\n",
    "            model_index = base_list.index('AdaBoost')\n",
    "            best_param[f'{i}'][f'AdaBoost'], _ = all_optuna(num_set = num_set, \n",
    "                                                            all_data = train_data[f'{i}'], \n",
    "                                                            mode = mode, \n",
    "                                                            TPE_multi = TPE_multi, \n",
    "                                                            n_iter = iter_list[model_index],\n",
    "                                                            filename = f'{filename}_{i}_AdaBoost',\n",
    "                                                            creator = AdaBoost_creator)\n",
    "            \n",
    "        if 'CatBoost' in base_list:\n",
    "            print('\\nStarting for CatBoost:\\n')\n",
    "            model_index = base_list.index('CatBoost')\n",
    "            best_param[f'{i}'][f'CatBoost'], _ = all_optuna(num_set = num_set, \n",
    "                                                            all_data = train_data[f'{i}'], \n",
    "                                                            mode = mode, \n",
    "                                                            TPE_multi = TPE_multi, \n",
    "                                                            n_iter = iter_list[model_index],\n",
    "                                                            filename = f'{filename}_{i}_CatBoost',\n",
    "                                                            creator = CatBoost_creator)\n",
    "            \n",
    "        if 'RandomForest' in base_list:\n",
    "            print('\\nStarting for RandomForest:\\n')\n",
    "            model_index = base_list.index('RandomForest')\n",
    "            best_param[f'{i}'][f'RandomForest'], _ = all_optuna(num_set = num_set, \n",
    "                                                                all_data = train_data[f'{i}'], \n",
    "                                                                mode = mode, \n",
    "                                                                TPE_multi = TPE_multi, \n",
    "                                                                n_iter = iter_list[model_index],\n",
    "                                                                filename = f'{filename}_{i}_RandomForest',\n",
    "                                                                creator = RandomForest_creator)\n",
    "\n",
    "        if 'ExtraTrees' in base_list:\n",
    "            print('\\nStarting for ExtraTrees:\\n')\n",
    "            model_index = base_list.index('ExtraTrees')\n",
    "            best_param[f'{i}'][f'ExtraTrees'], _ = all_optuna(num_set = num_set, \n",
    "                                                              all_data = train_data[f'{i}'], \n",
    "                                                              mode = mode, \n",
    "                                                              TPE_multi = TPE_multi, \n",
    "                                                              n_iter = iter_list[model_index],\n",
    "                                                              filename = f'{filename}_{i}_ExtraTrees',\n",
    "                                                              creator = ExtraTrees_creator)\n",
    "            \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data by base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:40.188052Z",
     "start_time": "2021-12-05T06:29:40.141569Z"
    }
   },
   "outputs": [],
   "source": [
    "def stratified_data(train_data, cv):\n",
    "    \n",
    "    good = train_data[train_data.GB == 0]\n",
    "    bad = train_data[train_data.GB == 1]\n",
    "    good_index = random.sample(good.index.to_list(), k = len(good))\n",
    "    bad_index = random.sample(bad.index.to_list(), k = len(bad))\n",
    "    \n",
    "    train_x_dict = {}\n",
    "    train_y_dict = {}\n",
    "    valid_x_dict = {}\n",
    "    valid_y_dict = {}\n",
    "    for i in range(cv):\n",
    "        \n",
    "        if (i+1) == cv:\n",
    "            good_valid_index = good_index[int(np.floor((i/cv)*len(good))): ]\n",
    "            bad_valid_index = bad_index[int(np.floor((i/cv)*len(bad))): ]\n",
    "        else:\n",
    "            good_valid_index = good_index[int(np.floor((i/cv)*len(good))) : int(np.floor(((i+1)/cv)*len(good)))]\n",
    "            bad_valid_index = bad_index[int(np.floor((i/cv)*len(bad))) : int(np.floor(((i+1)/cv)*len(bad)))]\n",
    "        good_train_index = [x for x in good_index if x not in good_valid_index]\n",
    "        bad_train_index = [x for x in bad_index if x not in bad_valid_index]\n",
    "        \n",
    "        good_train = good.loc[good_train_index]\n",
    "        good_valid = good.loc[good_valid_index]\n",
    "        bad_train = bad.loc[bad_train_index]\n",
    "        bad_valid = bad.loc[bad_valid_index]\n",
    "        train = pd.concat([good_train, bad_train], axis = 0)\n",
    "        valid = pd.concat([good_valid, bad_valid], axis = 0)\n",
    "        train_x_dict[i], train_y_dict[i], valid_x_dict[i], valid_y_dict[i] = label_divide(train, valid, train_only = False)\n",
    "\n",
    "    return train_x_dict, train_y_dict, valid_x_dict, valid_y_dict\n",
    "\n",
    "\n",
    "def transform_train(train_data, num_set, mode, base_param, cv):\n",
    "    \n",
    "    month_list = list(base_param.keys())\n",
    "    model_list = list(base_param[month_list[0]].keys())\n",
    "    set_dict = {}\n",
    "    for x in range(num_set):\n",
    "        set_dict[f'set{x}'] = pd.DataFrame()\n",
    "        \n",
    "    for month in tqdm(month_list):\n",
    "        \n",
    "        for i in tqdm(range(num_set)):\n",
    "            \n",
    "            train_x_dict, train_y_dict, valid_x_dict, valid_y_dict = stratified_data(train_data[month][f'set{i}'], cv = cv)\n",
    "            all_cv = pd.DataFrame()\n",
    "            for j in range(cv):\n",
    "                \n",
    "                model_predict = pd.DataFrame()\n",
    "                if mode == 'C':\n",
    "                    \n",
    "                    if 'NeuralNetwork' in model_list:\n",
    "                        temp_train = RunhistSet(train_x_dict[j], train_y_dict[j])\n",
    "                        temp_valid = RunhistSet(valid_x_dict[j], valid_y_dict[j])\n",
    "                        train_loader = DataLoader(temp_train, \n",
    "                                                  batch_size = base_param[month]['NeuralNetwork'][f'set{i}']['batch_size'], \n",
    "                                                  shuffle = True)\n",
    "                        valid_loader = DataLoader(temp_valid, batch_size = len(valid_x_dict[j]), shuffle = False)\n",
    "                        nn_model = NeuralNetworkC(dim = train_x_dict[j].shape[1])\n",
    "                        optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                                     lr = base_param[month]['NeuralNetwork'][f'set{i}']['learning_rate'], \n",
    "                                                     weight_decay = base_param[month]['NeuralNetwork'][f'set{i}']['weight_decay'])\n",
    "                        criterion = nn.CrossEntropyLoss(\n",
    "                            weight = torch.tensor([1-base_param[month]['NeuralNetwork'][f'set{i}']['bad_weight'], \n",
    "                                                   base_param[month]['NeuralNetwork'][f'set{i}']['bad_weight']])).to('cpu')\n",
    "                        network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 100, \n",
    "                                                  filename = 'none', early_stop = 10)\n",
    "                        for x, y in valid_loader:\n",
    "                            output = network(x)\n",
    "                            _, predict_y = torch.max(output.data, 1)\n",
    "                        predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'XGBoost' in model_list:                     \n",
    "                        clf = XGBClassifier(**base_param[month]['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'LightGBM' in model_list:                        \n",
    "                        clf = LGBMClassifier(**base_param[month]['LightGBM'][f'set{i}'])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'CatBoost' in model_list:\n",
    "                        clf = CatBoostClassifier(**base_param[month]['CatBoost'][f'set{i}'])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'AdaBoost' in model_list:\n",
    "                        tree_param = {\n",
    "                            'base_estimator': DecisionTreeClassifier(\n",
    "                                max_depth = base_param[month]['AdaBoost'][f'set{i}']['max_depth']\n",
    "                            )}\n",
    "                        boost_param = dict(\n",
    "                            (key, base_param[month]['AdaBoost'][f'set{i}'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                        )\n",
    "                        boost_param.update(tree_param)\n",
    "                        clf = AdaBoostClassifier(**boost_param)\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'RandomForest' in model_list:\n",
    "                        clf = RandomForestClassifier(**base_param[month]['RandomForest'][f'set{i}'])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'ExtraTrees' in model_list:\n",
    "                        clf = ExtraTreesClassifier(**base_param[month]['ExtraTrees'][f'set{i}'])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                elif mode == 'R':\n",
    "                    \n",
    "                    if 'XGBoost' in model_list:\n",
    "                        reg = XGBRegressor(**base_param[month]['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'X': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'LightGBM' in model_list:\n",
    "                        reg = LGBMRegressor(**base_param[month]['LightGBM'][f'set{i}'])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'L': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'CatBoost' in model_list:\n",
    "                        reg = CatBoostRegressor(**base_param[month]['CatBoost'][f'set{i}'])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'C': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'AdaBoost' in model_list:\n",
    "                        tree_param = {\n",
    "                            'base_estimator': DecisionTreeRegressor(\n",
    "                                max_depth = base_param[month]['AdaBoost'][f'set{i}']['max_depth']\n",
    "                            )}\n",
    "                        boost_param = dict(\n",
    "                            (key, base_param[month]['AdaBoost'][f'set{i}'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                        )\n",
    "                        boost_param.update(tree_param)\n",
    "                        reg = AdaBoostRegressor(**boost_param)\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'A': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                    if 'RandomForest' in model_list:\n",
    "                        reg = RandomForestRegressor(**base_param[month]['RandomForest'][f'set{i}'])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'R': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                    \n",
    "                    if 'ExtraTrees' in model_list:\n",
    "                        reg = ExtraTreesRegressor(**base_param[month]['ExtraTrees'][f'set{i}'])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'E': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                test_label = valid_y_dict[j].reset_index(drop = True)\n",
    "                done_cv = pd.concat([model_predict, test_label], axis = 1)\n",
    "                all_cv = pd.concat([all_cv, done_cv], axis = 0)\n",
    "                \n",
    "            set_dict[f'set{i}'] = pd.concat([set_dict[f'set{i}'], all_cv], axis = 0)\n",
    "    \n",
    "    return set_dict\n",
    "\n",
    "\n",
    "def transform_test(train_data, test_data, num_set, mode, base_param):\n",
    "    \n",
    "    month_list = list(base_param.keys())\n",
    "    model_list = list(base_param[month_list[0]].keys())\n",
    "    test_dict = {}\n",
    "    for i in tqdm(range(num_set)):\n",
    "        \n",
    "        month_test = pd.DataFrame()\n",
    "        for month in tqdm(month_list):\n",
    "\n",
    "            train_x, train_y, test_x, test_y = label_divide(train_data[f'set{i}'], test_data, train_only = False)\n",
    "            model_predict = pd.DataFrame()\n",
    "            if mode == 'C':\n",
    "\n",
    "                if 'NeuralNetwork' in model_list:\n",
    "                    temp_train = RunhistSet(train_x, train_y)\n",
    "                    temp_test = RunhistSet(test_x, test_y)\n",
    "                    train_loader = DataLoader(temp_train, \n",
    "                                              batch_size = base_param[month]['NeuralNetwork'][f'set{i}']['batch_size'], \n",
    "                                              shuffle = True)\n",
    "                    test_loader = DataLoader(temp_test, batch_size = len(test_x), shuffle = False)\n",
    "                    nn_model = NeuralNetworkC(dim = train_x.shape[1])\n",
    "                    optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                                 lr = base_param[month]['NeuralNetwork'][f'set{i}']['learning_rate'], \n",
    "                                                 weight_decay = base_param[month]['NeuralNetwork'][f'set{i}']['weight_decay'])\n",
    "                    criterion = nn.CrossEntropyLoss(\n",
    "                        weight = torch.tensor([1-base_param[month]['NeuralNetwork'][f'set{i}']['bad_weight'], \n",
    "                                               base_param[month]['NeuralNetwork'][f'set{i}']['bad_weight']])).to('cpu')\n",
    "                    network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 100, \n",
    "                                              filename = 'none', early_stop = 10)\n",
    "                    for X, Y in test_loader:\n",
    "                        X, Y = X.float(), Y.long()\n",
    "                        output = network(X)\n",
    "                        _, predict_y = torch.max(output.data, 1)\n",
    "                    predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                \n",
    "                if 'XGBoost' in model_list:\n",
    "                    clf = XGBClassifier(**base_param[month]['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:\n",
    "                    clf = LGBMClassifier(**base_param[month]['LightGBM'][f'set{i}'])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    clf = CatBoostClassifier(**base_param[month]['CatBoost'][f'set{i}'])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeClassifier(\n",
    "                            max_depth = base_param[month]['AdaBoost'][f'set{i}']['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[month]['AdaBoost'][f'set{i}'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    clf = AdaBoostClassifier(**boost_param)\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    clf = RandomForestClassifier(**base_param[month]['RandomForest'][f'set{i}'])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    clf = ExtraTreesClassifier(**base_param[month]['ExtraTrees'][f'set{i}'])\n",
    "                    clf.fit(train_x, train_y)\n",
    "                    predict_y = clf.predict_proba(test_x)\n",
    "                    predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            elif mode == 'R':\n",
    "\n",
    "                if 'XGBoost' in model_list:\n",
    "                    reg = XGBRegressor(**base_param[month]['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'X': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:\n",
    "                    reg = LGBMRegressor(**base_param[month]['LightGBM'][f'set{i}'])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'L': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    reg = CatBoostRegressor(**base_param[month]['CatBoost'][f'set{i}'])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'C': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeRegressor(\n",
    "                            max_depth = base_param[month]['AdaBoost'][f'set{i}']['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[month]['AdaBoost'][f'set{i}'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    reg = AdaBoostRegressor(**boost_param)\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'A': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    reg = RandomForestRegressor(**base_param[month]['RandomForest'][f'set{i}'])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'R': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    reg = ExtraTreesRegressor(**base_param[month]['ExtraTrees'][f'set{i}'])\n",
    "                    reg.fit(train_x, train_y)\n",
    "                    predict_y = reg.predict(test_x)\n",
    "                    predict = pd.DataFrame({'E': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            month_test = pd.concat([month_test, model_predict], axis = 1)\n",
    "        month_done = pd.concat([month_test, test_y], axis = 1)\n",
    "        test_dict[f'set{i}'] = month_done\n",
    "        \n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:41.844536Z",
     "start_time": "2021-12-05T06:29:41.736824Z"
    }
   },
   "outputs": [],
   "source": [
    "def LR(train_x, test_x, train_y, test_y, config):\n",
    "    \n",
    "    subconfig = config.copy()\n",
    "    del subconfig['meta_learner']\n",
    "    if config['meta_learner'] == 'LogisticRegression':\n",
    "        clf = LogisticRegression(**subconfig)\n",
    "        clf.fit(train_x, train_y)\n",
    "        coef = clf.coef_\n",
    "    elif config['meta_learner'] == 'ExtraTrees':\n",
    "        clf = ExtraTreesClassifier(**subconfig)\n",
    "        clf.fit(train_x, train_y)\n",
    "        coef = clf.feature_importances_\n",
    "    predict_y = clf.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result, coef\n",
    "\n",
    "\n",
    "def RidgeR(train_x, test_x, train_y, test_y, config):\n",
    "    \n",
    "    reg = Ridge(**config)\n",
    "    reg.fit(train_x, train_y)\n",
    "    coef = reg.coef_\n",
    "    predict_y = reg.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result, coef\n",
    "\n",
    "\n",
    "def runall_LR(num_set, trainset_x, testset_x, trainset_y, testset_y, config):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    coef_set = pd.DataFrame()\n",
    "    judge = list(config.keys())[0]\n",
    "\n",
    "    for i in tqdm(range(num_set)):\n",
    "        print('\\n', f'Dataset {i}:')\n",
    "        \n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[f'set{i}']\n",
    "        else :\n",
    "            best_config = config\n",
    "        model_name = trainset_x[f'set{i}'].columns.to_list()\n",
    "\n",
    "        result, coef = LR(trainset_x[f'set{i}'], testset_x[f'set{i}'], trainset_y[f'set{i}'], testset_y[f'set{i}'], \n",
    "                          best_config)\n",
    "        table = cf_matrix(result, trainset_y[f'set{i}'])\n",
    "        coef_df = pd.DataFrame({f'set{i}': coef.flatten()})\n",
    "        table_set = pd.concat([table_set, table]).rename(index = {0: f'dataset {i}'})\n",
    "        coef_set = pd.concat([coef_set, coef_df], axis = 1)\n",
    "    coef_set.index = model_name\n",
    "    \n",
    "    return table_set, coef_set\n",
    "\n",
    "\n",
    "def runall_RidgeR(num_set, trainset_x, testset_x, trainset_y, testset_y, config, thres_target = 'Recall', \n",
    "                    threshold = False):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    coef_set = pd.DataFrame()\n",
    "    pr_dict = {}\n",
    "    judge = list(config.keys())[0]\n",
    "\n",
    "    for i in range(num_set):\n",
    "        print('\\n', f'Dataset {i}:')\n",
    "        \n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[f'set{i}']\n",
    "        else :\n",
    "            best_config = config\n",
    "        model_name = trainset_x[f'set{i}'].columns.to_list()\n",
    "\n",
    "        predict, coef = RidgeR(trainset_x[f'set{i}'], testset_x[f'set{i}'], trainset_y[f'set{i}'], testset_y[f'set{i}'], \n",
    "                           best_config)\n",
    "        pr_matrix = PR_matrix(predict, trainset_y[f'set{i}'])\n",
    "        pr_dict[f'set{i}'] = pr_matrix\n",
    "        \n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = thres_target, threshold = threshold)\n",
    "        table_set = pd.concat([table_set, best_data]).rename(index = {best_data.index.values[0]: f'dataset {i}'})\n",
    "        coef_df = pd.DataFrame({f'set{i}': coef.flatten()})\n",
    "        coef_set = pd.concat([coef_set, coef_df], axis = 1)\n",
    "    coef_set.index = model_name\n",
    "        \n",
    "    return pr_dict, table_set, coef_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:43.331011Z",
     "start_time": "2021-12-05T06:29:43.311290Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_plot(target_data):\n",
    "    \n",
    "    correlation = target_data.iloc[:, :-1].corr()\n",
    "    plot = sn.heatmap(correlation, annot = True, cmap = 'magma')\n",
    "    plot.set_title('Correlation Coefficient of Base Learner Outputs')\n",
    "    \n",
    "    return correlation \n",
    "    \n",
    "    \n",
    "def vif(target_data):\n",
    "    \n",
    "    corr = target_data.corr()\n",
    "    vif = round(corr / (1 - corr), 2)   \n",
    "    return vif\n",
    "\n",
    "\n",
    "def forest_importance(target_data, mode = 'C'):\n",
    "    \n",
    "    colname = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    if mode == 'C':\n",
    "        clf = RandomForestClassifier(max_depth = 5, n_estimators = 500)\n",
    "    elif mode == 'R':\n",
    "        clf = RandomForestRegressor(max_depth = 5, n_estimators = 500)\n",
    "    clf.fit(X, Y)\n",
    "    importance = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis = 0)\n",
    "    importances = pd.DataFrame(dict(importance = importance, std = std), index = colname)\n",
    "    importances = importances.sort_values('importance', ascending = True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.barh(importances.index, importances['importance'].values, color = 'darkgreen', \n",
    "             xerr = importances['std'].values, ecolor = 'limegreen')\n",
    "    plt.title('Feature Importance by Random Forest')\n",
    "    plt.xlabel('importance')\n",
    "    plt.ylabel('model')\n",
    "    \n",
    "    return importances\n",
    "    \n",
    "    \n",
    "def forest_permutation(target_data, mode = 'C'):\n",
    "    \n",
    "    colnames = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "    if mode == 'C':\n",
    "        clf = RandomForestClassifier(max_depth = 5, n_estimators = 300)\n",
    "    elif mode == 'R':\n",
    "        clf = RandomForestRegressor(max_depth = 5, n_estimators = 300)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    importance = permutation_importance(clf, X_test, y_test, n_repeats = 20, n_jobs = 2)\n",
    "    importances = pd.DataFrame(dict(importance = importance.importances_mean, std = importance.importances_std), \n",
    "                               index = colnames)\n",
    "    importances = importances.sort_values('importance', ascending = True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.barh(importances.index, importances['importance'].values, color = 'firebrick', \n",
    "             xerr = importances['std'].values, ecolor = 'coral')\n",
    "    plt.title('Feature Importance by Permutation (Random Forest)')\n",
    "    plt.xlabel('importance')\n",
    "    plt.ylabel('model')\n",
    "    \n",
    "    return importances\n",
    "\n",
    "\n",
    "#####!!!!! forest_shap can only run for regressor !!!!!#####\n",
    "def forest_shap(target_data, mode = 'R'):\n",
    "    \n",
    "    colnames = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    if mode == 'C':\n",
    "        clf = RandomForestClassifier(max_depth = 5, n_estimators = 300)\n",
    "    elif mode == 'R':\n",
    "        clf = RandomForestRegressor(max_depth = 5, n_estimators = 300)\n",
    "    clf.fit(X, Y)\n",
    "    \n",
    "    explainer = shap.Explainer(clf)\n",
    "    shap_value = explainer(X)\n",
    "    values = abs(shap_value.values).mean(axis = 0)\n",
    "    values = values / sum(values)\n",
    "    shap_df = pd.DataFrame(dict(value = values), index = colnames).sort_values('value', ascending = True)\n",
    "    \n",
    "    plt.figure()\n",
    "    shap.plots.bar(shap_value)\n",
    "    shap.plots.beeswarm(shap_value)\n",
    "    \n",
    "    return shap_df\n",
    "\n",
    "\n",
    "def GLM_coefficient(target_data, mode = 'C'):\n",
    "    \n",
    "    colnames = target_data.columns.to_list()[:-1]\n",
    "    X, Y = label_divide(target_data, None, 'GB', train_only = True)\n",
    "    if mode == 'C':\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X, Y)\n",
    "        coefficient = abs(clf.coef_[0,:])\n",
    "    elif mode == 'R':\n",
    "        clf = Ridge()\n",
    "        clf.fit(X, Y)\n",
    "        coefficient = abs(clf.coef_)\n",
    "    coef_df = pd.DataFrame(dict(GLM = coefficient), index = colnames).sort_values('GLM', ascending = True)\n",
    "    \n",
    "    return coef_df\n",
    "\n",
    "\n",
    "#####!!!!! forest_shap can only run for regressor !!!!!#####\n",
    "def rank_importance(target_data, mode = 'C'):\n",
    "    \n",
    "    correlation = correlation_plot(target_data)\n",
    "    coefficient = GLM_coefficient(target_data, mode = mode).GLM\n",
    "    forest = forest_importance(target_data, mode = mode).importance\n",
    "    permutation = forest_permutation(target_data, mode = mode).importance\n",
    "    shapvalue = forest_shap(target_data).value\n",
    "    rank_df = pd.DataFrame()\n",
    "    rank_df['GLM'] = coefficient.rank(ascending = False)\n",
    "    rank_df['forest'] = forest.rank(ascending = False)\n",
    "    rank_df['permutation'] = permutation.rank(ascending = False)\n",
    "    rank_df['SHAP'] = shapvalue.rank(ascending = False)\n",
    "    rank_df['total_rank'] = rank_df.apply(sum, axis = 1).rank()\n",
    "    rank_df = rank_df.sort_values('total_rank', ascending = True)\n",
    "    \n",
    "    return rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:44.719138Z",
     "start_time": "2021-12-05T06:29:44.706195Z"
    }
   },
   "outputs": [],
   "source": [
    "def stackingCV_creator(train_data, mode, num_valid = 3, label = 'GB') :\n",
    "    \n",
    "    def objective(trial) :\n",
    "        # hyperparameters randomize setting\n",
    "        if mode == 'C' :\n",
    "            meta_learner = {'meta_learner': trial.suggest_categorical('meta_learner', ['LogisticRegression', 'ExtraTrees'])}\n",
    "            \n",
    "            if meta_learner['meta_learner'] == 'LogisticRegression' :      \n",
    "                param = {\n",
    "                    'solver': 'lbfgs',\n",
    "                    'C': trial.suggest_categorical('C', [100, 10 ,1 ,0.1, 0.01]),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['none', 'l2']),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "\n",
    "            elif meta_learner['meta_learner'] == 'ExtraTrees' :\n",
    "                param = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 200),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "            \n",
    "            param.update(meta_learner)\n",
    "\n",
    "        elif mode == 'R' :\n",
    "            meta_learner = 'RidgeCV'\n",
    "            \n",
    "            if meta_learner == 'RidgeCV' :\n",
    "                param = {\n",
    "                    'alpha': trial.suggest_float('alpha', 0, 1, step = 0.1)\n",
    "                }\n",
    "            \n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                param = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "        \n",
    "        # objective function\n",
    "        result_list = []\n",
    "        for i in range(num_valid):\n",
    "\n",
    "            train_good = train_data[train_data.GB == 0]\n",
    "            train_bad = train_data[train_data.GB == 1]\n",
    "            train_good_x, train_good_y = label_divide(train_good, None, label, train_only = True)\n",
    "            train_bad_x, train_bad_y = label_divide(train_bad, None, label, train_only = True)\n",
    "            train_g_x, valid_g_x, train_g_y, valid_g_y = train_test_split(train_good_x, train_good_y, test_size = 0.25)\n",
    "            train_b_x, valid_b_x, train_b_y, valid_b_y = train_test_split(train_bad_x, train_bad_y, test_size = 0.25)\n",
    "            train_x = pd.concat([train_g_x, train_b_x], axis = 0)\n",
    "            train_y = pd.concat([train_g_y, train_b_y], axis = 0)\n",
    "            valid_x = pd.concat([valid_g_x, valid_b_x], axis = 0)\n",
    "            valid_y = pd.concat([valid_g_y, valid_b_y], axis = 0)\n",
    "\n",
    "            if mode == 'C':\n",
    "                result, _ = LR(train_x, valid_x, train_y, valid_y, param)\n",
    "                table = cf_matrix(result, valid_y)\n",
    "                recall = table['Recall']\n",
    "                precision = table['Precision']\n",
    "                aging = table['Aging Rate']\n",
    "                result_list.append(recall+2*precision)\n",
    "\n",
    "            elif mode == 'R':\n",
    "                result, _ = RidgeR(train_x, valid_x, train_y, valid_y, param)\n",
    "                pr_matrix = PR_matrix(result, valid_y)\n",
    "                auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "                result_list.append((-1)*auc)\n",
    "\n",
    "        return np.mean(result_list)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:48.812260Z",
     "start_time": "2021-12-05T06:29:46.477265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Month 2:\n",
      "\n",
      "Dimension of dataset 0 : (39009, 85)  balance ratio: 590.0\n",
      "Dimension of dataset 1 : (1296, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2034, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1448, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1320, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (1310, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (1326, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (1320, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (1320, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (726, 85)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 3:\n",
      "\n",
      "Dimension of dataset 0 : (60396, 101)  balance ratio: 574.0\n",
      "Dimension of dataset 1 : (2088, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2568, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2306, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2100, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2101, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2325, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2100, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2100, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1155, 101)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 4:\n",
      "\n",
      "Dimension of dataset 0 : (57743, 103)  balance ratio: 524.0\n",
      "Dimension of dataset 1 : (2132, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2948, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2414, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2198, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2183, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2585, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2200, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2200, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1210, 103)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Combined training data:\n",
      "\n",
      "Dimension of dataset 0 : (157148, 125)  balance ratio: 558.0\n",
      "Dimension of dataset 1 : (5516, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (7550, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (6168, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (5618, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (5594, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (6236, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (5620, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (5620, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (3091, 125)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      " Dimension of testing data: (48649, 129)\n"
     ]
    }
   ],
   "source": [
    "### training data ### \n",
    "training_month = range(2, 5)\n",
    "\n",
    "data_dict, trainset_x, trainset_y = multiple_month(training_month, num_set = 10, filename = 'dataset')\n",
    "\n",
    "print('\\nCombined training data:\\n')\n",
    "run_train = multiple_set(num_set = 10)\n",
    "run_train_x, run_train_y = train_set(run_train, num_set = 10)\n",
    "\n",
    "### testing data ###\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of testing data:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize the base learners by one-month data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for training data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T00:48:50.540116Z",
     "start_time": "2021-12-02T15:33:08.123045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### by optuna ##### \n",
    "base_param_monthC = optimize_base(num_set = 10, \n",
    "                                  train_data = data_dict, \n",
    "                                  mode = 'C', \n",
    "                                  TPE_multi = True, \n",
    "                                  base_list = ['NeuralNetwork', 'XGBoost', 'LightGBM'],\n",
    "                                  iter_list = [20, 200, 200],\n",
    "                                  filename = 'runhist_array_m2m4_m5_3criteria')\n",
    "\n",
    "# base_param_monthR = optimize_base(num_set = 10, \n",
    "#                                   train_data = data_dict, \n",
    "#                                   mode = 'R', \n",
    "#                                   TPE_multi = True, \n",
    "#                                   base_list = ['XGBoost', 'LightGBM'],\n",
    "#                                   iter_list = [200, 200],\n",
    "#                                   filename = 'runhist_array_4criteria_m2m5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:29:55.218925Z",
     "start_time": "2021-12-05T06:29:55.207955Z"
    }
   },
   "outputs": [],
   "source": [
    "##### 'OR' by loading from stackingCV scheme 2 #####\n",
    "base_param_monthC = month_param(num_set = 10, \n",
    "                                date = '20211207', \n",
    "                                month_list = list(range(2, 5)), \n",
    "                                model_list = ['NeuralNetwork', 'LightGBM', 'XGBoost'], \n",
    "                                iter_list = [20, 200, 200], \n",
    "                                filename = 'runhist_array_m2m4_m5_3criteria', \n",
    "                                mode = 'C', \n",
    "                                TPE_multi = True)\n",
    "\n",
    "# base_param_monthR = month_param(num_set = 10, \n",
    "#                                 date = '20211019', \n",
    "#                                 month_list = [2, 3, 4], \n",
    "#                                 model_list = ['XGBoost', 'LightGBM'], \n",
    "#                                 iter_list = [200, 200], \n",
    "#                                 filename = 'runhist_array_4criteria_m2m5', \n",
    "#                                 mode = 'R', \n",
    "#                                 TPE_multi = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for testing data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T13:14:35.948977Z",
     "start_time": "2021-10-13T13:12:52.541490Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### by optuna ##### \n",
    "base_param_allC = optimize_base(num_set = 10, \n",
    "                                train_data = {'all': run_train}, \n",
    "                                mode = 'C', \n",
    "                                TPE_multi = False, \n",
    "                                base_list = ['LightGBM', 'CatBoost', 'RandomForest'], \n",
    "                                iter_list = [200, 200, 50],\n",
    "                                filename = 'runhist_array_m1m6_m7_3criteria')\n",
    "\n",
    "# base_param_allR = optimize_base(num_set = 10, \n",
    "#                                 train_data = {'all': run_train}, \n",
    "#                                 mode = 'R', \n",
    "#                                 TPE_multi = True, \n",
    "#                                 base_list = ['XGBoost', 'LightGBM'], \n",
    "#                                 iter_list = [200, 200],\n",
    "#                                 filename = 'runhist_array_4criteria_m2m5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:30:03.030962Z",
     "start_time": "2021-12-05T06:30:03.011991Z"
    }
   },
   "outputs": [],
   "source": [
    "##### 'OR' by loading from stackingCV scheme 1 #####\n",
    "base_param_allC = all_param(num_set = 10, \n",
    "                           date = '20211207', \n",
    "                           model_list = ['NeuralNetwork', 'LightGBM', 'XGBoost'], \n",
    "                           iter_list = [20, 200, 200], \n",
    "                           filename = 'runhist_array_m2m4_m5_3criteria', \n",
    "                           mode = 'C', \n",
    "                           TPE_multi = True)\n",
    "\n",
    "# base_param_allR = all_param(num_set = 10, \n",
    "#                            date = '20211123', \n",
    "#                            model_list = ['XGBoost', 'LightGBM'], \n",
    "#                            iter_list = [200, 200], \n",
    "#                            filename = 'runhist_array_m2m5_4selection', \n",
    "#                            mode = 'R', \n",
    "#                            TPE_multi = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### data transform for scheme 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:32:40.156657Z",
     "start_time": "2021-12-05T07:20:25.437151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564ac364e33d4b2abbfe10eada0d6a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e835899618924fc1be8ce62cb7de56cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2589db558d941fabe993bd2d0723a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.04790284217605385, Recall = 0.0071174377224199285, Aging Rate = 0.0018835747193728205, Precision = 0.006756756756756757, f1 = 0.006932409012131715\n",
      "Epoch 2: Train Loss = 0.026553097374780028, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.026076078963488673, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.025745182256259717, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.025630188081098224, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02461148397652342, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.024753559416162217, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.024499459375845693, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.024340795438940452, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.02413013841508628, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.023879977112014075, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.023452206983340978, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.023609663937455613, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.02328100263213374, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.02300368826666497, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.022786428541555995, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.022433996377476544, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02193858590127841, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.02210106792483098, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.021839721077330048, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 18: Train Loss = 0.02152643037797692, Recall = 0.0035587188612099642, Aging Rate = 6.363428105989259e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 19: Train Loss = 0.02120200819121579, Recall = 0.0071174377224199285, Aging Rate = 1.9090284317967775e-05, Precision = 0.6666666666666666, f1 = 0.014084507042253521\n",
      "Epoch 20: Train Loss = 0.020905327111948008, Recall = 0.0071174377224199285, Aging Rate = 1.9090284317967775e-05, Precision = 0.6666666666666666, f1 = 0.014084507042253521\n",
      "Test Loss = 0.02041656063437632, Recall = 0.021352313167259787, Aging Rate = 3.818056863593555e-05, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.020525088403093757, Recall = 0.0071174377224199285, Aging Rate = 1.9090284317967775e-05, Precision = 0.6666666666666666, f1 = 0.014084507042253521\n",
      "Epoch 22: Train Loss = 0.020312412875436902, Recall = 0.021352313167259787, Aging Rate = 3.818056863593555e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.019969593721672604, Recall = 0.014234875444839857, Aging Rate = 2.5453712423957035e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.019753513718925895, Recall = 0.017793594306049824, Aging Rate = 3.818056863593555e-05, Precision = 0.8333333333333334, f1 = 0.0348432055749129\n",
      "Epoch 25: Train Loss = 0.019465636401310518, Recall = 0.028469750889679714, Aging Rate = 5.727085295390333e-05, Precision = 0.8888888888888888, f1 = 0.05517241379310344\n",
      "Test Loss = 0.018974176322681516, Recall = 0.03558718861209965, Aging Rate = 6.363428105989258e-05, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.019150834603522297, Recall = 0.02491103202846975, Aging Rate = 5.090742484791407e-05, Precision = 0.875, f1 = 0.04844290657439446\n",
      "Epoch 27: Train Loss = 0.01891099980212311, Recall = 0.02491103202846975, Aging Rate = 5.090742484791407e-05, Precision = 0.875, f1 = 0.04844290657439446\n",
      "Epoch 28: Train Loss = 0.018656618057132375, Recall = 0.028469750889679714, Aging Rate = 5.727085295390333e-05, Precision = 0.8888888888888888, f1 = 0.05517241379310344\n",
      "Epoch 29: Train Loss = 0.018361131765021996, Recall = 0.042704626334519574, Aging Rate = 8.272456537786036e-05, Precision = 0.9230769230769231, f1 = 0.0816326530612245\n",
      "Epoch 30: Train Loss = 0.018088978936900327, Recall = 0.03558718861209965, Aging Rate = 6.999770916588185e-05, Precision = 0.9090909090909091, f1 = 0.06849315068493152\n",
      "Test Loss = 0.01766545681626396, Recall = 0.021352313167259787, Aging Rate = 4.4543996741924807e-05, precision = 0.8571428571428571\n",
      "\n",
      "Epoch 31: Train Loss = 0.017943203623313687, Recall = 0.02491103202846975, Aging Rate = 5.090742484791407e-05, Precision = 0.875, f1 = 0.04844290657439446\n",
      "Epoch 32: Train Loss = 0.017665758288862535, Recall = 0.03914590747330961, Aging Rate = 7.63611372718711e-05, Precision = 0.9166666666666666, f1 = 0.0750853242320819\n",
      "Epoch 33: Train Loss = 0.017480498412226934, Recall = 0.03558718861209965, Aging Rate = 6.999770916588185e-05, Precision = 0.9090909090909091, f1 = 0.06849315068493152\n",
      "Epoch 34: Train Loss = 0.017277284012689013, Recall = 0.03558718861209965, Aging Rate = 6.999770916588185e-05, Precision = 0.9090909090909091, f1 = 0.06849315068493152\n",
      "Epoch 35: Train Loss = 0.01709457339526235, Recall = 0.046263345195729534, Aging Rate = 8.908799348384961e-05, Precision = 0.9285714285714286, f1 = 0.08813559322033898\n",
      "Test Loss = 0.016593752316975057, Recall = 0.08185053380782918, Aging Rate = 0.00015908570264973148, precision = 0.92\n",
      "\n",
      "Epoch 36: Train Loss = 0.01685451130877524, Recall = 0.060498220640569395, Aging Rate = 0.00011454170590780665, Precision = 0.9444444444444444, f1 = 0.1137123745819398\n",
      "Epoch 37: Train Loss = 0.016732657807575598, Recall = 0.05693950177935943, Aging Rate = 0.00011454170590780665, Precision = 0.8888888888888888, f1 = 0.10702341137123744\n",
      "Epoch 38: Train Loss = 0.016499795273875285, Recall = 0.05693950177935943, Aging Rate = 0.00010817827780181739, Precision = 0.9411764705882353, f1 = 0.10738255033557045\n",
      "Epoch 39: Train Loss = 0.016333337016013482, Recall = 0.06405693950177936, Aging Rate = 0.00012090513401379592, Precision = 0.9473684210526315, f1 = 0.12000000000000001\n",
      "Epoch 40: Train Loss = 0.016186086074558285, Recall = 0.0711743772241993, Aging Rate = 0.00012726856211978517, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015650413304143337, Recall = 0.08185053380782918, Aging Rate = 0.00015908570264973148, precision = 0.92\n",
      "\n",
      "Epoch 41: Train Loss = 0.016010629159023616, Recall = 0.060498220640569395, Aging Rate = 0.00012090513401379592, Precision = 0.8947368421052632, f1 = 0.11333333333333333\n",
      "Epoch 42: Train Loss = 0.01579007814936486, Recall = 0.06405693950177936, Aging Rate = 0.00012090513401379592, Precision = 0.9473684210526315, f1 = 0.12000000000000001\n",
      "Epoch 43: Train Loss = 0.015771896639046053, Recall = 0.06761565836298933, Aging Rate = 0.00013363199022577442, Precision = 0.9047619047619048, f1 = 0.12582781456953643\n",
      "Epoch 44: Train Loss = 0.015537166994419777, Recall = 0.08540925266903915, Aging Rate = 0.00016544913075572073, Precision = 0.9230769230769231, f1 = 0.15635179153094464\n",
      "Epoch 45: Train Loss = 0.015480068189017046, Recall = 0.07473309608540925, Aging Rate = 0.00014635884643775295, Precision = 0.9130434782608695, f1 = 0.1381578947368421\n",
      "Test Loss = 0.014922664778910957, Recall = 0.10320284697508897, Aging Rate = 0.00020362969939165628, precision = 0.90625\n",
      "\n",
      "Epoch 46: Train Loss = 0.015321130895370617, Recall = 0.08540925266903915, Aging Rate = 0.00017181255886170998, Precision = 0.8888888888888888, f1 = 0.15584415584415584\n",
      "Epoch 47: Train Loss = 0.015122063483277524, Recall = 0.08540925266903915, Aging Rate = 0.00017817598696769923, Precision = 0.8571428571428571, f1 = 0.15533980582524273\n",
      "Epoch 48: Train Loss = 0.01510182118765756, Recall = 0.1103202846975089, Aging Rate = 0.00022271998370962406, Precision = 0.8857142857142857, f1 = 0.19620253164556964\n",
      "Epoch 49: Train Loss = 0.014964520266105768, Recall = 0.08540925266903915, Aging Rate = 0.00017181255886170998, Precision = 0.8888888888888888, f1 = 0.15584415584415584\n",
      "Epoch 50: Train Loss = 0.014807434991032379, Recall = 0.12455516014234876, Aging Rate = 0.00025453712423957034, Precision = 0.875, f1 = 0.21806853582554517\n",
      "Test Loss = 0.014438415607099212, Recall = 0.060498220640569395, Aging Rate = 0.00011454170590780665, precision = 0.9444444444444444\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss = 0.014713349564655216, Recall = 0.09608540925266904, Aging Rate = 0.00020362969939165628, Precision = 0.84375, f1 = 0.17252396166134185\n",
      "Epoch 52: Train Loss = 0.014654119454250781, Recall = 0.09608540925266904, Aging Rate = 0.00020362969939165628, Precision = 0.84375, f1 = 0.17252396166134185\n",
      "Epoch 53: Train Loss = 0.014556600505011164, Recall = 0.10320284697508897, Aging Rate = 0.000197266271285667, Precision = 0.9354838709677419, f1 = 0.1858974358974359\n",
      "Epoch 54: Train Loss = 0.014385467927341968, Recall = 0.1387900355871886, Aging Rate = 0.00026726398045154884, Precision = 0.9285714285714286, f1 = 0.2414860681114551\n",
      "Epoch 55: Train Loss = 0.01439149422314758, Recall = 0.11743772241992882, Aging Rate = 0.0002290834118156133, Precision = 0.9166666666666666, f1 = 0.20820189274447945\n",
      "Test Loss = 0.013867826549606842, Recall = 0.15302491103202848, Aging Rate = 0.00029908112098149514, precision = 0.9148936170212766\n",
      "\n",
      "Epoch 56: Train Loss = 0.014337810855858791, Recall = 0.1103202846975089, Aging Rate = 0.0002290834118156133, Precision = 0.8611111111111112, f1 = 0.19558359621451105\n",
      "Epoch 57: Train Loss = 0.014174508416733298, Recall = 0.11743772241992882, Aging Rate = 0.00024181026802759184, Precision = 0.868421052631579, f1 = 0.20689655172413796\n",
      "Epoch 58: Train Loss = 0.01403728652719575, Recall = 0.12099644128113879, Aging Rate = 0.0002481736961335811, Precision = 0.8717948717948718, f1 = 0.21249999999999997\n",
      "Epoch 59: Train Loss = 0.014008500353070448, Recall = 0.13523131672597866, Aging Rate = 0.00026726398045154884, Precision = 0.9047619047619048, f1 = 0.23529411764705882\n",
      "Epoch 60: Train Loss = 0.013967217624627541, Recall = 0.12811387900355872, Aging Rate = 0.00025453712423957034, Precision = 0.9, f1 = 0.22429906542056077\n",
      "Test Loss = 0.013520673170779074, Recall = 0.18149466192170818, Aging Rate = 0.00036271540204138776, precision = 0.8947368421052632\n",
      "\n",
      "Epoch 61: Train Loss = 0.013885364628616418, Recall = 0.11743772241992882, Aging Rate = 0.00024181026802759184, Precision = 0.868421052631579, f1 = 0.20689655172413796\n",
      "Epoch 62: Train Loss = 0.013807525264326957, Recall = 0.1423487544483986, Aging Rate = 0.00029908112098149514, Precision = 0.851063829787234, f1 = 0.24390243902439027\n",
      "Epoch 63: Train Loss = 0.01372210919320164, Recall = 0.12455516014234876, Aging Rate = 0.0002481736961335811, Precision = 0.8974358974358975, f1 = 0.21875\n",
      "Epoch 64: Train Loss = 0.013660402828745716, Recall = 0.12811387900355872, Aging Rate = 0.00026726398045154884, Precision = 0.8571428571428571, f1 = 0.22291021671826625\n",
      "Epoch 65: Train Loss = 0.013608520867607252, Recall = 0.1387900355871886, Aging Rate = 0.0002609005523455596, Precision = 0.9512195121951219, f1 = 0.2422360248447205\n",
      "Test Loss = 0.013066495658156948, Recall = 0.1387900355871886, Aging Rate = 0.00026726398045154884, precision = 0.9285714285714286\n",
      "\n",
      "Epoch 66: Train Loss = 0.013542220118476994, Recall = 0.13167259786476868, Aging Rate = 0.00028635426476951664, Precision = 0.8222222222222222, f1 = 0.22699386503067484\n",
      "Epoch 67: Train Loss = 0.013479261796720562, Recall = 0.15302491103202848, Aging Rate = 0.0003245348334054522, Precision = 0.8431372549019608, f1 = 0.25903614457831325\n",
      "Epoch 68: Train Loss = 0.013392263203333301, Recall = 0.15302491103202848, Aging Rate = 0.00029908112098149514, Precision = 0.9148936170212766, f1 = 0.2621951219512196\n",
      "Epoch 69: Train Loss = 0.013338242591681758, Recall = 0.1387900355871886, Aging Rate = 0.0002927176928755059, Precision = 0.8478260869565217, f1 = 0.2385321100917431\n",
      "Epoch 70: Train Loss = 0.013302247570849175, Recall = 0.14590747330960854, Aging Rate = 0.00028635426476951664, Precision = 0.9111111111111111, f1 = 0.25153374233128833\n",
      "Test Loss = 0.012739964012483243, Recall = 0.12455516014234876, Aging Rate = 0.00024181026802759184, precision = 0.9210526315789473\n",
      "\n",
      "Epoch 71: Train Loss = 0.013164592381984409, Recall = 0.15302491103202848, Aging Rate = 0.0003054445490874844, Precision = 0.8958333333333334, f1 = 0.26139817629179335\n",
      "Epoch 72: Train Loss = 0.013171727920360145, Recall = 0.15302491103202848, Aging Rate = 0.00031817140529946295, Precision = 0.86, f1 = 0.25981873111782483\n",
      "Epoch 73: Train Loss = 0.013117982174567147, Recall = 0.1387900355871886, Aging Rate = 0.00031817140529946295, Precision = 0.78, f1 = 0.23564954682779457\n",
      "Epoch 74: Train Loss = 0.01310939425107582, Recall = 0.14590747330960854, Aging Rate = 0.0003054445490874844, Precision = 0.8541666666666666, f1 = 0.24924012158054712\n",
      "Epoch 75: Train Loss = 0.012988870467680899, Recall = 0.1387900355871886, Aging Rate = 0.0003054445490874844, Precision = 0.8125, f1 = 0.2370820668693009\n",
      "Test Loss = 0.012598936005996294, Recall = 0.15302491103202848, Aging Rate = 0.0002927176928755059, precision = 0.9347826086956522\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a295e23267c4404db77745703c2cc5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db66d01421af4e92a1db6ea4973e5a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5429628784886471, Recall = 0.897389412617839, Aging Rate = 0.6680565627266135, Precision = 0.6716417910447762, f1 = 0.7682756479900669\n",
      "Epoch 2: Train Loss = 0.3368631757368983, Recall = 0.9002900652646846, Aging Rate = 0.5406091370558376, Precision = 0.8326626425217974, f1 = 0.8651567944250872\n",
      "Epoch 3: Train Loss = 0.2505217915626433, Recall = 0.9253081943437274, Aging Rate = 0.5251994198694706, Precision = 0.8809112875388333, f1 = 0.9025641025641027\n",
      "Epoch 4: Train Loss = 0.1883865756950489, Recall = 0.9586656997824511, Aging Rate = 0.5226613488034808, Precision = 0.9171002428026361, f1 = 0.9374224428292857\n",
      "Epoch 5: Train Loss = 0.14565593455908002, Recall = 0.9651921682378535, Aging Rate = 0.5145032632342277, Precision = 0.937984496124031, f1 = 0.9513938527519656\n",
      "Test Loss = 0.10591364639491772, Recall = 0.9840464104423495, Aging Rate = 0.5067077592458303, precision = 0.9710196779964222\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.09389632080693103, Recall = 0.9876722262509064, Aging Rate = 0.5092458303118201, Precision = 0.9697401210395158, f1 = 0.9786240344889526\n",
      "Epoch 7: Train Loss = 0.06945590494420754, Recall = 0.9923857868020305, Aging Rate = 0.5038071065989848, Precision = 0.9848866498740554, f1 = 0.988621997471555\n",
      "Epoch 8: Train Loss = 0.053026232619900214, Recall = 0.9949238578680203, Aging Rate = 0.5034445250181291, Precision = 0.9881166726683471, f1 = 0.9915085817524841\n",
      "Epoch 9: Train Loss = 0.04031604119293537, Recall = 0.9974619289340102, Aging Rate = 0.5021754894851341, Precision = 0.9931407942238267, f1 = 0.9952966714905934\n",
      "Epoch 10: Train Loss = 0.03220779549916686, Recall = 0.9978245105148659, Aging Rate = 0.5010877447425671, Precision = 0.9956584659913169, f1 = 0.9967403114813473\n",
      "Test Loss = 0.02730896758581522, Recall = 1.0, Aging Rate = 0.502356780275562, precision = 0.9953085528690003\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.02638060556072269, Recall = 0.9989122552574329, Aging Rate = 0.501269035532995, Precision = 0.9963833634719711, f1 = 0.997646206771682\n",
      "Epoch 12: Train Loss = 0.021255456100935908, Recall = 1.0, Aging Rate = 0.5010877447425671, Precision = 0.9978292329956585, f1 = 0.9989134371604491\n",
      "Epoch 13: Train Loss = 0.01739952145238894, Recall = 0.9996374184191443, Aging Rate = 0.5005438723712835, Precision = 0.9985512495472655, f1 = 0.9990940387751405\n",
      "Epoch 14: Train Loss = 0.014812455609159853, Recall = 1.0, Aging Rate = 0.5005438723712835, Precision = 0.9989134371604491, f1 = 0.9994564232650842\n",
      "Epoch 15: Train Loss = 0.0128401341551592, Recall = 1.0, Aging Rate = 0.5005438723712835, Precision = 0.9989134371604491, f1 = 0.9994564232650842\n",
      "Test Loss = 0.01063533917919039, Recall = 1.0, Aging Rate = 0.5005438723712835, precision = 0.9989134371604491\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.010937320693187345, Recall = 1.0, Aging Rate = 0.5005438723712835, Precision = 0.9989134371604491, f1 = 0.9994564232650842\n",
      "Epoch 17: Train Loss = 0.009258122349521274, Recall = 1.0, Aging Rate = 0.5005438723712835, Precision = 0.9989134371604491, f1 = 0.9994564232650842\n",
      "Epoch 18: Train Loss = 0.007975901637605896, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 19: Train Loss = 0.007007706733089565, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 20: Train Loss = 0.006224255360179379, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Test Loss = 0.005254124343919313, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.005452641344036936, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 22: Train Loss = 0.004924453789553321, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.004471932251838409, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.004172960137563438, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.0036487337786020838, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033237794447930676, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.0034589145210250113, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.003138446164964256, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.0029982946975361975, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.0027675224823351784, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.002579347828694871, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002286169859855905, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.002458289510134352, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.0023810596705398823, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.002363847290044833, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0022408543732030787, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.0021200574483003862, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018761247051866257, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.002018328812824288, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.0019553287160407923, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.001958369324397447, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0019893728738935683, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.001960088513579498, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017026709163044137, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.00203994192607482, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.001895474561573188, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0017786482607363638, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0021464893937298652, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0016371278203530825, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014375295835135854, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.001574405369215778, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.00165754405814044, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.002045306025587557, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0016986258636769705, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0016292840300171424, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014760748827168104, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0016850337688017573, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0016323302906584684, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0015780295828185797, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0016607184293902762, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0017585637429634554, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002376996233999902, Recall = 1.0, Aging Rate = 0.5003625815808557, precision = 0.9992753623188406\n",
      "\n",
      "Epoch 56: Train Loss = 0.0017458813513498794, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0015988303189712258, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0016807284620823507, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.002215501610216691, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0016129148639117228, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013257917366243839, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Train Loss = 0.0014044472429503912, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0015233698752833674, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.001641009350481529, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0015988169583080686, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0014879494523724096, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001269764714419134, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0015931689658504793, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.001569588850675275, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0015561493872721431, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.001837996001969253, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0016564192475603814, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013647776780355563, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84234964e36f47eca68e34e0bfc19b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788388137c6349a7862966963671b02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.527768557821678, Recall = 0.7478145695364239, Aging Rate = 0.5006622516556292, Precision = 0.7468253968253968, f1 = 0.7473196558570483\n",
      "Epoch 2: Train Loss = 0.33970873947964597, Recall = 0.8649006622516556, Aging Rate = 0.5013245033112583, Precision = 0.8626155878467635, f1 = 0.8637566137566137\n",
      "Epoch 3: Train Loss = 0.2589103692454218, Recall = 0.9059602649006623, Aging Rate = 0.5029139072847683, Precision = 0.9007110877008164, f1 = 0.9033280507131537\n",
      "Epoch 4: Train Loss = 0.2063593929354718, Recall = 0.9271523178807947, Aging Rate = 0.5026490066225165, Precision = 0.922266139657444, f1 = 0.9247027741083224\n",
      "Epoch 5: Train Loss = 0.1690731295568264, Recall = 0.9433112582781457, Aging Rate = 0.5002649006622517, Precision = 0.9428117553613979, f1 = 0.943061440677966\n",
      "Test Loss = 0.12809349602026654, Recall = 0.9570860927152318, Aging Rate = 0.4937748344370861, precision = 0.9691523605150214\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1197672149943595, Recall = 0.960794701986755, Aging Rate = 0.49642384105960263, Precision = 0.9677161152614728, f1 = 0.9642429881696132\n",
      "Epoch 7: Train Loss = 0.09737692289794518, Recall = 0.9692715231788079, Aging Rate = 0.4976158940397351, Precision = 0.973915357998403, f1 = 0.971587891662241\n",
      "Epoch 8: Train Loss = 0.08080438189159166, Recall = 0.9766887417218543, Aging Rate = 0.49920529801324504, Precision = 0.9782435659326081, f1 = 0.9774655355249205\n",
      "Epoch 9: Train Loss = 0.0680833668404857, Recall = 0.9811920529801325, Aging Rate = 0.5003973509933775, Precision = 0.980412916887242, f1 = 0.9808023301999207\n",
      "Epoch 10: Train Loss = 0.05846457305530839, Recall = 0.9835761589403974, Aging Rate = 0.4996026490066225, Precision = 0.9843584305408272, f1 = 0.9839671392606334\n",
      "Test Loss = 0.04840775556516963, Recall = 0.9894039735099338, Aging Rate = 0.4989403973509934, precision = 0.9915051765330501\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04998432928757952, Recall = 0.9880794701986755, Aging Rate = 0.49973509933774835, Precision = 0.9886032335011927, f1 = 0.9883412824589295\n",
      "Epoch 12: Train Loss = 0.044062015352067566, Recall = 0.9896688741721854, Aging Rate = 0.5001324503311259, Precision = 0.989406779661017, f1 = 0.9895378095616474\n",
      "Epoch 13: Train Loss = 0.038834032171609384, Recall = 0.9899337748344371, Aging Rate = 0.49933774834437084, Precision = 0.9912466843501326, f1 = 0.9905897945659379\n",
      "Epoch 14: Train Loss = 0.03497118790689486, Recall = 0.991523178807947, Aging Rate = 0.4996026490066225, Precision = 0.9923117709437964, f1 = 0.9919173181396582\n",
      "Epoch 15: Train Loss = 0.030697516110904564, Recall = 0.9939072847682119, Aging Rate = 0.5006622516556292, Precision = 0.9925925925925926, f1 = 0.9932495036399734\n",
      "Test Loss = 0.026862075068303293, Recall = 0.9920529801324504, Aging Rate = 0.49748344370860925, precision = 0.9970713525026624\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.028343206501214316, Recall = 0.9928476821192053, Aging Rate = 0.4989403973509934, Precision = 0.9949561985664985, f1 = 0.9939008220631131\n",
      "Epoch 17: Train Loss = 0.0246778724478274, Recall = 0.9954966887417218, Aging Rate = 0.5, Precision = 0.9954966887417218, f1 = 0.9954966887417218\n",
      "Epoch 18: Train Loss = 0.02221837816698267, Recall = 0.9952317880794702, Aging Rate = 0.4994701986754967, Precision = 0.9962874569079819, f1 = 0.9957593426981182\n",
      "Epoch 19: Train Loss = 0.02073245977033053, Recall = 0.9954966887417218, Aging Rate = 0.49973509933774835, Precision = 0.9960243837794858, f1 = 0.9957604663487016\n",
      "Epoch 20: Train Loss = 0.018849754737110327, Recall = 0.9970860927152317, Aging Rate = 0.5005298013245033, Precision = 0.9960306959513099, f1 = 0.9965581149060101\n",
      "Test Loss = 0.016158972616177915, Recall = 0.9973509933774835, Aging Rate = 0.5, precision = 0.9973509933774835\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.01733412523834121, Recall = 0.9968211920529801, Aging Rate = 0.49986754966887414, Precision = 0.9970853206147324, f1 = 0.9969532388395813\n",
      "Epoch 22: Train Loss = 0.016193664568346856, Recall = 0.9968211920529801, Aging Rate = 0.49986754966887414, Precision = 0.9970853206147324, f1 = 0.9969532388395813\n",
      "Epoch 23: Train Loss = 0.014771313698629275, Recall = 0.9968211920529801, Aging Rate = 0.4996026490066225, Precision = 0.9976139978791092, f1 = 0.9972174373923414\n",
      "Epoch 24: Train Loss = 0.013621906848340635, Recall = 0.9978807947019868, Aging Rate = 0.5001324503311259, Precision = 0.9976165254237288, f1 = 0.9977486425638988\n",
      "Epoch 25: Train Loss = 0.012710116805866459, Recall = 0.9981456953642384, Aging Rate = 0.5003973509933775, Precision = 0.9973530968766543, f1 = 0.9977492387130942\n",
      "Test Loss = 0.010794532729423796, Recall = 0.9978807947019868, Aging Rate = 0.49933774834437084, precision = 0.9992042440318303\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.011594972776261387, Recall = 0.9984105960264901, Aging Rate = 0.5001324503311259, Precision = 0.998146186440678, f1 = 0.9982783737253345\n",
      "Epoch 27: Train Loss = 0.011908663548775856, Recall = 0.9981456953642384, Aging Rate = 0.49973509933774835, Precision = 0.9986747945931619, f1 = 0.998410174880763\n",
      "Epoch 28: Train Loss = 0.010714642377385241, Recall = 0.999205298013245, Aging Rate = 0.5005298013245033, Precision = 0.9981476581106112, f1 = 0.998676198040773\n",
      "Epoch 29: Train Loss = 0.00989397918620449, Recall = 0.9984105960264901, Aging Rate = 0.49986754966887414, Precision = 0.9986751457339693, f1 = 0.9985428533580607\n",
      "Epoch 30: Train Loss = 0.010091978817783445, Recall = 0.9989403973509934, Aging Rate = 0.5006622516556292, Precision = 0.9976190476190476, f1 = 0.998279285241562\n",
      "Test Loss = 0.007838517065462194, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, precision = 0.9994700582935877\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.00904087747070963, Recall = 0.9989403973509934, Aging Rate = 0.5, Precision = 0.9989403973509934, f1 = 0.9989403973509934\n",
      "Epoch 32: Train Loss = 0.00926110673954846, Recall = 0.9989403973509934, Aging Rate = 0.5003973509933775, Precision = 0.998147167813658, f1 = 0.9985436250496492\n",
      "Epoch 33: Train Loss = 0.008008059510554107, Recall = 0.9989403973509934, Aging Rate = 0.49986754966887414, Precision = 0.9992050874403816, f1 = 0.9990727248642204\n",
      "Epoch 34: Train Loss = 0.008408444952589786, Recall = 0.999205298013245, Aging Rate = 0.5005298013245033, Precision = 0.9981476581106112, f1 = 0.998676198040773\n",
      "Epoch 35: Train Loss = 0.008163995990801035, Recall = 0.9989403973509934, Aging Rate = 0.5001324503311259, Precision = 0.9986758474576272, f1 = 0.9988081048867701\n",
      "Test Loss = 0.006692714470605187, Recall = 0.9984105960264901, Aging Rate = 0.49973509933774835, precision = 0.9989398356745296\n",
      "\n",
      "Epoch 36: Train Loss = 0.007286837857636, Recall = 0.9989403973509934, Aging Rate = 0.5001324503311259, Precision = 0.9986758474576272, f1 = 0.9988081048867701\n",
      "Epoch 37: Train Loss = 0.00709177735181439, Recall = 0.9989403973509934, Aging Rate = 0.5, Precision = 0.9989403973509934, f1 = 0.9989403973509934\n",
      "Epoch 38: Train Loss = 0.006999081046264989, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 39: Train Loss = 0.006565731472107551, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 40: Train Loss = 0.007833581048230462, Recall = 0.9989403973509934, Aging Rate = 0.5002649006622517, Precision = 0.9984114376489277, f1 = 0.998675847457627\n",
      "Test Loss = 0.0067856166664733005, Recall = 0.9989403973509934, Aging Rate = 0.4996026490066225, precision = 0.9997348886532343\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.0073459172349373825, Recall = 0.9989403973509934, Aging Rate = 0.5002649006622517, Precision = 0.9984114376489277, f1 = 0.998675847457627\n",
      "Epoch 42: Train Loss = 0.00621580002286675, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 43: Train Loss = 0.006451732616452192, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 44: Train Loss = 0.005542556627665875, Recall = 0.9997350993377483, Aging Rate = 0.5003973509933775, Precision = 0.9989412387506618, f1 = 0.9993380113862042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.005978532721356347, Recall = 0.9986754966887417, Aging Rate = 0.49973509933774835, Precision = 0.9992048767558972, f1 = 0.9989401165871753\n",
      "Test Loss = 0.004956204015699937, Recall = 0.999205298013245, Aging Rate = 0.5003973509933775, precision = 0.9984118581259925\n",
      "\n",
      "Epoch 46: Train Loss = 0.006310180595070617, Recall = 0.9989403973509934, Aging Rate = 0.5, Precision = 0.9989403973509934, f1 = 0.9989403973509934\n",
      "Epoch 47: Train Loss = 0.0060713301999309405, Recall = 0.9989403973509934, Aging Rate = 0.5001324503311259, Precision = 0.9986758474576272, f1 = 0.9988081048867701\n",
      "Epoch 48: Train Loss = 0.005267484795721557, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 49: Train Loss = 0.005677927411500586, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, Precision = 0.9989406779661016, f1 = 0.9990729704674877\n",
      "Epoch 50: Train Loss = 0.0058456945745582805, Recall = 0.999205298013245, Aging Rate = 0.5003973509933775, Precision = 0.9984118581259925, f1 = 0.9988084204951675\n",
      "Test Loss = 0.00580573884835035, Recall = 0.9984105960264901, Aging Rate = 0.4996026490066225, precision = 0.999204665959703\n",
      "\n",
      "Epoch 51: Train Loss = 0.006012835916353772, Recall = 0.9989403973509934, Aging Rate = 0.49973509933774835, Precision = 0.9994699178372648, f1 = 0.9992050874403815\n",
      "Epoch 52: Train Loss = 0.004980577312915629, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 53: Train Loss = 0.005119986139632614, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 54: Train Loss = 0.005165234869901116, Recall = 0.999205298013245, Aging Rate = 0.49973509933774835, Precision = 0.9997349589186324, f1 = 0.9994700582935877\n",
      "Epoch 55: Train Loss = 0.005949559139846424, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Test Loss = 0.004273780826479196, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, precision = 0.9994703389830508\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.005224121237445055, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 57: Train Loss = 0.005119586675489531, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 58: Train Loss = 0.0052354308100577614, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 59: Train Loss = 0.005307808886621369, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 60: Train Loss = 0.005038703088124363, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Test Loss = 0.0036345925760634295, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, precision = 0.9997350291467939\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.004657850840671716, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 62: Train Loss = 0.004841226675664846, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 63: Train Loss = 0.005283176972043534, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 64: Train Loss = 0.004337079170351194, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 65: Train Loss = 0.004063457039906489, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.003629040786108335, Recall = 1.0, Aging Rate = 0.500794701986755, precision = 0.9984131182226924\n",
      "\n",
      "Epoch 66: Train Loss = 0.004517101048683094, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 67: Train Loss = 0.004952033929625489, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 68: Train Loss = 0.00562072819616917, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 69: Train Loss = 0.006066622565907506, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, Precision = 0.9989406779661016, f1 = 0.9990729704674877\n",
      "Epoch 70: Train Loss = 0.004684101328599137, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Test Loss = 0.0035769033391608407, Recall = 0.9997350993377483, Aging Rate = 0.5003973509933775, precision = 0.9989412387506618\n",
      "\n",
      "Epoch 71: Train Loss = 0.004610044962313297, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 72: Train Loss = 0.005324726042442567, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 73: Train Loss = 0.004955119928591792, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 74: Train Loss = 0.004189829467549081, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 75: Train Loss = 0.004525979445701379, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Test Loss = 0.004224956238315447, Recall = 0.9994701986754967, Aging Rate = 0.49973509933774835, precision = 1.0\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.004031137566248707, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 77: Train Loss = 0.00422793045862029, Recall = 0.999205298013245, Aging Rate = 0.49973509933774835, Precision = 0.9997349589186324, f1 = 0.9994700582935877\n",
      "Epoch 78: Train Loss = 0.0048018481325518515, Recall = 0.9997350993377483, Aging Rate = 0.5003973509933775, Precision = 0.9989412387506618, f1 = 0.9993380113862042\n",
      "Epoch 79: Train Loss = 0.004561512099134902, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 80: Train Loss = 0.004899371698905794, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Test Loss = 0.003909111623189702, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 81: Train Loss = 0.004495231525167601, Recall = 0.999205298013245, Aging Rate = 0.4996026490066225, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.00449395384281775, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Epoch 83: Train Loss = 0.0038490401810368164, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 84: Train Loss = 0.004348835159834807, Recall = 1.0, Aging Rate = 0.5002649006622517, Precision = 0.9994704792163093, f1 = 0.9997351694915254\n",
      "Epoch 85: Train Loss = 0.0040766093052232895, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Test Loss = 0.003660550578219804, Recall = 1.0, Aging Rate = 0.5003973509933775, precision = 0.9992059290629963\n",
      "\n",
      "Epoch 86: Train Loss = 0.004406121528201693, Recall = 1.0, Aging Rate = 0.5003973509933775, Precision = 0.9992059290629963, f1 = 0.9996028068317225\n",
      "Epoch 87: Train Loss = 0.004347567050008485, Recall = 1.0, Aging Rate = 0.5003973509933775, Precision = 0.9992059290629963, f1 = 0.9996028068317225\n",
      "Epoch 88: Train Loss = 0.004770548482925094, Recall = 0.9989403973509934, Aging Rate = 0.4996026490066225, Precision = 0.9997348886532343, f1 = 0.9993374850934146\n",
      "Epoch 89: Train Loss = 0.004803841960992639, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, Precision = 0.9994700582935877, f1 = 0.9993376606173003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Train Loss = 0.004045987627869904, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Test Loss = 0.003349076027716726, Recall = 1.0, Aging Rate = 0.5005298013245033, precision = 0.9989415189203493\n",
      "\n",
      "Epoch 91: Train Loss = 0.003690788946577058, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 92: Train Loss = 0.004653766031148813, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 93: Train Loss = 0.005728254492166422, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 94: Train Loss = 0.003995190093922891, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 95: Train Loss = 0.004093370328083733, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Test Loss = 0.0030128189419790954, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 96: Train Loss = 0.004189454096721004, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 97: Train Loss = 0.004207732616029433, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 98: Train Loss = 0.004093994541056701, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 99: Train Loss = 0.0047477111737212994, Recall = 0.9989403973509934, Aging Rate = 0.49973509933774835, Precision = 0.9994699178372648, f1 = 0.9992050874403815\n",
      "Epoch 100: Train Loss = 0.0048642060035432606, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Test Loss = 0.003045462633341255, Recall = 0.999205298013245, Aging Rate = 0.4996026490066225, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fcb7a916cd47c1ad23a904dc50dc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daab788b72f4369a0987929f7b5767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5801356896352211, Recall = 0.9260700389105059, Aging Rate = 0.9246108949416343, Precision = 0.5007890583903208, f1 = 0.6500512120177534\n",
      "Epoch 2: Train Loss = 0.46540825144326176, Recall = 0.9753566796368353, Aging Rate = 0.8699740596627756, Precision = 0.5605665300037271, f1 = 0.7119526627218934\n",
      "Epoch 3: Train Loss = 0.39643302733796115, Recall = 0.9416342412451362, Aging Rate = 0.6948767833981842, Precision = 0.6775548296780215, f1 = 0.7880597014925373\n",
      "Epoch 4: Train Loss = 0.3427946466648161, Recall = 0.9442282749675746, Aging Rate = 0.6363488975356679, Precision = 0.7419108280254777, f1 = 0.8309316592951919\n",
      "Epoch 5: Train Loss = 0.29643416288761776, Recall = 0.9458495460440985, Aging Rate = 0.5964656290531777, Precision = 0.7928784995922805, f1 = 0.8626349253289959\n",
      "Test Loss = 0.258874706547084, Recall = 0.9691958495460441, Aging Rate = 0.5867380025940337, precision = 0.8259187620889749\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.23948868326407927, Recall = 0.9633592736705577, Aging Rate = 0.5586900129701686, Precision = 0.8621590249564712, f1 = 0.9099540581929555\n",
      "Epoch 7: Train Loss = 0.1979006727149978, Recall = 0.9717898832684825, Aging Rate = 0.5428015564202334, Precision = 0.8951612903225806, f1 = 0.9319029850746269\n",
      "Epoch 8: Train Loss = 0.16385586472800734, Recall = 0.9792477302204928, Aging Rate = 0.5298313878080415, Precision = 0.9241126070991432, f1 = 0.9508816120906801\n",
      "Epoch 9: Train Loss = 0.136485358417498, Recall = 0.98378728923476, Aging Rate = 0.5207522697795072, Precision = 0.9445828144458281, f1 = 0.9637865311308766\n",
      "Epoch 10: Train Loss = 0.11578836744281569, Recall = 0.9867055771725033, Aging Rate = 0.5145914396887159, Precision = 0.9587271581600504, f1 = 0.9725151805688719\n",
      "Test Loss = 0.10184356074504815, Recall = 0.9915693904020753, Aging Rate = 0.5137808041504539, precision = 0.9649731776585674\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.0974657582227822, Recall = 0.9909208819714657, Aging Rate = 0.5132944228274967, Precision = 0.965255843335439, f1 = 0.97792\n",
      "Epoch 12: Train Loss = 0.08398411831735172, Recall = 0.9941634241245136, Aging Rate = 0.5119974059662775, Precision = 0.9708676377454085, f1 = 0.9823774431272029\n",
      "Epoch 13: Train Loss = 0.07186997023769853, Recall = 0.9954604409857328, Aging Rate = 0.5090791180285343, Precision = 0.9777070063694268, f1 = 0.9865038560411311\n",
      "Epoch 14: Train Loss = 0.0625933164998831, Recall = 0.9961089494163424, Aging Rate = 0.5068093385214008, Precision = 0.982725527831094, f1 = 0.9893719806763285\n",
      "Epoch 15: Train Loss = 0.05505871350953087, Recall = 0.9970817120622568, Aging Rate = 0.5050259403372244, Precision = 0.9871589085072231, f1 = 0.9920954992740764\n",
      "Test Loss = 0.05037245218160088, Recall = 1.0, Aging Rate = 0.5089169909208819, precision = 0.9824784963364128\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04821495361017346, Recall = 0.9987029831387808, Aging Rate = 0.5058365758754864, Precision = 0.9871794871794872, f1 = 0.9929078014184397\n",
      "Epoch 17: Train Loss = 0.04280969790447547, Recall = 0.9990272373540856, Aging Rate = 0.5037289234760052, Precision = 0.9916317991631799, f1 = 0.9953157809723792\n",
      "Epoch 18: Train Loss = 0.03796206971318805, Recall = 1.0, Aging Rate = 0.5038910505836576, Precision = 0.9922779922779923, f1 = 0.9961240310077519\n",
      "Epoch 19: Train Loss = 0.03391096836630009, Recall = 0.9990272373540856, Aging Rate = 0.5021076523994812, Precision = 0.9948337100419761, f1 = 0.9969260637437307\n",
      "Epoch 20: Train Loss = 0.03039595806529086, Recall = 1.0, Aging Rate = 0.5021076523994812, Precision = 0.9958023894091056, f1 = 0.9978967804562368\n",
      "Test Loss = 0.02736492452106587, Recall = 1.0, Aging Rate = 0.501621271076524, precision = 0.9967679379444085\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.02716878247501669, Recall = 1.0, Aging Rate = 0.501621271076524, Precision = 0.9967679379444085, f1 = 0.9983813531887342\n",
      "Epoch 22: Train Loss = 0.02462287993869719, Recall = 1.0, Aging Rate = 0.501621271076524, Precision = 0.9967679379444085, f1 = 0.9983813531887342\n",
      "Epoch 23: Train Loss = 0.022333400454444645, Recall = 1.0, Aging Rate = 0.5014591439688716, Precision = 0.997090203685742, f1 = 0.9985429820301116\n",
      "Epoch 24: Train Loss = 0.020548936062952657, Recall = 1.0, Aging Rate = 0.5012970168612192, Precision = 0.9974126778783958, f1 = 0.9987046632124352\n",
      "Epoch 25: Train Loss = 0.01859395550962552, Recall = 1.0, Aging Rate = 0.500810635538262, Precision = 0.9983813531887342, f1 = 0.9991900210594524\n",
      "Test Loss = 0.01689399251939026, Recall = 1.0, Aging Rate = 0.5001621271076524, precision = 0.9996758508914101\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.016974193199831223, Recall = 1.0, Aging Rate = 0.5003242542153048, Precision = 0.9993519118600129, f1 = 0.99967585089141\n",
      "Epoch 27: Train Loss = 0.01549063182656983, Recall = 1.0, Aging Rate = 0.5004863813229572, Precision = 0.9990281827016521, f1 = 0.9995138551288284\n",
      "Epoch 28: Train Loss = 0.014285384267858494, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Epoch 29: Train Loss = 0.013143989456824755, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Epoch 30: Train Loss = 0.012151343081189965, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.011832398491103478, Recall = 1.0, Aging Rate = 0.5001621271076524, precision = 0.9996758508914101\n",
      "\n",
      "Epoch 31: Train Loss = 0.011298802391361402, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.010497444961582409, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.009767943927981974, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.009165009664886193, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.008718297976261216, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00792164230222745, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.008079828132770172, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.0075997889634395735, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.007117908797679824, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.006792072868083354, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.006411099773813546, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005988816147242539, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.006102772614400442, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.005841227003702974, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.00558430289827896, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0052560747343800415, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.005039902660336186, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00475747273249211, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.004857409334150849, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.004690321749402392, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.004494019680754336, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.004341226654554851, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.004199773584155489, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003980240697613718, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0040785286390740815, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.00386907135187159, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0037791707909916043, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0036475158002995797, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0035840961620110133, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033388945775954526, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: Train Loss = 0.0034528924057129697, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0033840406459578628, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0032722943141723073, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0032259485238604973, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0031296232369173943, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002958163104458542, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0030370684112295335, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0029735552502165507, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.00292503641889311, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0028638965658597097, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.002820480387958658, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026523192115997757, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.002789791017056874, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.002715392137881267, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.002682700199672974, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0025990748948305263, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0025664911223104376, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0023992920479219016, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0025106646674993207, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0025036205303880605, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0024907247943383305, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0024194996813058506, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.002403446869989893, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022546699375243936, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0024061287392713564, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0023348657966950175, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0023350849800203213, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0022995757239830865, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.002306653876111259, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002129209193433653, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.002249804387912734, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.002246962726145171, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0022471853167702832, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0022200421921198584, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0021580838034806434, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020121806323825445, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f71fa12eaf47e2b963ee2c0999de66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9be4bd758d44543b15042bf4c699f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5798268758898403, Recall = 0.7098611605553578, Aging Rate = 0.4587041651833393, Precision = 0.7737679472254559, f1 = 0.740438173041218\n",
      "Epoch 2: Train Loss = 0.37220201167868483, Recall = 0.8401566393734425, Aging Rate = 0.48576005695977215, Precision = 0.8647856357640161, f1 = 0.852293246659444\n",
      "Epoch 3: Train Loss = 0.2701854219361105, Recall = 0.9056603773584906, Aging Rate = 0.4996440014239943, Precision = 0.9063056644104026, f1 = 0.9059829059829061\n",
      "Epoch 4: Train Loss = 0.21376451209118547, Recall = 0.9305802776788893, Aging Rate = 0.4969740121039516, Precision = 0.9362464183381088, f1 = 0.9334047491519372\n",
      "Epoch 5: Train Loss = 0.1761859164474782, Recall = 0.9433962264150944, Aging Rate = 0.49305802776788893, Precision = 0.9566787003610109, f1 = 0.949991037820398\n",
      "Test Loss = 0.14553204315143278, Recall = 0.9644001423994304, Aging Rate = 0.5028479886080456, precision = 0.9589380530973451\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.13342827950939457, Recall = 0.9633321466714133, Aging Rate = 0.4944820220719117, Precision = 0.9740820734341252, f1 = 0.9686772865580812\n",
      "Epoch 7: Train Loss = 0.1124859293501714, Recall = 0.9661801352794589, Aging Rate = 0.49341402634389464, Precision = 0.9790764790764791, f1 = 0.9725855581437018\n",
      "Epoch 8: Train Loss = 0.0941219070058312, Recall = 0.970808116767533, Aging Rate = 0.49181203275186897, Precision = 0.9869706840390879, f1 = 0.9788226848528356\n",
      "Epoch 9: Train Loss = 0.08096037522069752, Recall = 0.9750800996796013, Aging Rate = 0.4928800284798861, Precision = 0.9891657638136512, f1 = 0.982072427393331\n",
      "Epoch 10: Train Loss = 0.07061939724598673, Recall = 0.9797080811676754, Aging Rate = 0.4937700249199003, Precision = 0.9920692141312184, f1 = 0.985849901486656\n",
      "Test Loss = 0.06292059617339185, Recall = 0.9832680669277323, Aging Rate = 0.49537201851192597, precision = 0.9924541861300754\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06191432649112546, Recall = 0.9814880740477038, Aging Rate = 0.494126023495906, Precision = 0.9931556195965417, f1 = 0.9872873769024171\n",
      "Epoch 12: Train Loss = 0.05517034494407918, Recall = 0.9839800640797437, Aging Rate = 0.4944820220719117, Precision = 0.9949604031677466, f1 = 0.9894397708967246\n",
      "Epoch 13: Train Loss = 0.050354992085587046, Recall = 0.9843360626557494, Aging Rate = 0.49466002135991455, Precision = 0.9949622166246851, f1 = 0.9896206156048676\n",
      "Epoch 14: Train Loss = 0.044376336237042346, Recall = 0.9878960484158064, Aging Rate = 0.4957280170879316, Precision = 0.9964093357271095, f1 = 0.9921344297461566\n",
      "Epoch 15: Train Loss = 0.04066781538391758, Recall = 0.9878960484158064, Aging Rate = 0.4957280170879316, Precision = 0.9964093357271095, f1 = 0.9921344297461566\n",
      "Test Loss = 0.037515162068908856, Recall = 0.9943040227839088, Aging Rate = 0.4989320042719829, precision = 0.9964323938637174\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.038118383850160975, Recall = 0.9886080455678178, Aging Rate = 0.4957280170879316, Precision = 0.9971274685816877, f1 = 0.9928494815874152\n",
      "Epoch 17: Train Loss = 0.03484511119471024, Recall = 0.9907440370238519, Aging Rate = 0.4966180135279459, Precision = 0.9974910394265233, f1 = 0.9941060903732809\n",
      "Epoch 18: Train Loss = 0.03204533712651306, Recall = 0.9925240299038803, Aging Rate = 0.49750800996796013, Precision = 0.9974955277280859, f1 = 0.9950035688793719\n",
      "Epoch 19: Train Loss = 0.030476185425866664, Recall = 0.9935920256318975, Aging Rate = 0.4973300106799573, Precision = 0.9989262705798139, f1 = 0.9962520078529359\n",
      "Epoch 20: Train Loss = 0.029207681119044128, Recall = 0.9935920256318975, Aging Rate = 0.49768600925596296, Precision = 0.998211731044349, f1 = 0.9958965209634255\n",
      "Test Loss = 0.025586775703313896, Recall = 0.9971520113919544, Aging Rate = 0.49911000355998575, precision = 0.9989300998573466\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.027031981429014185, Recall = 0.9953720185119259, Aging Rate = 0.4982200071199715, Precision = 0.9989281886387996, f1 = 0.9971469329529244\n",
      "Epoch 22: Train Loss = 0.02567191751877389, Recall = 0.996440014239943, Aging Rate = 0.4989320042719829, Precision = 0.998572957545487, f1 = 0.9975053456878118\n",
      "Epoch 23: Train Loss = 0.02484471516484105, Recall = 0.996440014239943, Aging Rate = 0.4985760056959772, Precision = 0.9992859692966798, f1 = 0.9978609625668449\n",
      "Epoch 24: Train Loss = 0.023710817138403496, Recall = 0.9967960128159488, Aging Rate = 0.49875400498398004, Precision = 0.9992862241256245, f1 = 0.9980395651399038\n",
      "Epoch 25: Train Loss = 0.022623999125197206, Recall = 0.9982200071199715, Aging Rate = 0.49928800284798863, Precision = 0.9996434937611408, f1 = 0.9989312433202707\n",
      "Test Loss = 0.021165711921929845, Recall = 0.9985760056959773, Aging Rate = 0.49928800284798863, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.022020668902388987, Recall = 0.9982200071199715, Aging Rate = 0.49928800284798863, Precision = 0.9996434937611408, f1 = 0.9989312433202707\n",
      "Epoch 27: Train Loss = 0.02131980830712669, Recall = 0.9967960128159488, Aging Rate = 0.4985760056959772, Precision = 0.9996429846483399, f1 = 0.9982174688057041\n",
      "Epoch 28: Train Loss = 0.020459045579105925, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.020337089146900193, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Epoch 30: Train Loss = 0.02006111276392709, Recall = 0.9982200071199715, Aging Rate = 0.49911000355998575, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01813233358703949, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.019655710455572618, Recall = 0.9985760056959773, Aging Rate = 0.49928800284798863, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.018939438511061474, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.018941701418693994, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Epoch 34: Train Loss = 0.01892303919698811, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.019019828200865445, Recall = 0.9985760056959773, Aging Rate = 0.4996440014239943, Precision = 0.9992874955468471, f1 = 0.9989316239316239\n",
      "Test Loss = 0.01728242957197334, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.017907084428017972, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.017860969978800104, Recall = 0.9985760056959773, Aging Rate = 0.49928800284798863, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.017417813215276107, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.017841220252486408, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01704038366550714, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Test Loss = 0.016276206579503637, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.0171563356740804, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.016772339198752925, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01728536716429268, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.01698018901594495, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 45: Train Loss = 0.016794902846434652, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Test Loss = 0.015358719587506413, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016808690353379254, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.016830700021615103, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016597289840358246, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.016563521237263436, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016325627939676783, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015157766615320097, Recall = 1.0, Aging Rate = 0.5001779992880029, precision = 0.999644128113879\n",
      "\n",
      "Epoch 51: Train Loss = 0.016602906875574915, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.016479941194723853, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Epoch 53: Train Loss = 0.01628405105285466, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.016185538489179238, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Epoch 55: Train Loss = 0.01597430240780635, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014679239986244895, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016288592337225204, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 57: Train Loss = 0.01607732100683457, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015995374190113477, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0159068798593946, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015825097621861153, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015064581562246068, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015746183692414317, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.016194919142812417, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01574579722239066, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 64: Train Loss = 0.015859093288295885, Recall = 1.0, Aging Rate = 0.5003559985760057, Precision = 0.9992885094272501, f1 = 0.999644128113879\n",
      "Epoch 65: Train Loss = 0.015927940388248248, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014671006531051127, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015978183956394398, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.015848438410322494, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015940357854701884, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015783684497951783, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.016173471328986416, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014478878034878856, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0158225908678525, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.01542262115770067, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.016381612890849242, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015863965720007242, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015710429112394113, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Test Loss = 0.015449648542643316, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.01564881012596962, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.015397687888037494, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.015621823671145578, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.015566939628961864, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 80: Train Loss = 0.01551763700329937, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01433615926480741, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 80.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ed85c2fedf42588d4af764209268fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c997c12dd5435093b4609f900017a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6150493776112059, Recall = 0.545617816091954, Aging Rate = 0.3494815874150876, Precision = 0.7769820971867007, f1 = 0.6410635155096012\n",
      "Epoch 2: Train Loss = 0.419931632516893, Recall = 0.8099856321839081, Aging Rate = 0.471576689309975, Precision = 0.854814253222138, f1 = 0.8317963850977498\n",
      "Epoch 3: Train Loss = 0.3113341314433769, Recall = 0.8588362068965517, Aging Rate = 0.47586700035752594, Precision = 0.8981968444778362, f1 = 0.8780756518545721\n",
      "Epoch 4: Train Loss = 0.24184799737213422, Recall = 0.9105603448275862, Aging Rate = 0.49177690382552736, Precision = 0.9214830970556162, f1 = 0.9159891598915989\n",
      "Epoch 5: Train Loss = 0.19407471388173522, Recall = 0.9342672413793104, Aging Rate = 0.49249195566678583, Precision = 0.9441016333938294, f1 = 0.9391586929048565\n",
      "Test Loss = 0.15592283121828612, Recall = 0.9561781609195402, Aging Rate = 0.487665355738291, precision = 0.9758064516129032\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14230095863629974, Recall = 0.9622844827586207, Aging Rate = 0.49374329638898823, Precision = 0.9699493120926864, f1 = 0.9661016949152542\n",
      "Epoch 7: Train Loss = 0.11226526920044043, Recall = 0.9748563218390804, Aging Rate = 0.49553092599213444, Precision = 0.9790764790764791, f1 = 0.9769618430525557\n",
      "Epoch 8: Train Loss = 0.08979534017089268, Recall = 0.9802442528735632, Aging Rate = 0.4953521630318198, Precision = 0.9848430169613858, f1 = 0.9825382538253825\n",
      "Epoch 9: Train Loss = 0.07294951751695004, Recall = 0.9852729885057471, Aging Rate = 0.4960672148730783, Precision = 0.9884684684684685, f1 = 0.9868681417521137\n",
      "Epoch 10: Train Loss = 0.05971574743634937, Recall = 0.9906609195402298, Aging Rate = 0.49749731855559526, Precision = 0.9910168882500898, f1 = 0.9908388719238368\n",
      "Test Loss = 0.05248833308512499, Recall = 0.9924568965517241, Aging Rate = 0.49588845191276365, precision = 0.9960346070656092\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05009257372816333, Recall = 0.9935344827586207, Aging Rate = 0.49731855559528065, Precision = 0.9942487419122933, f1 = 0.9938914840100611\n",
      "Epoch 12: Train Loss = 0.04205430050380195, Recall = 0.9942528735632183, Aging Rate = 0.4966035037540222, Precision = 0.9964002879769619, f1 = 0.9953254225098885\n",
      "Epoch 13: Train Loss = 0.03600957548704623, Recall = 0.9978448275862069, Aging Rate = 0.49892742223811226, Precision = 0.9953421712647796, f1 = 0.996591928251121\n",
      "Epoch 14: Train Loss = 0.031249083605224336, Recall = 0.9967672413793104, Aging Rate = 0.4976760815159099, Precision = 0.9967672413793104, f1 = 0.9967672413793104\n",
      "Epoch 15: Train Loss = 0.02707466991364551, Recall = 0.9982040229885057, Aging Rate = 0.4978548444762245, Precision = 0.9978456014362657, f1 = 0.9980247800323218\n",
      "Test Loss = 0.024257415186375392, Recall = 0.9996408045977011, Aging Rate = 0.498569896317483, precision = 0.9978486912871997\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.023667186464678826, Recall = 0.9978448275862069, Aging Rate = 0.49731855559528065, Precision = 0.9985621854780733, f1 = 0.998203377650018\n",
      "Epoch 17: Train Loss = 0.02080321542766847, Recall = 0.9992816091954023, Aging Rate = 0.4980336074365391, Precision = 0.9985642498205313, f1 = 0.9989228007181329\n",
      "Epoch 18: Train Loss = 0.01831615488799099, Recall = 0.9996408045977011, Aging Rate = 0.4982123703968538, Precision = 0.9985647649802655, f1 = 0.9991024950637227\n",
      "Epoch 19: Train Loss = 0.01625692380877251, Recall = 1.0, Aging Rate = 0.4983911333571684, Precision = 0.9985652797704447, f1 = 0.9992821249102656\n",
      "Epoch 20: Train Loss = 0.014774540863121391, Recall = 1.0, Aging Rate = 0.4982123703968538, Precision = 0.9989235737351991, f1 = 0.9994614970382337\n",
      "Test Loss = 0.013287571721008027, Recall = 1.0, Aging Rate = 0.4980336074365391, precision = 0.9992821249102656\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.013848968729920204, Recall = 0.9996408045977011, Aging Rate = 0.4980336074365391, Precision = 0.9989231873653984, f1 = 0.9992818671454219\n",
      "Epoch 22: Train Loss = 0.011879764153153598, Recall = 1.0, Aging Rate = 0.4982123703968538, Precision = 0.9989235737351991, f1 = 0.9994614970382337\n",
      "Epoch 23: Train Loss = 0.01071224477066178, Recall = 1.0, Aging Rate = 0.4982123703968538, Precision = 0.9989235737351991, f1 = 0.9994614970382337\n",
      "Epoch 24: Train Loss = 0.0099803608118646, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Epoch 25: Train Loss = 0.00872715030675957, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00803897604742512, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.008206290445627807, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.0075162101259290594, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.00698051016550001, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.006503405721713634, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.005998027412547814, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005779886591951784, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.0056266212332454334, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.005304741189030289, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.004934474062021392, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0047046340930232, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.004370485158243593, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00405200781667932, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.004155976986322748, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.003967182121572908, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.003776709902245774, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0035747566102568725, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.003389368768469096, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0031475191919248277, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.0033000915346738427, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0031296323064526873, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0030120296796381243, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0029908827775546568, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.002918731935757146, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002640755697342304, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.002759410242703575, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.002813600481450851, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0026300361671830965, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.002489324154739838, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.002401904188929619, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002258576728849085, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0024101352290818744, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.002266664449045615, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.002320478228611747, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Train Loss = 0.002261811587826339, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0021385472291340656, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00197433426737796, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.002150842355225271, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0021349600412871876, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.002046993770545051, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.002020084827656874, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.002076327566588201, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018743845061825247, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0019714731393760876, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0018930463637696485, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0019906923528807286, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0019092960476283947, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0019623542915061408, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018141824715631931, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.001914402297596405, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0020132584128857477, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0018551704719526813, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0017819919042481715, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.001766799369613478, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016762014320866072, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0017905849988815701, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0017733405046844253, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0017764770400799973, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0017498785398952453, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.002008915196989007, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016223637537077112, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6647e53f7274410195c597f77a230559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a262c0428d546dd87e390bd8a9882c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.4436366252978694, Recall = 0.982778750729714, Aging Rate = 0.86241180243746, Precision = 0.6260691706954258, f1 = 0.7648796001817356\n",
      "Epoch 2: Train Loss = 0.27867888146443887, Recall = 0.9643899591360187, Aging Rate = 0.6675753688261706, Precision = 0.79365841940908, f1 = 0.8707339570430886\n",
      "Epoch 3: Train Loss = 0.22313814920404923, Recall = 0.9708114419147694, Aging Rate = 0.6372674791533034, Precision = 0.8369401107196779, f1 = 0.898918918918919\n",
      "Epoch 4: Train Loss = 0.18408266093158968, Recall = 0.9690601284296556, Aging Rate = 0.6112892880051315, Precision = 0.8709338929695698, f1 = 0.917380491848577\n",
      "Epoch 5: Train Loss = 0.15350040764744424, Recall = 0.9775248102743724, Aging Rate = 0.6024695317511225, Precision = 0.8914027149321267, f1 = 0.9324794654044272\n",
      "Test Loss = 0.12181550106171081, Recall = 0.9862813776999416, Aging Rate = 0.5920461834509301, precision = 0.9152221018418202\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.11578651134347671, Recall = 0.982778750729714, Aging Rate = 0.5846696600384862, Precision = 0.9234777838727373, f1 = 0.9522058823529412\n",
      "Epoch 7: Train Loss = 0.09187257413664561, Recall = 0.98861646234676, Aging Rate = 0.5763309813983323, Precision = 0.9424040066777963, f1 = 0.9649572649572651\n",
      "Epoch 8: Train Loss = 0.07454132447120775, Recall = 0.9918272037361354, Aging Rate = 0.5676715843489416, Precision = 0.9598870056497175, f1 = 0.9755957507895492\n",
      "Epoch 9: Train Loss = 0.0629823196098696, Recall = 0.993286631640397, Aging Rate = 0.5643040410519564, Precision = 0.9670360897982382, f1 = 0.9799856011519078\n",
      "Epoch 10: Train Loss = 0.05354938426457054, Recall = 0.9944541739638062, Aging Rate = 0.5607761385503528, Precision = 0.9742636545610524, f1 = 0.9842553806153401\n",
      "Test Loss = 0.044039574841836696, Recall = 0.9964973730297724, Aging Rate = 0.5572482360487492, precision = 0.9824460431654676\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.044219450247333626, Recall = 0.99620548744892, Aging Rate = 0.5588518280949326, Precision = 0.9793400286944046, f1 = 0.9877007668933584\n",
      "Epoch 12: Train Loss = 0.038642723906625154, Recall = 0.99620548744892, Aging Rate = 0.5553239255933291, Precision = 0.9855616517470401, f1 = 0.9908549862098999\n",
      "Epoch 13: Train Loss = 0.03272208392696552, Recall = 0.9982486865148862, Aging Rate = 0.5558050032071841, Precision = 0.986728216964801, f1 = 0.9924550203134068\n",
      "Epoch 14: Train Loss = 0.029349454078223553, Recall = 0.997081144191477, Aging Rate = 0.553880692751764, Precision = 0.9889982628836133, f1 = 0.9930232558139536\n",
      "Epoch 15: Train Loss = 0.02536985935133149, Recall = 0.9982486865148862, Aging Rate = 0.5529185375240538, Precision = 0.9918793503480279, f1 = 0.9950538260110563\n",
      "Test Loss = 0.022546562813031246, Recall = 0.9985405720957384, Aging Rate = 0.5508338678640154, precision = 0.995924308588064\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.023171087159435284, Recall = 0.9982486865148862, Aging Rate = 0.5532392559332906, Precision = 0.991304347826087, f1 = 0.9947643979057593\n",
      "Epoch 17: Train Loss = 0.02017913708335471, Recall = 0.9994162288382954, Aging Rate = 0.5521167415009621, Precision = 0.9944815567818762, f1 = 0.996942786431795\n",
      "Epoch 18: Train Loss = 0.018688698284891916, Recall = 0.9994162288382954, Aging Rate = 0.5525978191148172, Precision = 0.9936157864190366, f1 = 0.9965075669383003\n",
      "Epoch 19: Train Loss = 0.016118290798883165, Recall = 0.9997081144191476, Aging Rate = 0.5519563822963438, Precision = 0.9950610110400929, f1 = 0.9973791496796739\n",
      "Epoch 20: Train Loss = 0.014735010234523156, Recall = 1.0, Aging Rate = 0.5516356638871072, Precision = 0.9959302325581395, f1 = 0.997960967084183\n",
      "Test Loss = 0.012807107621038033, Recall = 1.0, Aging Rate = 0.5519563822963438, precision = 0.9953515398024404\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.013842445190678123, Recall = 1.0, Aging Rate = 0.5522771007055804, Precision = 0.9947735191637631, f1 = 0.9973799126637556\n",
      "Epoch 22: Train Loss = 0.012284391368049328, Recall = 0.9997081144191476, Aging Rate = 0.5511545862732521, Precision = 0.9965085830666278, f1 = 0.9981057846422847\n",
      "Epoch 23: Train Loss = 0.011526013841593281, Recall = 0.9997081144191476, Aging Rate = 0.5511545862732521, Precision = 0.9965085830666278, f1 = 0.9981057846422847\n",
      "Epoch 24: Train Loss = 0.011123048991079588, Recall = 1.0, Aging Rate = 0.5517960230917255, Precision = 0.995640802092415, f1 = 0.9978156400174749\n",
      "Epoch 25: Train Loss = 0.009803791758481884, Recall = 0.9997081144191476, Aging Rate = 0.550673508659397, Precision = 0.9973791496796739, f1 = 0.9985422740524781\n",
      "Test Loss = 0.00898032706434587, Recall = 1.0, Aging Rate = 0.5513149454778704, precision = 0.9965095986038395\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.009308338060143516, Recall = 1.0, Aging Rate = 0.550673508659397, Precision = 0.9976703552708212, f1 = 0.9988338192419826\n",
      "Epoch 27: Train Loss = 0.008306479857215386, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Epoch 28: Train Loss = 0.008571289650360487, Recall = 0.9994162288382954, Aging Rate = 0.5503527902501604, Precision = 0.9976689976689976, f1 = 0.9985418489355496\n",
      "Epoch 29: Train Loss = 0.007616431154836401, Recall = 1.0, Aging Rate = 0.550673508659397, Precision = 0.9976703552708212, f1 = 0.9988338192419826\n",
      "Epoch 30: Train Loss = 0.007016113377997629, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Test Loss = 0.005443746345762864, Recall = 1.0, Aging Rate = 0.5498717126363053, precision = 0.9991251093613298\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.006165377429947543, Recall = 1.0, Aging Rate = 0.550192431045542, Precision = 0.9985426989215972, f1 = 0.9992708181420447\n",
      "Epoch 32: Train Loss = 0.005985439226635856, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 33: Train Loss = 0.005553202876836692, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Epoch 34: Train Loss = 0.005614164275206229, Recall = 1.0, Aging Rate = 0.550192431045542, Precision = 0.9985426989215972, f1 = 0.9992708181420447\n",
      "Epoch 35: Train Loss = 0.004873235338989399, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Test Loss = 0.003871143064799171, Recall = 1.0, Aging Rate = 0.549711353431687, precision = 0.9994165694282381\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.00481544641345964, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 37: Train Loss = 0.004962307307212073, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 38: Train Loss = 0.004405575511320551, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 39: Train Loss = 0.00478099740598401, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Epoch 40: Train Loss = 0.00429763548082724, Recall = 0.9997081144191476, Aging Rate = 0.5495509942270687, Precision = 0.9994163991829589, f1 = 0.9995622355172917\n",
      "Test Loss = 0.0032974240301992933, Recall = 1.0, Aging Rate = 0.549711353431687, precision = 0.9994165694282381\n",
      "\n",
      "Epoch 41: Train Loss = 0.0038695166909301355, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 42: Train Loss = 0.003447905580733913, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 43: Train Loss = 0.0035048987203581718, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 44: Train Loss = 0.0039714049673212105, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 45: Train Loss = 0.003756601321454219, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.005776364698804762, Recall = 1.0, Aging Rate = 0.5498717126363053, precision = 0.9991251093613298\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss = 0.0035846087074407865, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 47: Train Loss = 0.003189441886257961, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 48: Train Loss = 0.0033184541710440337, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 49: Train Loss = 0.003383461513506957, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 50: Train Loss = 0.003009949421756223, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.002512192318136514, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.0029094581248960325, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 52: Train Loss = 0.003284580650129442, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 53: Train Loss = 0.0036907000700161195, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Epoch 54: Train Loss = 0.004123611979197288, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Epoch 55: Train Loss = 0.0031178290019321987, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Test Loss = 0.0022441637118342807, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 56: Train Loss = 0.0026388908440702387, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 57: Train Loss = 0.002852380092030557, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 58: Train Loss = 0.002782090678912629, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 59: Train Loss = 0.0025043639520480705, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 60: Train Loss = 0.0029062387029293296, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0021429610853229792, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.00245926123604685, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 62: Train Loss = 0.0035982701096760206, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0030673841224186423, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Epoch 64: Train Loss = 0.00283597960259892, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 65: Train Loss = 0.0028294578445594424, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Test Loss = 0.0024000937888258977, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 66: Train Loss = 0.002809395814389755, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 67: Train Loss = 0.002579775579737271, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 68: Train Loss = 0.0027378414086388176, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 69: Train Loss = 0.0024335014793933434, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 70: Train Loss = 0.002540185197774438, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019886062445860974, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0026911141485202223, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 72: Train Loss = 0.0025773673748294033, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 73: Train Loss = 0.002729738961903449, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 74: Train Loss = 0.002683223936736053, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0025228748444200153, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022442561975722706, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 76: Train Loss = 0.002533345664325669, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 77: Train Loss = 0.002486416343760852, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 78: Train Loss = 0.002929028359584085, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 79: Train Loss = 0.0028537892010074882, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 80: Train Loss = 0.0028327710225651277, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.002426463394498093, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.002539837728132033, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.003351380567516202, Recall = 1.0, Aging Rate = 0.550192431045542, Precision = 0.9985426989215972, f1 = 0.9992708181420447\n",
      "Epoch 83: Train Loss = 0.0035079291544456185, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 84: Train Loss = 0.0022389590399694644, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0022496312500931673, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0017822638365802682, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0020822552309059097, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0027307284737694017, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 88: Train Loss = 0.002521201588704494, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0023791632578664024, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.002279546922174324, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018728369372915368, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.002444635115795655, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.002869684532771008, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 93: Train Loss = 0.002379888333075556, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0036427531373610027, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 95: Train Loss = 0.002530592615430012, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Test Loss = 0.002824065294755937, Recall = 1.0, Aging Rate = 0.5500320718409236, precision = 0.9988338192419826\n",
      "\n",
      "Epoch 96: Train Loss = 0.002861010346046725, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: Train Loss = 0.002574049351997363, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 98: Train Loss = 0.0021393315212752113, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.002096669380585923, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0023101044998273036, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019474737611261744, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86298165c66b4807a2d5fca6b7fbf5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53556be88c5940758456973cfa44067c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5193890139937825, Recall = 0.9064056939501779, Aging Rate = 0.7064056939501779, Precision = 0.6415617128463476, f1 = 0.7513274336283186\n",
      "Epoch 2: Train Loss = 0.3420782950212947, Recall = 0.8857651245551601, Aging Rate = 0.5322064056939502, Precision = 0.8321631561350719, f1 = 0.85812790898121\n",
      "Epoch 3: Train Loss = 0.2604065543816183, Recall = 0.9153024911032028, Aging Rate = 0.5154804270462633, Precision = 0.8878149810148429, f1 = 0.9013492202558261\n",
      "Epoch 4: Train Loss = 0.20663406548232793, Recall = 0.9288256227758007, Aging Rate = 0.504270462633452, Precision = 0.9209597741707833, f1 = 0.92487597448618\n",
      "Epoch 5: Train Loss = 0.1680026232612939, Recall = 0.9530249110320285, Aging Rate = 0.5083629893238434, Precision = 0.9373468673433671, f1 = 0.9451208752426328\n",
      "Test Loss = 0.1327337333050911, Recall = 0.9839857651245552, Aging Rate = 0.5206405693950178, precision = 0.9449760765550239\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.11687476243624907, Recall = 0.9733096085409253, Aging Rate = 0.503202846975089, Precision = 0.9671145685997171, f1 = 0.9702021993614757\n",
      "Epoch 7: Train Loss = 0.09083404976914362, Recall = 0.9839857651245552, Aging Rate = 0.500711743772242, Precision = 0.9825870646766169, f1 = 0.9832859174964438\n",
      "Epoch 8: Train Loss = 0.0718737018940924, Recall = 0.9893238434163701, Aging Rate = 0.500711743772242, Precision = 0.9879175550817342, f1 = 0.9886201991465149\n",
      "Epoch 9: Train Loss = 0.05909480370077374, Recall = 0.9932384341637011, Aging Rate = 0.5008896797153025, Precision = 0.9914742451154529, f1 = 0.9923555555555555\n",
      "Epoch 10: Train Loss = 0.04951227751835598, Recall = 0.99644128113879, Aging Rate = 0.501067615658363, Precision = 0.9943181818181818, f1 = 0.9953785993601137\n",
      "Test Loss = 0.04215485487941958, Recall = 0.998932384341637, Aging Rate = 0.501423487544484, precision = 0.9960965223562811\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04236848790768627, Recall = 0.997508896797153, Aging Rate = 0.5001779359430605, Precision = 0.9971540377090004, f1 = 0.9973314356876002\n",
      "Epoch 12: Train Loss = 0.03717923835715365, Recall = 0.998932384341637, Aging Rate = 0.5005338078291814, Precision = 0.997867045858514, f1 = 0.9983994309087676\n",
      "Epoch 13: Train Loss = 0.033399232503464216, Recall = 0.999288256227758, Aging Rate = 0.5005338078291814, Precision = 0.9982225382154284, f1 = 0.9987551129290414\n",
      "Epoch 14: Train Loss = 0.029964456967081464, Recall = 0.998932384341637, Aging Rate = 0.5001779359430605, Precision = 0.9985770188545002, f1 = 0.9987546699875466\n",
      "Epoch 15: Train Loss = 0.027880232355613724, Recall = 0.999644128113879, Aging Rate = 0.5001779359430605, Precision = 0.9992885094272501, f1 = 0.99946628713752\n",
      "Test Loss = 0.024828414103261517, Recall = 1.0, Aging Rate = 0.5005338078291814, precision = 0.998933522929257\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.02535711008379256, Recall = 0.999644128113879, Aging Rate = 0.5001779359430605, Precision = 0.9992885094272501, f1 = 0.99946628713752\n",
      "Epoch 17: Train Loss = 0.02375601166826126, Recall = 0.999288256227758, Aging Rate = 0.5, Precision = 0.999288256227758, f1 = 0.999288256227758\n",
      "Epoch 18: Train Loss = 0.022837938922473967, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Epoch 19: Train Loss = 0.022514680472742833, Recall = 0.999644128113879, Aging Rate = 0.5001779359430605, Precision = 0.9992885094272501, f1 = 0.99946628713752\n",
      "Epoch 20: Train Loss = 0.020360288714320527, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.018862734968339845, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.020447130950691436, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 22: Train Loss = 0.019666391933250683, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.019365708134882816, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 24: Train Loss = 0.018633086875982557, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 25: Train Loss = 0.01846547553918964, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.01673460835027525, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.01856656764564353, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 27: Train Loss = 0.01776686352064495, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 28: Train Loss = 0.018586008513207113, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.01789995874294819, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.01756142185623111, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015903722780866023, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.01822244366465196, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 32: Train Loss = 0.017191974367959644, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 33: Train Loss = 0.01714988443669051, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.017470951303441964, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 35: Train Loss = 0.017264842243328214, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.01556836790123497, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 36: Train Loss = 0.01702840971856567, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 37: Train Loss = 0.017526825414009364, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 38: Train Loss = 0.016961562084663807, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 39: Train Loss = 0.017527764480150042, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 40: Train Loss = 0.01679185768025944, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Test Loss = 0.016266533108660122, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.017121555355254867, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 42: Train Loss = 0.016889150152347477, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 43: Train Loss = 0.017193540892964793, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 44: Train Loss = 0.01677949185950476, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.01620679009056176, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Test Loss = 0.01578848308144515, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.017096637557940126, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 47: Train Loss = 0.016996599085549444, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 48: Train Loss = 0.01630485793873933, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.01697722148428608, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss = 0.016636715641119303, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.015222022269464685, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.016402021111436585, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.016560841243037975, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016334596476467903, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.01637711721303412, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 55: Train Loss = 0.016241909930767538, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014238134965804335, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016421897344794986, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 57: Train Loss = 0.01658085964858744, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 58: Train Loss = 0.016187285083044466, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 59: Train Loss = 0.016263922692352765, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 60: Train Loss = 0.016180874305210504, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014336465952712447, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 61: Train Loss = 0.01630179553799782, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 62: Train Loss = 0.016278607449974877, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01595823776069697, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.01610402228423391, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 65: Train Loss = 0.01597278120122134, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.014587432356862835, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.016961501333265967, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01599406401669852, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 68: Train Loss = 0.016325000998335375, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 69: Train Loss = 0.016328852333367082, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 70: Train Loss = 0.015704467081302426, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.01426574298415956, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.016759698201647008, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 72: Train Loss = 0.016690072217074577, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 73: Train Loss = 0.015540413790485189, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.016142448719285985, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015884322251501458, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.013840598570269091, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c927d7eda52b44769fa3013f54625e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23735758675432494ae53834100fa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5639265970191073, Recall = 0.6715302491103203, Aging Rate = 0.41565836298932385, Precision = 0.807791095890411, f1 = 0.7333851535172952\n",
      "Epoch 2: Train Loss = 0.327363717874174, Recall = 0.8822064056939501, Aging Rate = 0.5005338078291814, Precision = 0.881265552790615, f1 = 0.8817357282589365\n",
      "Epoch 3: Train Loss = 0.2381631650640447, Recall = 0.9263345195729538, Aging Rate = 0.504270462633452, Precision = 0.9184897671136203, f1 = 0.9223954642097804\n",
      "Epoch 4: Train Loss = 0.1949467635663803, Recall = 0.9355871886120997, Aging Rate = 0.4962633451957295, Precision = 0.942631767658659, f1 = 0.9390962671905698\n",
      "Epoch 5: Train Loss = 0.16138664218878832, Recall = 0.9441281138790035, Aging Rate = 0.49644128113879005, Precision = 0.9508960573476702, f1 = 0.9475\n",
      "Test Loss = 0.13960633325619207, Recall = 0.9555160142348754, Aging Rate = 0.4994661921708185, precision = 0.9565372283576772\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1287746268818387, Recall = 0.9476868327402135, Aging Rate = 0.4875444839857651, Precision = 0.9718978102189781, f1 = 0.9596396396396396\n",
      "Epoch 7: Train Loss = 0.11033682249406902, Recall = 0.9572953736654805, Aging Rate = 0.4898576512455516, Precision = 0.9771158735924446, f1 = 0.967104080532087\n",
      "Epoch 8: Train Loss = 0.09223911289748772, Recall = 0.9686832740213523, Aging Rate = 0.49217081850533806, Precision = 0.9840925524222705, f1 = 0.9763271162123386\n",
      "Epoch 9: Train Loss = 0.0793314596989401, Recall = 0.9736654804270463, Aging Rate = 0.4919928825622776, Precision = 0.989511754068716, f1 = 0.98152466367713\n",
      "Epoch 10: Train Loss = 0.06991909101499358, Recall = 0.9765124555160143, Aging Rate = 0.4925266903914591, Precision = 0.9913294797687862, f1 = 0.9838651846539979\n",
      "Test Loss = 0.06139821616462117, Recall = 0.9825622775800712, Aging Rate = 0.4962633451957295, precision = 0.9899605593402653\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06070394257209479, Recall = 0.9786476868327402, Aging Rate = 0.4928825622775801, Precision = 0.9927797833935018, f1 = 0.985663082437276\n",
      "Epoch 12: Train Loss = 0.05418289676573777, Recall = 0.9814946619217082, Aging Rate = 0.4935943060498221, Precision = 0.9942321557317952, f1 = 0.9878223495702005\n",
      "Epoch 13: Train Loss = 0.0491137643646303, Recall = 0.9857651245551602, Aging Rate = 0.49590747330960855, Precision = 0.9939002511661285, f1 = 0.9898159728425944\n",
      "Epoch 14: Train Loss = 0.044266088444779354, Recall = 0.9864768683274021, Aging Rate = 0.49572953736654807, Precision = 0.9949748743718593, f1 = 0.9907076483202287\n",
      "Epoch 15: Train Loss = 0.040121040400449066, Recall = 0.9893238434163701, Aging Rate = 0.49697508896797155, Precision = 0.9953455066237021, f1 = 0.992325539889345\n",
      "Test Loss = 0.03622778123277787, Recall = 0.996797153024911, Aging Rate = 0.500355871886121, precision = 0.9960881934566145\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.036401998553942105, Recall = 0.9918149466192171, Aging Rate = 0.49697508896797155, Precision = 0.9978517722878625, f1 = 0.994824201320721\n",
      "Epoch 17: Train Loss = 0.035171622949987116, Recall = 0.9911032028469751, Aging Rate = 0.4973309608540925, Precision = 0.9964221824686941, f1 = 0.9937555753791257\n",
      "Epoch 18: Train Loss = 0.031793741124857784, Recall = 0.9928825622775801, Aging Rate = 0.4973309608540925, Precision = 0.998211091234347, f1 = 0.9955396966993756\n",
      "Epoch 19: Train Loss = 0.0296243349189648, Recall = 0.995373665480427, Aging Rate = 0.4983985765124555, Precision = 0.9985719385933595, f1 = 0.9969702370343966\n",
      "Epoch 20: Train Loss = 0.028701781354234738, Recall = 0.994306049822064, Aging Rate = 0.49750889679715304, Precision = 0.9992846924177397, f1 = 0.9967891544773458\n",
      "Test Loss = 0.026725714767742836, Recall = 0.995373665480427, Aging Rate = 0.4976868327402135, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.026256851029989984, Recall = 0.9950177935943061, Aging Rate = 0.497864768683274, Precision = 0.9992852037169406, f1 = 0.9971469329529244\n",
      "Epoch 22: Train Loss = 0.025037966440580917, Recall = 0.995729537366548, Aging Rate = 0.498220640569395, Precision = 0.9992857142857143, f1 = 0.9975044563279857\n",
      "Epoch 23: Train Loss = 0.024407049191592003, Recall = 0.996797153024911, Aging Rate = 0.49857651245551604, Precision = 0.9996431120628123, f1 = 0.9982181040627227\n",
      "Epoch 24: Train Loss = 0.02325199050729385, Recall = 0.996797153024911, Aging Rate = 0.4987544483985765, Precision = 0.9992864787727435, f1 = 0.9980402636736148\n",
      "Epoch 25: Train Loss = 0.02235738952839714, Recall = 0.996797153024911, Aging Rate = 0.49857651245551604, Precision = 0.9996431120628123, f1 = 0.9982181040627227\n",
      "Test Loss = 0.019801722841463268, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.021280038609994688, Recall = 0.997508896797153, Aging Rate = 0.498932384341637, Precision = 0.9996433666191156, f1 = 0.9985749910936943\n",
      "Epoch 27: Train Loss = 0.02050064252523765, Recall = 0.9971530249110321, Aging Rate = 0.498932384341637, Precision = 0.9992867332382311, f1 = 0.998218738867118\n",
      "Epoch 28: Train Loss = 0.020098887234130787, Recall = 0.998220640569395, Aging Rate = 0.499288256227758, Precision = 0.9996436208125445, f1 = 0.9989316239316239\n",
      "Epoch 29: Train Loss = 0.019877779788712167, Recall = 0.998576512455516, Aging Rate = 0.4998220640569395, Precision = 0.9989320042719829, f1 = 0.9987542267307351\n",
      "Epoch 30: Train Loss = 0.019446776929274996, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n",
      "Test Loss = 0.018287924136237317, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.01882666686278009, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.01881546251131122, Recall = 0.998220640569395, Aging Rate = 0.499288256227758, Precision = 0.9996436208125445, f1 = 0.9989316239316239\n",
      "Epoch 33: Train Loss = 0.018131464862950756, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 34: Train Loss = 0.017908742925105782, Recall = 0.999288256227758, Aging Rate = 0.5, Precision = 0.999288256227758, f1 = 0.999288256227758\n",
      "Epoch 35: Train Loss = 0.018045333917401863, Recall = 0.9971530249110321, Aging Rate = 0.4987544483985765, Precision = 0.9996432393863718, f1 = 0.9983965793693212\n",
      "Test Loss = 0.016134172228572632, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.017826291937314744, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.017379861334968504, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.017223211395093554, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 39: Train Loss = 0.01738716051366936, Recall = 0.997864768683274, Aging Rate = 0.498932384341637, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01679030181806919, Recall = 0.998220640569395, Aging Rate = 0.499288256227758, Precision = 0.9996436208125445, f1 = 0.9989316239316239\n",
      "Test Loss = 0.015025744682635277, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.016617125616776772, Recall = 0.999644128113879, Aging Rate = 0.500355871886121, Precision = 0.9989331436699858, f1 = 0.9992885094272501\n",
      "Epoch 42: Train Loss = 0.016655473362031357, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.017120368381650635, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.016515648452518673, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016248662622638868, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Test Loss = 0.01512722477144512, Recall = 1.0, Aging Rate = 0.500355871886121, precision = 0.9992887624466572\n",
      "\n",
      "Epoch 46: Train Loss = 0.015703953361781677, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.016023011500300884, Recall = 0.999644128113879, Aging Rate = 0.5001779359430605, Precision = 0.9992885094272501, f1 = 0.99946628713752\n",
      "Epoch 48: Train Loss = 0.016645782470066775, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n",
      "Epoch 49: Train Loss = 0.01558106348806747, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016055914348660837, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.014210709453530583, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.015428734129274866, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.015664649044836332, Recall = 0.999288256227758, Aging Rate = 0.5, Precision = 0.999288256227758, f1 = 0.999288256227758\n",
      "Epoch 53: Train Loss = 0.015669929082389403, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015621421365498224, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 55: Train Loss = 0.01586687776700882, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014362082719219536, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 56: Train Loss = 0.015833906475388283, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 57: Train Loss = 0.015546704128755794, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Epoch 58: Train Loss = 0.015590224647065923, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Epoch 59: Train Loss = 0.016433926173270386, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n",
      "Epoch 60: Train Loss = 0.015387219463697107, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.013821630537828093, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015281044577788628, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 62: Train Loss = 0.015692528406177974, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 63: Train Loss = 0.014983941751177624, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.01524650616790052, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015120331209245738, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013832237617878303, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015257552942904077, Recall = 0.999644128113879, Aging Rate = 0.5001779359430605, Precision = 0.9992885094272501, f1 = 0.99946628713752\n",
      "Epoch 67: Train Loss = 0.015314987671704054, Recall = 0.998932384341637, Aging Rate = 0.4998220640569395, Precision = 0.9992880028479886, f1 = 0.9991101619505249\n",
      "Epoch 68: Train Loss = 0.015386600364198463, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015976013173266343, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 70: Train Loss = 0.0157272817905051, Recall = 0.998932384341637, Aging Rate = 0.4998220640569395, Precision = 0.9992880028479886, f1 = 0.9991101619505249\n",
      "Test Loss = 0.01591129706794681, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.014943566971402152, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 72: Train Loss = 0.015375988811148444, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 73: Train Loss = 0.014786204341732735, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 74: Train Loss = 0.015025717811493262, Recall = 0.999644128113879, Aging Rate = 0.5001779359430605, Precision = 0.9992885094272501, f1 = 0.99946628713752\n",
      "Epoch 75: Train Loss = 0.015371789856154299, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013421119937799155, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.014930359523385445, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.015100956531871256, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 78: Train Loss = 0.01530222890507496, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.015139488931546432, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Epoch 80: Train Loss = 0.014826318554686269, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01434096156742119, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.01493039349312778, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014714754629363157, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.014914677892662452, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 84: Train Loss = 0.014793146829190416, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 85: Train Loss = 0.014833955833973409, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.013493653304636267, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.015050794467250436, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Epoch 87: Train Loss = 0.015361431921132943, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.014963371395481439, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.01460154992909406, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 90: Train Loss = 0.014909433617233382, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013965402550836262, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Training Finished at epoch 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98185970556146fa8b18d0d02a67c5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acddc8ba909243d080b4515f81100ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5332539150029161, Recall = 0.010676156583629894, Aging Rate = 0.0061468780329990294, Precision = 0.15789473684210525, f1 = 0.02\n",
      "Epoch 2: Train Loss = 0.45182452819197744, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.41870886052707684, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.377284329171244, Recall = 0.0498220640569395, Aging Rate = 0.005499838240051763, Precision = 0.8235294117647058, f1 = 0.09395973154362415\n",
      "Epoch 5: Train Loss = 0.335111475379403, Recall = 0.2597864768683274, Aging Rate = 0.025558071821417016, Precision = 0.9240506329113924, f1 = 0.40555555555555556\n",
      "Test Loss = 0.30760432375321994, Recall = 0.37722419928825623, Aging Rate = 0.03849886768036234, precision = 0.8907563025210085\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.29559454371033655, Recall = 0.3879003558718861, Aging Rate = 0.03979294726625687, Precision = 0.8861788617886179, f1 = 0.5396039603960396\n",
      "Epoch 7: Train Loss = 0.26083488129163707, Recall = 0.5302491103202847, Aging Rate = 0.055321902296991264, Precision = 0.8713450292397661, f1 = 0.65929203539823\n",
      "Epoch 8: Train Loss = 0.23532761844548158, Recall = 0.6120996441281139, Aging Rate = 0.06373341960530572, Precision = 0.8730964467005076, f1 = 0.7196652719665273\n",
      "Epoch 9: Train Loss = 0.21234672993604745, Recall = 0.6512455516014235, Aging Rate = 0.06890973794888386, Precision = 0.8591549295774648, f1 = 0.7408906882591093\n",
      "Epoch 10: Train Loss = 0.19067762870930885, Recall = 0.7046263345195729, Aging Rate = 0.07279197670656745, Precision = 0.88, f1 = 0.7826086956521738\n",
      "Test Loss = 0.17751393207104527, Recall = 0.7508896797153025, Aging Rate = 0.07893885473956648, precision = 0.8647540983606558\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.1728035004557008, Recall = 0.7188612099644128, Aging Rate = 0.07408605629246198, Precision = 0.8820960698689956, f1 = 0.792156862745098\n",
      "Epoch 12: Train Loss = 0.15779676810220314, Recall = 0.800711743772242, Aging Rate = 0.08185053380782918, Precision = 0.8893280632411067, f1 = 0.8426966292134831\n",
      "Epoch 13: Train Loss = 0.14413497656516294, Recall = 0.8185053380782918, Aging Rate = 0.08443869297961824, Precision = 0.8812260536398467, f1 = 0.8487084870848708\n",
      "Epoch 14: Train Loss = 0.13224055590327988, Recall = 0.8256227758007118, Aging Rate = 0.08087997411840828, Precision = 0.928, f1 = 0.8738229755178908\n",
      "Epoch 15: Train Loss = 0.12065435923329837, Recall = 0.8576512455516014, Aging Rate = 0.08249757360077645, Precision = 0.9450980392156862, f1 = 0.8992537313432836\n",
      "Test Loss = 0.11172253307194935, Recall = 0.8683274021352313, Aging Rate = 0.08346813329019735, precision = 0.9457364341085271\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.11053556444401187, Recall = 0.8754448398576512, Aging Rate = 0.08314461339372371, Precision = 0.9571984435797666, f1 = 0.9144981412639405\n",
      "Epoch 17: Train Loss = 0.10149889595539043, Recall = 0.8932384341637011, Aging Rate = 0.08443869297961824, Precision = 0.9616858237547893, f1 = 0.9261992619926199\n",
      "Epoch 18: Train Loss = 0.09379343446964818, Recall = 0.900355871886121, Aging Rate = 0.08508573277256551, Precision = 0.9619771863117871, f1 = 0.9301470588235293\n",
      "Epoch 19: Train Loss = 0.0851124632815565, Recall = 0.9288256227758007, Aging Rate = 0.08670333225493368, Precision = 0.9738805970149254, f1 = 0.9508196721311475\n",
      "Epoch 20: Train Loss = 0.0786454175372642, Recall = 0.9288256227758007, Aging Rate = 0.08670333225493368, Precision = 0.9738805970149254, f1 = 0.9508196721311475\n",
      "Test Loss = 0.07320270855949944, Recall = 0.9430604982206405, Aging Rate = 0.08832093173730185, precision = 0.9706959706959707\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.07286762062668685, Recall = 0.9430604982206405, Aging Rate = 0.08735037204788094, Precision = 0.9814814814814815, f1 = 0.9618874773139745\n",
      "Epoch 22: Train Loss = 0.0672229109473577, Recall = 0.9430604982206405, Aging Rate = 0.08735037204788094, Precision = 0.9814814814814815, f1 = 0.9618874773139745\n",
      "Epoch 23: Train Loss = 0.06255749847579642, Recall = 0.9644128113879004, Aging Rate = 0.08993853121967, Precision = 0.9748201438848921, f1 = 0.9695885509838998\n",
      "Epoch 24: Train Loss = 0.05874054794344691, Recall = 0.9466192170818505, Aging Rate = 0.08799741184082821, Precision = 0.9779411764705882, f1 = 0.9620253164556961\n",
      "Epoch 25: Train Loss = 0.05297986909009538, Recall = 0.9572953736654805, Aging Rate = 0.08832093173730185, Precision = 0.9853479853479854, f1 = 0.9711191335740071\n",
      "Test Loss = 0.05116353118060014, Recall = 0.9750889679715302, Aging Rate = 0.09090909090909091, precision = 0.9750889679715302\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.05065181139351596, Recall = 0.9679715302491103, Aging Rate = 0.09026205111614365, Precision = 0.974910394265233, f1 = 0.9714285714285714\n",
      "Epoch 27: Train Loss = 0.04602539388927508, Recall = 0.9715302491103203, Aging Rate = 0.08929149142672274, Precision = 0.9891304347826086, f1 = 0.9802513464991024\n",
      "Epoch 28: Train Loss = 0.04333077519722295, Recall = 0.9715302491103203, Aging Rate = 0.08929149142672274, Precision = 0.9891304347826086, f1 = 0.9802513464991024\n",
      "Epoch 29: Train Loss = 0.039260127900734146, Recall = 0.9822064056939501, Aging Rate = 0.09026205111614365, Precision = 0.989247311827957, f1 = 0.9857142857142857\n",
      "Epoch 30: Train Loss = 0.03760999747028015, Recall = 0.9822064056939501, Aging Rate = 0.08993853121967, Precision = 0.9928057553956835, f1 = 0.9874776386404293\n",
      "Test Loss = 0.03374689965307308, Recall = 0.9857651245551602, Aging Rate = 0.09058557101261727, precision = 0.9892857142857143\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.03429374043458837, Recall = 0.9822064056939501, Aging Rate = 0.09026205111614365, Precision = 0.989247311827957, f1 = 0.9857142857142857\n",
      "Epoch 32: Train Loss = 0.03219121958474701, Recall = 0.9822064056939501, Aging Rate = 0.09026205111614365, Precision = 0.989247311827957, f1 = 0.9857142857142857\n",
      "Epoch 33: Train Loss = 0.02990942972280454, Recall = 0.9857651245551602, Aging Rate = 0.09058557101261727, Precision = 0.9892857142857143, f1 = 0.9875222816399287\n",
      "Epoch 34: Train Loss = 0.028100776564135826, Recall = 0.9893238434163701, Aging Rate = 0.09058557101261727, Precision = 0.9928571428571429, f1 = 0.9910873440285205\n",
      "Epoch 35: Train Loss = 0.02713829879936726, Recall = 0.9928825622775801, Aging Rate = 0.09123261080556454, Precision = 0.9893617021276596, f1 = 0.9911190053285969\n",
      "Test Loss = 0.024327889973655058, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, precision = 0.9928825622775801\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.024428900070331775, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 37: Train Loss = 0.023167891310367027, Recall = 0.9928825622775801, Aging Rate = 0.09123261080556454, Precision = 0.9893617021276596, f1 = 0.9911190053285969\n",
      "Epoch 38: Train Loss = 0.02174495813017696, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 39: Train Loss = 0.020600878427769906, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 40: Train Loss = 0.019608389748310393, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Test Loss = 0.018503186240218596, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, precision = 0.9928825622775801\n",
      "\n",
      "Epoch 41: Train Loss = 0.018343491723055314, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 42: Train Loss = 0.017528897905508337, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 43: Train Loss = 0.01667753067554557, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 44: Train Loss = 0.015777573433280744, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 45: Train Loss = 0.015324069341612706, Recall = 0.99644128113879, Aging Rate = 0.09123261080556454, Precision = 0.9929078014184397, f1 = 0.9946714031971581\n",
      "Test Loss = 0.013912615346243537, Recall = 1.0, Aging Rate = 0.09155613070203818, precision = 0.9929328621908127\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.014105564491917924, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.013636074985842399, Recall = 0.99644128113879, Aging Rate = 0.09123261080556454, Precision = 0.9929078014184397, f1 = 0.9946714031971581\n",
      "Epoch 48: Train Loss = 0.012809041049903899, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 49: Train Loss = 0.01319719569480986, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 50: Train Loss = 0.01196574220139761, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.011047204497229008, Recall = 1.0, Aging Rate = 0.09123261080556454, precision = 0.9964539007092199\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.011126314896839514, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 52: Train Loss = 0.010497029511747922, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 53: Train Loss = 0.010208616895370356, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 54: Train Loss = 0.010017000122037387, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 55: Train Loss = 0.00941364856395269, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.008776711541056555, Recall = 1.0, Aging Rate = 0.09123261080556454, precision = 0.9964539007092199\n",
      "\n",
      "Epoch 56: Train Loss = 0.009262009390935046, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 57: Train Loss = 0.00872984738608889, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 58: Train Loss = 0.008473830047810455, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 59: Train Loss = 0.007989592779854409, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 60: Train Loss = 0.00774841775653829, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.007228501968746219, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.0074030666767861495, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.007359520103589583, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 63: Train Loss = 0.006939234853888172, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0066047507048675215, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.006471238836925687, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006082688192365403, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.006198456502803976, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.006213331721308489, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.005924153196664882, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0056702095406736775, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.005617859250730593, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.0054414441156138095, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.005439709235432873, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.005138703781312355, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.005156922013270155, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.004899127979978885, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.004725942744315152, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00449732736847386, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0046376968708054436, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.004476408023927728, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.004316299297810396, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.004310110442139105, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.004264193460157256, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004013249588228186, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0040993283362501754, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.00395561150002945, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0038507702055805104, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0037461233319879177, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.003713405083042224, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003506637074751403, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0036219548792869034, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.00355578118394994, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0035004832161773048, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0033670860821449307, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.003381394278450918, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003154620523946417, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.003253464682018056, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.0031731250406333597, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.003104412948448465, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.003085407443113043, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.0030629318625558033, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029486789104755605, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.002938560202748815, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0029048035177394664, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.003314212407369621, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 99: Train Loss = 0.0027940901264033677, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0027675591898821137, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002609870918785092, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Labels of  10 datasets are divided.\n"
     ]
    }
   ],
   "source": [
    "train_firstC = transform_train(data_dict, num_set = 10, mode = 'C', base_param = base_param_monthC, cv = 5)\n",
    "test_firstC = transform_test(run_train, run_test, num_set = 10, mode = 'C', base_param = dict(all = base_param_allC))\n",
    "train_firstC_x, train_firstC_y = train_set(train_firstC, num_set = 10)\n",
    "test_firstC_x, test_firstC_y = train_set(test_firstC, num_set = 10) \n",
    "\n",
    "# train_firstR = transform_train(data_dict, num_set = 10, mode = 'R', base_param = base_param_monthR, cv = 5)\n",
    "# test_firstR = transform_test(run_train, run_test, num_set = 10, mode = 'R', base_param = dict(all = base_param_allR))\n",
    "# train_firstR_x, train_firstR_y = train_set(train_firstR, num_set = 10)\n",
    "# test_firstR_x, test_firstR_y = train_set(test_firstR, num_set = 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### searching for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:46:39.460717Z",
     "start_time": "2021-12-05T07:39:26.030905Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0d564a5379485f8dcf513e2ec641e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:39:26,053]\u001b[0m A new study created in memory with name: no-name-32f899fa-c105-47c8-9e0a-074a76ac4d0f\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dd8014b4f64b3f9e6692ed172ef861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9491525423728814 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.00150173080838933\n",
      "Precision: 0.9814814814814815 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "Precision: 0.9444444444444444 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "\u001b[32m[I 2021-12-05 15:39:27,996]\u001b[0m Trial 0 finished with value: 2.6678926877860594 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 0 with value: 2.6678926877860594.\u001b[0m\n",
      "Precision: 0.9814814814814815 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "Precision: 0.9122807017543859 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9344262295081968 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0015526369374872734\n",
      "\u001b[32m[I 2021-12-05 15:39:30,180]\u001b[0m Trial 1 finished with value: 2.646022322111066 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 0 with value: 2.6678926877860594.\u001b[0m\n",
      "Precision: 0.9482758620689655 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.9523809523809523 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.001603543066585217\n",
      "Precision: 0.9629629629629629 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "\u001b[32m[I 2021-12-05 15:39:34,836]\u001b[0m Trial 2 finished with value: 2.693117410294033 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 2 with value: 2.693117410294033.\u001b[0m\n",
      "Precision: 0.9591836734693877 \n",
      "Recall: 0.6619718309859155 \n",
      "Aging Rate: 0.001247200162899613\n",
      "Precision: 0.9803921568627451 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0012981062919975566\n",
      "Precision: 0.9818181818181818 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "\u001b[32m[I 2021-12-05 15:39:39,039]\u001b[0m Trial 3 finished with value: 2.6565161958936367 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 2 with value: 2.693117410294033.\u001b[0m\n",
      "Precision: 0.9824561403508771 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9375 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.0016289961311341885\n",
      "Precision: 0.9622641509433962 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013490124210955\n",
      "\u001b[32m[I 2021-12-05 15:39:43,360]\u001b[0m Trial 4 finished with value: 2.705517752881628 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 12}. Best is trial 4 with value: 2.705517752881628.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9285714285714286 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014253716147424149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8947368421052632 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0014508246792913867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9285714285714286 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "\u001b[32m[I 2021-12-05 15:39:47,155]\u001b[0m Trial 5 finished with value: 2.5622859966818456 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 4 with value: 2.705517752881628.\u001b[0m\n",
      "Precision: 0.9818181818181818 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 0.9333333333333333 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "Precision: 0.9655172413793104 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "\u001b[32m[I 2021-12-05 15:39:50,809]\u001b[0m Trial 6 finished with value: 2.699788560691912 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 4 with value: 2.705517752881628.\u001b[0m\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.0015780900020362452\n",
      "Precision: 0.9375 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.0016289961311341885\n",
      "Precision: 0.9516129032258065 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.0015780900020362452\n",
      "\u001b[32m[I 2021-12-05 15:39:52,737]\u001b[0m Trial 7 finished with value: 2.7449454793275785 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9473684210526315 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 1.0 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0012726532274485848\n",
      "Precision: 0.9482758620689655 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "\u001b[32m[I 2021-12-05 15:39:56,754]\u001b[0m Trial 8 finished with value: 2.6769083953205013 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n",
      "\u001b[32m[I 2021-12-05 15:40:00,159]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9107142857142857 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9622641509433962 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013490124210955\n",
      "Precision: 0.9811320754716981 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013490124210955\n",
      "\u001b[32m[I 2021-12-05 15:40:04,707]\u001b[0m Trial 10 finished with value: 2.6257450362552675 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.8571428571428571 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.001603543066585217\n",
      "Precision: 0.9482758620689655 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.9655172413793104 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "\u001b[32m[I 2021-12-05 15:40:09,709]\u001b[0m Trial 11 finished with value: 2.6219385277180325 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "Precision: 0.9444444444444444 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "Precision: 0.9818181818181818 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "\u001b[32m[I 2021-12-05 15:40:15,214]\u001b[0m Trial 12 finished with value: 2.7096836913738316 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9642857142857143 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9464285714285714 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9523809523809523 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.001603543066585217\n",
      "\u001b[32m[I 2021-12-05 15:40:20,765]\u001b[0m Trial 13 finished with value: 2.6927677174156046 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9821428571428571 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9655172413793104 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.95 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "\u001b[32m[I 2021-12-05 15:40:22,890]\u001b[0m Trial 14 finished with value: 2.7205057933809758 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 6}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9642857142857143 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.0015780900020362452\n",
      "Precision: 0.9649122807017544 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "\u001b[32m[I 2021-12-05 15:40:24,765]\u001b[0m Trial 15 finished with value: 2.724720517027842 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9622641509433962 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013490124210955\n",
      "Precision: 0.9649122807017544 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9272727272727272 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "\u001b[32m[I 2021-12-05 15:40:26,643]\u001b[0m Trial 16 finished with value: 2.640055307823186 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 3}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9454545454545454 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 0.95 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "Precision: 0.9615384615384616 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "\u001b[32m[I 2021-12-05 15:40:29,072]\u001b[0m Trial 17 finished with value: 2.6511408779014416 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 18}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9814814814814815 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0013744654856444715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0014508246792913867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.90625 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.0016289961311341885\n",
      "\u001b[32m[I 2021-12-05 15:40:32,767]\u001b[0m Trial 18 finished with value: 2.7138867153538513 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9454545454545454 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 1.0 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9811320754716981 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013490124210955\n",
      "\u001b[32m[I 2021-12-05 15:40:34,486]\u001b[0m Trial 19 finished with value: 2.70223145620435 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 7 with value: 2.7449454793275785.\u001b[0m\n",
      "Precision: 0.9824561403508771 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 1.0 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.9482758620689655 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "\u001b[32m[I 2021-12-05 15:40:36,524]\u001b[0m Trial 20 finished with value: 2.74724856499351 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 20 with value: 2.74724856499351.\u001b[0m\n",
      "Precision: 0.9636363636363636 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 1.0 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "Precision: 0.9333333333333333 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "\u001b[32m[I 2021-12-05 15:40:38,565]\u001b[0m Trial 21 finished with value: 2.6871816759140703 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 20 with value: 2.74724856499351.\u001b[0m\n",
      "Precision: 0.9636363636363636 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 0.9824561403508771 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9137931034482759 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "\u001b[32m[I 2021-12-05 15:40:40,592]\u001b[0m Trial 22 finished with value: 2.667153785238701 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 20 with value: 2.74724856499351.\u001b[0m\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "Precision: 0.9818181818181818 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "\u001b[32m[I 2021-12-05 15:40:42,623]\u001b[0m Trial 23 finished with value: 2.7731683027457676 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 23 with value: 2.7731683027457676.\u001b[0m\n",
      "Precision: 0.9122807017543859 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9285714285714286 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014253716147424149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:40:44,837]\u001b[0m A new study created in memory with name: no-name-ac3b9279-882e-4a5c-9072-28426d2cf484\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9818181818181818 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "\u001b[32m[I 2021-12-05 15:40:44,706]\u001b[0m Trial 24 finished with value: 2.623564245654683 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 23 with value: 2.7731683027457676.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4009a69b88704014a723d0bd08b98810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 1.0 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "\u001b[32m[I 2021-12-05 15:40:46,063]\u001b[0m Trial 0 finished with value: 2.992260694912352 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9956709956709957 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5021739130434782\n",
      "Precision: 0.9956521739130435 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:40:47,360]\u001b[0m Trial 1 finished with value: 2.9893817239182296 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9942279942279942 \n",
      "Recall: 0.9985507246376811 \n",
      "Aging Rate: 0.5021739130434782\n",
      "\u001b[32m[I 2021-12-05 15:40:49,317]\u001b[0m Trial 2 finished with value: 2.9884112764831285 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9985401459854014 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 15:40:51,747]\u001b[0m Trial 3 finished with value: 2.9898395940093665 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 15:40:53,013]\u001b[0m Trial 4 finished with value: 2.989846658673566 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 15:40:55,536]\u001b[0m Trial 5 finished with value: 2.989353693584659 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9927849927849928 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5021739130434782\n",
      "Precision: 0.9942112879884226 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.5007246376811594\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5014492753623189\n",
      "\u001b[32m[I 2021-12-05 15:40:57,952]\u001b[0m Trial 6 finished with value: 2.9869886126381426 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9942196531791907 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5014492753623189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 15:40:59,234]\u001b[0m Trial 7 finished with value: 2.9888972540444656 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.998546511627907 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 15:41:01,692]\u001b[0m Trial 8 finished with value: 2.9898410290978545 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9956458635703919 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 15:41:01,871]\u001b[0m Trial 9 finished with value: 2.98839598101288 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9941944847605225 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9942028985507246 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-05 15:41:02,071]\u001b[0m Trial 10 finished with value: 2.984046744474659 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 1.0 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9956458635703919 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 15:41:03,366]\u001b[0m Trial 11 finished with value: 2.9912987386326186 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9985507246376811 \n",
      "Aging Rate: 0.5007246376811594\n",
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.9970887918486172 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.49782608695652175\n",
      "\u001b[32m[I 2021-12-05 15:41:04,676]\u001b[0m Trial 12 finished with value: 2.990328303303695 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9941944847605225 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:41:06,774]\u001b[0m Trial 13 finished with value: 2.987428370609633 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9985507246376811 \n",
      "Recall: 0.9985507246376811 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 15:41:08,876]\u001b[0m Trial 14 finished with value: 2.991295925882987 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 9}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9985507246376811 \n",
      "Aging Rate: 0.5007246376811594\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:41:10,140]\u001b[0m Trial 15 finished with value: 2.9917902360927595 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985401459854014 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9942196531791907 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5014492753623189\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 15:41:11,995]\u001b[0m Trial 16 finished with value: 2.9864662798222112 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 15:41:13,289]\u001b[0m Trial 17 finished with value: 2.9893607132293916 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 15:41:13,484]\u001b[0m Trial 18 finished with value: 2.9903325550577393 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 1.0 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 15:41:14,776]\u001b[0m Trial 19 finished with value: 2.991295925882987 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 1.0 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4956521739130435\n",
      "\u001b[32m[I 2021-12-05 15:41:16,647]\u001b[0m Trial 20 finished with value: 2.9888762437270917 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9970760233918129 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4956521739130435\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 15:41:17,945]\u001b[0m Trial 21 finished with value: 2.9883860419156822 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9956268221574344 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5014492753623189\n",
      "Precision: 0.9985380116959064 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4956521739130435\n",
      "\u001b[32m[I 2021-12-05 15:41:19,206]\u001b[0m Trial 22 finished with value: 2.9874198219378028 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 0 with value: 2.992260694912352.\u001b[0m\n",
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.998546511627907 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9985507246376811 \n",
      "Recall: 0.9985507246376811 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:41:20,470]\u001b[0m Trial 23 finished with value: 2.993229687160414 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 12}. Best is trial 23 with value: 2.993229687160414.\u001b[0m\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5014492753623189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:41:21,935]\u001b[0m A new study created in memory with name: no-name-bb93bb1d-fabd-4bfa-aa44-0520e845d02e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "\u001b[32m[I 2021-12-05 15:41:21,801]\u001b[0m Trial 24 finished with value: 2.992271897134873 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 23 with value: 2.993229687160414.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a31a8ebbe0048acaadecef21780e898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9915611814345991 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5021186440677966\n",
      "Precision: 0.9936440677966102 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9957446808510638 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4978813559322034\n",
      "\u001b[32m[I 2021-12-05 15:41:24,634]\u001b[0m Trial 0 finished with value: 2.980944021184792 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 0 with value: 2.980944021184792.\u001b[0m\n",
      "Precision: 0.996822033898305 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9905163329820864 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4978813559322034\n",
      "\u001b[32m[I 2021-12-05 15:41:24,845]\u001b[0m Trial 1 finished with value: 2.98236309444869 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 1 with value: 2.98236309444869.\u001b[0m\n",
      "Precision: 0.9915433403805497 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9957356076759062 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.4968220338983051\n",
      "Precision: 0.9935965848452508 \n",
      "Recall: 0.986228813559322 \n",
      "Aging Rate: 0.4962923728813559\n",
      "\u001b[32m[I 2021-12-05 15:41:25,045]\u001b[0m Trial 2 finished with value: 2.9770102422734546 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 1 with value: 2.98236309444869.\u001b[0m\n",
      "Precision: 0.9925768822905621 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9947089947089947 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9915522703273495 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5015889830508474\n",
      "\u001b[32m[I 2021-12-05 15:41:25,242]\u001b[0m Trial 3 finished with value: 2.9798892733591806 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 1 with value: 2.98236309444869.\u001b[0m\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9915164369034994 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9946977730646872 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4994703389830508\n",
      "\u001b[32m[I 2021-12-05 15:41:25,437]\u001b[0m Trial 4 finished with value: 2.9770382868714464 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 1 with value: 2.98236309444869.\u001b[0m\n",
      "Precision: 0.9915700737618546 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9957537154989384 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4989406779661017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9936507936507937 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5005296610169492\n",
      "\u001b[32m[I 2021-12-05 15:41:25,639]\u001b[0m Trial 5 finished with value: 2.9823728857828655 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 5 with value: 2.9823728857828655.\u001b[0m\n",
      "Precision: 0.9936575052854123 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9925768822905621 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9915611814345991 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5021186440677966\n",
      "\u001b[32m[I 2021-12-05 15:41:26,894]\u001b[0m Trial 6 finished with value: 2.979547328492925 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 5 with value: 2.9823728857828655.\u001b[0m\n",
      "Precision: 0.9936373276776246 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9989339019189766 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4968220338983051\n",
      "Precision: 0.9925768822905621 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4994703389830508\n",
      "\u001b[32m[I 2021-12-05 15:41:28,160]\u001b[0m Trial 7 finished with value: 2.982330379676188 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 12}. Best is trial 5 with value: 2.9823728857828655.\u001b[0m\n",
      "Precision: 0.9936575052854123 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9926238145416227 \n",
      "Recall: 0.9978813559322034 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9936642027455121 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5015889830508474\n",
      "\u001b[32m[I 2021-12-05 15:41:30,822]\u001b[0m Trial 8 finished with value: 2.9834523822800034 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 6}. Best is trial 8 with value: 2.9834523822800034.\u001b[0m\n",
      "Precision: 0.9915433403805497 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9957582184517497 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9904761904761905 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.5005296610169492\n",
      "\u001b[32m[I 2021-12-05 15:41:31,027]\u001b[0m Trial 9 finished with value: 2.9784761266576374 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 8 with value: 2.9834523822800034.\u001b[0m\n",
      "Precision: 0.9915164369034994 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9947033898305084 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9968186638388123 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.4994703389830508\n",
      "\u001b[32m[I 2021-12-05 15:41:33,627]\u001b[0m Trial 10 finished with value: 2.982336394845157 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 8 with value: 2.9834523822800034.\u001b[0m\n",
      "Precision: 0.9968253968253968 \n",
      "Recall: 0.9978813559322034 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9936507936507937 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9989350372736954 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4973516949152542\n",
      "\u001b[32m[I 2021-12-05 15:41:36,349]\u001b[0m Trial 11 finished with value: 2.988350423019698 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.996822033898305 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9968051118210862 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4973516949152542\n",
      "Precision: 0.9936440677966102 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:41:39,231]\u001b[0m Trial 12 finished with value: 2.985511317485244 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9936642027455121 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9957582184517497 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9904862579281184 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5010593220338984\n",
      "\u001b[32m[I 2021-12-05 15:41:41,339]\u001b[0m Trial 13 finished with value: 2.9813091759140953 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9936575052854123 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9957310565635006 \n",
      "Recall: 0.9883474576271186 \n",
      "Aging Rate: 0.4962923728813559\n",
      "Precision: 0.9925373134328358 \n",
      "Recall: 0.986228813559322 \n",
      "Aging Rate: 0.4968220338983051\n",
      "\u001b[32m[I 2021-12-05 15:41:43,406]\u001b[0m Trial 14 finished with value: 2.9780635778714486 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9957671957671957 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9947033898305084 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9946865037194474 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4984110169491525\n",
      "\u001b[32m[I 2021-12-05 15:41:46,165]\u001b[0m Trial 15 finished with value: 2.98445500869731 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 18}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9947033898305084 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9936642027455121 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9968017057569296 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4968220338983051\n",
      "\u001b[32m[I 2021-12-05 15:41:48,038]\u001b[0m Trial 16 finished with value: 2.984110040696543 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 21}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9936642027455121 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9915074309978769 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9968186638388123 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.4994703389830508\n",
      "\u001b[32m[I 2021-12-05 15:41:50,740]\u001b[0m Trial 17 finished with value: 2.9819907068627103 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 15}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9915343915343915 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.997874601487779 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.4984110169491525\n",
      "Precision: 0.9947033898305084 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:41:53,458]\u001b[0m Trial 18 finished with value: 2.983405430376362 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 18}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9915522703273495 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9936440677966102 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9936440677966102 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:41:55,305]\u001b[0m Trial 19 finished with value: 2.9798907790882896 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.992600422832981 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 1.0 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4962923728813559\n",
      "Precision: 0.9894736842105263 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.503177966101695\n",
      "\u001b[32m[I 2021-12-05 15:41:58,069]\u001b[0m Trial 20 finished with value: 2.982399687181547 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 21}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9915522703273495 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9957537154989384 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9936305732484076 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4989406779661017\n",
      "\u001b[32m[I 2021-12-05 15:42:00,750]\u001b[0m Trial 21 finished with value: 2.9805819998351075 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 18}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9947201689545935 \n",
      "Recall: 0.9978813559322034 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9936373276776246 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9936373276776246 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4994703389830508\n",
      "\u001b[32m[I 2021-12-05 15:42:03,483]\u001b[0m Trial 22 finished with value: 2.9823468320257707 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 18}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9957537154989384 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9936507936507937 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9947145877378436 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5010593220338984\n",
      "\u001b[32m[I 2021-12-05 15:42:06,187]\u001b[0m Trial 23 finished with value: 2.9844692284335252 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 15}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Precision: 0.9936642027455121 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.992600422832981 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5010593220338984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:42:09,035]\u001b[0m A new study created in memory with name: no-name-60f3b3bb-7770-4b5b-8758-9478ff50de1b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9925925925925926 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5005296610169492\n",
      "\u001b[32m[I 2021-12-05 15:42:08,905]\u001b[0m Trial 24 finished with value: 2.980961309289198 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 11 with value: 2.988350423019698.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset3 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465493d89d514979b7ff640e7d9f93f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9859335038363172 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5071335927367056\n",
      "Precision: 0.9935567010309279 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "\u001b[32m[I 2021-12-05 15:42:11,361]\u001b[0m Trial 0 finished with value: 2.9811787980968254 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 0 with value: 2.9811787980968254.\u001b[0m\n",
      "Precision: 0.9935567010309279 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9910025706940874 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5045395590142672\n",
      "\u001b[32m[I 2021-12-05 15:42:13,175]\u001b[0m Trial 1 finished with value: 2.9845581760020052 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 1 with value: 2.9845581760020052.\u001b[0m\n",
      "Precision: 0.9960886571056062 \n",
      "Recall: 0.9909208819714657 \n",
      "Aging Rate: 0.4974059662775616\n",
      "Precision: 0.9986979166666666 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.9960988296488946 \n",
      "Recall: 0.993514915693904 \n",
      "Aging Rate: 0.4987029831387808\n",
      "\u001b[32m[I 2021-12-05 15:42:13,357]\u001b[0m Trial 2 finished with value: 2.9870061790209426 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 2 with value: 2.9870061790209426.\u001b[0m\n",
      "Precision: 0.9987012987012988 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.9922380336351876 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 15:42:13,546]\u001b[0m Trial 3 finished with value: 2.9861942110474637 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 2 with value: 2.9870061790209426.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.9935567010309279 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9961190168175937 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9961240310077519 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 15:42:14,812]\u001b[0m Trial 4 finished with value: 2.9901008269504423 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m\n",
      "Precision: 0.9961089494163424 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9973992197659298 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9935483870967742 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 15:42:16,624]\u001b[0m Trial 5 finished with value: 2.9879123258894467 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 21}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m\n",
      "Precision: 0.9922680412371134 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9961190168175937 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9922480620155039 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 15:42:19,369]\u001b[0m Trial 6 finished with value: 2.9849283852781086 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m\n",
      "Precision: 0.9897039897039897 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9846350832266325 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.506485084306096\n",
      "Precision: 0.990979381443299 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.503242542153048\n",
      "\u001b[32m[I 2021-12-05 15:42:20,651]\u001b[0m Trial 7 finished with value: 2.974284935860176 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m\n",
      "Precision: 0.9922480620155039 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9948119325551232 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9973924380704041 \n",
      "Recall: 0.9922178988326849 \n",
      "Aging Rate: 0.4974059662775616\n",
      "\u001b[32m[I 2021-12-05 15:42:20,844]\u001b[0m Trial 8 finished with value: 2.9840145486954044 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.9961038961038962 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.9947916666666666 \n",
      "Recall: 0.9909208819714657 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.9935316946959897 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5012970168612192\n",
      "\u001b[32m[I 2021-12-05 15:42:21,046]\u001b[0m Trial 9 finished with value: 2.983565426292012 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m\n",
      "Precision: 0.9884615384615385 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5058365758754864\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9935567010309279 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.503242542153048\n",
      "\u001b[32m[I 2021-12-05 15:42:22,319]\u001b[0m Trial 10 finished with value: 2.9845712994465905 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 21}. Best is trial 4 with value: 2.9901008269504423.\u001b[0m\n",
      "Precision: 0.9987012987012988 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9974059662775616 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:42:24,176]\u001b[0m Trial 11 finished with value: 2.9905274823562764 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 11 with value: 2.9905274823562764.\u001b[0m\n",
      "Precision: 0.9974059662775616 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9973992197659298 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 15:42:26,037]\u001b[0m Trial 12 finished with value: 2.9905018967581687 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 11 with value: 2.9905274823562764.\u001b[0m\n",
      "Precision: 0.9974093264248705 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9986979166666666 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4980544747081712\n",
      "\u001b[32m[I 2021-12-05 15:42:27,863]\u001b[0m Trial 13 finished with value: 2.9918022737439394 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 13 with value: 2.9918022737439394.\u001b[0m\n",
      "Precision: 0.9986996098829649 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9987046632124352 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9987046632124352 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5006485084306096\n",
      "\u001b[32m[I 2021-12-05 15:42:29,723]\u001b[0m Trial 14 finished with value: 2.996108940677338 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9986996098829649 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9986962190352021 \n",
      "Recall: 0.993514915693904 \n",
      "Aging Rate: 0.4974059662775616\n",
      "Precision: 0.9973958333333334 \n",
      "Recall: 0.993514915693904 \n",
      "Aging Rate: 0.4980544747081712\n",
      "\u001b[32m[I 2021-12-05 15:42:32,382]\u001b[0m Trial 15 finished with value: 2.9909073684357175 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9834183673469388 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5084306095979247\n",
      "\u001b[32m[I 2021-12-05 15:42:34,257]\u001b[0m Trial 16 finished with value: 2.980356712868234 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 18}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9948320413436692 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9935483870967742 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9935483870967742 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 15:42:36,102]\u001b[0m Trial 17 finished with value: 2.9866555268302597 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9973958333333334 \n",
      "Recall: 0.993514915693904 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.99609375 \n",
      "Recall: 0.9922178988326849 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.9921773142112125 \n",
      "Recall: 0.9870298313878081 \n",
      "Aging Rate: 0.4974059662775616\n",
      "\u001b[32m[I 2021-12-05 15:42:36,288]\u001b[0m Trial 18 finished with value: 2.981365480334496 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9922680412371134 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9961139896373057 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9961038961038962 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4993514915693904\n",
      "\u001b[32m[I 2021-12-05 15:42:39,047]\u001b[0m Trial 19 finished with value: 2.9866309119760324 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.9987029831387808 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 15:42:40,328]\u001b[0m Trial 20 finished with value: 2.9948297839699873 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9974059662775616 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9974093264248705 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 1.0 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.4993514915693904\n",
      "\u001b[32m[I 2021-12-05 15:42:41,606]\u001b[0m Trial 21 finished with value: 2.9948141726533293 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9973992197659298 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9973924380704041 \n",
      "Recall: 0.9922178988326849 \n",
      "Aging Rate: 0.4974059662775616\n",
      "Precision: 1.0 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:42:42,888]\u001b[0m Trial 22 finished with value: 2.992204382353492 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9922680412371134 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 15:42:44,185]\u001b[0m Trial 23 finished with value: 2.984093706601909 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Precision: 0.9987012987012988 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.9948253557567918 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5012970168612192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:42:45,568]\u001b[0m A new study created in memory with name: no-name-0ed8aa2d-ecd4-47f5-b448-b1335b12dfa3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9961190168175937 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5012970168612192\n",
      "\u001b[32m[I 2021-12-05 15:42:45,449]\u001b[0m Trial 24 finished with value: 2.990935419415091 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 14 with value: 2.996108940677338.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset4 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80566ce826d04a0086a2b6c4a187375c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.992867332382311 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9985714285714286 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:42:46,745]\u001b[0m Trial 0 finished with value: 2.98385010731068 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 12}. Best is trial 0 with value: 2.98385010731068.\u001b[0m\n",
      "Precision: 0.9957203994293866 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9900990099009901 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 15:42:46,940]\u001b[0m Trial 1 finished with value: 2.9800944094717003 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 0 with value: 2.98385010731068.\u001b[0m\n",
      "Precision: 0.9957081545064378 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9928876244665719 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:42:48,757]\u001b[0m Trial 2 finished with value: 2.981954848455865 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 21}. Best is trial 0 with value: 2.98385010731068.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49359886201991465\n",
      "Precision: 0.9943100995732574 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9928876244665719 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:42:48,934]\u001b[0m Trial 3 finished with value: 2.982930298719772 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 0 with value: 2.98385010731068.\u001b[0m\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 15:42:51,460]\u001b[0m Trial 4 finished with value: 2.985744004461287 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9943100995732574 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 15:42:51,664]\u001b[0m Trial 5 finished with value: 2.98051600764544 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.9872701555869873 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.492176386913229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 15:42:51,858]\u001b[0m Trial 6 finished with value: 2.9763062195176544 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9943019943019943 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:42:52,051]\u001b[0m Trial 7 finished with value: 2.98480254684155 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.9914529914529915 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.99002849002849 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:42:52,252]\u001b[0m Trial 8 finished with value: 2.9753126171966975 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.9943019943019943 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9914772727272727 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:42:52,447]\u001b[0m Trial 9 finished with value: 2.9791193311998367 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 15:42:54,986]\u001b[0m Trial 10 finished with value: 2.981475085424227 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 3}. Best is trial 4 with value: 2.985744004461287.\u001b[0m\n",
      "Precision: 0.9985693848354793 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:42:57,629]\u001b[0m Trial 11 finished with value: 2.9905087041597844 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9943100995732574 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9914651493598862 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9943019943019943 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:43:00,408]\u001b[0m Trial 12 finished with value: 2.97960578662333 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9900990099009901 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 15:43:02,910]\u001b[0m Trial 13 finished with value: 2.9805644452989024 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9943019943019943 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9985632183908046 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49359886201991465\n",
      "\u001b[32m[I 2021-12-05 15:43:04,724]\u001b[0m Trial 14 finished with value: 2.981942444688112 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 15}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9985714285714286 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.992867332382311 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 15:43:07,101]\u001b[0m Trial 15 finished with value: 2.9833746721741314 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9914407988587732 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9957203994293866 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9971469329529244 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 15:43:08,960]\u001b[0m Trial 16 finished with value: 2.9814780618895043 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9928469241773963 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9928977272727273 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:43:11,570]\u001b[0m Trial 17 finished with value: 2.9814874252910735 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9985714285714286 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9914893617021276 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9900568181818182 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:43:12,899]\u001b[0m Trial 18 finished with value: 2.98010685513905 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9971469329529244 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 1.0 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:43:15,506]\u001b[0m Trial 19 finished with value: 2.9890929874154026 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9872881355932204 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 15:43:17,331]\u001b[0m Trial 20 finished with value: 2.9805994019447013 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9914772727272727 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:43:19,957]\u001b[0m Trial 21 finished with value: 2.9843500258851137 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:43:22,485]\u001b[0m Trial 22 finished with value: 2.9862250220144957 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9929078014184397 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9957325746799431 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:43:24,971]\u001b[0m Trial 23 finished with value: 2.984836330860967 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:43:27,629]\u001b[0m A new study created in memory with name: no-name-9df697cd-e70e-49fb-bd63-cc8897c19c4c\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 15:43:27,505]\u001b[0m Trial 24 finished with value: 2.9881256925102444 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 11 with value: 2.9905087041597844.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset5 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d8c0318b19472eac500f4cdaa3c369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 15:43:28,824]\u001b[0m Trial 0 finished with value: 2.9860931745510872 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 0 with value: 2.9860931745510872.\u001b[0m\n",
      "Precision: 0.9942363112391931 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9956458635703919 \n",
      "Recall: 0.985632183908046 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.498927805575411\n",
      "\u001b[32m[I 2021-12-05 15:43:29,027]\u001b[0m Trial 1 finished with value: 2.9808136542006918 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9860931745510872.\u001b[0m\n",
      "Precision: 0.9956584659913169 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.496783416726233\n",
      "\u001b[32m[I 2021-12-05 15:43:31,598]\u001b[0m Trial 2 finished with value: 2.9846438813165066 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 0 with value: 2.9860931745510872.\u001b[0m\n",
      "Precision: 0.994261119081779 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4982130092923517\n",
      "Precision: 0.9942279942279942 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 15:43:31,798]\u001b[0m Trial 3 finished with value: 2.983699865110001 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9860931745510872.\u001b[0m\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.498927805575411\n",
      "Precision: 0.994261119081779 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4982130092923517\n",
      "\u001b[32m[I 2021-12-05 15:43:31,986]\u001b[0m Trial 4 finished with value: 2.985640366082674 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.9860931745510872.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.9942528735632183 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49749821300929237\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 1.0 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49249463902787705\n",
      "\u001b[32m[I 2021-12-05 15:43:32,182]\u001b[0m Trial 5 finished with value: 2.987545136304749 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.987545136304749.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49463902787705505\n",
      "\u001b[32m[I 2021-12-05 15:43:34,764]\u001b[0m Trial 6 finished with value: 2.98848916083398 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 18}. Best is trial 6 with value: 2.98848916083398.\u001b[0m\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9841954022988506 \n",
      "Aging Rate: 0.49177984274481773\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.49749821300929237\n",
      "\u001b[32m[I 2021-12-05 15:43:37,343]\u001b[0m Trial 7 finished with value: 2.982705744229628 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 6 with value: 2.98848916083398.\u001b[0m\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49320943531093636\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.9928469241773963 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.4996426018584703\n",
      "\u001b[32m[I 2021-12-05 15:43:39,151]\u001b[0m Trial 8 finished with value: 2.9837106401067537 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 6 with value: 2.98848916083398.\u001b[0m\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9928366762177651 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.498927805575411\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 15:43:40,996]\u001b[0m Trial 9 finished with value: 2.988992876397322 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 9 with value: 2.988992876397322.\u001b[0m\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 15:43:42,246]\u001b[0m Trial 10 finished with value: 2.9894470272588127 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 10 with value: 2.9894470272588127.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9913793103448276 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49749821300929237\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "\u001b[32m[I 2021-12-05 15:43:43,525]\u001b[0m Trial 11 finished with value: 2.9865872818986188 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 10 with value: 2.9894470272588127.\u001b[0m\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 15:43:45,383]\u001b[0m Trial 12 finished with value: 2.9908865532993496 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 1.0 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49320943531093636\n",
      "\u001b[32m[I 2021-12-05 15:43:46,703]\u001b[0m Trial 13 finished with value: 2.989446979286964 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 15:43:48,548]\u001b[0m Trial 14 finished with value: 2.990407638046013 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 3}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 1.0 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49320943531093636\n",
      "Precision: 0.9928366762177651 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.498927805575411\n",
      "\u001b[32m[I 2021-12-05 15:43:50,420]\u001b[0m Trial 15 finished with value: 2.9894773243750614 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 21}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 0.9942446043165467 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 1.0 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 15:43:52,271]\u001b[0m Trial 16 finished with value: 2.9880130139805154 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.994261119081779 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4982130092923517\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 15:43:54,145]\u001b[0m Trial 17 finished with value: 2.987066169417874 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 15}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 0.99 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.5003573981415297\n",
      "Precision: 0.994261119081779 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4982130092923517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.498927805575411\n",
      "\u001b[32m[I 2021-12-05 15:43:54,340]\u001b[0m Trial 18 finished with value: 2.9818555557461397 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 0.9956584659913169 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 1.0 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 15:43:56,155]\u001b[0m Trial 19 finished with value: 2.986084797549038 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 12 with value: 2.9908865532993496.\u001b[0m\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.4982130092923517\n",
      "Precision: 1.0 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9971264367816092 \n",
      "Aging Rate: 0.4982130092923517\n",
      "\u001b[32m[I 2021-12-05 15:43:58,661]\u001b[0m Trial 20 finished with value: 2.9913875558633882 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 20 with value: 2.9913875558633882.\u001b[0m\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.4982130092923517\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49320943531093636\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 15:44:01,195]\u001b[0m Trial 21 finished with value: 2.990402025298422 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 20 with value: 2.9913875558633882.\u001b[0m\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9942528735632183 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49749821300929237\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49463902787705505\n",
      "\u001b[32m[I 2021-12-05 15:44:03,969]\u001b[0m Trial 22 finished with value: 2.9860972812342106 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 20 with value: 2.9913875558633882.\u001b[0m\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49463902787705505\n",
      "\u001b[32m[I 2021-12-05 15:44:05,813]\u001b[0m Trial 23 finished with value: 2.988007469271796 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 3}. Best is trial 20 with value: 2.9913875558633882.\u001b[0m\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9956521739130435 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49320943531093636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:44:08,445]\u001b[0m A new study created in memory with name: no-name-efdd60ca-c748-4ab7-9696-849bebb84414\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 15:44:08,315]\u001b[0m Trial 24 finished with value: 2.986075070105785 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 20 with value: 2.9913875558633882.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset6 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010432bfbf414336925fd8a97489341e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9953051643192489 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.5461538461538461\n",
      "Precision: 0.9965034965034965 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.991812865497076 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 15:44:08,543]\u001b[0m Trial 0 finished with value: 2.9813019432563883 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 0 with value: 2.9813019432563883.\u001b[0m\n",
      "Precision: 0.9906759906759907 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9964953271028038 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9906976744186047 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5512820512820513\n",
      "\u001b[32m[I 2021-12-05 15:44:08,739]\u001b[0m Trial 1 finished with value: 2.9790227353661383 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9813019432563883.\u001b[0m\n",
      "Precision: 0.9964871194379391 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9976498237367802 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9988290398126464 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5474358974358975\n",
      "\u001b[32m[I 2021-12-05 15:44:10,548]\u001b[0m Trial 2 finished with value: 2.988309488463767 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 15}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.991822429906542 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9929577464788732 \n",
      "Recall: 0.9871645274212368 \n",
      "Aging Rate: 0.5461538461538461\n",
      "Precision: 0.9941588785046729 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5487179487179488\n",
      "\u001b[32m[I 2021-12-05 15:44:12,352]\u001b[0m Trial 3 finished with value: 2.9762355270640257 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9907192575406032 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5525641025641026\n",
      "Precision: 0.9906432748538012 \n",
      "Recall: 0.9883313885647608 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9907300115874855 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5532051282051282\n",
      "\u001b[32m[I 2021-12-05 15:44:12,546]\u001b[0m Trial 4 finished with value: 2.9755607236036403 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9907084785133565 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.551923076923077\n",
      "Precision: 0.9941656942823804 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5493589743589744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9929906542056075 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5487179487179488\n",
      "\u001b[32m[I 2021-12-05 15:44:12,747]\u001b[0m Trial 5 finished with value: 2.9790199585687684 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9953106682297772 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5467948717948717\n",
      "Precision: 0.9929988331388565 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9953106682297772 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5467948717948717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-05 15:44:12,945]\u001b[0m Trial 6 finished with value: 2.9805231313464318 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9918793503480279 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5525641025641026\n",
      "Precision: 0.9953271028037384 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5487179487179488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9941792782305006 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5506410256410257\n",
      "\u001b[32m[I 2021-12-05 15:44:13,144]\u001b[0m Trial 7 finished with value: 2.983700950443098 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9964912280701754 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9976553341148886 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5467948717948717\n",
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 15:44:14,409]\u001b[0m Trial 8 finished with value: 2.9852124440292442 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9941245593419507 \n",
      "Recall: 0.9871645274212368 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9953161592505855 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9964747356051704 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.5455128205128205\n",
      "\u001b[32m[I 2021-12-05 15:44:16,251]\u001b[0m Trial 9 finished with value: 2.980108552506756 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9976498237367802 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9953325554259043 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9918414918414918 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 15:44:18,906]\u001b[0m Trial 10 finished with value: 2.9828814138083075 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9953434225844005 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5506410256410257\n",
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9976525821596244 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5461538461538461\n",
      "\u001b[32m[I 2021-12-05 15:44:20,186]\u001b[0m Trial 11 finished with value: 2.9852233131642336 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9964994165694282 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9941520467836257 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9964994165694282 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 15:44:21,478]\u001b[0m Trial 12 finished with value: 2.9863775216597177 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9964871194379391 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9906759906759907 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9941656942823804 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 15:44:23,335]\u001b[0m Trial 13 finished with value: 2.980551369403063 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 21}. Best is trial 2 with value: 2.988309488463767.\u001b[0m\n",
      "Precision: 0.9976662777129521 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9976608187134502 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9941656942823804 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 15:44:26,081]\u001b[0m Trial 14 finished with value: 2.988716702946268 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9906759906759907 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9964788732394366 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5461538461538461\n",
      "Precision: 0.9964871194379391 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5474358974358975\n",
      "\u001b[32m[I 2021-12-05 15:44:28,801]\u001b[0m Trial 15 finished with value: 2.980926627564244 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 21}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9952996474735605 \n",
      "Recall: 0.9883313885647608 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9930232558139535 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5512820512820513\n",
      "Precision: 0.9953271028037384 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5487179487179488\n",
      "\u001b[32m[I 2021-12-05 15:44:31,538]\u001b[0m Trial 16 finished with value: 2.9820988371996915 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9941520467836257 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9964912280701754 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9953161592505855 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5474358974358975\n",
      "\u001b[32m[I 2021-12-05 15:44:34,330]\u001b[0m Trial 17 finished with value: 2.983249502160606 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 18}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9976580796252927 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9929988331388565 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9964871194379391 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5474358974358975\n",
      "\u001b[32m[I 2021-12-05 15:44:36,185]\u001b[0m Trial 18 finished with value: 2.984817141654757 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.991822429906542 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9953271028037384 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9953325554259043 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 15:44:38,926]\u001b[0m Trial 19 finished with value: 2.981709178944154 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9964830011723329 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5467948717948717\n",
      "Precision: 0.9941588785046729 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9918414918414918 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 15:44:40,757]\u001b[0m Trial 20 finished with value: 2.980932127103347 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9953051643192489 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.5461538461538461\n",
      "Precision: 0.9976387249114522 \n",
      "Recall: 0.9859976662777129 \n",
      "Aging Rate: 0.5429487179487179\n",
      "Precision: 0.9953216374269006 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 15:44:42,027]\u001b[0m Trial 21 finished with value: 2.981675267480019 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9941588785046729 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9976525821596244 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5461538461538461\n",
      "Precision: 0.9976553341148886 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5467948717948717\n",
      "\u001b[32m[I 2021-12-05 15:44:43,310]\u001b[0m Trial 22 finished with value: 2.985587742610472 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 9}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Precision: 0.9964664310954063 \n",
      "Recall: 0.9871645274212368 \n",
      "Aging Rate: 0.5442307692307692\n",
      "Precision: 0.9929824561403509 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9953271028037384 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5487179487179488\n",
      "\u001b[32m[I 2021-12-05 15:44:44,588]\u001b[0m Trial 23 finished with value: 2.9805157708781387 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 15}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9976498237367802 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5455128205128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:44:46,607]\u001b[0m A new study created in memory with name: no-name-87e2a42f-dab4-4942-9a2e-0f04f06a1fa9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9941656942823804 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 15:44:46,475]\u001b[0m Trial 24 finished with value: 2.9828805071562923 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 21}. Best is trial 14 with value: 2.988716702946268.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset7 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66ba901f0794555a2cfb8a7a006943c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9929378531073446 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5035561877667141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9915014164305949 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "\u001b[32m[I 2021-12-05 15:44:46,713]\u001b[0m Trial 0 finished with value: 2.9820798287022545 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.9820798287022545.\u001b[0m\n",
      "Precision: 0.9957203994293866 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9914529914529915 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:44:46,906]\u001b[0m Trial 1 finished with value: 2.9800527995709403 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9820798287022545.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9929378531073446 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "\u001b[32m[I 2021-12-05 15:44:48,712]\u001b[0m Trial 2 finished with value: 2.987745551081858 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:44:48,911]\u001b[0m Trial 3 finished with value: 2.9838419091609456 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9985507246376811 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.4907539118065434\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:44:49,092]\u001b[0m Trial 4 finished with value: 2.9843281525966496 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9859747545582047 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5071123755334281\n",
      "Precision: 0.9929378531073446 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 15:44:51,614]\u001b[0m Trial 5 finished with value: 2.9812109157957436 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9915373765867419 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5042674253200569\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:44:51,809]\u001b[0m Trial 6 finished with value: 2.9858112077739425 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:44:52,012]\u001b[0m Trial 7 finished with value: 2.9857952936855576 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9943019943019943 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 1.0 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.4907539118065434\n",
      "Precision: 0.9942857142857143 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:44:52,200]\u001b[0m Trial 8 finished with value: 2.980537846502759 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 2 with value: 2.987745551081858.\u001b[0m\n",
      "Precision: 0.9929178470254958 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 1.0 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9985734664764622 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 15:44:54,915]\u001b[0m Trial 9 finished with value: 2.9914825921212675 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9957507082152974 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9915134370579916 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9900990099009901 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 15:44:57,584]\u001b[0m Trial 10 finished with value: 2.98253797827171 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9915373765867419 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5042674253200569\n",
      "Precision: 0.9901408450704225 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5049786628733998\n",
      "Precision: 0.9901269393511989 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5042674253200569\n",
      "\u001b[32m[I 2021-12-05 15:44:58,852]\u001b[0m Trial 11 finished with value: 2.980729282303347 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 15:45:00,704]\u001b[0m Trial 12 finished with value: 2.9872460649780765 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 6}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9929178470254958 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5021337126600285\n",
      "\u001b[32m[I 2021-12-05 15:45:02,500]\u001b[0m Trial 13 finished with value: 2.9863071355719057 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9929378531073446 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5035561877667141\n",
      "\u001b[32m[I 2021-12-05 15:45:03,781]\u001b[0m Trial 14 finished with value: 2.986331180760659 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9943181818181818 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:45:06,376]\u001b[0m Trial 15 finished with value: 2.9834391122884507 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9929378531073446 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 15:45:08,207]\u001b[0m Trial 16 finished with value: 2.989151998556725 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 15:45:10,866]\u001b[0m Trial 17 finished with value: 2.984915402806077 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9929178470254958 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9943100995732574 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 15:45:12,687]\u001b[0m Trial 18 finished with value: 2.9838947111945466 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9957446808510638 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9943181818181818 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:45:13,966]\u001b[0m Trial 19 finished with value: 2.9877068025519122 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9957446808510638 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "\u001b[32m[I 2021-12-05 15:45:16,675]\u001b[0m Trial 20 finished with value: 2.9881863489545526 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9901408450704225 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5049786628733998\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9929078014184397 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 15:45:19,334]\u001b[0m Trial 21 finished with value: 2.9811527466695367 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9957507082152974 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:45:22,012]\u001b[0m Trial 22 finished with value: 2.98721373911772 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9943181818181818 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 15:45:23,787]\u001b[0m Trial 23 finished with value: 2.9862870177054757 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9971631205673759 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5014224751066856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:45:26,262]\u001b[0m A new study created in memory with name: no-name-ae692198-60cf-46ad-913b-4dc1bd6eca10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "\u001b[32m[I 2021-12-05 15:45:26,128]\u001b[0m Trial 24 finished with value: 2.990068214594254 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 9 with value: 2.9914825921212675.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset8 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c645185d0be04428a6a5b89b66e17440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9914163090128756 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971469329529244 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 15:45:26,354]\u001b[0m Trial 0 finished with value: 2.978129036756117 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.978129036756117.\u001b[0m\n",
      "Precision: 0.9928366762177651 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49359886201991465\n",
      "\u001b[32m[I 2021-12-05 15:45:29,076]\u001b[0m Trial 1 finished with value: 2.9752524555785342 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 21}. Best is trial 0 with value: 2.978129036756117.\u001b[0m\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9928263988522238 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 15:45:29,266]\u001b[0m Trial 2 finished with value: 2.974771563362283 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.978129036756117.\u001b[0m\n",
      "Precision: 0.9914529914529915 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9928571428571429 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-05 15:45:29,461]\u001b[0m Trial 3 finished with value: 2.9743398493360793 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 0 with value: 2.978129036756117.\u001b[0m\n",
      "Precision: 0.9928263988522238 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9928571428571429 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:45:29,651]\u001b[0m Trial 4 finished with value: 2.975740458206667 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 0 with value: 2.978129036756117.\u001b[0m\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9914163090128756 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9886201991465149 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 15:45:29,845]\u001b[0m Trial 5 finished with value: 2.970541007892137 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.978129036756117.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9956709956709957 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9942528735632183 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 15:45:31,798]\u001b[0m Trial 6 finished with value: 2.9781027185849887 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 18}. Best is trial 0 with value: 2.978129036756117.\u001b[0m\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 15:45:33,660]\u001b[0m Trial 7 finished with value: 2.9843159696719135 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9928057553956835 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9914163090128756 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9885877318116976 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 15:45:33,840]\u001b[0m Trial 8 finished with value: 2.9662259713066295 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9928774928774928 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9942857142857143 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 1.0 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 15:45:35,691]\u001b[0m Trial 9 finished with value: 2.9824331290997956 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9957081545064378 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.49359886201991465\n",
      "\u001b[32m[I 2021-12-05 15:45:36,960]\u001b[0m Trial 10 finished with value: 2.9790647821492287 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9971469329529244 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "\u001b[32m[I 2021-12-05 15:45:38,800]\u001b[0m Trial 11 finished with value: 2.984314614975633 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9927953890489913 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.49359886201991465\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9928469241773963 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 15:45:40,642]\u001b[0m Trial 12 finished with value: 2.9714347016624196 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9956521739130435 \n",
      "Recall: 0.9772403982930299 \n",
      "Aging Rate: 0.4907539118065434\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49359886201991465\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 15:45:41,907]\u001b[0m Trial 13 finished with value: 2.974769703005039 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49359886201991465\n",
      "\u001b[32m[I 2021-12-05 15:45:44,441]\u001b[0m Trial 14 finished with value: 2.980509151307416 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9957203994293866 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 15:45:46,297]\u001b[0m Trial 15 finished with value: 2.9814561411969636 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "\u001b[32m[I 2021-12-05 15:45:47,594]\u001b[0m Trial 16 finished with value: 2.980976379981179 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 15:45:50,091]\u001b[0m Trial 17 finished with value: 2.982882606423388 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 15:45:51,876]\u001b[0m Trial 18 finished with value: 2.9838433301119616 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 7 with value: 2.9843159696719135.\u001b[0m\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.4914651493598862\n",
      "Precision: 1.0 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 1.0 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 15:45:53,754]\u001b[0m Trial 19 finished with value: 2.985758780335671 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 19 with value: 2.985758780335671.\u001b[0m\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9942857142857143 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.49288762446657186\n",
      "\u001b[32m[I 2021-12-05 15:45:56,388]\u001b[0m Trial 20 finished with value: 2.980981911508227 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 15}. Best is trial 19 with value: 2.985758780335671.\u001b[0m\n",
      "Precision: 0.9971469329529244 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9786628733997155 \n",
      "Aging Rate: 0.4907539118065434\n",
      "\u001b[32m[I 2021-12-05 15:45:58,265]\u001b[0m Trial 21 finished with value: 2.9814503518859468 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 19 with value: 2.985758780335671.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9928057553956835 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49431009957325744\n",
      "\u001b[32m[I 2021-12-05 15:46:00,119]\u001b[0m Trial 22 finished with value: 2.975241497095157 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 19 with value: 2.985758780335671.\u001b[0m\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.4914651493598862\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 15:46:01,953]\u001b[0m Trial 23 finished with value: 2.983366273862332 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 19 with value: 2.985758780335671.\u001b[0m\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49359886201991465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 15:46:03,348]\u001b[0m A new study created in memory with name: no-name-752d0086-54d4-408b-877e-f7f39d14cd15\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9928571428571429 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 15:46:03,220]\u001b[0m Trial 24 finished with value: 2.979084033340458 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 19 with value: 2.985758780335671.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset9 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3a61ce0ea54915b84db23755f7381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.967741935483871 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.08010335917312661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9411764705882353 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07751937984496124\n",
      "\u001b[32m[I 2021-12-05 15:46:03,442]\u001b[0m Trial 0 finished with value: 2.787322755847758 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.9464285714285714 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.07235142118863049\n",
      "Precision: 0.9354838709677419 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07881136950904392\n",
      "\u001b[32m[I 2021-12-05 15:46:03,622]\u001b[0m Trial 1 finished with value: 2.713162901257359 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9661016949152542 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.07622739018087855\n",
      "Precision: 0.9649122807017544 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.07364341085271318\n",
      "\u001b[32m[I 2021-12-05 15:46:03,818]\u001b[0m Trial 2 finished with value: 2.7400156778231217 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.787322755847758.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.9354838709677419 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9393939393939394 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08527131782945736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07493540051679587\n",
      "\u001b[32m[I 2021-12-05 15:46:04,011]\u001b[0m Trial 3 finished with value: 2.7522659580814963 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.9538461538461539 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9523809523809523 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9666666666666667 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07751937984496124\n",
      "\u001b[32m[I 2021-12-05 15:46:06,385]\u001b[0m Trial 4 finished with value: 2.760332937797726 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 9}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.9298245614035088 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.07364341085271318\n",
      "Precision: 0.9655172413793104 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.07493540051679587\n",
      "Precision: 0.8985507246376812 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08914728682170543\n",
      "\u001b[32m[I 2021-12-05 15:46:06,582]\u001b[0m Trial 5 finished with value: 2.665411919688784 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.9393939393939394 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08527131782945736\n",
      "Precision: 0.9122807017543859 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.07364341085271318\n",
      "Precision: 0.9090909090909091 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 15:46:08,952]\u001b[0m Trial 6 finished with value: 2.6574117752768602 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.9516129032258065 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9508196721311475 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9354838709677419 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08010335917312661\n",
      "\u001b[32m[I 2021-12-05 15:46:11,346]\u001b[0m Trial 7 finished with value: 2.713540541681253 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 0 with value: 2.787322755847758.\u001b[0m\n",
      "Precision: 0.9848484848484849 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08527131782945736\n",
      "Precision: 0.9701492537313433 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9142857142857143 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.09043927648578812\n",
      "\u001b[32m[I 2021-12-05 15:46:13,209]\u001b[0m Trial 8 finished with value: 2.8236537573094225 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9705882352941176 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 0.9104477611940298 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9516129032258065 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.08010335917312661\n",
      "\u001b[32m[I 2021-12-05 15:46:14,491]\u001b[0m Trial 9 finished with value: 2.761672036429021 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 9}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9538461538461539 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9295774647887324 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.0917312661498708\n",
      "Precision: 0.9054054054054054 \n",
      "Recall: 0.9436619718309859 \n",
      "Aging Rate: 0.09560723514211886\n",
      "\u001b[32m[I 2021-12-05 15:46:15,770]\u001b[0m Trial 10 finished with value: 2.774712307106673 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n",
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n",
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n",
      "\u001b[32m[I 2021-12-05 15:46:15,950]\u001b[0m Trial 11 finished with value: 0.0 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.9838709677419355 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9420289855072463 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08914728682170543\n",
      "Precision: 0.9393939393939394 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 15:46:17,756]\u001b[0m Trial 12 finished with value: 2.7928250364099685 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.96875 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.082687338501292\n",
      "Precision: 0.927536231884058 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08914728682170543\n",
      "Precision: 0.9552238805970149 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08656330749354005\n",
      "\u001b[32m[I 2021-12-05 15:46:19,603]\u001b[0m Trial 13 finished with value: 2.793025520996771 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9552238805970149 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9402985074626866 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9516129032258065 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.08010335917312661\n",
      "\u001b[32m[I 2021-12-05 15:46:21,470]\u001b[0m Trial 14 finished with value: 2.771329630810057 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9354838709677419 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9365079365079365 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.08139534883720931\n",
      "\u001b[32m[I 2021-12-05 15:46:23,468]\u001b[0m Trial 15 finished with value: 2.729177934993473 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9393939393939394 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08527131782945736\n",
      "Precision: 0.9206349206349206 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9117647058823529 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08785529715762273\n",
      "\u001b[32m[I 2021-12-05 15:46:25,219]\u001b[0m Trial 16 finished with value: 2.7023224711708558 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9850746268656716 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9491525423728814 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.07622739018087855\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.08010335917312661\n",
      "\u001b[32m[I 2021-12-05 15:46:26,470]\u001b[0m Trial 17 finished with value: 2.7891061637116628 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9565217391304348 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.08914728682170543\n",
      "Precision: 0.9047619047619048 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9333333333333333 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.07751937984496124\n",
      "\u001b[32m[I 2021-12-05 15:46:28,311]\u001b[0m Trial 18 finished with value: 2.7034535716715755 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 18}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9117647058823529 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 0.9014084507042254 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.0917312661498708\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07751937984496124\n",
      "\u001b[32m[I 2021-12-05 15:46:30,153]\u001b[0m Trial 19 finished with value: 2.732882260885575 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 18}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9206349206349206 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.90625 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.082687338501292\n",
      "Precision: 0.9365079365079365 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.08139534883720931\n",
      "\u001b[32m[I 2021-12-05 15:46:32,547]\u001b[0m Trial 20 finished with value: 2.66385814889336 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 12}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9428571428571428 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.09043927648578812\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9411764705882353 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08785529715762273\n",
      "\u001b[32m[I 2021-12-05 15:46:34,387]\u001b[0m Trial 21 finished with value: 2.8037788932243415 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9402985074626866 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9705882352941176 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 0.9558823529411765 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08785529715762273\n",
      "\u001b[32m[I 2021-12-05 15:46:36,201]\u001b[0m Trial 22 finished with value: 2.8219775191977146 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9545454545454546 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.08527131782945736\n",
      "Precision: 0.9354838709677419 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9661016949152542 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.07622739018087855\n",
      "\u001b[32m[I 2021-12-05 15:46:38,062]\u001b[0m Trial 23 finished with value: 2.7397680981260097 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 18}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Precision: 0.9701492537313433 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9411764705882353 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 0.927536231884058 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08914728682170543\n",
      "\u001b[32m[I 2021-12-05 15:46:39,326]\u001b[0m Trial 24 finished with value: 2.798677923854067 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 8 with value: 2.8236537573094225.\u001b[0m\n",
      "Sampler is TPESampler\n"
     ]
    }
   ],
   "source": [
    "best_paramC, _ = all_optuna(num_set = 10, \n",
    "                            all_data = train_firstC, \n",
    "                            mode = 'C', \n",
    "                            TPE_multi = True, \n",
    "                            n_iter = 25,\n",
    "                            filename = 'runhist_array_m2m4_m5_3criteria_StackingCV3',\n",
    "                            creator = stackingCV_creator\n",
    ")\n",
    "\n",
    "# best_paramR, _ = all_optuna(num_set = 10, \n",
    "#                             all_data = train_firstR, \n",
    "#                             mode = 'R', \n",
    "#                             TPE_multi = True, \n",
    "#                             n_iter = 10,\n",
    "#                             filename = f'runhist_array_4criteria_m2m5_StackingCV3',\n",
    "#                             creator = stackingCV_creator\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection by feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:48:08.400994Z",
     "start_time": "2021-12-05T05:48:01.591412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank_importance(train_firstC['set7'], mode = 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:47:00.267012Z",
     "start_time": "2021-12-05T07:46:42.206378Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f2458a4fb426996c5af0488bf469a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset 0:\n",
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 6.166622129951284e-05\n",
      "\n",
      " Dataset 1:\n",
      "Precision: 0.0015473572766511931 \n",
      "Recall: 0.37254901960784315 \n",
      "Aging Rate: 0.25239984377890606\n",
      "\n",
      " Dataset 2:\n",
      "Precision: 0.0011852036678934564 \n",
      "Recall: 0.37254901960784315 \n",
      "Aging Rate: 0.3295237312174968\n",
      "\n",
      " Dataset 3:\n",
      "Precision: 0.001386544489992766 \n",
      "Recall: 0.45098039215686275 \n",
      "Aging Rate: 0.3409730929721063\n",
      "\n",
      " Dataset 4:\n",
      "Precision: 0.0013141683778234085 \n",
      "Recall: 0.3137254901960784 \n",
      "Aging Rate: 0.2502620814405229\n",
      "\n",
      " Dataset 5:\n",
      "Precision: 0.0020429009193054137 \n",
      "Recall: 0.47058823529411764 \n",
      "Aging Rate: 0.24148492260889226\n",
      "\n",
      " Dataset 6:\n",
      "Precision: 0.0012866563019353456 \n",
      "Recall: 0.47058823529411764 \n",
      "Aging Rate: 0.38342000863327097\n",
      "\n",
      " Dataset 7:\n",
      "Precision: 0.001277864546358086 \n",
      "Recall: 0.5294117647058824 \n",
      "Aging Rate: 0.4343151966124689\n",
      "\n",
      " Dataset 8:\n",
      "Precision: 0.0013470029185063234 \n",
      "Recall: 0.35294117647058826 \n",
      "Aging Rate: 0.27468190507513\n",
      "\n",
      " Dataset 9:\n",
      "Precision: 0.0010508717458801052 \n",
      "Recall: 0.43137254901960786 \n",
      "Aging Rate: 0.4303274476351004\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_setC, coefC = runall_LR(10, train_firstC_x, test_firstC_x, train_firstC_y, test_firstC_y, best_paramC)\n",
    "line_chart(table_setC, title = 'StackingCV Classifier (scheme 3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:53:12.543700Z",
     "start_time": "2021-12-05T07:53:12.528741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set0</th>\n",
       "      <th>set1</th>\n",
       "      <th>set2</th>\n",
       "      <th>set3</th>\n",
       "      <th>set4</th>\n",
       "      <th>set5</th>\n",
       "      <th>set6</th>\n",
       "      <th>set7</th>\n",
       "      <th>set8</th>\n",
       "      <th>set9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meta_learner</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>ExtraTrees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_split</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         set0        set1        set2        set3        set4  \\\n",
       "meta_learner       ExtraTrees  ExtraTrees  ExtraTrees  ExtraTrees  ExtraTrees   \n",
       "n_estimators              100         100         500         300         500   \n",
       "min_samples_split           7           2          32           2           2   \n",
       "max_depth                   9          12          21          18           9   \n",
       "\n",
       "                         set5        set6        set7        set8        set9  \n",
       "meta_learner       ExtraTrees  ExtraTrees  ExtraTrees  ExtraTrees  ExtraTrees  \n",
       "n_estimators              500         500         500         300         300  \n",
       "min_samples_split          12          32           2          12           2  \n",
       "max_depth                   3          21          15           9          21  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(best_paramC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:47:03.065532Z",
     "start_time": "2021-12-05T07:47:03.035612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balance Ratio</th>\n",
       "      <th>Train_OK</th>\n",
       "      <th>Train_NG</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Aging Rate</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset 0</th>\n",
       "      <td>558.245552</td>\n",
       "      <td>156867.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>48595.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12260.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36338.0</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.252400</td>\n",
       "      <td>1.476027</td>\n",
       "      <td>0.337774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16012.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32586.0</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.329524</td>\n",
       "      <td>1.130568</td>\n",
       "      <td>0.281792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16565.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32033.0</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.340973</td>\n",
       "      <td>1.322628</td>\n",
       "      <td>0.380041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12159.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36439.0</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.250262</td>\n",
       "      <td>1.253588</td>\n",
       "      <td>0.254909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 5</th>\n",
       "      <td>1.009339</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11724.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36874.0</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.241485</td>\n",
       "      <td>1.948727</td>\n",
       "      <td>0.508882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 6</th>\n",
       "      <td>0.820198</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18629.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29969.0</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.383420</td>\n",
       "      <td>1.227344</td>\n",
       "      <td>0.376852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21102.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27496.0</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.434315</td>\n",
       "      <td>1.218957</td>\n",
       "      <td>0.421961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13345.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35253.0</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.274682</td>\n",
       "      <td>1.284909</td>\n",
       "      <td>0.291644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 9</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20913.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27685.0</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.430327</td>\n",
       "      <td>1.002429</td>\n",
       "      <td>0.299528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Balance Ratio  Train_OK  Train_NG    TP       FP    FN       TN  \\\n",
       "dataset 0     558.245552  156867.0     281.0   0.0      3.0  51.0  48595.0   \n",
       "dataset 1       1.000000    2758.0    2758.0  19.0  12260.0  32.0  36338.0   \n",
       "dataset 2       1.000000    3775.0    3775.0  19.0  16012.0  32.0  32586.0   \n",
       "dataset 3       1.000000    3084.0    3084.0  23.0  16565.0  28.0  32033.0   \n",
       "dataset 4       1.000000    2809.0    2809.0  16.0  12159.0  35.0  36439.0   \n",
       "dataset 5       1.009339    2810.0    2784.0  24.0  11724.0  27.0  36874.0   \n",
       "dataset 6       0.820198    2810.0    3426.0  24.0  18629.0  27.0  29969.0   \n",
       "dataset 7       1.000000    2810.0    2810.0  27.0  21102.0  24.0  27496.0   \n",
       "dataset 8       1.000000    2810.0    2810.0  18.0  13345.0  33.0  35253.0   \n",
       "dataset 9      10.000000    2810.0     281.0  22.0  20913.0  29.0  27685.0   \n",
       "\n",
       "           Precision    Recall  Aging Rate  Efficiency     Score  \n",
       "dataset 0   0.000000  0.000000    0.000062    0.000000  0.000000  \n",
       "dataset 1   0.001547  0.372549    0.252400    1.476027  0.337774  \n",
       "dataset 2   0.001185  0.372549    0.329524    1.130568  0.281792  \n",
       "dataset 3   0.001387  0.450980    0.340973    1.322628  0.380041  \n",
       "dataset 4   0.001314  0.313725    0.250262    1.253588  0.254909  \n",
       "dataset 5   0.002043  0.470588    0.241485    1.948727  0.508882  \n",
       "dataset 6   0.001287  0.470588    0.383420    1.227344  0.376852  \n",
       "dataset 7   0.001278  0.529412    0.434315    1.218957  0.421961  \n",
       "dataset 8   0.001347  0.352941    0.274682    1.284909  0.291644  \n",
       "dataset 9   0.001051  0.431373    0.430327    1.002429  0.299528  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T07:04:38.242253Z",
     "start_time": "2021-10-17T07:04:37.642731Z"
    }
   },
   "outputs": [],
   "source": [
    "pr_dict, table_setR, coefR = runall_RidgeR(10, train_firstR_x, test_firstR_x, train_firstR_y, test_firstR_y, \n",
    "                                           best_paramR, thres_target = 'Recall', threshold = 0.7)\n",
    "line_chart(table_setR, title = 'StackingCV Regressor (scheme 3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T07:04:47.085372Z",
     "start_time": "2021-10-17T07:04:44.350165Z"
    }
   },
   "outputs": [],
   "source": [
    "multiple_curve(4, 3, pr_dict, table_setR, target = 'Aging Rate')\n",
    "multiple_curve(4, 3, pr_dict, table_setR, target = 'Precision')\n",
    "print(coefR)\n",
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:47:06.299394Z",
     "start_time": "2021-12-05T07:47:05.753852Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20211207'\n",
    "TPE_multi = True\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'StackingCV_3_tree'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'StackingCV_3_tree')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
