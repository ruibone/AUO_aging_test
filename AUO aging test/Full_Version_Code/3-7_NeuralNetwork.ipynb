{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T08:40:33.560229Z",
     "start_time": "2021-11-07T08:40:33.553288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio, train_col\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, AUC, PR_curve, multiple_curve, \\\n",
    "    best_threshold\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:32:00.997027Z",
     "start_time": "2021-11-07T06:32:00.983032Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "##### GPU ??? #####\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:32:01.951308Z",
     "start_time": "2021-11-07T06:32:01.936348Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunhistSet(Dataset):\n",
    "    \n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.x = torch.tensor(train_x.values.astype(np.float32))\n",
    "        self.y = torch.tensor(train_y.values.astype(np.float32)).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.x[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T08:25:10.435315Z",
     "start_time": "2021-11-07T08:25:10.417149Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkC(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super(NeuralNetworkC, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "#             nn.Linear(96, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class NeuralNetworkR(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkR, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(114, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "#             nn.Linear(32, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:32:05.607186Z",
     "start_time": "2021-11-07T06:32:05.591229Z"
    }
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.cls = classes\n",
    "        self.dim = dim                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            pred = pred * self.weight.unsqueeze(0)   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:32:12.185064Z",
     "start_time": "2021-11-07T06:32:12.154017Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainingC(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop):\n",
    "    \n",
    "    network.train()\n",
    "    best_model = network\n",
    "    best_objective = 0\n",
    "    stop_trigger = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        \n",
    "        total_loss = 0\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0  \n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)         \n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            TP += torch.dot((predicted == y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "            TN += torch.dot((predicted == y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "            FN += torch.dot((predicted != y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "            FP += torch.dot((predicted != y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "            total_loss += loss.item()*len(y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss.append(total_loss)\n",
    "        recall = TP / (TP + FN)\n",
    "        aging = (TP + FP) / (TP + TN + FP + FN)\n",
    "            \n",
    "        print(f'Epoch {i+1}: Train Loss = {total_loss / (TP + TN + FP + FN)}, Recall = {recall}, Aging Rate = {aging}')\n",
    "        \n",
    "        if ((i+1) % 5 == 0):\n",
    "            five_loss, valid_recall, _ = testingC(network, validloader, criterion)\n",
    "            valid_loss.append(five_loss)\n",
    "            \n",
    "            if valid_recall > best_objective:\n",
    "                best_objective = valid_recall\n",
    "                best_model = network\n",
    "                torch.save(best_model, f'{filename}_NeuralNetworkC_{epoch}.ckpt')\n",
    "                print(f'Model in epoch {i+1} is saved.\\n')\n",
    "                stop_trigger = 0\n",
    "            else:\n",
    "                stop_trigger += 1\n",
    "                print('')\n",
    "                \n",
    "            if stop_trigger == early_stop:\n",
    "                print(f'Training Finished at epoch {i+1}.')\n",
    "                return network, train_loss, valid_loss\n",
    "            \n",
    "    return network, train_loss, valid_loss\n",
    "\n",
    "\n",
    "def PR_matrix(predict):\n",
    "    \n",
    "    Y_new = predict.sort_values(['predict', 'truth'], ascending = [False, True]).reset_index(drop = True)\n",
    "    Y_new.loc[Y_new['truth'] != 1, 'truth'] = 0\n",
    "    \n",
    "    matrix = pd.DataFrame(Y_new.groupby('predict').sum()).rename(columns = {'truth': 'Bad_Count'})\n",
    "    matrix = matrix.sort_index(ascending = False)\n",
    "    matrix['All_Count'] = Y_new.groupby('predict').count()\n",
    "    matrix['Class_Prob'] = matrix.index\n",
    "    \n",
    "    matrix['TP'] = matrix['Bad_Count'].cumsum()\n",
    "    matrix['FP'] = matrix['All_Count'].cumsum() - matrix['TP']\n",
    "    matrix['FN'] = matrix['TP'].values[-1] - matrix['TP']\n",
    "    matrix['TN'] = matrix['FP'].values[-1] - matrix['FP']\n",
    "    \n",
    "    matrix['Precision'] = matrix['TP'] / (matrix['TP'] + matrix['FP'])\n",
    "    matrix['Recall'] = matrix['TP'] / (matrix['TP'] + matrix['FN'])\n",
    "    matrix['Aging Rate'] = (matrix['TP'] + matrix['FP']) / (matrix['TP'] + matrix['FP'] + matrix['FN'] + matrix['TN'])\n",
    "    matrix['Efficiency'] = matrix['Recall'] / matrix['Aging Rate']\n",
    "    matrix['Score'] = score1(matrix['Recall'], matrix['Aging Rate'])            \n",
    "    matrix = matrix.drop(columns = ['Bad_Count', 'All_Count']).reset_index(drop = True)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def trainingR(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop):\n",
    "    \n",
    "    network.train()\n",
    "    best_model = network\n",
    "    best_objective = 1\n",
    "    stop_trigger = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        \n",
    "        total_loss = 0\n",
    "        predict_vector = torch.tensor([0])\n",
    "        y_vector = torch.tensor([0])\n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.type(torch.FloatTensor).to(device)\n",
    "            y = y.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()*len(y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            predict_vector = torch.cat((predict_vector, output.data[:,0]), axis = 0)\n",
    "            y_vector = torch.cat((y_vector, y[:,0]), axis = 0)       \n",
    "        result_df = pd.DataFrame(dict(predict = predict_vector, truth = y_vector))\n",
    "        pr_matrix = PR_matrix(result_df.iloc[1:, :])\n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = 'Recall', threshold = 0.7)\n",
    "        auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "        train_loss.append(total_loss)\n",
    "        \n",
    "        recall = best_data[\"Recall\"].values\n",
    "        aging = best_data[\"Aging Rate\"].values\n",
    "        print(f'Epoch {i+1}: Train Loss = {total_loss}, AUC = {auc}, Recall(0.7) = {recall}, Aging Rate = {aging}')\n",
    "        \n",
    "        if ((i+1) % 5 == 0):\n",
    "            five_loss, valid_auc, _ = testingR(network, validloader, criterion)\n",
    "            valid_loss.append(five_loss)\n",
    "            \n",
    "            if valid_auc < best_objective:\n",
    "                best_objective = valid_auc\n",
    "                best_model = network\n",
    "                torch.save(best_model, f'{filename}_NeuralNetworkR_{epoch}.ckpt')\n",
    "                print(f'Model in epoch {i+1} is saved.\\n')\n",
    "                stop_trigger = 0\n",
    "            else:\n",
    "                stop_trigger += 1\n",
    "                print('')\n",
    "                \n",
    "            if stop_trigger == early_stop:\n",
    "                print(f'Training Finished at epoch {i+1}.')\n",
    "                return network, train_loss, valid_loss\n",
    "      \n",
    "    return network, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T08:55:22.919907Z",
     "start_time": "2021-11-07T08:55:22.892001Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingC(network, dataloader, criterion):\n",
    "    \n",
    "    network.eval()\n",
    "    total_loss = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = network(x)\n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        TP += torch.dot((predicted == y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "        TN += torch.dot((predicted == y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "        FN += torch.dot((predicted != y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "        FP += torch.dot((predicted != y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "        total_loss += loss.item()*len(y)\n",
    "        \n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    aging = (TP + FP) / (TP + TN + FP + FN)\n",
    "    if (TP + FP) != 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if aging != 0:\n",
    "        efficiency = recall / aging\n",
    "        score = score1(recall, aging)\n",
    "    else:\n",
    "        efficiency = 0\n",
    "        score = 0\n",
    "        \n",
    "    print(f'Test Loss = {total_loss / (TP + TN + FP + FN)}, Recall = {recall}, Aging Rate = {aging}, Efficiency = {recall / (aging + 1e-8)}')\n",
    "    \n",
    "    valid_objective = recall - 0.5*aging\n",
    "    table = pd.Series({'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN, 'Precision': precision, 'Recall': recall, \n",
    "                       'Aging Rate': aging, 'Efficiency': efficiency, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    return total_loss, valid_objective, table\n",
    "\n",
    "\n",
    "def testingR(network, dataloader, criterion):\n",
    "    \n",
    "    network.eval()   \n",
    "    total_loss = 0\n",
    "    predict_vector = torch.tensor([0])\n",
    "    y_vector = torch.tensor([0])\n",
    "    for x, y in dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y = y.unsqueeze(1)\n",
    "        output = network(x)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.item()*len(y)\n",
    "        \n",
    "        predict_vector = torch.cat((predict_vector, output.data[:,0]), axis = 0)\n",
    "        y_vector = torch.cat((y_vector, y[:,0]), axis = 0)\n",
    "    result_df = pd.DataFrame(dict(predict = predict_vector, truth = y_vector))\n",
    "    pr_matrix = PR_matrix(result_df.iloc[1:, :])\n",
    "    best_data, best_thres = best_threshold(pr_matrix, target = 'Recall', threshold = 0.7)\n",
    "    auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "    recall = best_data['Recall'].values[0]\n",
    "    aging = best_data['Aging Rate'].values[0]\n",
    "    precision = best_data['Precision'].values[0]\n",
    "    efficiency = best_data['Efficiency'].values[0]\n",
    "    score = best_data['Score'].values[0]\n",
    "    TP = best_data['TP'].values[0]\n",
    "    FP = best_data['FP'].values[0]\n",
    "    TN = best_data['TN'].values[0]\n",
    "    FN = best_data['FN'].values[0]\n",
    "        \n",
    "    print(f'Test Loss = {total_loss}, Recall = {recall}, Aging Rate = {aging}, Efficiency = {efficiency}')\n",
    "    \n",
    "    valid_objective = auc\n",
    "    table = pd.Series({'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN, 'Precision': precision, 'Recall': recall,\n",
    "                       'Aging Rate': aging,'Efficiency': efficiency, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    return total_loss, valid_objective, table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:32:14.731617Z",
     "start_time": "2021-11-07T06:32:14.718652Z"
    }
   },
   "outputs": [],
   "source": [
    "def runall_nn(train_x, train_y, test_x, test_y, n_epoch, batch_size, model, optimizer, criterion, filename,\n",
    "              train_ratio, early_stop, mode):\n",
    "    \n",
    "    set_name = list(train_x.keys())\n",
    "    result_table = pd.DataFrame()\n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    for num, i in enumerate(tqdm(set_name)):\n",
    "        print(f'\\nStarting training Dataset {num}:')\n",
    "        \n",
    "        # data preparation\n",
    "        train_ratio = train_ratio\n",
    "        train_data = RunhistSet(train_x[i], train_y[i])\n",
    "        test_data = RunhistSet(test_x, test_y)\n",
    "        train_size = int(len(train_data)*train_ratio)\n",
    "        valid_size = len(train_data) - train_size\n",
    "        train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "        valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = False)\n",
    "        test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "        \n",
    "        # training\n",
    "        if mode == 'C':\n",
    "            done_model, train_loss, valid_loss = trainingC(network = model, \n",
    "                                                           trainloader = train_loader, \n",
    "                                                           validloader = valid_loader, \n",
    "                                                           optimizer = optimizer, \n",
    "                                                           criterion = criterion, \n",
    "                                                           epoch = n_epoch, \n",
    "                                                           filename = filename, \n",
    "                                                           early_stop = early_stop)\n",
    "        elif mode == 'R':\n",
    "            done_model, train_loss, valid_loss = trainingR(network = model, \n",
    "                                                           trainloader = train_loader, \n",
    "                                                           validloader = valid_loader, \n",
    "                                                           optimizer = optimizer, \n",
    "                                                           criterion = criterion, \n",
    "                                                           epoch = n_epoch, \n",
    "                                                           filename = filename, \n",
    "                                                           early_stop = early_stop)\n",
    "        train_dict[i] = train_loss\n",
    "        valid_dict[i] = valid_loss\n",
    "        \n",
    "        # testing\n",
    "        if mode == 'C':\n",
    "            _, _, table = testingC(done_model, test_loader, criterion)\n",
    "        elif mode == 'R':\n",
    "            _, _, table = testingR(done_model, test_loader, criterion)\n",
    "        result_table = pd.concat([result_table, table], axis = 0).rename({0: f'dataset {num}'})\n",
    "    loss_dict = dict(train = train_dict, valid = valid_dict)\n",
    "        \n",
    "    return result_table, loss_dict\n",
    "\n",
    "\n",
    "def loss_plot(train_loss, valid_loss, num_row, num_col):\n",
    "    \n",
    "    fig , axes = plt.subplots(num_row, num_col, sharex = False, sharey = False, figsize = (num_row*8 + 1, num_col*6))\n",
    "    plt.suptitle('Training & Validation Loss Curve', y = 0.94, fontsize = 30)\n",
    "    for row in range(num_row):\n",
    "        for col in range(num_col):\n",
    "            \n",
    "            index = num_col*row + col\n",
    "            if index < len(train_loss):\n",
    "                \n",
    "                train = train_loss[f'set{index}']\n",
    "                valid = valid_loss[f'set{index}']\n",
    "                axes[row, col].plot(range(len(train)), train, 'b-', linewidth = 5, label = 'train')\n",
    "                axes[row, col].plot(range(4, len(train)+1, 5), valid, 'r-', linewidth = 5, label = 'valid')\n",
    "                axes[row, col].set_xlabel('Epoch')\n",
    "                axes[row, col].set_ylabel('Total Loss')\n",
    "                axes[row, col].set_title(f'dataset {index}')\n",
    "                axes[row, col].legend(loc = 'upper right', fancybox = True, prop = dict(size = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:32:30.450715Z",
     "start_time": "2021-11-07T06:32:25.233978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Month 1:\n",
      "\n",
      "Dimension of dataset 0 : (17735, 84)  balance ratio: 1181.0\n",
      "Dimension of dataset 1 : (296, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (420, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (328, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (300, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (298, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (370, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (300, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (300, 84)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (165, 84)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 2:\n",
      "\n",
      "Dimension of dataset 0 : (39009, 90)  balance ratio: 533.0\n",
      "Dimension of dataset 1 : (1460, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (1908, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1604, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1460, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (1465, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (1670, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (1460, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (1460, 90)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (803, 90)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 3:\n",
      "\n",
      "Dimension of dataset 0 : (60396, 92)  balance ratio: 443.0\n",
      "Dimension of dataset 1 : (2736, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (3816, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2986, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2720, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2716, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (3293, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2720, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2720, 92)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1496, 92)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 4:\n",
      "\n",
      "Dimension of dataset 0 : (57743, 99)  balance ratio: 433.0\n",
      "Dimension of dataset 1 : (2600, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (3066, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2920, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2660, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2643, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2880, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2660, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2660, 99)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1463, 99)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 5:\n",
      "\n",
      "Dimension of dataset 0 : (48649, 93)  balance ratio: 415.0\n",
      "Dimension of dataset 1 : (2316, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (3178, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2566, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2338, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2348, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2783, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2340, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2340, 93)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1287, 93)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 6:\n",
      "\n",
      "Dimension of dataset 0 : (7792, 71)  balance ratio: 1112.0\n",
      "Dimension of dataset 1 : (140, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (164, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (152, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (140, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (138, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (154, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (140, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (140, 71)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (77, 71)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Combined training data:\n",
      "\n",
      "Dimension of dataset 0 : (231324, 134)  balance ratio: 480.0\n",
      "Dimension of dataset 1 : (9548, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (12552, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (10556, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (9618, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (9608, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (11150, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (9620, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (9620, 134)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (5291, 134)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      " Dimension of testing data: (43243, 134)\n"
     ]
    }
   ],
   "source": [
    "### training data ### \n",
    "training_month = range(1, 7)\n",
    "\n",
    "data_dict, trainset_x, trainset_y = multiple_month(training_month, num_set = 10, filename = 'dataset')\n",
    "\n",
    "print('\\nCombined training data:\\n')\n",
    "run_train = multiple_set(num_set = 10)\n",
    "run_train_x, run_train_y = train_set(run_train, num_set = 10)\n",
    "\n",
    "### testing data ###\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of testing data:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:15.452762Z",
     "start_time": "2021-11-02T02:49:15.422734Z"
    }
   },
   "outputs": [],
   "source": [
    "##### data preparation #####\n",
    "train_data = RunhistSet(run_train_x['set7'], run_train_y['set7'])\n",
    "test_data = RunhistSet(run_test_x, run_test_y)\n",
    "train_ratio = 0.75\n",
    "train_size = int(len(train_data)*train_ratio)\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = 64, shuffle = False)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:13:32.498950Z",
     "start_time": "2021-11-02T02:13:25.418192Z"
    }
   },
   "outputs": [],
   "source": [
    "##### model preparation #####\n",
    "# hyperparameter: learning rate, weight decay, weight\n",
    "modelC = NeuralNetworkC().to(device)\n",
    "optimizerC = torch.optim.Adam(modelC.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "criterionC = nn.CrossEntropyLoss(weight = torch.tensor([0.5, 0.5])).to(device)\n",
    "\n",
    "### label smoothing ###\n",
    "#criterionC = LabelSmoothingLoss(classes = 2, smoothing = 0.2)\n",
    "\n",
    "##### training #####\n",
    "done_modelC, train_lossC, valid_lossC = trainingC(network = modelC, \n",
    "                                                  trainloader = train_loader, \n",
    "                                                  validloader = valid_loader, \n",
    "                                                  optimizer = optimizerC, \n",
    "                                                  criterion = criterionC, \n",
    "                                                  epoch = 150, \n",
    "                                                  filename = 'tamama',\n",
    "                                                  early_stop = 10)\n",
    "\n",
    "##### testing #####\n",
    "_, _, result_tableC = testingC(done_modelC, test_loader, criterionC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:51:36.859170Z",
     "start_time": "2021-11-02T02:51:26.514487Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea820ae21ecc4cda888c41afad08269a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.43432676792144775 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.63361662] ,   Aging Rate: [0.54517375]\n",
      "Epoch 1: Train Loss = 846.9388972967863, AUC = 0.3952542841727463, Recall(0.7) = [0.70041752], Aging Rate = [0.54517375]\n",
      "Best Threshold: 0.5488395690917969 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.94908062] ,   Aging Rate: [0.36396396]\n",
      "Epoch 2: Train Loss = 518.1432443782687, AUC = 0.27935565393543216, Recall(0.7) = [0.70041752], Aging Rate = [0.36396396]\n",
      "Best Threshold: 0.6151427626609802 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.98676471] ,   Aging Rate: [0.35006435]\n",
      "Epoch 3: Train Loss = 382.7067674770951, AUC = 0.26039226766551476, Recall(0.7) = [0.70041752], Aging Rate = [0.35006435]\n",
      "Best Threshold: 0.6658647656440735 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.99628805] ,   Aging Rate: [0.34671815]\n",
      "Epoch 4: Train Loss = 303.71694777160883, AUC = 0.2535262898694735, Recall(0.7) = [0.70041752], Aging Rate = [0.34671815]\n",
      "Best Threshold: 0.6969634890556335 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.9992554] ,   Aging Rate: [0.34568855]\n",
      "Epoch 5: Train Loss = 252.0662296190858, AUC = 0.24894238948501232, Recall(0.7) = [0.70041752], Aging Rate = [0.34568855]\n",
      "Best Threshold: 0.7454460263252258 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 61.76131880655885, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Best Threshold: 0.8326825499534607 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 6: Train Loss = 113.00535123050213, AUC = 0.2466625958288485, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.8661829233169556 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 7: Train Loss = 83.7673932556063, AUC = 0.24659394711127025, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.8891046047210693 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 8: Train Loss = 66.52354558743536, AUC = 0.2465900512074706, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9067725539207458 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 9: Train Loss = 52.79196160752326, AUC = 0.24658951381444802, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9172561168670654 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 10: Train Loss = 43.28453846462071, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.8625966310501099 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 27.10233525931835, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Best Threshold: 0.9286236763000488 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 11: Train Loss = 35.48507585376501, AUC = 0.2465895138451325, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.936020016670227 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 12: Train Loss = 30.113216229714453, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9405725002288818 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 13: Train Loss = 26.774640945717692, AUC = 0.24658951384513253, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9463627934455872 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 14: Train Loss = 23.633905421942472, AUC = 0.24658951384513256, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9497013688087463 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 15: Train Loss = 20.77750749886036, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9149306416511536 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 18.42299883440137, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Best Threshold: 0.9512624144554138 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 16: Train Loss = 19.11801414284855, AUC = 0.24658951382211913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9547045230865479 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 17: Train Loss = 17.27199205569923, AUC = 0.24658951383746147, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9553464651107788 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 18: Train Loss = 15.861247757915407, AUC = 0.24658951382979033, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9581364393234253 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 19: Train Loss = 15.471164492890239, AUC = 0.2465895138278725, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9609731435775757 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 20: Train Loss = 13.97874150564894, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9355906844139099 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 15.489769376814365, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Best Threshold: 0.962775468826294 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 21: Train Loss = 12.951939071062952, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9628905057907104 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 22: Train Loss = 12.491330340970308, AUC = 0.24658951386047473, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9642407894134521 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 23: Train Loss = 12.697923412546515, AUC = 0.24658951383554364, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9664119482040405 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 24: Train Loss = 11.16673456528224, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9679551720619202 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 25: Train Loss = 10.539420448243618, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9510689973831177 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 13.779261209070683, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9684223532676697 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 26: Train Loss = 9.850985727272928, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.969421923160553 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 27: Train Loss = 9.349297927110456, AUC = 0.24658951382211913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9696387052536011 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 28: Train Loss = 9.763006628141738, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9712596535682678 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 29: Train Loss = 9.336533795576543, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9724233150482178 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 30: Train Loss = 9.217923182761297, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9424140453338623 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 15.832925887778401, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9732487797737122 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 31: Train Loss = 9.04700749879703, AUC = 0.2465895138489681, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.9722369313240051 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 32: Train Loss = 9.031352465739474, AUC = 0.24658951385088584, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9751859903335571 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 33: Train Loss = 8.162652850616723, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9743627309799194 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 34: Train Loss = 8.170216631609946, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9755672216415405 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 35: Train Loss = 8.700050393119454, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9651535153388977 \n",
      "\n",
      "Recall: [0.70178042] ,   Precision: [1.] ,   Aging Rate: [0.36525097]\n",
      "Test Loss = 13.113624626770616, Recall = 0.701780415430267, Aging Rate = 0.36525096525096523, Efficiency = 1.9213649851632049\n",
      "\n",
      "Best Threshold: 0.9758872985839844 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 36: Train Loss = 7.982390616089106, AUC = 0.2465895138393792, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9755007028579712 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 37: Train Loss = 8.199290924705565, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9782941937446594 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 38: Train Loss = 8.40997535130009, AUC = 0.24658951383937913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9764113426208496 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 39: Train Loss = 7.424406706704758, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9778127670288086 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 40: Train Loss = 7.111044444609433, AUC = 0.24658951385472144, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.964168906211853 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 14.232608878985047, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9790011048316956 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 41: Train Loss = 7.259600186836906, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9789282083511353 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 42: Train Loss = 6.919962025131099, AUC = 0.24658951384129696, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9800938963890076 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 43: Train Loss = 6.518845319515094, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9788238406181335 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 44: Train Loss = 7.142630195594393, AUC = 0.24658951381444805, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9802493453025818 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 45: Train Loss = 6.322114697191864, AUC = 0.246589513841297, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.972071647644043 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 12.548936162143946, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9809168577194214 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 46: Train Loss = 6.771576856262982, AUC = 0.24658951382211913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9804174900054932 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 47: Train Loss = 6.613125075935386, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9801037311553955 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 48: Train Loss = 6.758818219532259, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9798376560211182 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 49: Train Loss = 8.296786832506768, AUC = 0.24658951382787248, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9797123670578003 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 50: Train Loss = 7.790700750192627, AUC = 0.2465895138451325, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9643718004226685 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 12.645020667463541, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9815775156021118 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 51: Train Loss = 7.510928211035207, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9814843535423279 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 52: Train Loss = 6.609729846473783, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9812673926353455 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 53: Train Loss = 7.399489681236446, AUC = 0.24658951384513247, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9807630777359009 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 54: Train Loss = 7.4163997187279165, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9813729524612427 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 55: Train Loss = 6.600580210331827, AUC = 0.246589513814448, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.979337751865387 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 13.42629144154489, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9830928444862366 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 56: Train Loss = 6.32335099845659, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9817944169044495 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 57: Train Loss = 7.089116318617016, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9819741249084473 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 58: Train Loss = 6.890363401733339, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.982314944267273 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 59: Train Loss = 7.109322623582557, AUC = 0.24658951385280364, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9832709431648254 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 60: Train Loss = 6.210180180845782, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9742255806922913 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 13.452005784958601, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9822063446044922 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 61: Train Loss = 6.069727279245853, AUC = 0.24658951386047478, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9822067022323608 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 62: Train Loss = 6.2594345971010625, AUC = 0.24658951383170802, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.9831069707870483 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 63: Train Loss = 5.964000734151341, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.982894778251648 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 64: Train Loss = 6.6053667610976845, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9815890789031982 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 65: Train Loss = 5.97409304860048, AUC = 0.24658951382787245, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9652794599533081 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 14.8444560226053, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9824767112731934 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 66: Train Loss = 6.472427227767184, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9832783937454224 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 67: Train Loss = 6.716100656194612, AUC = 0.24658951382979025, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9822962284088135 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 68: Train Loss = 6.012331276433542, AUC = 0.24658951383746144, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9817267060279846 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 69: Train Loss = 7.817374554928392, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9823286533355713 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 70: Train Loss = 7.8439161505084485, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9587316513061523 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 15.946332313120365, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Training Finished at epoch 70.\n",
      "Best Threshold: 0.3367293179035187 \n",
      "\n",
      "Recall: [0.71153846] ,   Precision: [0.00133314] ,   Aging Rate: [0.57043614]\n",
      "Test Loss = 13083.386966064572, Recall = 0.7115384615384616, Aging Rate = 0.5704361409133885, Efficiency = 1.2473586620916735\n"
     ]
    }
   ],
   "source": [
    "##### model preparation #####\n",
    "# hyperparameter: learning rate, weight decay, weight\n",
    "modelR = NeuralNetworkR().to(device)\n",
    "optimizerR = torch.optim.Adam(modelR.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterionR = nn.MSELoss().to(device)\n",
    "\n",
    "##### training #####\n",
    "done_modelR, train_lossR, valid_lossR = trainingR(network = modelR, \n",
    "                                                  trainloader = train_loader, \n",
    "                                                  validloader = valid_loader, \n",
    "                                                  optimizer = optimizerR, \n",
    "                                                  criterion = criterionR, \n",
    "                                                  epoch = 150, \n",
    "                                                  filename = 'tamama',\n",
    "                                                  early_stop = 10)\n",
    "\n",
    "##### testing #####\n",
    "_, _, result_tableR = testingR(done_modelR, test_loader, criterionR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T09:03:27.574750Z",
     "start_time": "2021-11-07T08:58:33.786887Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3028b63805364580a3483d9db1bce8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training Dataset 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0531eb1e8f6d45ef9b215a1a3659b02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.05195593168263867, Recall = 0.0, Aging Rate = 0.0002651403803035281\n",
      "Epoch 2: Train Loss = 0.04930674600282732, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 3: Train Loss = 0.04900512052182173, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 4: Train Loss = 0.0491876664311161, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 5: Train Loss = 0.04875879685507351, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.041577422972488655, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 6: Train Loss = 0.048394555800856456, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 7: Train Loss = 0.04857432504067837, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 8: Train Loss = 0.04851370183075093, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 9: Train Loss = 0.04840678849536985, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 10: Train Loss = 0.048229019748548575, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.043955876181587424, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 11: Train Loss = 0.04827040197834597, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 12: Train Loss = 0.048400710281342046, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 13: Train Loss = 0.04816529492064103, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 14: Train Loss = 0.04811800692550207, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 15: Train Loss = 0.04802183181457539, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.04126852446576111, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 16: Train Loss = 0.04788352251073577, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 17: Train Loss = 0.0479558643422154, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 18: Train Loss = 0.048270662415352654, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 19: Train Loss = 0.04807968974949981, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 20: Train Loss = 0.048111392075084115, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.04171009249169057, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 21: Train Loss = 0.04807114608397179, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 22: Train Loss = 0.0480374590035751, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 23: Train Loss = 0.04799852701154475, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 24: Train Loss = 0.04789073053189415, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 25: Train Loss = 0.048066658199723146, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.04126869160375661, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.04793775462705761, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 27: Train Loss = 0.04795342916914513, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 28: Train Loss = 0.04778390474337955, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 29: Train Loss = 0.04806491901098386, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 30: Train Loss = 0.047876914913530584, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.04266471036845394, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.04807430647846451, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 32: Train Loss = 0.04797442345300033, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 33: Train Loss = 0.047837763499495686, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 34: Train Loss = 0.04788416146521946, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 35: Train Loss = 0.04775212156388681, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.043499894645406156, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.047911638621690206, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 37: Train Loss = 0.04783382251238635, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 38: Train Loss = 0.047874067906359924, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 39: Train Loss = 0.047933250211085764, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 40: Train Loss = 0.04781632416854732, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.04357019766480562, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.04810783802660091, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 42: Train Loss = 0.04808358372174197, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 43: Train Loss = 0.04788006437372093, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 44: Train Loss = 0.047777040494507886, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 45: Train Loss = 0.047801721727959434, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.041888558131890595, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.04784782034900153, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 47: Train Loss = 0.047878745336871635, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 48: Train Loss = 0.047813605853576656, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 49: Train Loss = 0.0479119186640589, Recall = 0.0, Aging Rate = 0.0\n",
      "Epoch 50: Train Loss = 0.04777275635765228, Recall = 0.0, Aging Rate = 0.0\n",
      "Test Loss = 0.04329899019452183, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Training Finished at epoch 50.\n",
      "Test Loss = 0.028526900719461092, Recall = 0.0, Aging Rate = 0.0, Efficiency = 0.0\n",
      "\n",
      "Starting training Dataset 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f213d4e641d24acca5ec674c0e0b8f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.768496951441278, Recall = 0.8242595204513399, Aging Rate = 0.8078480659125821\n",
      "Epoch 2: Train Loss = 0.3980089911592521, Recall = 0.9627644569816644, Aging Rate = 0.8031001256807708\n",
      "Epoch 3: Train Loss = 0.3418382134097629, Recall = 0.9596614950634696, Aging Rate = 0.7342549923195084\n",
      "Epoch 4: Train Loss = 0.32059768805322725, Recall = 0.9590973201692524, Aging Rate = 0.7142857142857143\n",
      "Epoch 5: Train Loss = 0.30212615921650166, Recall = 0.9616361071932299, Aging Rate = 0.6972489875715682\n",
      "Test Loss = 0.28433863756219163, Recall = 0.9715215622457283, Aging Rate = 0.6987850858818601, Efficiency = 1.3903009207998023\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2766706375515739, Recall = 0.9675599435825106, Aging Rate = 0.6775590001396453\n",
      "Epoch 7: Train Loss = 0.26425977915030185, Recall = 0.9689703808180536, Aging Rate = 0.6661080854629242\n",
      "Epoch 8: Train Loss = 0.2529164185372166, Recall = 0.9712270803949224, Aging Rate = 0.6584275939114649\n",
      "Epoch 9: Train Loss = 0.24322374981802414, Recall = 0.9703808180535967, Aging Rate = 0.6482334869431644\n",
      "Epoch 10: Train Loss = 0.23588928279389285, Recall = 0.9746121297602257, Aging Rate = 0.6434855467113532\n",
      "Test Loss = 0.24075658942147884, Recall = 0.9593165174938975, Aging Rate = 0.6124842899036448, Efficiency = 1.5662711969022158\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.22951458840459407, Recall = 0.9760225669957687, Aging Rate = 0.6374807987711214\n",
      "Epoch 12: Train Loss = 0.22499097304801305, Recall = 0.9791255289139633, Aging Rate = 0.6345482474514733\n",
      "Epoch 13: Train Loss = 0.22049551075221674, Recall = 0.9771509167842031, Aging Rate = 0.629102080714984\n",
      "Epoch 14: Train Loss = 0.2174915483986394, Recall = 0.9779971791255289, Aging Rate = 0.6261695293953359\n",
      "Epoch 15: Train Loss = 0.21333561402561776, Recall = 0.9833568406205924, Aging Rate = 0.6284038542103059\n",
      "Test Loss = 0.2207448854324555, Recall = 0.9585028478437754, Aging Rate = 0.6028487641390867, Efficiency = 1.589955705247289\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.21233224127982153, Recall = 0.9813822284908321, Aging Rate = 0.6242144951822372\n",
      "Epoch 17: Train Loss = 0.21034529642042643, Recall = 0.9827926657263751, Aging Rate = 0.6233766233766234\n",
      "Epoch 18: Train Loss = 0.20793608953842585, Recall = 0.9833568406205924, Aging Rate = 0.6208630079597821\n",
      "Epoch 19: Train Loss = 0.20561402659686878, Recall = 0.9825105782792666, Aging Rate = 0.6173718754363916\n",
      "Epoch 20: Train Loss = 0.2048123649583817, Recall = 0.9842031029619182, Aging Rate = 0.6215612344644603\n",
      "Test Loss = 0.21062135313737826, Recall = 0.9820992676973149, Aging Rate = 0.6162547130289066, Efficiency = 1.5936579972487248\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.20413696428173297, Recall = 0.9825105782792666, Aging Rate = 0.6155564865242285\n",
      "Epoch 22: Train Loss = 0.20217093514873402, Recall = 0.983638928067701, Aging Rate = 0.6186286831448122\n",
      "Epoch 23: Train Loss = 0.2015234549407469, Recall = 0.9833568406205924, Aging Rate = 0.6166736489317135\n",
      "Epoch 24: Train Loss = 0.19941381532467373, Recall = 0.9858956276445698, Aging Rate = 0.6186286831448122\n",
      "Epoch 25: Train Loss = 0.1973574814883345, Recall = 0.9870239774330042, Aging Rate = 0.6147186147186147\n",
      "Test Loss = 0.20428327758491013, Recall = 0.9820992676973149, Aging Rate = 0.6225387515710097, Efficiency = 1.5775712747892758\n",
      "\n",
      "Epoch 26: Train Loss = 0.19699801282166202, Recall = 0.9881523272214386, Aging Rate = 0.6173718754363916\n",
      "Epoch 27: Train Loss = 0.1971472932454711, Recall = 0.9861777150916784, Aging Rate = 0.6131825164083229\n",
      "Epoch 28: Train Loss = 0.19674674113058807, Recall = 0.9842031029619182, Aging Rate = 0.6127635805055159\n",
      "Epoch 29: Train Loss = 0.1952900532495208, Recall = 0.985049365303244, Aging Rate = 0.6123446446027091\n",
      "Epoch 30: Train Loss = 0.1958392958741101, Recall = 0.9847672778561354, Aging Rate = 0.6136014523111297\n",
      "Test Loss = 0.2029582196179423, Recall = 0.9788445890968267, Aging Rate = 0.5994972769166318, Efficiency = 1.6327756779872604\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.19337235372828265, Recall = 0.9873060648801129, Aging Rate = 0.609412093283061\n",
      "Epoch 32: Train Loss = 0.1930010254540235, Recall = 0.9842031029619182, Aging Rate = 0.6074570590699623\n",
      "Epoch 33: Train Loss = 0.19207787526010087, Recall = 0.9856135401974612, Aging Rate = 0.6096913838849323\n",
      "Epoch 34: Train Loss = 0.19251120216585774, Recall = 0.9867418899858956, Aging Rate = 0.6042452171484429\n",
      "Epoch 35: Train Loss = 0.19198821381622977, Recall = 0.9839210155148096, Aging Rate = 0.6108085462924172\n",
      "Test Loss = 0.1994912698521592, Recall = 0.9820992676973149, Aging Rate = 0.6032677000418936, Efficiency = 1.6279659118985725\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.19164847139455873, Recall = 0.9833568406205924, Aging Rate = 0.6056416701577992\n",
      "Epoch 37: Train Loss = 0.18993753245374276, Recall = 0.9870239774330042, Aging Rate = 0.607596704370898\n",
      "Epoch 38: Train Loss = 0.1897804261256889, Recall = 0.9844851904090268, Aging Rate = 0.6063398966624773\n",
      "Epoch 39: Train Loss = 0.18987911995180393, Recall = 0.9881523272214386, Aging Rate = 0.6062002513615417\n",
      "Epoch 40: Train Loss = 0.18848920152925144, Recall = 0.9870239774330042, Aging Rate = 0.6042452171484429\n",
      "Test Loss = 0.19898943794687574, Recall = 0.9804719283970708, Aging Rate = 0.5898617511520737, Efficiency = 1.662206287931068\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.18744220352092947, Recall = 0.9870239774330042, Aging Rate = 0.6042452171484429\n",
      "Epoch 42: Train Loss = 0.18802187191950612, Recall = 0.9867418899858956, Aging Rate = 0.6059209607596704\n",
      "Epoch 43: Train Loss = 0.18628150253609863, Recall = 0.9881523272214386, Aging Rate = 0.6039659265465717\n",
      "Epoch 44: Train Loss = 0.18702993853151123, Recall = 0.9889985895627644, Aging Rate = 0.6073174137690267\n",
      "Epoch 45: Train Loss = 0.1856656316883686, Recall = 0.9875881523272214, Aging Rate = 0.6028487641390867\n",
      "Test Loss = 0.1925889217846055, Recall = 0.9861676159479251, Aging Rate = 0.611646418098031, Efficiency = 1.6123164799874676\n",
      "\n",
      "Epoch 46: Train Loss = 0.18529756339530906, Recall = 0.9881523272214386, Aging Rate = 0.6042452171484429\n",
      "Epoch 47: Train Loss = 0.18570654868305503, Recall = 0.9858956276445698, Aging Rate = 0.6025694735372155\n",
      "Epoch 48: Train Loss = 0.18472113210493052, Recall = 0.9889985895627644, Aging Rate = 0.6015919564306661\n",
      "Epoch 49: Train Loss = 0.18481211483503518, Recall = 0.9867418899858956, Aging Rate = 0.6003351487222455\n",
      "Epoch 50: Train Loss = 0.18491751154305433, Recall = 0.9878702397743301, Aging Rate = 0.6055020248568636\n",
      "Test Loss = 0.2046054105255261, Recall = 0.9601301871440195, Aging Rate = 0.5609551738583997, Efficiency = 1.7115987422381722\n",
      "\n",
      "Epoch 51: Train Loss = 0.1848467047250965, Recall = 0.9864598025387871, Aging Rate = 0.6013126658287948\n",
      "Epoch 52: Train Loss = 0.18433211278963682, Recall = 0.9875881523272214, Aging Rate = 0.6015919564306661\n",
      "Epoch 53: Train Loss = 0.18376218768000818, Recall = 0.9873060648801129, Aging Rate = 0.6022901829353442\n",
      "Epoch 54: Train Loss = 0.1826180671741636, Recall = 0.9898448519040902, Aging Rate = 0.5990783410138248\n",
      "Epoch 55: Train Loss = 0.1828711629038732, Recall = 0.9895627644569817, Aging Rate = 0.5971233068007261\n",
      "Test Loss = 0.18863341553744584, Recall = 0.9894222945484134, Aging Rate = 0.6204440720569753, Efficiency = 1.5947001884007894\n",
      "\n",
      "Epoch 56: Train Loss = 0.18222507999864243, Recall = 0.9875881523272214, Aging Rate = 0.6010333752269236\n",
      "Epoch 57: Train Loss = 0.1817088010128509, Recall = 0.9895627644569817, Aging Rate = 0.6024298282362799\n",
      "Epoch 58: Train Loss = 0.18341347271795796, Recall = 0.9861777150916784, Aging Rate = 0.5982404692082112\n",
      "Epoch 59: Train Loss = 0.1808555216625106, Recall = 0.9904090267983074, Aging Rate = 0.598659405111018\n",
      "Epoch 60: Train Loss = 0.18103991716896217, Recall = 0.9889985895627644, Aging Rate = 0.5994972769166318\n",
      "Test Loss = 0.18861079945767031, Recall = 0.9829129373474369, Aging Rate = 0.5953079178885631, Efficiency = 1.6511000295823883\n",
      "\n",
      "Epoch 61: Train Loss = 0.18130214499238648, Recall = 0.9878702397743301, Aging Rate = 0.5987990504119537\n",
      "Epoch 62: Train Loss = 0.18239706967350092, Recall = 0.9870239774330042, Aging Rate = 0.6014523111297305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: Train Loss = 0.18173790336354387, Recall = 0.9875881523272214, Aging Rate = 0.6000558581203742\n",
      "Epoch 64: Train Loss = 0.18049980472368063, Recall = 0.9881523272214386, Aging Rate = 0.6004747940231812\n",
      "Epoch 65: Train Loss = 0.18039835424933282, Recall = 0.9870239774330042, Aging Rate = 0.596425080296048\n",
      "Test Loss = 0.18604276190380398, Recall = 0.9877949552481693, Aging Rate = 0.6162547130289066, Efficiency = 1.602900421424981\n",
      "\n",
      "Epoch 66: Train Loss = 0.179490212173689, Recall = 0.9892806770098731, Aging Rate = 0.5987990504119537\n",
      "Epoch 67: Train Loss = 0.18045245186940634, Recall = 0.9887165021156559, Aging Rate = 0.5990783410138248\n",
      "Epoch 68: Train Loss = 0.17880802007381386, Recall = 0.9878702397743301, Aging Rate = 0.5936321742773356\n",
      "Epoch 69: Train Loss = 0.17896448011680316, Recall = 0.9901269393511989, Aging Rate = 0.5969836614997905\n",
      "Epoch 70: Train Loss = 0.17872441616941973, Recall = 0.9887165021156559, Aging Rate = 0.5948889819857561\n",
      "Test Loss = 0.18539340726041773, Recall = 0.9861676159479251, Aging Rate = 0.6045245077503142, Efficiency = 1.6313111991187101\n",
      "\n",
      "Epoch 71: Train Loss = 0.17896863313555933, Recall = 0.9887165021156559, Aging Rate = 0.5974025974025974\n",
      "Epoch 72: Train Loss = 0.17923989820007571, Recall = 0.9895627644569817, Aging Rate = 0.5989386957128893\n",
      "Epoch 73: Train Loss = 0.17927173832544582, Recall = 0.9904090267983074, Aging Rate = 0.5979611786063399\n",
      "Epoch 74: Train Loss = 0.17840214378714578, Recall = 0.9906911142454161, Aging Rate = 0.5993576316156961\n",
      "Epoch 75: Train Loss = 0.17918919635092043, Recall = 0.9895627644569817, Aging Rate = 0.5958664990923055\n",
      "Test Loss = 0.1858827541213581, Recall = 0.9877949552481693, Aging Rate = 0.5982404692082112, Efficiency = 1.6511670299467947\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.17876903129895794, Recall = 0.9892806770098731, Aging Rate = 0.5958664990923055\n",
      "Epoch 77: Train Loss = 0.17842798534093532, Recall = 0.9904090267983074, Aging Rate = 0.5961457896941768\n",
      "Epoch 78: Train Loss = 0.17880441639621802, Recall = 0.9887165021156559, Aging Rate = 0.5996369222175674\n",
      "Epoch 79: Train Loss = 0.17741853409706757, Recall = 0.9892806770098731, Aging Rate = 0.5950286272866918\n",
      "Epoch 80: Train Loss = 0.1780941502423014, Recall = 0.9889985895627644, Aging Rate = 0.5940511101801424\n",
      "Test Loss = 0.18565941292045005, Recall = 0.9829129373474369, Aging Rate = 0.5911185588604944, Efficiency = 1.6628016596436968\n",
      "\n",
      "Epoch 81: Train Loss = 0.17806448682886714, Recall = 0.9884344146685472, Aging Rate = 0.5947493366848206\n",
      "Epoch 82: Train Loss = 0.17763224934968946, Recall = 0.9895627644569817, Aging Rate = 0.5974025974025974\n",
      "Epoch 83: Train Loss = 0.17757420373968433, Recall = 0.9912552891396332, Aging Rate = 0.5930735930735931\n",
      "Epoch 84: Train Loss = 0.17762619628201748, Recall = 0.9892806770098731, Aging Rate = 0.5953079178885631\n",
      "Epoch 85: Train Loss = 0.1766174736593609, Recall = 0.9904090267983074, Aging Rate = 0.5951682725876274\n",
      "Test Loss = 0.18406524081883394, Recall = 0.985353946297803, Aging Rate = 0.5990783410138248, Efficiency = 1.6447830982880303\n",
      "\n",
      "Epoch 86: Train Loss = 0.17642368124676988, Recall = 0.9909732016925247, Aging Rate = 0.5953079178885631\n",
      "Epoch 87: Train Loss = 0.1769753539238441, Recall = 0.9898448519040902, Aging Rate = 0.596425080296048\n",
      "Epoch 88: Train Loss = 0.17725470967589768, Recall = 0.9887165021156559, Aging Rate = 0.5969836614997905\n",
      "Epoch 89: Train Loss = 0.1771157770150368, Recall = 0.9889985895627644, Aging Rate = 0.5947493366848206\n",
      "Epoch 90: Train Loss = 0.17613954057599926, Recall = 0.9895627644569817, Aging Rate = 0.5894428152492669\n",
      "Test Loss = 0.18355042864125695, Recall = 0.9845402766476811, Aging Rate = 0.604943443653121, Efficiency = 1.6274914137879464\n",
      "\n",
      "Epoch 91: Train Loss = 0.17681216231405927, Recall = 0.9906911142454161, Aging Rate = 0.5968440161988549\n",
      "Epoch 92: Train Loss = 0.17668529387551501, Recall = 0.9906911142454161, Aging Rate = 0.5960061443932412\n",
      "Epoch 93: Train Loss = 0.17628011315012257, Recall = 0.9906911142454161, Aging Rate = 0.5957268537913699\n",
      "Epoch 94: Train Loss = 0.17661996345799025, Recall = 0.9873060648801129, Aging Rate = 0.591956430666108\n",
      "Epoch 95: Train Loss = 0.17660452740732777, Recall = 0.9915373765867419, Aging Rate = 0.5974025974025974\n",
      "Test Loss = 0.18815355310641826, Recall = 0.9764035801464606, Aging Rate = 0.578969417679095, Efficiency = 1.6864510170434266\n",
      "\n",
      "Epoch 96: Train Loss = 0.17689080629588805, Recall = 0.9909732016925247, Aging Rate = 0.5934925289763999\n",
      "Epoch 97: Train Loss = 0.17587971850873838, Recall = 0.9889985895627644, Aging Rate = 0.5932132383745287\n",
      "Epoch 98: Train Loss = 0.17582679274711108, Recall = 0.9895627644569817, Aging Rate = 0.5943304007820137\n",
      "Epoch 99: Train Loss = 0.1744892707817915, Recall = 0.9901269393511989, Aging Rate = 0.5943304007820137\n",
      "Epoch 100: Train Loss = 0.17510795582857333, Recall = 0.9887165021156559, Aging Rate = 0.5906996229576875\n",
      "Test Loss = 0.1824197004192985, Recall = 0.9910496338486574, Aging Rate = 0.6170925848345203, Efficiency = 1.6059982604627063\n",
      "\n",
      "Epoch 101: Train Loss = 0.1753665047631785, Recall = 0.9904090267983074, Aging Rate = 0.5953079178885631\n",
      "Epoch 102: Train Loss = 0.17565377640284702, Recall = 0.9901269393511989, Aging Rate = 0.5940511101801424\n",
      "Epoch 103: Train Loss = 0.17553651692430794, Recall = 0.9912552891396332, Aging Rate = 0.594609691383885\n",
      "Epoch 104: Train Loss = 0.1751762311034255, Recall = 0.9884344146685472, Aging Rate = 0.5902806870548806\n",
      "Epoch 105: Train Loss = 0.17520155240463622, Recall = 0.9906911142454161, Aging Rate = 0.5927943024717218\n",
      "Test Loss = 0.18192317330242952, Recall = 0.9886086248982913, Aging Rate = 0.618349392542941, Efficiency = 1.598786415629532\n",
      "\n",
      "Epoch 106: Train Loss = 0.1752878382912296, Recall = 0.9887165021156559, Aging Rate = 0.5954475631894987\n",
      "Epoch 107: Train Loss = 0.17503225721388685, Recall = 0.9878702397743301, Aging Rate = 0.5930735930735931\n",
      "Epoch 108: Train Loss = 0.1749359017380821, Recall = 0.9895627644569817, Aging Rate = 0.5947493366848206\n",
      "Epoch 109: Train Loss = 0.17554574638642573, Recall = 0.9881523272214386, Aging Rate = 0.5936321742773356\n",
      "Epoch 110: Train Loss = 0.1767840766175102, Recall = 0.9898448519040902, Aging Rate = 0.5926546571707862\n",
      "Test Loss = 0.18149500273739289, Recall = 0.9894222945484134, Aging Rate = 0.6338500209467951, Efficiency = 1.5609722272482853\n",
      "\n",
      "Epoch 111: Train Loss = 0.17488245498724547, Recall = 0.9901269393511989, Aging Rate = 0.5943304007820137\n",
      "Epoch 112: Train Loss = 0.17582084043759105, Recall = 0.9887165021156559, Aging Rate = 0.594190755481078\n",
      "Epoch 113: Train Loss = 0.17535322783271654, Recall = 0.9892806770098731, Aging Rate = 0.5912582041614299\n",
      "Epoch 114: Train Loss = 0.17486989713509213, Recall = 0.9895627644569817, Aging Rate = 0.5958664990923055\n",
      "Epoch 115: Train Loss = 0.17467527422986873, Recall = 0.9895627644569817, Aging Rate = 0.5951682725876274\n",
      "Test Loss = 0.18306214282165076, Recall = 0.9829129373474369, Aging Rate = 0.5940511101801424, Efficiency = 1.6545931889655798\n",
      "\n",
      "Epoch 116: Train Loss = 0.17519497663935474, Recall = 0.9881523272214386, Aging Rate = 0.5932132383745287\n",
      "Epoch 117: Train Loss = 0.1745295652571112, Recall = 0.9915373765867419, Aging Rate = 0.5932132383745287\n",
      "Epoch 118: Train Loss = 0.17400061303452896, Recall = 0.9892806770098731, Aging Rate = 0.5939114648792068\n",
      "Epoch 119: Train Loss = 0.17404283679925003, Recall = 0.9904090267983074, Aging Rate = 0.5932132383745287\n",
      "Epoch 120: Train Loss = 0.17453051831964972, Recall = 0.9892806770098731, Aging Rate = 0.5918167853651725\n",
      "Test Loss = 0.17990733484593605, Recall = 0.9861676159479251, Aging Rate = 0.6011730205278593, Efficiency = 1.6404056168022403\n",
      "\n",
      "Epoch 121: Train Loss = 0.1741794858285459, Recall = 0.9887165021156559, Aging Rate = 0.5936321742773356\n",
      "Epoch 122: Train Loss = 0.1740627662668746, Recall = 0.9892806770098731, Aging Rate = 0.5904203323558163\n",
      "Epoch 123: Train Loss = 0.17434017938486177, Recall = 0.9887165021156559, Aging Rate = 0.5930735930735931\n",
      "Epoch 124: Train Loss = 0.17503567881473298, Recall = 0.9898448519040902, Aging Rate = 0.5971233068007261\n",
      "Epoch 125: Train Loss = 0.17390328661104668, Recall = 0.9887165021156559, Aging Rate = 0.5905599776567518\n",
      "Test Loss = 0.1810737891356017, Recall = 0.9877949552481693, Aging Rate = 0.5982404692082112, Efficiency = 1.6511670299467947\n",
      "\n",
      "Training Finished at epoch 125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 1.11010107373705, Recall = 0.5405405405405406, Aging Rate = 0.5856439192470457, Efficiency = 0.9229849632958819\n",
      "\n",
      "Starting training Dataset 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4bab0926f14fdc890277649908822f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.35246378170701886, Recall = 0.9559011504047721, Aging Rate = 0.7701295942213724\n",
      "Epoch 2: Train Loss = 0.32014235101018157, Recall = 0.9737963357477631, Aging Rate = 0.7724665391969407\n",
      "Epoch 3: Train Loss = 0.31180031511189454, Recall = 0.9782701320835109, Aging Rate = 0.759613341831315\n",
      "Epoch 4: Train Loss = 0.30949913189115863, Recall = 0.9789092458457606, Aging Rate = 0.7594008922880816\n",
      "Epoch 5: Train Loss = 0.30603563069461487, Recall = 0.9821048146570089, Aging Rate = 0.7625876354365838\n",
      "Test Loss = 0.2924421656048169, Recall = 0.9898862199747156, Aging Rate = 0.7820267686424475, Efficiency = 1.265795810334142\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.28332789335023933, Recall = 0.9853003834682573, Aging Rate = 0.7382621627363501\n",
      "Epoch 7: Train Loss = 0.2759911459938898, Recall = 0.985939497230507, Aging Rate = 0.7233906947100064\n",
      "Epoch 8: Train Loss = 0.2701590016614056, Recall = 0.9857264593097571, Aging Rate = 0.7179732313575525\n",
      "Epoch 9: Train Loss = 0.2652909717211329, Recall = 0.9865786109927567, Aging Rate = 0.716167410240068\n",
      "Epoch 10: Train Loss = 0.2615358524193854, Recall = 0.9880698764380059, Aging Rate = 0.7099001487146802\n",
      "Test Loss = 0.2619541419433742, Recall = 0.9873577749683944, Aging Rate = 0.7332695984703633, Efficiency = 1.3465139746185193\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.2571834938156268, Recall = 0.9880698764380059, Aging Rate = 0.7065009560229446\n",
      "Epoch 12: Train Loss = 0.25328181437405656, Recall = 0.9887089902002556, Aging Rate = 0.7006585935840238\n",
      "Epoch 13: Train Loss = 0.2507747108101313, Recall = 0.9874307626757562, Aging Rate = 0.6947100063734863\n",
      "Epoch 14: Train Loss = 0.2489055254503955, Recall = 0.9887089902002556, Aging Rate = 0.6953473550031868\n",
      "Epoch 15: Train Loss = 0.24585841012679774, Recall = 0.9882829143587558, Aging Rate = 0.6898236668791162\n",
      "Test Loss = 0.24914286944272032, Recall = 0.9823008849557522, Aging Rate = 0.7001274697259401, Efficiency = 1.4030314669840795\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.2452607406934457, Recall = 0.9882829143587558, Aging Rate = 0.6889738687061823\n",
      "Epoch 17: Train Loss = 0.24207770928268804, Recall = 0.9904132935662548, Aging Rate = 0.685362226471213\n",
      "Epoch 18: Train Loss = 0.24155950066337722, Recall = 0.9887089902002556, Aging Rate = 0.6857871255576801\n",
      "Epoch 19: Train Loss = 0.23916802129371673, Recall = 0.9904132935662548, Aging Rate = 0.6855746760144465\n",
      "Epoch 20: Train Loss = 0.23839588626658864, Recall = 0.9889220281210055, Aging Rate = 0.6822817080943276\n",
      "Test Loss = 0.2401829427256563, Recall = 0.9879898862199747, Aging Rate = 0.7023581899298916, Efficiency = 1.4066752353978276\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.23663589365927562, Recall = 0.9895611418832552, Aging Rate = 0.6832377310388783\n",
      "Epoch 22: Train Loss = 0.2357798878278402, Recall = 0.9906263314870047, Aging Rate = 0.6826003824091779\n",
      "Epoch 23: Train Loss = 0.23401646593054162, Recall = 0.9912654452492544, Aging Rate = 0.6817505842362439\n",
      "Epoch 24: Train Loss = 0.23508933867482792, Recall = 0.9895611418832552, Aging Rate = 0.6819630337794774\n",
      "Epoch 25: Train Loss = 0.23217848703084742, Recall = 0.9902002556455048, Aging Rate = 0.6787762906309751\n",
      "Test Loss = 0.2365240322662205, Recall = 0.9835651074589128, Aging Rate = 0.6775015933715742, Efficiency = 1.451753180456281\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.23389582441368248, Recall = 0.9902002556455048, Aging Rate = 0.6773953685999575\n",
      "Epoch 27: Train Loss = 0.23228036588687462, Recall = 0.9912654452492544, Aging Rate = 0.6796260888039091\n",
      "Epoch 28: Train Loss = 0.231768938003451, Recall = 0.9902002556455048, Aging Rate = 0.6766517951986403\n",
      "Epoch 29: Train Loss = 0.23093150576031687, Recall = 0.9910524073285045, Aging Rate = 0.6775015933715742\n",
      "Epoch 30: Train Loss = 0.2315897981192824, Recall = 0.9912654452492544, Aging Rate = 0.6802634374336095\n",
      "Test Loss = 0.2347647533286066, Recall = 0.9873577749683944, Aging Rate = 0.6889738687061823, Efficiency = 1.4330844832935385\n",
      "\n",
      "Epoch 31: Train Loss = 0.23068155364446233, Recall = 0.989987217724755, Aging Rate = 0.678032717229658\n",
      "Epoch 32: Train Loss = 0.23033568764507883, Recall = 0.9906263314870047, Aging Rate = 0.6773953685999575\n",
      "Epoch 33: Train Loss = 0.2286955912053167, Recall = 0.9912654452492544, Aging Rate = 0.6728277034204376\n",
      "Epoch 34: Train Loss = 0.22874452260134706, Recall = 0.9908393694077546, Aging Rate = 0.677182919056724\n",
      "Epoch 35: Train Loss = 0.2288449814371782, Recall = 0.9910524073285045, Aging Rate = 0.6735712768217549\n",
      "Test Loss = 0.2332491040457731, Recall = 0.9898862199747156, Aging Rate = 0.7033142128744423, Efficiency = 1.4074594082984055\n",
      "\n",
      "Epoch 36: Train Loss = 0.22905259151253093, Recall = 0.989987217724755, Aging Rate = 0.6721903547907372\n",
      "Epoch 37: Train Loss = 0.2282743133447822, Recall = 0.9912654452492544, Aging Rate = 0.6750584236243892\n",
      "Epoch 38: Train Loss = 0.2287827359501764, Recall = 0.9912654452492544, Aging Rate = 0.6758019970257064\n",
      "Epoch 39: Train Loss = 0.22849226782079815, Recall = 0.9910524073285045, Aging Rate = 0.6720841300191205\n",
      "Epoch 40: Train Loss = 0.2277373053578275, Recall = 0.9914784831700043, Aging Rate = 0.6742086254514553\n",
      "Test Loss = 0.23295937025524235, Recall = 0.9854614412136536, Aging Rate = 0.674314850223072, Efficiency = 1.4614262555145978\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.2277534108886755, Recall = 0.9910524073285045, Aging Rate = 0.6726152538772041\n",
      "Epoch 42: Train Loss = 0.22743097441526186, Recall = 0.9902002556455048, Aging Rate = 0.6716592309326535\n",
      "Epoch 43: Train Loss = 0.22688844050347032, Recall = 0.9908393694077546, Aging Rate = 0.669003611642235\n",
      "Epoch 44: Train Loss = 0.2279885643058353, Recall = 0.9908393694077546, Aging Rate = 0.671021882302953\n",
      "Epoch 45: Train Loss = 0.22679396751505923, Recall = 0.9908393694077546, Aging Rate = 0.6746335245379222\n",
      "Test Loss = 0.23097946850489173, Recall = 0.9879898862199747, Aging Rate = 0.6912045889101338, Efficiency = 1.4293740055806936\n",
      "\n",
      "Epoch 46: Train Loss = 0.2271306518032365, Recall = 0.9906263314870047, Aging Rate = 0.6686849373273848\n",
      "Epoch 47: Train Loss = 0.22715742805023367, Recall = 0.9912654452492544, Aging Rate = 0.6691098364138517\n",
      "Epoch 48: Train Loss = 0.22556072106932742, Recall = 0.9910524073285045, Aging Rate = 0.6708094327597196\n",
      "Epoch 49: Train Loss = 0.2251188708245691, Recall = 0.9914784831700043, Aging Rate = 0.6719779052475037\n",
      "Epoch 50: Train Loss = 0.22608663288934586, Recall = 0.9919045590115041, Aging Rate = 0.6722965795623539\n",
      "Test Loss = 0.2343287220838497, Recall = 0.9766118836915297, Aging Rate = 0.6332058636073933, Efficiency = 1.5423291608584453\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.22776342141721972, Recall = 0.9902002556455048, Aging Rate = 0.6656044189504993\n",
      "Epoch 52: Train Loss = 0.2257956238164754, Recall = 0.992117596932254, Aging Rate = 0.6704907584448694\n",
      "Epoch 53: Train Loss = 0.2256600194507175, Recall = 0.9908393694077546, Aging Rate = 0.6741024006798385\n",
      "Epoch 54: Train Loss = 0.2261092872035359, Recall = 0.9910524073285045, Aging Rate = 0.6717654557042703\n",
      "Epoch 55: Train Loss = 0.22539758330220144, Recall = 0.9916915210907542, Aging Rate = 0.6711281070745698\n",
      "Test Loss = 0.23038908563805963, Recall = 0.988621997471555, Aging Rate = 0.6880178457616316, Efficiency = 1.4369132853058826\n",
      "\n",
      "Epoch 56: Train Loss = 0.2251190052864175, Recall = 0.9906263314870047, Aging Rate = 0.6665604418950499\n",
      "Epoch 57: Train Loss = 0.22527375203460956, Recall = 0.9919045590115041, Aging Rate = 0.6707032079881028\n",
      "Epoch 58: Train Loss = 0.22545001405406612, Recall = 0.992117596932254, Aging Rate = 0.6744210749946887\n",
      "Epoch 59: Train Loss = 0.22405891024511737, Recall = 0.9916915210907542, Aging Rate = 0.6692160611854685\n",
      "Epoch 60: Train Loss = 0.22580433127377122, Recall = 0.9916915210907542, Aging Rate = 0.6731463777352878\n",
      "Test Loss = 0.23126705227344943, Recall = 0.9936788874841972, Aging Rate = 0.7151051625239006, Efficiency = 1.389556285793732\n",
      "\n",
      "Epoch 61: Train Loss = 0.22488390668729624, Recall = 0.992117596932254, Aging Rate = 0.6698534098151689\n",
      "Epoch 62: Train Loss = 0.2245212094480378, Recall = 0.9914784831700043, Aging Rate = 0.6663479923518164\n",
      "Epoch 63: Train Loss = 0.2241525329429429, Recall = 0.9914784831700043, Aging Rate = 0.6702783089016359\n",
      "Epoch 64: Train Loss = 0.22427425908199003, Recall = 0.9914784831700043, Aging Rate = 0.6696409602719354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: Train Loss = 0.22448457092749532, Recall = 0.9910524073285045, Aging Rate = 0.6654981941788826\n",
      "Test Loss = 0.2340674198228436, Recall = 0.9943109987357776, Aging Rate = 0.7297641810070108, Efficiency = 1.3625099874573392\n",
      "\n",
      "Epoch 66: Train Loss = 0.22558149819493623, Recall = 0.9916915210907542, Aging Rate = 0.6704907584448694\n",
      "Epoch 67: Train Loss = 0.22378570810102824, Recall = 0.9927567106945037, Aging Rate = 0.6680475886976843\n",
      "Epoch 68: Train Loss = 0.22565953843477604, Recall = 0.9912654452492544, Aging Rate = 0.6691098364138517\n",
      "Epoch 69: Train Loss = 0.2270714795010749, Recall = 0.9895611418832552, Aging Rate = 0.6634799235181644\n",
      "Epoch 70: Train Loss = 0.22370317265856715, Recall = 0.992117596932254, Aging Rate = 0.671021882302953\n",
      "Test Loss = 0.23079636351100347, Recall = 0.9848293299620733, Aging Rate = 0.6708094327597196, Efficiency = 1.4681208509982675\n",
      "\n",
      "Epoch 71: Train Loss = 0.22477752592043435, Recall = 0.9908393694077546, Aging Rate = 0.6699596345867856\n",
      "Epoch 72: Train Loss = 0.22335168334970257, Recall = 0.9919045590115041, Aging Rate = 0.668153813469301\n",
      "Epoch 73: Train Loss = 0.22475471568381125, Recall = 0.9916915210907542, Aging Rate = 0.6688973868706183\n",
      "Epoch 74: Train Loss = 0.22518053298825186, Recall = 0.9908393694077546, Aging Rate = 0.6692160611854685\n",
      "Epoch 75: Train Loss = 0.22338742124115, Recall = 0.9912654452492544, Aging Rate = 0.6668791162099001\n",
      "Test Loss = 0.2294091880701392, Recall = 0.9879898862199747, Aging Rate = 0.680050987890376, Efficiency = 1.4528173464708867\n",
      "\n",
      "Epoch 76: Train Loss = 0.22346900996443622, Recall = 0.9912654452492544, Aging Rate = 0.6668791162099001\n",
      "Epoch 77: Train Loss = 0.2231060866593352, Recall = 0.992117596932254, Aging Rate = 0.6699596345867856\n",
      "Epoch 78: Train Loss = 0.2239807057601012, Recall = 0.9912654452492544, Aging Rate = 0.6660293180369662\n",
      "Epoch 79: Train Loss = 0.22313601561468424, Recall = 0.9925436727737538, Aging Rate = 0.6660293180369662\n",
      "Epoch 80: Train Loss = 0.22333459838170622, Recall = 0.992117596932254, Aging Rate = 0.668578712555768\n",
      "Test Loss = 0.22879644949167227, Recall = 0.9911504424778761, Aging Rate = 0.6937539834289357, Efficiency = 1.4286770986052766\n",
      "\n",
      "Epoch 81: Train Loss = 0.2241833390185654, Recall = 0.9914784831700043, Aging Rate = 0.6698534098151689\n",
      "Epoch 82: Train Loss = 0.22356288757753748, Recall = 0.9914784831700043, Aging Rate = 0.6658168684937328\n",
      "Epoch 83: Train Loss = 0.22460066781179247, Recall = 0.9912654452492544, Aging Rate = 0.6671977905247504\n",
      "Epoch 84: Train Loss = 0.22355610575421692, Recall = 0.9916915210907542, Aging Rate = 0.6674102400679839\n",
      "Epoch 85: Train Loss = 0.22247891047969856, Recall = 0.992117596932254, Aging Rate = 0.6675164648396006\n",
      "Test Loss = 0.229706896099792, Recall = 0.9943109987357776, Aging Rate = 0.7065009560229446, Efficiency = 1.4073738700358491\n",
      "\n",
      "Epoch 86: Train Loss = 0.223334852018021, Recall = 0.992117596932254, Aging Rate = 0.6668791162099001\n",
      "Epoch 87: Train Loss = 0.22354534477951524, Recall = 0.9910524073285045, Aging Rate = 0.6650732950924155\n",
      "Epoch 88: Train Loss = 0.22344730224912274, Recall = 0.9929697486152536, Aging Rate = 0.6670915657531337\n",
      "Epoch 89: Train Loss = 0.2230090964352069, Recall = 0.9923306348530039, Aging Rate = 0.666135542808583\n",
      "Epoch 90: Train Loss = 0.2238729043335628, Recall = 0.992117596932254, Aging Rate = 0.6684724877841512\n",
      "Test Loss = 0.23209498840733803, Recall = 0.9797724399494311, Aging Rate = 0.6398980242192479, Efficiency = 1.5311383807341594\n",
      "\n",
      "Epoch 91: Train Loss = 0.22385740460598827, Recall = 0.9910524073285045, Aging Rate = 0.6644359464627151\n",
      "Epoch 92: Train Loss = 0.22301024525870125, Recall = 0.9906263314870047, Aging Rate = 0.6665604418950499\n",
      "Epoch 93: Train Loss = 0.2231534597585985, Recall = 0.9927567106945037, Aging Rate = 0.6664542171234332\n",
      "Epoch 94: Train Loss = 0.22289814007796374, Recall = 0.9919045590115041, Aging Rate = 0.6654981941788826\n",
      "Epoch 95: Train Loss = 0.22255169647892797, Recall = 0.9914784831700043, Aging Rate = 0.6658168684937328\n",
      "Test Loss = 0.22897111397020523, Recall = 0.9854614412136536, Aging Rate = 0.6755895474824729, Efficiency = 1.4586688475261398\n",
      "\n",
      "Epoch 96: Train Loss = 0.2225323860239319, Recall = 0.9916915210907542, Aging Rate = 0.6639048226046314\n",
      "Epoch 97: Train Loss = 0.2223367179107433, Recall = 0.9923306348530039, Aging Rate = 0.6641172721478649\n",
      "Epoch 98: Train Loss = 0.2232602925415958, Recall = 0.9927567106945037, Aging Rate = 0.6678351391544508\n",
      "Epoch 99: Train Loss = 0.22253651174930142, Recall = 0.9914784831700043, Aging Rate = 0.6648608455491821\n",
      "Epoch 100: Train Loss = 0.22340790665919597, Recall = 0.992117596932254, Aging Rate = 0.6669853409815168\n",
      "Test Loss = 0.22897176196652663, Recall = 0.9873577749683944, Aging Rate = 0.6845124282982792, Efficiency = 1.4424248848172851\n",
      "\n",
      "Training Finished at epoch 100.\n",
      "Test Loss = 1.9524026806980908, Recall = 0.8648648648648649, Aging Rate = 0.8465647619267859, Efficiency = 1.021616884549103\n",
      "\n",
      "Starting training Dataset 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1201d62ef84db3b790cccae91e729e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.33280975515303085, Recall = 0.9515474378488077, Aging Rate = 0.7476316786661614\n",
      "Epoch 2: Train Loss = 0.29783302730941497, Recall = 0.9660071029934043, Aging Rate = 0.73992673992674\n",
      "Epoch 3: Train Loss = 0.2845197087498465, Recall = 0.9731100963977676, Aging Rate = 0.7307060755336617\n",
      "Epoch 4: Train Loss = 0.2787593394589421, Recall = 0.9728564180618975, Aging Rate = 0.7227485158519641\n",
      "Epoch 5: Train Loss = 0.26726203597870324, Recall = 0.9774226281075596, Aging Rate = 0.7079701907288114\n",
      "Test Loss = 0.25175721465118367, Recall = 0.9842814371257484, Aging Rate = 0.7112542629784009, Efficiency = 1.3838671689156077\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.23977657991654497, Recall = 0.987062404870624, Aging Rate = 0.6829607174434761\n",
      "Epoch 7: Train Loss = 0.2303843472199532, Recall = 0.984779299847793, Aging Rate = 0.6617405582922824\n",
      "Epoch 8: Train Loss = 0.22209416273583724, Recall = 0.9868087265347539, Aging Rate = 0.6510041682455475\n",
      "Epoch 9: Train Loss = 0.21618349996799016, Recall = 0.989091831557585, Aging Rate = 0.6460780598711633\n",
      "Epoch 10: Train Loss = 0.21148070663277724, Recall = 0.9875697615423643, Aging Rate = 0.6383731211317418\n",
      "Test Loss = 0.22114722951879642, Recall = 0.9723053892215568, Aging Rate = 0.6312997347480106, Efficiency = 1.5401643946643158\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.20786571931632702, Recall = 0.9878234398782344, Aging Rate = 0.6309208033345964\n",
      "Epoch 12: Train Loss = 0.20497533259283973, Recall = 0.9898528665651953, Aging Rate = 0.6275104206138689\n",
      "Epoch 13: Train Loss = 0.20202601525320374, Recall = 0.9898528665651953, Aging Rate = 0.6241000378931414\n",
      "Epoch 14: Train Loss = 0.20009043783766312, Recall = 0.989345509893455, Aging Rate = 0.6227106227106227\n",
      "Epoch 15: Train Loss = 0.19834509676185177, Recall = 0.9901065449010654, Aging Rate = 0.6172792724516862\n",
      "Test Loss = 0.20975399867699607, Recall = 0.9738023952095808, Aging Rate = 0.6248579007199697, Efficiency = 1.5584381320987877\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.19637883746015675, Recall = 0.9895991882293252, Aging Rate = 0.6129847164329923\n",
      "Epoch 17: Train Loss = 0.19626058696066775, Recall = 0.9895991882293252, Aging Rate = 0.6182897562207907\n",
      "Epoch 18: Train Loss = 0.1939080210092755, Recall = 0.9901065449010654, Aging Rate = 0.6137425792598207\n",
      "Epoch 19: Train Loss = 0.19345080181137547, Recall = 0.9901065449010654, Aging Rate = 0.6115953012504737\n",
      "Epoch 20: Train Loss = 0.19253076155769625, Recall = 0.9906139015728057, Aging Rate = 0.6115953012504737\n",
      "Test Loss = 0.2077657556208574, Recall = 0.9708083832335329, Aging Rate = 0.6040166729821902, Efficiency = 1.607254254038804\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.19116812150135357, Recall = 0.9895991882293252, Aging Rate = 0.6126057850195781\n",
      "Epoch 22: Train Loss = 0.19070787787874102, Recall = 0.9916286149162862, Aging Rate = 0.6098269546545408\n",
      "Epoch 23: Train Loss = 0.18962944930733883, Recall = 0.9911212582445459, Aging Rate = 0.6086901604142984\n",
      "Epoch 24: Train Loss = 0.18985576989020725, Recall = 0.9908675799086758, Aging Rate = 0.6074270557029178\n",
      "Epoch 25: Train Loss = 0.18855877467195267, Recall = 0.9918822932521563, Aging Rate = 0.6098269546545408\n",
      "Test Loss = 0.2045275627008303, Recall = 0.967814371257485, Aging Rate = 0.5877226222053809, Efficiency = 1.6467195888404724\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.18884242314069397, Recall = 0.991374936580416, Aging Rate = 0.6070481242895036\n",
      "Epoch 27: Train Loss = 0.18741489028906572, Recall = 0.9921359715880264, Aging Rate = 0.6066691928760894\n",
      "Epoch 28: Train Loss = 0.18744166157749154, Recall = 0.9903602232369355, Aging Rate = 0.6085638499431603\n",
      "Epoch 29: Train Loss = 0.18687642013445427, Recall = 0.9926433282597666, Aging Rate = 0.6032588101553619\n",
      "Epoch 30: Train Loss = 0.18733077674814286, Recall = 0.9911212582445459, Aging Rate = 0.6074270557029178\n",
      "Test Loss = 0.1997035243105554, Recall = 0.9797904191616766, Aging Rate = 0.6252368321333839, Efficiency = 1.567070833219797\n",
      "\n",
      "Epoch 31: Train Loss = 0.18613120940138297, Recall = 0.9911212582445459, Aging Rate = 0.6052797776935708\n",
      "Epoch 32: Train Loss = 0.1852027123566452, Recall = 0.9903602232369355, Aging Rate = 0.6019957054439813\n",
      "Epoch 33: Train Loss = 0.18554676115866697, Recall = 0.9923896499238964, Aging Rate = 0.6075533661740559\n",
      "Epoch 34: Train Loss = 0.1851353494656783, Recall = 0.989091831557585, Aging Rate = 0.5975748389541493\n",
      "Epoch 35: Train Loss = 0.18518532842521912, Recall = 0.9916286149162862, Aging Rate = 0.6075533661740559\n",
      "Test Loss = 0.20202725745141845, Recall = 0.967814371257485, Aging Rate = 0.5869647593785525, Efficiency = 1.648845759997071\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.18474882000531906, Recall = 0.9921359715880264, Aging Rate = 0.6027535682708096\n",
      "Epoch 37: Train Loss = 0.18386790300727085, Recall = 0.9923896499238964, Aging Rate = 0.6046482253378805\n",
      "Epoch 38: Train Loss = 0.18434204068940682, Recall = 0.9921359715880264, Aging Rate = 0.6019957054439813\n",
      "Epoch 39: Train Loss = 0.1837527433873066, Recall = 0.9916286149162862, Aging Rate = 0.6009852216748769\n",
      "Epoch 40: Train Loss = 0.182885694881348, Recall = 0.991374936580416, Aging Rate = 0.6001010483769105\n",
      "Test Loss = 0.19769617504086986, Recall = 0.9752994011976048, Aging Rate = 0.5975748389541493, Efficiency = 1.6320958000567347\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.18373434891171628, Recall = 0.9921359715880264, Aging Rate = 0.6018693949728432\n",
      "Epoch 42: Train Loss = 0.18329233175631318, Recall = 0.991374936580416, Aging Rate = 0.6018693949728432\n",
      "Epoch 43: Train Loss = 0.18277542212702974, Recall = 0.9939117199391172, Aging Rate = 0.6038903625110522\n",
      "Epoch 44: Train Loss = 0.18333292545935895, Recall = 0.991374936580416, Aging Rate = 0.6026272577996716\n",
      "Epoch 45: Train Loss = 0.1827397347042412, Recall = 0.9908675799086758, Aging Rate = 0.5985853227232537\n",
      "Test Loss = 0.196153610346217, Recall = 0.9865269461077845, Aging Rate = 0.6468359226979916, Efficiency = 1.5251579206382693\n",
      "\n",
      "Epoch 46: Train Loss = 0.18266941641253986, Recall = 0.9923896499238964, Aging Rate = 0.6033851206265\n",
      "Epoch 47: Train Loss = 0.18247575507563624, Recall = 0.9916286149162862, Aging Rate = 0.5984590122521157\n",
      "Epoch 48: Train Loss = 0.18268799861979873, Recall = 0.9934043632673769, Aging Rate = 0.6009852216748769\n",
      "Epoch 49: Train Loss = 0.18103847129147388, Recall = 0.9926433282597666, Aging Rate = 0.5984590122521157\n",
      "Epoch 50: Train Loss = 0.1812220512672523, Recall = 0.991374936580416, Aging Rate = 0.601490463559429\n",
      "Test Loss = 0.19799562665313064, Recall = 0.9745508982035929, Aging Rate = 0.5930276619931792, Efficiency = 1.6433481003139456\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.1821544928556442, Recall = 0.9928970065956367, Aging Rate = 0.5985853227232537\n",
      "Epoch 52: Train Loss = 0.18189618016292128, Recall = 0.9926433282597666, Aging Rate = 0.5988379436655299\n",
      "Epoch 53: Train Loss = 0.18138161676353437, Recall = 0.9923896499238964, Aging Rate = 0.601364153088291\n",
      "Epoch 54: Train Loss = 0.18148101004210176, Recall = 0.9936580416032471, Aging Rate = 0.5984590122521157\n",
      "Epoch 55: Train Loss = 0.18111056088648375, Recall = 0.9928970065956367, Aging Rate = 0.5984590122521157\n",
      "Test Loss = 0.19514918177134163, Recall = 0.9775449101796407, Aging Rate = 0.610079575596817, Efficiency = 1.6023235873781068\n",
      "\n",
      "Epoch 56: Train Loss = 0.1808737513448187, Recall = 0.9936580416032471, Aging Rate = 0.5973222180118731\n",
      "Epoch 57: Train Loss = 0.1809946467750103, Recall = 0.9926433282597666, Aging Rate = 0.5978274598964254\n",
      "Epoch 58: Train Loss = 0.1804564963883638, Recall = 0.9931506849315068, Aging Rate = 0.5988379436655299\n",
      "Epoch 59: Train Loss = 0.18022605816688866, Recall = 0.9921359715880264, Aging Rate = 0.5970695970695971\n",
      "Epoch 60: Train Loss = 0.18069664151647166, Recall = 0.9921359715880264, Aging Rate = 0.5997221169634963\n",
      "Test Loss = 0.19618529903477636, Recall = 0.9760479041916168, Aging Rate = 0.5994694960212201, Efficiency = 1.6281860785043274\n",
      "\n",
      "Epoch 61: Train Loss = 0.17976525364175686, Recall = 0.9928970065956367, Aging Rate = 0.5959328028293546\n",
      "Epoch 62: Train Loss = 0.17998776896645569, Recall = 0.9931506849315068, Aging Rate = 0.5980800808387016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: Train Loss = 0.180516428973498, Recall = 0.9928970065956367, Aging Rate = 0.5974485284830112\n",
      "Epoch 64: Train Loss = 0.18008211272577754, Recall = 0.9931506849315068, Aging Rate = 0.5931539724643173\n",
      "Epoch 65: Train Loss = 0.1795146184782496, Recall = 0.9923896499238964, Aging Rate = 0.5974485284830112\n",
      "Test Loss = 0.19554194950298542, Recall = 0.9745508982035929, Aging Rate = 0.5862068965517241, Efficiency = 1.6624691509287142\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.17934077313162544, Recall = 0.9918822932521563, Aging Rate = 0.5937855248200076\n",
      "Epoch 67: Train Loss = 0.17866810024710666, Recall = 0.9936580416032471, Aging Rate = 0.5949223190602501\n",
      "Epoch 68: Train Loss = 0.17944410096045108, Recall = 0.9928970065956367, Aging Rate = 0.5956801818870784\n",
      "Epoch 69: Train Loss = 0.17897545131556505, Recall = 0.9949264332825977, Aging Rate = 0.5954275609448023\n",
      "Epoch 70: Train Loss = 0.17875360066509463, Recall = 0.9931506849315068, Aging Rate = 0.5973222180118731\n",
      "Test Loss = 0.19341148520202608, Recall = 0.9812874251497006, Aging Rate = 0.6195528609321713, Efficiency = 1.5838638979644595\n",
      "\n",
      "Epoch 71: Train Loss = 0.17923496431498512, Recall = 0.9923896499238964, Aging Rate = 0.5953012504736642\n",
      "Epoch 72: Train Loss = 0.1788342572213183, Recall = 0.9931506849315068, Aging Rate = 0.5970695970695971\n",
      "Epoch 73: Train Loss = 0.1786245354855318, Recall = 0.9918822932521563, Aging Rate = 0.5960591133004927\n",
      "Epoch 74: Train Loss = 0.17907068181349592, Recall = 0.9941653982749873, Aging Rate = 0.5980800808387016\n",
      "Epoch 75: Train Loss = 0.1795527009224973, Recall = 0.9931506849315068, Aging Rate = 0.5951749400025262\n",
      "Test Loss = 0.19605021774430037, Recall = 0.9760479041916168, Aging Rate = 0.596816976127321, Efficiency = 1.635422460954208\n",
      "\n",
      "Epoch 76: Train Loss = 0.17819511926360876, Recall = 0.9926433282597666, Aging Rate = 0.5963117342427687\n",
      "Epoch 77: Train Loss = 0.17839721108538434, Recall = 0.9939117199391172, Aging Rate = 0.5926487305797651\n",
      "Epoch 78: Train Loss = 0.1786330963390952, Recall = 0.9934043632673769, Aging Rate = 0.5951749400025262\n",
      "Epoch 79: Train Loss = 0.1786066889913452, Recall = 0.9928970065956367, Aging Rate = 0.5955538714159404\n",
      "Epoch 80: Train Loss = 0.1777640370402927, Recall = 0.9931506849315068, Aging Rate = 0.5905014525704181\n",
      "Test Loss = 0.1932733588459908, Recall = 0.9835329341317365, Aging Rate = 0.6328154604016673, Efficiency = 1.5542175881184732\n",
      "\n",
      "Epoch 81: Train Loss = 0.17850249122983053, Recall = 0.9928970065956367, Aging Rate = 0.5931539724643173\n",
      "Epoch 82: Train Loss = 0.17874379278177924, Recall = 0.9944190766108574, Aging Rate = 0.5953012504736642\n",
      "Epoch 83: Train Loss = 0.1775242254237748, Recall = 0.9928970065956367, Aging Rate = 0.5937855248200076\n",
      "Epoch 84: Train Loss = 0.17734884933998027, Recall = 0.9928970065956367, Aging Rate = 0.5936592143488695\n",
      "Epoch 85: Train Loss = 0.17754908851470794, Recall = 0.9939117199391172, Aging Rate = 0.5942907667045598\n",
      "Test Loss = 0.1941206076010559, Recall = 0.9760479041916168, Aging Rate = 0.5945433876468359, Efficiency = 1.6416764664358412\n",
      "\n",
      "Epoch 86: Train Loss = 0.1772507728011948, Recall = 0.9921359715880264, Aging Rate = 0.5921434886952128\n",
      "Epoch 87: Train Loss = 0.17810966258354977, Recall = 0.9921359715880264, Aging Rate = 0.5941644562334217\n",
      "Epoch 88: Train Loss = 0.17838559281544447, Recall = 0.9944190766108574, Aging Rate = 0.5960591133004927\n",
      "Epoch 89: Train Loss = 0.17698006654901458, Recall = 0.9936580416032471, Aging Rate = 0.5944170771756979\n",
      "Epoch 90: Train Loss = 0.17666992251132493, Recall = 0.9931506849315068, Aging Rate = 0.5927750410509032\n",
      "Test Loss = 0.19215900072910516, Recall = 0.9805389221556886, Aging Rate = 0.6134899583175445, Efficiency = 1.5982965864050742\n",
      "\n",
      "Epoch 91: Train Loss = 0.17738149725526545, Recall = 0.9934043632673769, Aging Rate = 0.5927750410509032\n",
      "Epoch 92: Train Loss = 0.17708383154068028, Recall = 0.9928970065956367, Aging Rate = 0.5945433876468359\n",
      "Epoch 93: Train Loss = 0.17728459929631668, Recall = 0.9939117199391172, Aging Rate = 0.5944170771756979\n",
      "Epoch 94: Train Loss = 0.1769727245566067, Recall = 0.9939117199391172, Aging Rate = 0.5959328028293546\n",
      "Epoch 95: Train Loss = 0.17656132504315755, Recall = 0.9939117199391172, Aging Rate = 0.5936592143488695\n",
      "Test Loss = 0.19401261937713118, Recall = 0.9790419161676647, Aging Rate = 0.5896172792724517, Efficiency = 1.6604701625621203\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.17676897305753836, Recall = 0.9936580416032471, Aging Rate = 0.5899962106858658\n",
      "Epoch 97: Train Loss = 0.17689678381229268, Recall = 0.9934043632673769, Aging Rate = 0.5955538714159404\n",
      "Epoch 98: Train Loss = 0.17683094645334763, Recall = 0.9936580416032471, Aging Rate = 0.5939118352911457\n",
      "Epoch 99: Train Loss = 0.17713259744933266, Recall = 0.9931506849315068, Aging Rate = 0.5926487305797651\n",
      "Epoch 100: Train Loss = 0.1763674685098885, Recall = 0.9934043632673769, Aging Rate = 0.5922697991663509\n",
      "Test Loss = 0.19173606121210313, Recall = 0.9797904191616766, Aging Rate = 0.6157635467980296, Efficiency = 1.5911796148778057\n",
      "\n",
      "Epoch 101: Train Loss = 0.176715580476675, Recall = 0.9936580416032471, Aging Rate = 0.5958064923582165\n",
      "Epoch 102: Train Loss = 0.17681307990178777, Recall = 0.9936580416032471, Aging Rate = 0.5939118352911457\n",
      "Epoch 103: Train Loss = 0.17658323504089993, Recall = 0.9944190766108574, Aging Rate = 0.5944170771756979\n",
      "Epoch 104: Train Loss = 0.17724704555908327, Recall = 0.9941653982749873, Aging Rate = 0.5930276619931792\n",
      "Epoch 105: Train Loss = 0.1766999810990532, Recall = 0.9936580416032471, Aging Rate = 0.5934065934065934\n",
      "Test Loss = 0.19037246045152903, Recall = 0.9835329341317365, Aging Rate = 0.6093217127699886, Efficiency = 1.6141438871743752\n",
      "\n",
      "Epoch 106: Train Loss = 0.1765990059992664, Recall = 0.9934043632673769, Aging Rate = 0.5926487305797651\n",
      "Epoch 107: Train Loss = 0.17710331993315637, Recall = 0.9931506849315068, Aging Rate = 0.5936592143488695\n",
      "Epoch 108: Train Loss = 0.17662212321127968, Recall = 0.9931506849315068, Aging Rate = 0.5905014525704181\n",
      "Epoch 109: Train Loss = 0.17632363137910353, Recall = 0.9949264332825977, Aging Rate = 0.5921434886952128\n",
      "Epoch 110: Train Loss = 0.17599862389331425, Recall = 0.9941653982749873, Aging Rate = 0.5931539724643173\n",
      "Test Loss = 0.19105922975925027, Recall = 0.9805389221556886, Aging Rate = 0.6089427813565744, Efficiency = 1.610231594943902\n",
      "\n",
      "Epoch 111: Train Loss = 0.1766378049828932, Recall = 0.9926433282597666, Aging Rate = 0.5905014525704181\n",
      "Epoch 112: Train Loss = 0.17575003748042756, Recall = 0.9936580416032471, Aging Rate = 0.5922697991663509\n",
      "Epoch 113: Train Loss = 0.17665425383398686, Recall = 0.9944190766108574, Aging Rate = 0.5940381457622836\n",
      "Epoch 114: Train Loss = 0.17606896213981046, Recall = 0.9934043632673769, Aging Rate = 0.5916382468106606\n",
      "Epoch 115: Train Loss = 0.17611143535039062, Recall = 0.9941653982749873, Aging Rate = 0.5916382468106606\n",
      "Test Loss = 0.1956710990584678, Recall = 0.9708083832335329, Aging Rate = 0.5706707086017431, Efficiency = 1.7011708356304083\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.175983511634219, Recall = 0.9923896499238964, Aging Rate = 0.5910066944549703\n",
      "Epoch 117: Train Loss = 0.17578515049610774, Recall = 0.995687468290208, Aging Rate = 0.5939118352911457\n",
      "Epoch 118: Train Loss = 0.17605759743993987, Recall = 0.9941653982749873, Aging Rate = 0.5918908677529368\n",
      "Epoch 119: Train Loss = 0.1763414533937521, Recall = 0.9931506849315068, Aging Rate = 0.5934065934065934\n",
      "Epoch 120: Train Loss = 0.1751421280219366, Recall = 0.9941653982749873, Aging Rate = 0.592522420108627\n",
      "Test Loss = 0.19350741123805312, Recall = 0.9767964071856288, Aging Rate = 0.5854490337248958, Efficiency = 1.6684567472700975\n",
      "\n",
      "Epoch 121: Train Loss = 0.17507145752490613, Recall = 0.9939117199391172, Aging Rate = 0.592522420108627\n",
      "Epoch 122: Train Loss = 0.1757446974704132, Recall = 0.9939117199391172, Aging Rate = 0.5921434886952128\n",
      "Epoch 123: Train Loss = 0.17512510909461396, Recall = 0.9941653982749873, Aging Rate = 0.5913856258683845\n",
      "Epoch 124: Train Loss = 0.17569634955843635, Recall = 0.9946727549467276, Aging Rate = 0.5929013515220412\n",
      "Epoch 125: Train Loss = 0.17550726470184458, Recall = 0.9946727549467276, Aging Rate = 0.5922697991663509\n",
      "Test Loss = 0.19213578160873368, Recall = 0.9775449101796407, Aging Rate = 0.5934065934065934, Efficiency = 1.647344172727113\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126: Train Loss = 0.17544118855823784, Recall = 0.9934043632673769, Aging Rate = 0.5912593153972464\n",
      "Epoch 127: Train Loss = 0.17584537817392293, Recall = 0.9954337899543378, Aging Rate = 0.5944170771756979\n",
      "Epoch 128: Train Loss = 0.17608455291778524, Recall = 0.9949264332825977, Aging Rate = 0.5930276619931792\n",
      "Epoch 129: Train Loss = 0.17563431302232094, Recall = 0.9931506849315068, Aging Rate = 0.588101553618795\n",
      "Epoch 130: Train Loss = 0.1756004310492179, Recall = 0.9928970065956367, Aging Rate = 0.5934065934065934\n",
      "Test Loss = 0.1899109450354184, Recall = 0.9857784431137725, Aging Rate = 0.6335733232284957, Efficiency = 1.5559026736345536\n",
      "\n",
      "Epoch 131: Train Loss = 0.17597275276809415, Recall = 0.9951801116184678, Aging Rate = 0.5929013515220412\n",
      "Epoch 132: Train Loss = 0.17506141530356203, Recall = 0.9936580416032471, Aging Rate = 0.5913856258683845\n",
      "Epoch 133: Train Loss = 0.17518995178834468, Recall = 0.9946727549467276, Aging Rate = 0.5906277630415562\n",
      "Epoch 134: Train Loss = 0.17486110290290582, Recall = 0.9939117199391172, Aging Rate = 0.5896172792724517\n",
      "Epoch 135: Train Loss = 0.17511007249980198, Recall = 0.9951801116184678, Aging Rate = 0.5934065934065934\n",
      "Test Loss = 0.1946079160463363, Recall = 0.9715568862275449, Aging Rate = 0.5721864342553997, Efficiency = 1.6979725681754934\n",
      "\n",
      "Epoch 136: Train Loss = 0.17452314481359396, Recall = 0.9946727549467276, Aging Rate = 0.5905014525704181\n",
      "Epoch 137: Train Loss = 0.17536938670037808, Recall = 0.9939117199391172, Aging Rate = 0.5908803839838322\n",
      "Epoch 138: Train Loss = 0.17516423074098733, Recall = 0.9941653982749873, Aging Rate = 0.5927750410509032\n",
      "Epoch 139: Train Loss = 0.1752633050477488, Recall = 0.9926433282597666, Aging Rate = 0.5896172792724517\n",
      "Epoch 140: Train Loss = 0.17460847733048487, Recall = 0.9944190766108574, Aging Rate = 0.5932802829354553\n",
      "Test Loss = 0.19337298150494406, Recall = 0.9738023952095808, Aging Rate = 0.5706707086017431, Efficiency = 1.70641731469172\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.17550574304427524, Recall = 0.9941653982749873, Aging Rate = 0.5906277630415562\n",
      "Epoch 142: Train Loss = 0.17441348056382447, Recall = 0.9946727549467276, Aging Rate = 0.592522420108627\n",
      "Epoch 143: Train Loss = 0.1743939196851137, Recall = 0.9936580416032471, Aging Rate = 0.5867121384362763\n",
      "Epoch 144: Train Loss = 0.17469943611891686, Recall = 0.9934043632673769, Aging Rate = 0.5889857269167614\n",
      "Epoch 145: Train Loss = 0.17410367960104425, Recall = 0.9939117199391172, Aging Rate = 0.5910066944549703\n",
      "Test Loss = 0.18885149653434935, Recall = 0.9842814371257484, Aging Rate = 0.6218264494126563, Efficiency = 1.5828876726401244\n",
      "\n",
      "Epoch 146: Train Loss = 0.17464919995294914, Recall = 0.9949264332825977, Aging Rate = 0.592522420108627\n",
      "Epoch 147: Train Loss = 0.17463539735948977, Recall = 0.9954337899543378, Aging Rate = 0.5926487305797651\n",
      "Epoch 148: Train Loss = 0.17415181951223485, Recall = 0.9939117199391172, Aging Rate = 0.59037514209928\n",
      "Epoch 149: Train Loss = 0.1743895148495404, Recall = 0.9939117199391172, Aging Rate = 0.5882278640899331\n",
      "Epoch 150: Train Loss = 0.17403481862954423, Recall = 0.9939117199391172, Aging Rate = 0.5920171782240747\n",
      "Test Loss = 0.18982133671906737, Recall = 0.9812874251497006, Aging Rate = 0.5987116331943918, Efficiency = 1.6389983998208177\n",
      "\n",
      "Test Loss = 1.52187322063147, Recall = 0.7297297297297297, Aging Rate = 0.7911106999976875, Efficiency = 0.9224116429062915\n",
      "\n",
      "Starting training Dataset 4:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f399f444a44f388503305c8bf88fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.31908539766811156, Recall = 0.9481894150417828, Aging Rate = 0.7117704145293221\n",
      "Epoch 2: Train Loss = 0.24101981035378578, Recall = 0.9766016713091922, Aging Rate = 0.652294468321087\n",
      "Epoch 3: Train Loss = 0.22277766833246854, Recall = 0.9793871866295265, Aging Rate = 0.6377374185498406\n",
      "Epoch 4: Train Loss = 0.21367377014261302, Recall = 0.9768802228412257, Aging Rate = 0.6247053930403438\n",
      "Epoch 5: Train Loss = 0.20797468840241284, Recall = 0.9802228412256268, Aging Rate = 0.6256758630250936\n",
      "Test Loss = 0.19663371863831106, Recall = 0.9811320754716981, Aging Rate = 0.6266112266112266, Efficiency = 1.5657747868961847\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1870985111428517, Recall = 0.9880222841225627, Aging Rate = 0.6047414390683488\n",
      "Epoch 7: Train Loss = 0.180995604449166, Recall = 0.9871866295264624, Aging Rate = 0.5993345348676001\n",
      "Epoch 8: Train Loss = 0.1764610000693251, Recall = 0.9885793871866295, Aging Rate = 0.5950367392208512\n",
      "Epoch 9: Train Loss = 0.17264716924228468, Recall = 0.9894150417827298, Aging Rate = 0.5922639678358519\n",
      "Epoch 10: Train Loss = 0.17079263384292942, Recall = 0.9888579387186629, Aging Rate = 0.5915707749896021\n",
      "Test Loss = 0.17754131826318476, Recall = 0.9794913863822805, Aging Rate = 0.5817047817047817, Efficiency = 1.6838289805241584\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.16909354795831824, Recall = 0.9891364902506964, Aging Rate = 0.5903230278663524\n",
      "Epoch 12: Train Loss = 0.1678722618927857, Recall = 0.9883008356545961, Aging Rate = 0.5860252322196035\n",
      "Epoch 13: Train Loss = 0.166653273946598, Recall = 0.9880222841225627, Aging Rate = 0.5871343407736033\n",
      "Epoch 14: Train Loss = 0.16554539441095548, Recall = 0.9885793871866295, Aging Rate = 0.584222930819354\n",
      "Epoch 15: Train Loss = 0.16573448357702078, Recall = 0.9894150417827298, Aging Rate = 0.5890752807431028\n",
      "Test Loss = 0.1725725686661667, Recall = 0.9762100082034455, Aging Rate = 0.6020790020790021, Efficiency = 1.621398501888572\n",
      "\n",
      "Epoch 16: Train Loss = 0.1647825915462991, Recall = 0.9880222841225627, Aging Rate = 0.5818660751421045\n",
      "Epoch 17: Train Loss = 0.16554268401719885, Recall = 0.9874651810584958, Aging Rate = 0.5856093165118536\n",
      "Epoch 18: Train Loss = 0.16434276269656617, Recall = 0.9891364902506964, Aging Rate = 0.5854706779426037\n",
      "Epoch 19: Train Loss = 0.16401666738934745, Recall = 0.9885793871866295, Aging Rate = 0.5821433522806044\n",
      "Epoch 20: Train Loss = 0.1647839742255214, Recall = 0.9883008356545961, Aging Rate = 0.5867184250658534\n",
      "Test Loss = 0.1721778920931033, Recall = 0.977850697292863, Aging Rate = 0.5738045738045738, Efficiency = 1.7041528159452608\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.16333452360641273, Recall = 0.9891364902506964, Aging Rate = 0.584361569388604\n",
      "Epoch 22: Train Loss = 0.1635723920510874, Recall = 0.9877437325905293, Aging Rate = 0.5826979065576043\n",
      "Epoch 23: Train Loss = 0.16313280615969536, Recall = 0.9896935933147633, Aging Rate = 0.5867184250658534\n",
      "Epoch 24: Train Loss = 0.16364193026895218, Recall = 0.9885793871866295, Aging Rate = 0.5846388465271039\n",
      "Epoch 25: Train Loss = 0.16303859134206622, Recall = 0.9885793871866295, Aging Rate = 0.5849161236656037\n",
      "Test Loss = 0.17117354010469948, Recall = 0.9803117309269893, Aging Rate = 0.5833679833679833, Efficiency = 1.6804345491553516\n",
      "\n",
      "Epoch 26: Train Loss = 0.16255583806547266, Recall = 0.9888579387186629, Aging Rate = 0.5849161236656037\n",
      "Epoch 27: Train Loss = 0.16345042622644693, Recall = 0.9880222841225627, Aging Rate = 0.583807015111604\n",
      "Epoch 28: Train Loss = 0.1636176993157552, Recall = 0.9888579387186629, Aging Rate = 0.5850547622348538\n",
      "Epoch 29: Train Loss = 0.16374937589846486, Recall = 0.9869080779944289, Aging Rate = 0.5857479550811036\n",
      "Epoch 30: Train Loss = 0.16326990570115363, Recall = 0.9888579387186629, Aging Rate = 0.584361569388604\n",
      "Test Loss = 0.17208327677666274, Recall = 0.9835931091058244, Aging Rate = 0.6257796257796258, Efficiency = 1.571788298736212\n",
      "\n",
      "Epoch 31: Train Loss = 0.16372293489626868, Recall = 0.9883008356545961, Aging Rate = 0.5833910994038541\n",
      "Epoch 32: Train Loss = 0.16238145149069785, Recall = 0.9891364902506964, Aging Rate = 0.5871343407736033\n",
      "Epoch 33: Train Loss = 0.16444437422915006, Recall = 0.9880222841225627, Aging Rate = 0.5864411479273534\n",
      "Epoch 34: Train Loss = 0.16353723385719754, Recall = 0.9880222841225627, Aging Rate = 0.5850547622348538\n",
      "Epoch 35: Train Loss = 0.1636203631591354, Recall = 0.9874651810584958, Aging Rate = 0.5832524608346041\n",
      "Test Loss = 0.1715130112027428, Recall = 0.9811320754716981, Aging Rate = 0.5862785862785863, Efficiency = 1.673491206568773\n",
      "\n",
      "Epoch 36: Train Loss = 0.16291535936495344, Recall = 0.9894150417827298, Aging Rate = 0.5854706779426037\n",
      "Epoch 37: Train Loss = 0.1630542937985872, Recall = 0.9871866295264624, Aging Rate = 0.5867184250658534\n",
      "Epoch 38: Train Loss = 0.16300522702858813, Recall = 0.9880222841225627, Aging Rate = 0.5818660751421045\n",
      "Epoch 39: Train Loss = 0.16208068704813341, Recall = 0.9883008356545961, Aging Rate = 0.583807015111604\n",
      "Epoch 40: Train Loss = 0.1630704365056083, Recall = 0.9883008356545961, Aging Rate = 0.5854706779426037\n",
      "Test Loss = 0.1700611300503142, Recall = 0.9811320754716981, Aging Rate = 0.5920997920997921, Efficiency = 1.6570383438607172\n",
      "\n",
      "Epoch 41: Train Loss = 0.16268155065932088, Recall = 0.9902506963788301, Aging Rate = 0.5860252322196035\n",
      "Epoch 42: Train Loss = 0.16298350165823405, Recall = 0.9885793871866295, Aging Rate = 0.5835297379731041\n",
      "Epoch 43: Train Loss = 0.1627831538023837, Recall = 0.9869080779944289, Aging Rate = 0.5857479550811036\n",
      "Epoch 44: Train Loss = 0.16289265753780607, Recall = 0.9888579387186629, Aging Rate = 0.583945653680854\n",
      "Epoch 45: Train Loss = 0.1621486872174316, Recall = 0.9874651810584958, Aging Rate = 0.583668376542354\n",
      "Test Loss = 0.1714842635975558, Recall = 0.9827727645611156, Aging Rate = 0.6103950103950104, Efficiency = 1.6100602588879656\n",
      "\n",
      "Epoch 46: Train Loss = 0.16325866284837146, Recall = 0.9880222841225627, Aging Rate = 0.583945653680854\n",
      "Epoch 47: Train Loss = 0.16264477815299824, Recall = 0.9888579387186629, Aging Rate = 0.5833910994038541\n",
      "Epoch 48: Train Loss = 0.16207652706247452, Recall = 0.9894150417827298, Aging Rate = 0.584361569388604\n",
      "Epoch 49: Train Loss = 0.16349376392037268, Recall = 0.9891364902506964, Aging Rate = 0.584361569388604\n",
      "Epoch 50: Train Loss = 0.16300830470818217, Recall = 0.9888579387186629, Aging Rate = 0.5847774850963539\n",
      "Test Loss = 0.17162740436884072, Recall = 0.9794913863822805, Aging Rate = 0.5837837837837838, Efficiency = 1.6778324386734431\n",
      "\n",
      "Epoch 51: Train Loss = 0.16248183387477438, Recall = 0.9888579387186629, Aging Rate = 0.5847774850963539\n",
      "Epoch 52: Train Loss = 0.16203773261752752, Recall = 0.9894150417827298, Aging Rate = 0.5854706779426037\n",
      "Epoch 53: Train Loss = 0.16114669477957785, Recall = 0.9877437325905293, Aging Rate = 0.5808956051573547\n",
      "Epoch 54: Train Loss = 0.16288147310075723, Recall = 0.9891364902506964, Aging Rate = 0.5868570636351033\n",
      "Epoch 55: Train Loss = 0.16192052551889557, Recall = 0.9894150417827298, Aging Rate = 0.5845002079578538\n",
      "Test Loss = 0.1703087958377513, Recall = 0.9811320754716981, Aging Rate = 0.5933471933471933, Efficiency = 1.6535547314235757\n",
      "\n",
      "Epoch 56: Train Loss = 0.16219251127013648, Recall = 0.9896935933147633, Aging Rate = 0.583668376542354\n",
      "Epoch 57: Train Loss = 0.16145588664171948, Recall = 0.9896935933147633, Aging Rate = 0.583668376542354\n",
      "Epoch 58: Train Loss = 0.16254588320766625, Recall = 0.9891364902506964, Aging Rate = 0.5832524608346041\n",
      "Epoch 59: Train Loss = 0.163493727492787, Recall = 0.9888579387186629, Aging Rate = 0.583807015111604\n",
      "Epoch 60: Train Loss = 0.16336936971259716, Recall = 0.9883008356545961, Aging Rate = 0.5826979065576043\n",
      "Test Loss = 0.17007845251699, Recall = 0.9819524200164069, Aging Rate = 0.6087318087318088, Efficiency = 1.6131117017377883\n",
      "\n",
      "Epoch 61: Train Loss = 0.16280337226724684, Recall = 0.9880222841225627, Aging Rate = 0.5835297379731041\n",
      "Epoch 62: Train Loss = 0.16166108283506722, Recall = 0.9894150417827298, Aging Rate = 0.5824206294191044\n",
      "Epoch 63: Train Loss = 0.16234581573246157, Recall = 0.9899721448467966, Aging Rate = 0.5845002079578538\n",
      "Epoch 64: Train Loss = 0.16170168804069604, Recall = 0.9888579387186629, Aging Rate = 0.5847774850963539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: Train Loss = 0.16158287849808706, Recall = 0.9888579387186629, Aging Rate = 0.5833910994038541\n",
      "Test Loss = 0.1697426452148481, Recall = 0.9827727645611156, Aging Rate = 0.6174636174636174, Efficiency = 1.591628592923108\n",
      "\n",
      "Epoch 66: Train Loss = 0.1626337581462244, Recall = 0.9902506963788301, Aging Rate = 0.5833910994038541\n",
      "Epoch 67: Train Loss = 0.162707974797774, Recall = 0.9883008356545961, Aging Rate = 0.5846388465271039\n",
      "Epoch 68: Train Loss = 0.16233069275819725, Recall = 0.9888579387186629, Aging Rate = 0.5853320393733537\n",
      "Epoch 69: Train Loss = 0.1623744419208433, Recall = 0.9894150417827298, Aging Rate = 0.583945653680854\n",
      "Epoch 70: Train Loss = 0.16354029888330313, Recall = 0.9885793871866295, Aging Rate = 0.5869957022043533\n",
      "Test Loss = 0.1692033647933274, Recall = 0.9827727645611156, Aging Rate = 0.5975051975051975, Efficiency = 1.6447936389785636\n",
      "\n",
      "Training Finished at epoch 70.\n",
      "Test Loss = 1.1411144946496643, Recall = 0.5135135135135135, Aging Rate = 0.564137548273709, Efficiency = 0.9102629420471351\n",
      "\n",
      "Starting training Dataset 5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc9dc402d1544b28564807480e02ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.25380883025289275, Recall = 0.9633002207505519, Aging Rate = 0.6652789342214821\n",
      "Epoch 2: Train Loss = 0.23813950574599865, Recall = 0.9721302428256071, Aging Rate = 0.6550097141271163\n",
      "Epoch 3: Train Loss = 0.23325881964128745, Recall = 0.9790286975717439, Aging Rate = 0.6561199000832639\n",
      "Epoch 4: Train Loss = 0.22448971167789908, Recall = 0.9801324503311258, Aging Rate = 0.6437690813211213\n",
      "Epoch 5: Train Loss = 0.22705505749036498, Recall = 0.9751655629139073, Aging Rate = 0.6455731334998612\n",
      "Test Loss = 0.22712279403338723, Recall = 0.9948892674616695, Aging Rate = 0.704412989175687, Efficiency = 1.4123664222918961\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.20179440014120145, Recall = 0.9870309050772627, Aging Rate = 0.6229530946433528\n",
      "Epoch 7: Train Loss = 0.19556648384075578, Recall = 0.9859271523178808, Aging Rate = 0.6164307521509853\n",
      "Epoch 8: Train Loss = 0.19015003492641608, Recall = 0.9862030905077263, Aging Rate = 0.6099084096586178\n",
      "Epoch 9: Train Loss = 0.18612839087591612, Recall = 0.9870309050772627, Aging Rate = 0.607549264501804\n",
      "Epoch 10: Train Loss = 0.182268639149151, Recall = 0.9884105960264901, Aging Rate = 0.609075770191507\n",
      "Test Loss = 0.19422364361478725, Recall = 0.9787052810902896, Aging Rate = 0.5691090757701915, Efficiency = 1.719714735823941\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.18337540354557974, Recall = 0.9845474613686535, Aging Rate = 0.6065778517901749\n",
      "Epoch 12: Train Loss = 0.17994489884115145, Recall = 0.9878587196467992, Aging Rate = 0.6051901193449903\n",
      "Epoch 13: Train Loss = 0.1781955155115673, Recall = 0.9875827814569537, Aging Rate = 0.6006106022758813\n",
      "Epoch 14: Train Loss = 0.17658100214942177, Recall = 0.9873068432671082, Aging Rate = 0.6018595614765473\n",
      "Epoch 15: Train Loss = 0.17677586854678737, Recall = 0.9870309050772627, Aging Rate = 0.6032472939217319\n",
      "Test Loss = 0.18617034258691595, Recall = 0.989778534923339, Aging Rate = 0.5870108243130724, Efficiency = 1.6861333336063387\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.17641783765611005, Recall = 0.9884105960264901, Aging Rate = 0.599639189564252\n",
      "Epoch 17: Train Loss = 0.17545569107213552, Recall = 0.9875827814569537, Aging Rate = 0.6013044684984735\n",
      "Epoch 18: Train Loss = 0.17502826443983718, Recall = 0.9875827814569537, Aging Rate = 0.5982514571190675\n",
      "Epoch 19: Train Loss = 0.1753764998290792, Recall = 0.9889624724061811, Aging Rate = 0.6025534276991396\n",
      "Epoch 20: Train Loss = 0.17346733724354638, Recall = 0.9862030905077263, Aging Rate = 0.5940882597835138\n",
      "Test Loss = 0.18450408583378217, Recall = 0.989778534923339, Aging Rate = 0.609075770191507, Efficiency = 1.6250498987369526\n",
      "\n",
      "Epoch 21: Train Loss = 0.17295588129863057, Recall = 0.9873068432671082, Aging Rate = 0.6013044684984735\n",
      "Epoch 22: Train Loss = 0.17460746419221337, Recall = 0.9886865342163356, Aging Rate = 0.598112683874549\n",
      "Epoch 23: Train Loss = 0.1731787013686633, Recall = 0.9864790286975718, Aging Rate = 0.5985290036081043\n",
      "Epoch 24: Train Loss = 0.17297551106602624, Recall = 0.9878587196467992, Aging Rate = 0.5976963641409936\n",
      "Epoch 25: Train Loss = 0.1731787927794086, Recall = 0.9881346578366446, Aging Rate = 0.6000555092978074\n",
      "Test Loss = 0.18141175101788018, Recall = 0.989778534923339, Aging Rate = 0.5990840965861782, Efficiency = 1.6521528847818956\n",
      "\n",
      "Epoch 26: Train Loss = 0.17206392786898547, Recall = 0.9875827814569537, Aging Rate = 0.5971412711629198\n",
      "Epoch 27: Train Loss = 0.17260615226132586, Recall = 0.9870309050772627, Aging Rate = 0.5979739106300306\n",
      "Epoch 28: Train Loss = 0.17224045168135915, Recall = 0.9870309050772627, Aging Rate = 0.5985290036081043\n",
      "Epoch 29: Train Loss = 0.17179694497979023, Recall = 0.9892384105960265, Aging Rate = 0.5989453233416597\n",
      "Epoch 30: Train Loss = 0.17220068176421993, Recall = 0.9870309050772627, Aging Rate = 0.5940882597835138\n",
      "Test Loss = 0.18516463816761475, Recall = 0.9906303236797275, Aging Rate = 0.6144879267277269, Efficiency = 1.6121233053899076\n",
      "\n",
      "Epoch 31: Train Loss = 0.1721850795246911, Recall = 0.9878587196467992, Aging Rate = 0.5990840965861782\n",
      "Epoch 32: Train Loss = 0.1718289890025941, Recall = 0.9867549668874173, Aging Rate = 0.5945045795170691\n",
      "Epoch 33: Train Loss = 0.17159455160184137, Recall = 0.9878587196467992, Aging Rate = 0.5978351373855121\n",
      "Epoch 34: Train Loss = 0.17153531888640988, Recall = 0.9892384105960265, Aging Rate = 0.5954759922286983\n",
      "Epoch 35: Train Loss = 0.17145499679700157, Recall = 0.9867549668874173, Aging Rate = 0.5979739106300306\n",
      "Test Loss = 0.18497700362677974, Recall = 0.9838160136286201, Aging Rate = 0.5691090757701915, Efficiency = 1.7286949694313767\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.17046807587659593, Recall = 0.9864790286975718, Aging Rate = 0.5933943935609215\n",
      "Epoch 37: Train Loss = 0.17243684753424057, Recall = 0.9873068432671082, Aging Rate = 0.5968637246738828\n",
      "Epoch 38: Train Loss = 0.17143756691123457, Recall = 0.9884105960264901, Aging Rate = 0.5968637246738828\n",
      "Epoch 39: Train Loss = 0.170550695474433, Recall = 0.9881346578366446, Aging Rate = 0.5929780738273661\n",
      "Epoch 40: Train Loss = 0.17020153504277546, Recall = 0.9881346578366446, Aging Rate = 0.5924229808492922\n",
      "Test Loss = 0.18212961390006552, Recall = 0.989778534923339, Aging Rate = 0.5965861781848459, Efficiency = 1.6590704822295794\n",
      "\n",
      "Epoch 41: Train Loss = 0.17040663389755295, Recall = 0.9878587196467992, Aging Rate = 0.5957535387177352\n",
      "Epoch 42: Train Loss = 0.17125943021876197, Recall = 0.9884105960264901, Aging Rate = 0.5960310852067722\n",
      "Epoch 43: Train Loss = 0.17129013455976158, Recall = 0.9884105960264901, Aging Rate = 0.598112683874549\n",
      "Epoch 44: Train Loss = 0.1712156160528581, Recall = 0.9873068432671082, Aging Rate = 0.5938107132944769\n",
      "Epoch 45: Train Loss = 0.17076778995835248, Recall = 0.9862030905077263, Aging Rate = 0.5938107132944769\n",
      "Test Loss = 0.1817346922587991, Recall = 0.9838160136286201, Aging Rate = 0.5691090757701915, Efficiency = 1.7286949694313767\n",
      "\n",
      "Epoch 46: Train Loss = 0.17066222258054842, Recall = 0.9878587196467992, Aging Rate = 0.5954759922286983\n",
      "Epoch 47: Train Loss = 0.17009611711083866, Recall = 0.9878587196467992, Aging Rate = 0.5950596724951429\n",
      "Epoch 48: Train Loss = 0.17024586415691176, Recall = 0.9886865342163356, Aging Rate = 0.5953372189841799\n",
      "Epoch 49: Train Loss = 0.17016477517674045, Recall = 0.9864790286975718, Aging Rate = 0.5915903413821815\n",
      "Epoch 50: Train Loss = 0.16972701374752602, Recall = 0.9889624724061811, Aging Rate = 0.5976963641409936\n",
      "Test Loss = 0.18022207539841892, Recall = 0.9906303236797275, Aging Rate = 0.5903413821815154, Efficiency = 1.6780634676809736\n",
      "\n",
      "Epoch 51: Train Loss = 0.16971868600840043, Recall = 0.9873068432671082, Aging Rate = 0.5911740216486261\n",
      "Epoch 52: Train Loss = 0.170327215613407, Recall = 0.9875827814569537, Aging Rate = 0.5929780738273661\n",
      "Epoch 53: Train Loss = 0.16979662918925384, Recall = 0.9884105960264901, Aging Rate = 0.5920066611157369\n",
      "Epoch 54: Train Loss = 0.1695501681147368, Recall = 0.9878587196467992, Aging Rate = 0.5954759922286983\n",
      "Epoch 55: Train Loss = 0.16990682196938062, Recall = 0.9897902869757175, Aging Rate = 0.5956147654732168\n",
      "Test Loss = 0.17995216575242598, Recall = 0.9906303236797275, Aging Rate = 0.601165695253955, Efficiency = 1.6478490290148002\n",
      "\n",
      "Epoch 56: Train Loss = 0.16966654370220177, Recall = 0.9875827814569537, Aging Rate = 0.5933943935609215\n",
      "Epoch 57: Train Loss = 0.169502506703097, Recall = 0.989514348785872, Aging Rate = 0.5945045795170691\n",
      "Epoch 58: Train Loss = 0.17010519797276563, Recall = 0.9881346578366446, Aging Rate = 0.5938107132944769\n",
      "Epoch 59: Train Loss = 0.16896465096892563, Recall = 0.9884105960264901, Aging Rate = 0.5931168470718845\n",
      "Epoch 60: Train Loss = 0.16932887286816312, Recall = 0.9878587196467992, Aging Rate = 0.5925617540938107\n",
      "Test Loss = 0.18073161888082856, Recall = 0.9863713798977853, Aging Rate = 0.5836802664446294, Efficiency = 1.6899172709862107\n",
      "\n",
      "Epoch 61: Train Loss = 0.1697607934020011, Recall = 0.9873068432671082, Aging Rate = 0.5913127948931446\n",
      "Epoch 62: Train Loss = 0.16939249667001705, Recall = 0.9892384105960265, Aging Rate = 0.5939494865389953\n",
      "Epoch 63: Train Loss = 0.17098578654773494, Recall = 0.9864790286975718, Aging Rate = 0.5974188176519567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss = 0.1699451293724959, Recall = 0.9892384105960265, Aging Rate = 0.5922842076047738\n",
      "Epoch 65: Train Loss = 0.16928059273367618, Recall = 0.9886865342163356, Aging Rate = 0.5939494865389953\n",
      "Test Loss = 0.18187403887336598, Recall = 0.9872231686541738, Aging Rate = 0.5736885928393006, Efficiency = 1.7208345499077498\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.16936902574059567, Recall = 0.9859271523178808, Aging Rate = 0.5893699694698862\n",
      "Epoch 67: Train Loss = 0.17019903395958752, Recall = 0.9862030905077263, Aging Rate = 0.5965861781848459\n",
      "Epoch 68: Train Loss = 0.1692247010939094, Recall = 0.9870309050772627, Aging Rate = 0.5903413821815154\n",
      "Epoch 69: Train Loss = 0.16958620156674065, Recall = 0.9886865342163356, Aging Rate = 0.594782126006106\n",
      "Epoch 70: Train Loss = 0.1702088396143258, Recall = 0.9850993377483444, Aging Rate = 0.5925617540938107\n",
      "Test Loss = 0.18039957172765422, Recall = 0.989778534923339, Aging Rate = 0.5895087427144047, Efficiency = 1.6789887009580164\n",
      "\n",
      "Epoch 71: Train Loss = 0.1701820106629428, Recall = 0.9884105960264901, Aging Rate = 0.5982514571190675\n",
      "Epoch 72: Train Loss = 0.170592037519911, Recall = 0.9878587196467992, Aging Rate = 0.5951984457396614\n",
      "Epoch 73: Train Loss = 0.16896217625951093, Recall = 0.9886865342163356, Aging Rate = 0.5942270330280321\n",
      "Epoch 74: Train Loss = 0.1697319091656722, Recall = 0.9889624724061811, Aging Rate = 0.5961698584512906\n",
      "Epoch 75: Train Loss = 0.16959110622005003, Recall = 0.9884105960264901, Aging Rate = 0.5942270330280321\n",
      "Test Loss = 0.18406248714256843, Recall = 0.9770017035775128, Aging Rate = 0.5628642797668609, Efficiency = 1.735767788683464\n",
      "\n",
      "Epoch 76: Train Loss = 0.16886467170440717, Recall = 0.9878587196467992, Aging Rate = 0.5933943935609215\n",
      "Epoch 77: Train Loss = 0.16899030908110935, Recall = 0.9908940397350994, Aging Rate = 0.5940882597835138\n",
      "Epoch 78: Train Loss = 0.16874097818273523, Recall = 0.9864790286975718, Aging Rate = 0.5925617540938107\n",
      "Epoch 79: Train Loss = 0.17050215383966266, Recall = 0.9897902869757175, Aging Rate = 0.5971412711629198\n",
      "Epoch 80: Train Loss = 0.16850306274012133, Recall = 0.9870309050772627, Aging Rate = 0.5897862892034416\n",
      "Test Loss = 0.19001135887651022, Recall = 0.9974446337308348, Aging Rate = 0.6511240632805995, Efficiency = 1.531881057177546\n",
      "\n",
      "Epoch 81: Train Loss = 0.16981557762659227, Recall = 0.9897902869757175, Aging Rate = 0.5972800444074382\n",
      "Epoch 82: Train Loss = 0.1695938029538715, Recall = 0.9878587196467992, Aging Rate = 0.5943658062725506\n",
      "Epoch 83: Train Loss = 0.16925617105599214, Recall = 0.9873068432671082, Aging Rate = 0.5929780738273661\n",
      "Epoch 84: Train Loss = 0.16874701296277356, Recall = 0.9892384105960265, Aging Rate = 0.593255620316403\n",
      "Epoch 85: Train Loss = 0.16944116163743458, Recall = 0.9900662251655629, Aging Rate = 0.5961698584512906\n",
      "Test Loss = 0.17955806329039908, Recall = 0.9863713798977853, Aging Rate = 0.582014987510408, Efficiency = 1.6947525134524497\n",
      "\n",
      "Epoch 86: Train Loss = 0.16893159552411904, Recall = 0.9900662251655629, Aging Rate = 0.5943658062725506\n",
      "Epoch 87: Train Loss = 0.16876234263057613, Recall = 0.9875827814569537, Aging Rate = 0.5933943935609215\n",
      "Epoch 88: Train Loss = 0.1694842300794206, Recall = 0.9884105960264901, Aging Rate = 0.5936719400499584\n",
      "Epoch 89: Train Loss = 0.16961291735118672, Recall = 0.9889624724061811, Aging Rate = 0.5953372189841799\n",
      "Epoch 90: Train Loss = 0.16828132954085168, Recall = 0.9897902869757175, Aging Rate = 0.5942270330280321\n",
      "Test Loss = 0.17999799558264726, Recall = 0.9872231686541738, Aging Rate = 0.5865945045795171, Efficiency = 1.682973747822779\n",
      "\n",
      "Epoch 91: Train Loss = 0.16901205882508658, Recall = 0.9881346578366446, Aging Rate = 0.5958923119622537\n",
      "Epoch 92: Train Loss = 0.16859889986645404, Recall = 0.9886865342163356, Aging Rate = 0.5927005273383291\n",
      "Epoch 93: Train Loss = 0.16881092299678543, Recall = 0.9884105960264901, Aging Rate = 0.5936719400499584\n",
      "Epoch 94: Train Loss = 0.16929988798179, Recall = 0.9878587196467992, Aging Rate = 0.5938107132944769\n",
      "Epoch 95: Train Loss = 0.16926577900668963, Recall = 0.9881346578366446, Aging Rate = 0.5920066611157369\n",
      "Test Loss = 0.18167743569458653, Recall = 0.9863713798977853, Aging Rate = 0.5703580349708576, Efficiency = 1.7293897904923632\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.16862140284489832, Recall = 0.9870309050772627, Aging Rate = 0.5915903413821815\n",
      "Epoch 97: Train Loss = 0.16935166552385753, Recall = 0.9889624724061811, Aging Rate = 0.5997779628087705\n",
      "Epoch 98: Train Loss = 0.16918147897310068, Recall = 0.9875827814569537, Aging Rate = 0.5925617540938107\n",
      "Epoch 99: Train Loss = 0.16940501381085046, Recall = 0.9862030905077263, Aging Rate = 0.5924229808492922\n",
      "Epoch 100: Train Loss = 0.16891539518266727, Recall = 0.9875827814569537, Aging Rate = 0.5940882597835138\n",
      "Test Loss = 0.18154656256763863, Recall = 0.9931856899488927, Aging Rate = 0.6219816819317235, Efficiency = 1.5968085601753002\n",
      "\n",
      "Epoch 101: Train Loss = 0.16862294737114167, Recall = 0.9884105960264901, Aging Rate = 0.5927005273383291\n",
      "Epoch 102: Train Loss = 0.1689529167408087, Recall = 0.9881346578366446, Aging Rate = 0.5938107132944769\n",
      "Epoch 103: Train Loss = 0.16935902406714076, Recall = 0.9878587196467992, Aging Rate = 0.5950596724951429\n",
      "Epoch 104: Train Loss = 0.1698482493551248, Recall = 0.9884105960264901, Aging Rate = 0.5964474049403276\n",
      "Epoch 105: Train Loss = 0.17014145332923902, Recall = 0.9875827814569537, Aging Rate = 0.594782126006106\n",
      "Test Loss = 0.18130865344447358, Recall = 0.9914821124361158, Aging Rate = 0.5928393005828476, Efficiency = 1.6724297709970415\n",
      "\n",
      "Epoch 106: Train Loss = 0.16980023298318472, Recall = 0.9897902869757175, Aging Rate = 0.5976963641409936\n",
      "Epoch 107: Train Loss = 0.16939959580571395, Recall = 0.9875827814569537, Aging Rate = 0.5942270330280321\n",
      "Epoch 108: Train Loss = 0.16853603220237418, Recall = 0.9884105960264901, Aging Rate = 0.5927005273383291\n",
      "Epoch 109: Train Loss = 0.16891457359227677, Recall = 0.9892384105960265, Aging Rate = 0.5936719400499584\n",
      "Epoch 110: Train Loss = 0.16832297991587988, Recall = 0.9892384105960265, Aging Rate = 0.5949208992506245\n",
      "Test Loss = 0.18000549867091628, Recall = 0.9829642248722317, Aging Rate = 0.5616153205661948, Efficiency = 1.7502446449979423\n",
      "Model in epoch 110 is saved.\n",
      "\n",
      "Epoch 111: Train Loss = 0.16927244489205534, Recall = 0.9862030905077263, Aging Rate = 0.5922842076047738\n",
      "Epoch 112: Train Loss = 0.16828006688766997, Recall = 0.9886865342163356, Aging Rate = 0.5917291146267\n",
      "Epoch 113: Train Loss = 0.16873711215649054, Recall = 0.9881346578366446, Aging Rate = 0.5957535387177352\n",
      "Epoch 114: Train Loss = 0.16935441511275534, Recall = 0.9884105960264901, Aging Rate = 0.5935331668054399\n",
      "Epoch 115: Train Loss = 0.16905586838374825, Recall = 0.9870309050772627, Aging Rate = 0.5908964751595892\n",
      "Test Loss = 0.18275443667376864, Recall = 0.989778534923339, Aging Rate = 0.6019983347210658, Efficiency = 1.644154911060345\n",
      "\n",
      "Epoch 116: Train Loss = 0.1694085147358992, Recall = 0.9884105960264901, Aging Rate = 0.5931168470718845\n",
      "Epoch 117: Train Loss = 0.16879742373170833, Recall = 0.9873068432671082, Aging Rate = 0.5945045795170691\n",
      "Epoch 118: Train Loss = 0.16944613712812642, Recall = 0.9892384105960265, Aging Rate = 0.5967249514293644\n",
      "Epoch 119: Train Loss = 0.1695623719713736, Recall = 0.9870309050772627, Aging Rate = 0.5933943935609215\n",
      "Epoch 120: Train Loss = 0.16831512518005837, Recall = 0.9881346578366446, Aging Rate = 0.5933943935609215\n",
      "Test Loss = 0.1827110617136975, Recall = 0.9940374787052811, Aging Rate = 0.6128226477935054, Efficiency = 1.6220638484294236\n",
      "\n",
      "Epoch 121: Train Loss = 0.16809127386943717, Recall = 0.9889624724061811, Aging Rate = 0.5946433527615875\n",
      "Epoch 122: Train Loss = 0.16891954549726035, Recall = 0.9889624724061811, Aging Rate = 0.5946433527615875\n",
      "Epoch 123: Train Loss = 0.1704945385935834, Recall = 0.9881346578366446, Aging Rate = 0.5997779628087705\n",
      "Epoch 124: Train Loss = 0.16841880567713047, Recall = 0.9881346578366446, Aging Rate = 0.5927005273383291\n",
      "Epoch 125: Train Loss = 0.16798583639187512, Recall = 0.9892384105960265, Aging Rate = 0.593255620316403\n",
      "Test Loss = 0.18075252176075554, Recall = 0.9829642248722317, Aging Rate = 0.5857618651124064, Efficiency = 1.6780952578786439\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126: Train Loss = 0.1697829757304513, Recall = 0.9884105960264901, Aging Rate = 0.5956147654732168\n",
      "Epoch 127: Train Loss = 0.17039705795147272, Recall = 0.9884105960264901, Aging Rate = 0.5961698584512906\n",
      "Epoch 128: Train Loss = 0.1689803592081107, Recall = 0.9873068432671082, Aging Rate = 0.5910352484041077\n",
      "Epoch 129: Train Loss = 0.16771357559244598, Recall = 0.9878587196467992, Aging Rate = 0.5943658062725506\n",
      "Epoch 130: Train Loss = 0.16861550854025303, Recall = 0.9870309050772627, Aging Rate = 0.5938107132944769\n",
      "Test Loss = 0.17947929737768403, Recall = 0.9906303236797275, Aging Rate = 0.601165695253955, Efficiency = 1.6478490290148002\n",
      "\n",
      "Epoch 131: Train Loss = 0.169122649705678, Recall = 0.9886865342163356, Aging Rate = 0.5943658062725506\n",
      "Epoch 132: Train Loss = 0.16804549681571268, Recall = 0.9886865342163356, Aging Rate = 0.5943658062725506\n",
      "Epoch 133: Train Loss = 0.16883756975531414, Recall = 0.9862030905077263, Aging Rate = 0.5915903413821815\n",
      "Epoch 134: Train Loss = 0.16886562028759688, Recall = 0.9884105960264901, Aging Rate = 0.5927005273383291\n",
      "Epoch 135: Train Loss = 0.16849960524060478, Recall = 0.9875827814569537, Aging Rate = 0.5921454343602554\n",
      "Test Loss = 0.18195641377039695, Recall = 0.9787052810902896, Aging Rate = 0.5524562864279767, Efficiency = 1.7715524058976537\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.16944079502254866, Recall = 0.9867549668874173, Aging Rate = 0.5917291146267\n",
      "Epoch 137: Train Loss = 0.16834223634777154, Recall = 0.9886865342163356, Aging Rate = 0.5925617540938107\n",
      "Epoch 138: Train Loss = 0.1682919855791762, Recall = 0.9889624724061811, Aging Rate = 0.593255620316403\n",
      "Epoch 139: Train Loss = 0.17014387871965647, Recall = 0.9873068432671082, Aging Rate = 0.5915903413821815\n",
      "Epoch 140: Train Loss = 0.16849363179809676, Recall = 0.9886865342163356, Aging Rate = 0.5914515681376631\n",
      "Test Loss = 0.17985518251876847, Recall = 0.9838160136286201, Aging Rate = 0.5874271440466278, Efficiency = 1.6747881109195855\n",
      "\n",
      "Epoch 141: Train Loss = 0.16839431195235272, Recall = 0.9884105960264901, Aging Rate = 0.5940882597835138\n",
      "Epoch 142: Train Loss = 0.16893336084231117, Recall = 0.9906181015452539, Aging Rate = 0.596308631695809\n",
      "Epoch 143: Train Loss = 0.1681599560509845, Recall = 0.9881346578366446, Aging Rate = 0.593255620316403\n",
      "Epoch 144: Train Loss = 0.16902049746888564, Recall = 0.9884105960264901, Aging Rate = 0.5956147654732168\n",
      "Epoch 145: Train Loss = 0.16824622455154098, Recall = 0.9892384105960265, Aging Rate = 0.5922842076047738\n",
      "Test Loss = 0.18284285054863939, Recall = 0.9940374787052811, Aging Rate = 0.6248959200666112, Efficiency = 1.5907248405335606\n",
      "\n",
      "Epoch 146: Train Loss = 0.16881237932990864, Recall = 0.9886865342163356, Aging Rate = 0.5956147654732168\n",
      "Epoch 147: Train Loss = 0.16809441671581887, Recall = 0.9881346578366446, Aging Rate = 0.5946433527615875\n",
      "Epoch 148: Train Loss = 0.16897536050750586, Recall = 0.9873068432671082, Aging Rate = 0.5918678878712185\n",
      "Epoch 149: Train Loss = 0.16873171544227208, Recall = 0.9886865342163356, Aging Rate = 0.5967249514293644\n",
      "Epoch 150: Train Loss = 0.169202282917258, Recall = 0.9878587196467992, Aging Rate = 0.5958923119622537\n",
      "Test Loss = 0.18154285300979012, Recall = 0.989778534923339, Aging Rate = 0.6165695253955037, Efficiency = 1.6052991237856686\n",
      "\n",
      "Test Loss = 1.3001844158248252, Recall = 0.6486486486486487, Aging Rate = 0.6645006128159471, Efficiency = 0.9761445307603736\n",
      "\n",
      "Starting training Dataset 6:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2056307abeee4a27a20f2b6debf3b1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.2858043546041955, Recall = 0.9758606213266163, Aging Rate = 0.8249222674001435\n",
      "Epoch 2: Train Loss = 0.2546153400210039, Recall = 0.985936188077246, Aging Rate = 0.8098541018895001\n",
      "Epoch 3: Train Loss = 0.25438478971120704, Recall = 0.987615449202351, Aging Rate = 0.8097345132743363\n",
      "Epoch 4: Train Loss = 0.2529091897855618, Recall = 0.9899244332493703, Aging Rate = 0.8109303994259747\n",
      "Epoch 5: Train Loss = 0.2524073278432085, Recall = 0.9867758186397985, Aging Rate = 0.8060272662042574\n",
      "Test Loss = 0.24810686049023523, Recall = 0.9980964467005076, Aging Rate = 0.836441893830703, Efficiency = 1.1932645197825043\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2396546390403888, Recall = 0.9907640638119227, Aging Rate = 0.7961014111456589\n",
      "Epoch 7: Train Loss = 0.23704597935583055, Recall = 0.9895046179680941, Aging Rate = 0.7831858407079646\n",
      "Epoch 8: Train Loss = 0.23319557523348206, Recall = 0.9897145256087322, Aging Rate = 0.782827074862473\n",
      "Epoch 9: Train Loss = 0.2311381075091979, Recall = 0.9905541561712846, Aging Rate = 0.7779239416407558\n",
      "Epoch 10: Train Loss = 0.22858674558005646, Recall = 0.9907640638119227, Aging Rate = 0.7758909351829706\n",
      "Test Loss = 0.23101819549263636, Recall = 0.9955583756345178, Aging Rate = 0.7754662840746055, Efficiency = 1.2838190173340245\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.22721160435414264, Recall = 0.9909739714525608, Aging Rate = 0.7739775173403493\n",
      "Epoch 12: Train Loss = 0.22662240948240256, Recall = 0.9916036943744753, Aging Rate = 0.7725424539583832\n",
      "Epoch 13: Train Loss = 0.2253768542463829, Recall = 0.9920235096557515, Aging Rate = 0.7688352068883042\n",
      "Epoch 14: Train Loss = 0.22268384475509564, Recall = 0.9903442485306465, Aging Rate = 0.7685960296579766\n",
      "Epoch 15: Train Loss = 0.22239141547916552, Recall = 0.9918136020151134, Aging Rate = 0.7679980865821574\n",
      "Test Loss = 0.22503643180410696, Recall = 0.9955583756345178, Aging Rate = 0.7747489239598279, Efficiency = 1.2850077386310275\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.22051202298222874, Recall = 0.9916036943744753, Aging Rate = 0.7650083712030614\n",
      "Epoch 17: Train Loss = 0.2194683447932832, Recall = 0.9922334172963896, Aging Rate = 0.7656063142788806\n",
      "Epoch 18: Train Loss = 0.2191931095470569, Recall = 0.9911838790931989, Aging Rate = 0.7662042573546999\n",
      "Epoch 19: Train Loss = 0.21931175125597538, Recall = 0.9907640638119227, Aging Rate = 0.7632145419756039\n",
      "Epoch 20: Train Loss = 0.21782077832160399, Recall = 0.9924433249370277, Aging Rate = 0.7640516622817508\n",
      "Test Loss = 0.22073518075423057, Recall = 0.9949238578680203, Aging Rate = 0.7514347202295553, Efficiency = 1.3240323049269784\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.21726731768181323, Recall = 0.9924433249370277, Aging Rate = 0.7608227696723272\n",
      "Epoch 22: Train Loss = 0.21629997221113706, Recall = 0.993073047858942, Aging Rate = 0.7648887825878976\n",
      "Epoch 23: Train Loss = 0.21605626686683999, Recall = 0.9918136020151134, Aging Rate = 0.7608227696723272\n",
      "Epoch 24: Train Loss = 0.21618923229824497, Recall = 0.9916036943744753, Aging Rate = 0.7578330542932313\n",
      "Epoch 25: Train Loss = 0.2151421945336275, Recall = 0.9932829554995802, Aging Rate = 0.7634537192059316\n",
      "Test Loss = 0.21715610103422464, Recall = 0.9955583756345178, Aging Rate = 0.7614777618364419, Efficiency = 1.3074030686851803\n",
      "\n",
      "Epoch 26: Train Loss = 0.21513652480039205, Recall = 0.9907640638119227, Aging Rate = 0.7566371681415929\n",
      "Epoch 27: Train Loss = 0.2146538692715635, Recall = 0.9920235096557515, Aging Rate = 0.7569959339870844\n",
      "Epoch 28: Train Loss = 0.214377264205533, Recall = 0.9913937867338372, Aging Rate = 0.7617794785936379\n",
      "Epoch 29: Train Loss = 0.21350735650838662, Recall = 0.9916036943744753, Aging Rate = 0.7561588136809376\n",
      "Epoch 30: Train Loss = 0.21380582135335535, Recall = 0.9928631402183039, Aging Rate = 0.7597464721358527\n",
      "Test Loss = 0.21850613619025844, Recall = 0.9955583756345178, Aging Rate = 0.7539454806312769, Efficiency = 1.32046465958824\n",
      "\n",
      "Epoch 31: Train Loss = 0.21322966539682997, Recall = 0.9913937867338372, Aging Rate = 0.75436498445348\n",
      "Epoch 32: Train Loss = 0.21345147918183618, Recall = 0.9928631402183039, Aging Rate = 0.7567567567567568\n",
      "Epoch 33: Train Loss = 0.2133116849556234, Recall = 0.9926532325776658, Aging Rate = 0.7579526429083951\n",
      "Epoch 34: Train Loss = 0.21230607475355692, Recall = 0.9920235096557515, Aging Rate = 0.7569959339870844\n",
      "Epoch 35: Train Loss = 0.21229550348631762, Recall = 0.9926532325776658, Aging Rate = 0.758072231523559\n",
      "Test Loss = 0.2164489341261055, Recall = 0.9942893401015228, Aging Rate = 0.7417503586800573, Efficiency = 1.3404635603631148\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.21173667434947796, Recall = 0.9926532325776658, Aging Rate = 0.7549629275292992\n",
      "Epoch 37: Train Loss = 0.21244057123383334, Recall = 0.9928631402183039, Aging Rate = 0.7561588136809376\n",
      "Epoch 38: Train Loss = 0.2112005857567672, Recall = 0.9926532325776658, Aging Rate = 0.7523319779956948\n",
      "Epoch 39: Train Loss = 0.21180473697139898, Recall = 0.9922334172963896, Aging Rate = 0.7532886869170055\n",
      "Epoch 40: Train Loss = 0.21043620626198223, Recall = 0.9922334172963896, Aging Rate = 0.7538866299928246\n",
      "Test Loss = 0.2190832603439539, Recall = 0.993020304568528, Aging Rate = 0.7299139167862266, Efficiency = 1.3604621971534987\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.21106386927790233, Recall = 0.9918136020151134, Aging Rate = 0.7548433389141354\n",
      "Epoch 42: Train Loss = 0.20989596660172533, Recall = 0.9932829554995802, Aging Rate = 0.7552021047596269\n",
      "Epoch 43: Train Loss = 0.21064164805814314, Recall = 0.9913937867338372, Aging Rate = 0.7502989715379096\n",
      "Epoch 44: Train Loss = 0.209937633851646, Recall = 0.9911838790931989, Aging Rate = 0.7531690983018416\n",
      "Epoch 45: Train Loss = 0.21031286188772608, Recall = 0.9922334172963896, Aging Rate = 0.7537670413776608\n",
      "Test Loss = 0.2158525651975547, Recall = 0.9955583756345178, Aging Rate = 0.7514347202295553, Efficiency = 1.3248767132847123\n",
      "\n",
      "Epoch 46: Train Loss = 0.20955888869602182, Recall = 0.9924433249370277, Aging Rate = 0.7549629275292992\n",
      "Epoch 47: Train Loss = 0.20874092425848986, Recall = 0.993073047858942, Aging Rate = 0.7487443195407797\n",
      "Epoch 48: Train Loss = 0.20978477265426515, Recall = 0.9916036943744753, Aging Rate = 0.7547237502989715\n",
      "Epoch 49: Train Loss = 0.20914966208716382, Recall = 0.9928631402183039, Aging Rate = 0.7501793829227458\n",
      "Epoch 50: Train Loss = 0.20899676303634288, Recall = 0.9918136020151134, Aging Rate = 0.7473092561588137\n",
      "Test Loss = 0.21366948368217545, Recall = 0.9955583756345178, Aging Rate = 0.7453371592539455, Efficiency = 1.3357154542970588\n",
      "\n",
      "Epoch 51: Train Loss = 0.20901490092505623, Recall = 0.9920235096557515, Aging Rate = 0.7483855536952883\n",
      "Epoch 52: Train Loss = 0.20807656946578, Recall = 0.9932829554995802, Aging Rate = 0.7530495096866778\n",
      "Epoch 53: Train Loss = 0.2078283655033303, Recall = 0.9922334172963896, Aging Rate = 0.7475484333891413\n",
      "Epoch 54: Train Loss = 0.20767137096761318, Recall = 0.9928631402183039, Aging Rate = 0.7485051423104521\n",
      "Epoch 55: Train Loss = 0.20777798468802597, Recall = 0.9928631402183039, Aging Rate = 0.750059794307582\n",
      "Test Loss = 0.21242311579767226, Recall = 0.9961928934010152, Aging Rate = 0.7564562410329986, Efficiency = 1.3169206970537133\n",
      "\n",
      "Epoch 56: Train Loss = 0.20836012722259217, Recall = 0.9926532325776658, Aging Rate = 0.7520928007653671\n",
      "Epoch 57: Train Loss = 0.2068329128073437, Recall = 0.9934928631402183, Aging Rate = 0.7501793829227458\n",
      "Epoch 58: Train Loss = 0.20733910358157406, Recall = 0.9918136020151134, Aging Rate = 0.7471896675436498\n",
      "Epoch 59: Train Loss = 0.20656914423809927, Recall = 0.9926532325776658, Aging Rate = 0.7491030853862712\n",
      "Epoch 60: Train Loss = 0.20704486151042095, Recall = 0.993073047858942, Aging Rate = 0.7501793829227458\n",
      "Test Loss = 0.21111957440246298, Recall = 0.9949238578680203, Aging Rate = 0.7446197991391679, Efficiency = 1.3361501341445927\n",
      "\n",
      "Epoch 61: Train Loss = 0.2056686568455501, Recall = 0.9924433249370277, Aging Rate = 0.7440803635493901\n",
      "Epoch 62: Train Loss = 0.2062118954847829, Recall = 0.9920235096557515, Aging Rate = 0.7469504903133222\n",
      "Epoch 63: Train Loss = 0.20654923081241194, Recall = 0.9922334172963896, Aging Rate = 0.7457546041616838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss = 0.20569463345785513, Recall = 0.9926532325776658, Aging Rate = 0.7461133700071754\n",
      "Epoch 65: Train Loss = 0.20607189159805597, Recall = 0.9926532325776658, Aging Rate = 0.7471896675436498\n",
      "Test Loss = 0.21085259805792886, Recall = 0.9955583756345178, Aging Rate = 0.7586083213773315, Efficiency = 1.3123483284542619\n",
      "\n",
      "Epoch 66: Train Loss = 0.20601485223041371, Recall = 0.9920235096557515, Aging Rate = 0.746352547237503\n",
      "Epoch 67: Train Loss = 0.2068596507260686, Recall = 0.9899244332493703, Aging Rate = 0.746352547237503\n",
      "Epoch 68: Train Loss = 0.2051439077300023, Recall = 0.9922334172963896, Aging Rate = 0.746352547237503\n",
      "Epoch 69: Train Loss = 0.2057234600410881, Recall = 0.993073047858942, Aging Rate = 0.7450370724707008\n",
      "Epoch 70: Train Loss = 0.20530142149153122, Recall = 0.9920235096557515, Aging Rate = 0.7458741927768476\n",
      "Test Loss = 0.21022037283077816, Recall = 0.9974619289340102, Aging Rate = 0.7664992826398852, Efficiency = 1.3013213952209546\n",
      "\n",
      "Epoch 71: Train Loss = 0.20516096118234384, Recall = 0.9911838790931989, Aging Rate = 0.7425257115522602\n",
      "Epoch 72: Train Loss = 0.20363905962803408, Recall = 0.9920235096557515, Aging Rate = 0.7445587180100455\n",
      "Epoch 73: Train Loss = 0.20474699265048482, Recall = 0.9928631402183039, Aging Rate = 0.7437215977038986\n",
      "Epoch 74: Train Loss = 0.20407023077175337, Recall = 0.9926532325776658, Aging Rate = 0.7436020090887348\n",
      "Epoch 75: Train Loss = 0.20329716026469355, Recall = 0.9924433249370277, Aging Rate = 0.7455154269313561\n",
      "Test Loss = 0.20783088586080706, Recall = 0.9968274111675127, Aging Rate = 0.7657819225251076, Efficiency = 1.3017118435799997\n",
      "\n",
      "Epoch 76: Train Loss = 0.20306858149506607, Recall = 0.9926532325776658, Aging Rate = 0.7433628318584071\n",
      "Epoch 77: Train Loss = 0.2033853159833827, Recall = 0.9928631402183039, Aging Rate = 0.7453958383161923\n",
      "Epoch 78: Train Loss = 0.2033324106383683, Recall = 0.9920235096557515, Aging Rate = 0.7409710595551303\n",
      "Epoch 79: Train Loss = 0.20333060235162867, Recall = 0.9928631402183039, Aging Rate = 0.7433628318584071\n",
      "Epoch 80: Train Loss = 0.20247370854315933, Recall = 0.993073047858942, Aging Rate = 0.7418081798612772\n",
      "Test Loss = 0.20773374679440917, Recall = 0.9974619289340102, Aging Rate = 0.7600430416068866, Efficiency = 1.3123755645488389\n",
      "\n",
      "Epoch 81: Train Loss = 0.20203712815675004, Recall = 0.9928631402183039, Aging Rate = 0.740492705094475\n",
      "Epoch 82: Train Loss = 0.2037454946592532, Recall = 0.9926532325776658, Aging Rate = 0.7449174838555369\n",
      "Epoch 83: Train Loss = 0.2034788155592791, Recall = 0.9924433249370277, Aging Rate = 0.7421669457067687\n",
      "Epoch 84: Train Loss = 0.20128303143260182, Recall = 0.9928631402183039, Aging Rate = 0.7385792872518536\n",
      "Epoch 85: Train Loss = 0.20243341805575082, Recall = 0.9913937867338372, Aging Rate = 0.742764888782588\n",
      "Test Loss = 0.20683294286686857, Recall = 0.9961928934010152, Aging Rate = 0.7543041606886657, Efficiency = 1.320677960048278\n",
      "\n",
      "Epoch 86: Train Loss = 0.20230796283097166, Recall = 0.9922334172963896, Aging Rate = 0.7373834011002153\n",
      "Epoch 87: Train Loss = 0.20194303876074612, Recall = 0.9932829554995802, Aging Rate = 0.742764888782588\n",
      "Epoch 88: Train Loss = 0.20170753285434936, Recall = 0.993073047858942, Aging Rate = 0.7409710595551303\n",
      "Epoch 89: Train Loss = 0.20116965064618098, Recall = 0.9924433249370277, Aging Rate = 0.7395359961731643\n",
      "Epoch 90: Train Loss = 0.20108280494306624, Recall = 0.9922334172963896, Aging Rate = 0.7382205214063621\n",
      "Test Loss = 0.20610501041029242, Recall = 0.9961928934010152, Aging Rate = 0.746413199426112, Efficiency = 1.334639956555631\n",
      "\n",
      "Training Finished at epoch 90.\n",
      "Test Loss = 2.2711614366955293, Recall = 0.8918918918918919, Aging Rate = 0.8855999814998959, Efficiency = 1.0071046753075727\n",
      "\n",
      "Starting training Dataset 7:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a993eb024245dabc2fcba98bf785ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.29062984937689657, Recall = 0.9549524342473419, Aging Rate = 0.7060291060291061\n",
      "Epoch 2: Train Loss = 0.273813507268599, Recall = 0.9706211527700056, Aging Rate = 0.7021483021483022\n",
      "Epoch 3: Train Loss = 0.26821521214123656, Recall = 0.9697817571348629, Aging Rate = 0.6948024948024948\n",
      "Epoch 4: Train Loss = 0.26508021632458545, Recall = 0.9714605484051483, Aging Rate = 0.6942480942480942\n",
      "Epoch 5: Train Loss = 0.25874932070036194, Recall = 0.9731393396754336, Aging Rate = 0.6864864864864865\n",
      "Test Loss = 0.23247878825342333, Recall = 0.9830097087378641, Aging Rate = 0.7076923076923077, Efficiency = 1.389035438371481\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2352390339018484, Recall = 0.9829322887520985, Aging Rate = 0.6625086625086625\n",
      "Epoch 7: Train Loss = 0.22568386092998877, Recall = 0.9829322887520985, Aging Rate = 0.6501732501732502\n",
      "Epoch 8: Train Loss = 0.21916956890191128, Recall = 0.985170677112479, Aging Rate = 0.6418572418572419\n",
      "Epoch 9: Train Loss = 0.21282523213701784, Recall = 0.9862898712926693, Aging Rate = 0.6354816354816355\n",
      "Epoch 10: Train Loss = 0.2082761547297797, Recall = 0.985170677112479, Aging Rate = 0.6267498267498267\n",
      "Test Loss = 0.20308466184411872, Recall = 0.9797734627831716, Aging Rate = 0.637006237006237, Efficiency = 1.5380908231086443\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.20602013642227823, Recall = 0.9879686625629547, Aging Rate = 0.6232848232848233\n",
      "Epoch 12: Train Loss = 0.201322820970604, Recall = 0.9874090654728596, Aging Rate = 0.6212058212058212\n",
      "Epoch 13: Train Loss = 0.20019635825172036, Recall = 0.9882484611080022, Aging Rate = 0.616909216909217\n",
      "Epoch 14: Train Loss = 0.19836029811857148, Recall = 0.9893676552881925, Aging Rate = 0.6167706167706167\n",
      "Epoch 15: Train Loss = 0.19542365142519244, Recall = 0.9885282596530498, Aging Rate = 0.6138600138600139\n",
      "Test Loss = 0.19549014676137674, Recall = 0.9797734627831716, Aging Rate = 0.6191268191268191, Efficiency = 1.5825084888745455\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.19377909872322055, Recall = 0.9885282596530498, Aging Rate = 0.6153846153846154\n",
      "Epoch 17: Train Loss = 0.19164585299113518, Recall = 0.98964745383324, Aging Rate = 0.6088704088704089\n",
      "Epoch 18: Train Loss = 0.19042348155567237, Recall = 0.9879686625629547, Aging Rate = 0.608038808038808\n",
      "Epoch 19: Train Loss = 0.189605334083074, Recall = 0.9893676552881925, Aging Rate = 0.6074844074844075\n",
      "Epoch 20: Train Loss = 0.1886397509321271, Recall = 0.9899272523782876, Aging Rate = 0.606930006930007\n",
      "Test Loss = 0.1974722829653171, Recall = 0.9724919093851133, Aging Rate = 0.577962577962578, Efficiency = 1.6826208644634284\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.1883359296302868, Recall = 0.98964745383324, Aging Rate = 0.6031878031878032\n",
      "Epoch 22: Train Loss = 0.18710541041336867, Recall = 0.9902070509233352, Aging Rate = 0.6056826056826057\n",
      "Epoch 23: Train Loss = 0.18717449620335117, Recall = 0.9910464465584778, Aging Rate = 0.6051282051282051\n",
      "Epoch 24: Train Loss = 0.18609123664520222, Recall = 0.9913262451035255, Aging Rate = 0.604019404019404\n",
      "Epoch 25: Train Loss = 0.18455432743061276, Recall = 0.9921656407386682, Aging Rate = 0.6026334026334026\n",
      "Test Loss = 0.18913184135470718, Recall = 0.9797734627831716, Aging Rate = 0.5987525987525988, Efficiency = 1.636357735166058\n",
      "\n",
      "Epoch 26: Train Loss = 0.18467797070886224, Recall = 0.9910464465584778, Aging Rate = 0.6031878031878032\n",
      "Epoch 27: Train Loss = 0.1837746399273413, Recall = 0.9910464465584778, Aging Rate = 0.5983367983367983\n",
      "Epoch 28: Train Loss = 0.1841411184041928, Recall = 0.9918858421936206, Aging Rate = 0.6033264033264033\n",
      "Epoch 29: Train Loss = 0.18319174523619589, Recall = 0.9899272523782876, Aging Rate = 0.6027720027720028\n",
      "Epoch 30: Train Loss = 0.1832845359028666, Recall = 0.9918858421936206, Aging Rate = 0.5993069993069993\n",
      "Test Loss = 0.19168815948611237, Recall = 0.9797734627831716, Aging Rate = 0.5858627858627858, Efficiency = 1.6723599274473182\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.18271773877816322, Recall = 0.9921656407386682, Aging Rate = 0.5987525987525988\n",
      "Epoch 32: Train Loss = 0.18326961934483885, Recall = 0.9916060436485731, Aging Rate = 0.5970893970893971\n",
      "Epoch 33: Train Loss = 0.18305192651512386, Recall = 0.9910464465584778, Aging Rate = 0.6020790020790021\n",
      "Epoch 34: Train Loss = 0.18236117756622827, Recall = 0.9921656407386682, Aging Rate = 0.5993069993069993\n",
      "Epoch 35: Train Loss = 0.1811647739578929, Recall = 0.9927252378287633, Aging Rate = 0.6015246015246015\n",
      "Test Loss = 0.18915308853940507, Recall = 0.9773462783171522, Aging Rate = 0.5891891891891892, Efficiency = 1.6587987010999592\n",
      "\n",
      "Epoch 36: Train Loss = 0.18028026462551297, Recall = 0.9921656407386682, Aging Rate = 0.5983367983367983\n",
      "Epoch 37: Train Loss = 0.1806689988695245, Recall = 0.9930050363738109, Aging Rate = 0.5966735966735967\n",
      "Epoch 38: Train Loss = 0.1801592798197509, Recall = 0.9924454392837158, Aging Rate = 0.5981981981981982\n",
      "Epoch 39: Train Loss = 0.1792035511485091, Recall = 0.9921656407386682, Aging Rate = 0.5993069993069993\n",
      "Epoch 40: Train Loss = 0.1794529611297721, Recall = 0.9916060436485731, Aging Rate = 0.5941787941787942\n",
      "Test Loss = 0.1850550485995604, Recall = 0.9854368932038835, Aging Rate = 0.6103950103950104, Efficiency = 1.6144248564907504\n",
      "\n",
      "Epoch 41: Train Loss = 0.1788780106163157, Recall = 0.9921656407386682, Aging Rate = 0.5941787941787942\n",
      "Epoch 42: Train Loss = 0.1789305000299393, Recall = 0.9932848349188584, Aging Rate = 0.5963963963963964\n",
      "Epoch 43: Train Loss = 0.1797368020149708, Recall = 0.9902070509233352, Aging Rate = 0.5963963963963964\n",
      "Epoch 44: Train Loss = 0.1792822370079765, Recall = 0.9921656407386682, Aging Rate = 0.5952875952875953\n",
      "Epoch 45: Train Loss = 0.17902908811780702, Recall = 0.9910464465584778, Aging Rate = 0.5930699930699931\n",
      "Test Loss = 0.18488665534775867, Recall = 0.982200647249191, Aging Rate = 0.6, Efficiency = 1.6370010514653006\n",
      "\n",
      "Epoch 46: Train Loss = 0.17918467949804795, Recall = 0.9932848349188584, Aging Rate = 0.5970893970893971\n",
      "Epoch 47: Train Loss = 0.17825279773438216, Recall = 0.9918858421936206, Aging Rate = 0.5894663894663895\n",
      "Epoch 48: Train Loss = 0.17797164174102695, Recall = 0.9932848349188584, Aging Rate = 0.5981981981981982\n",
      "Epoch 49: Train Loss = 0.17773709566082627, Recall = 0.9924454392837158, Aging Rate = 0.5923769923769924\n",
      "Epoch 50: Train Loss = 0.17870932040698645, Recall = 0.993564633463906, Aging Rate = 0.5977823977823978\n",
      "Test Loss = 0.1843308883744317, Recall = 0.9830097087378641, Aging Rate = 0.6070686070686071, Efficiency = 1.6192728154596243\n",
      "\n",
      "Epoch 51: Train Loss = 0.178012904400885, Recall = 0.9927252378287633, Aging Rate = 0.595010395010395\n",
      "Epoch 52: Train Loss = 0.17787697063868688, Recall = 0.9932848349188584, Aging Rate = 0.5896049896049896\n",
      "Epoch 53: Train Loss = 0.17768736736938018, Recall = 0.9924454392837158, Aging Rate = 0.5951489951489951\n",
      "Epoch 54: Train Loss = 0.17653248321654808, Recall = 0.9938444320089536, Aging Rate = 0.5970893970893971\n",
      "Epoch 55: Train Loss = 0.17719052284852713, Recall = 0.9924454392837158, Aging Rate = 0.5920997920997921\n",
      "Test Loss = 0.18396463254137496, Recall = 0.9862459546925566, Aging Rate = 0.6270270270270271, Efficiency = 1.572892230243727\n",
      "\n",
      "Epoch 56: Train Loss = 0.17712730716498296, Recall = 0.9927252378287633, Aging Rate = 0.5930699930699931\n",
      "Epoch 57: Train Loss = 0.17734982042740552, Recall = 0.9918858421936206, Aging Rate = 0.5922383922383923\n",
      "Epoch 58: Train Loss = 0.176953390075445, Recall = 0.9930050363738109, Aging Rate = 0.5943173943173943\n",
      "Epoch 59: Train Loss = 0.1770709776291811, Recall = 0.9932848349188584, Aging Rate = 0.5926541926541926\n",
      "Epoch 60: Train Loss = 0.17687476859353768, Recall = 0.9930050363738109, Aging Rate = 0.5948717948717949\n",
      "Test Loss = 0.18321264690396197, Recall = 0.9830097087378641, Aging Rate = 0.6116424116424116, Efficiency = 1.6071640454536151\n",
      "\n",
      "Epoch 61: Train Loss = 0.17688203857990908, Recall = 0.9924454392837158, Aging Rate = 0.5915453915453915\n",
      "Epoch 62: Train Loss = 0.17682032470932788, Recall = 0.993564633463906, Aging Rate = 0.5944559944559945\n",
      "Epoch 63: Train Loss = 0.17609996204977638, Recall = 0.9927252378287633, Aging Rate = 0.5916839916839917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss = 0.1762806420142834, Recall = 0.9918858421936206, Aging Rate = 0.5925155925155925\n",
      "Epoch 65: Train Loss = 0.1758640331521434, Recall = 0.9927252378287633, Aging Rate = 0.5945945945945946\n",
      "Test Loss = 0.1836488186073898, Recall = 0.9846278317152104, Aging Rate = 0.6033264033264033, Efficiency = 1.631998549983126\n",
      "\n",
      "Epoch 66: Train Loss = 0.17684898221980178, Recall = 0.9930050363738109, Aging Rate = 0.5933471933471933\n",
      "Epoch 67: Train Loss = 0.175909797367088, Recall = 0.9932848349188584, Aging Rate = 0.591961191961192\n",
      "Epoch 68: Train Loss = 0.17583897141268579, Recall = 0.9924454392837158, Aging Rate = 0.5932085932085932\n",
      "Epoch 69: Train Loss = 0.17472340304324466, Recall = 0.9924454392837158, Aging Rate = 0.5925155925155925\n",
      "Epoch 70: Train Loss = 0.17562341853222413, Recall = 0.9932848349188584, Aging Rate = 0.5911295911295912\n",
      "Test Loss = 0.18439266181041694, Recall = 0.9805825242718447, Aging Rate = 0.5954261954261955, Efficiency = 1.6468581922254515\n",
      "\n",
      "Epoch 71: Train Loss = 0.17536651494184138, Recall = 0.9932848349188584, Aging Rate = 0.5911295911295912\n",
      "Epoch 72: Train Loss = 0.17465484527069416, Recall = 0.9941242305540011, Aging Rate = 0.5904365904365905\n",
      "Epoch 73: Train Loss = 0.17538961268347003, Recall = 0.9918858421936206, Aging Rate = 0.5875259875259875\n",
      "Epoch 74: Train Loss = 0.17445197374103635, Recall = 0.9932848349188584, Aging Rate = 0.5907137907137907\n",
      "Epoch 75: Train Loss = 0.17463882101424766, Recall = 0.9932848349188584, Aging Rate = 0.589050589050589\n",
      "Test Loss = 0.1831708679513971, Recall = 0.9862459546925566, Aging Rate = 0.6245322245322246, Efficiency = 1.5791754214756522\n",
      "\n",
      "Epoch 76: Train Loss = 0.17515728332809666, Recall = 0.9930050363738109, Aging Rate = 0.5914067914067914\n",
      "Epoch 77: Train Loss = 0.1749808406652218, Recall = 0.9930050363738109, Aging Rate = 0.5904365904365905\n",
      "Epoch 78: Train Loss = 0.1749136348193665, Recall = 0.9930050363738109, Aging Rate = 0.5916839916839917\n",
      "Epoch 79: Train Loss = 0.17421190207482046, Recall = 0.9927252378287633, Aging Rate = 0.5891891891891892\n",
      "Epoch 80: Train Loss = 0.17437016419131807, Recall = 0.9921656407386682, Aging Rate = 0.5880803880803881\n",
      "Test Loss = 0.18665972043471624, Recall = 0.9773462783171522, Aging Rate = 0.5738045738045738, Efficiency = 1.703273737963056\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.17467097837273973, Recall = 0.9918858421936206, Aging Rate = 0.590990990990991\n",
      "Epoch 82: Train Loss = 0.1744427526195431, Recall = 0.9924454392837158, Aging Rate = 0.5908523908523908\n",
      "Epoch 83: Train Loss = 0.17378281078360103, Recall = 0.9930050363738109, Aging Rate = 0.586001386001386\n",
      "Epoch 84: Train Loss = 0.17452978157823645, Recall = 0.9938444320089536, Aging Rate = 0.5908523908523908\n",
      "Epoch 85: Train Loss = 0.17356522197700258, Recall = 0.9938444320089536, Aging Rate = 0.5872487872487873\n",
      "Test Loss = 0.18287037601094236, Recall = 0.9813915857605178, Aging Rate = 0.5966735966735967, Efficiency = 1.644771236374423\n",
      "\n",
      "Epoch 86: Train Loss = 0.17428328068985016, Recall = 0.9941242305540011, Aging Rate = 0.5893277893277893\n",
      "Epoch 87: Train Loss = 0.17387946694366485, Recall = 0.9916060436485731, Aging Rate = 0.591961191961192\n",
      "Epoch 88: Train Loss = 0.1741286326388229, Recall = 0.9930050363738109, Aging Rate = 0.5916839916839917\n",
      "Epoch 89: Train Loss = 0.1740472295097568, Recall = 0.9918858421936206, Aging Rate = 0.5880803880803881\n",
      "Epoch 90: Train Loss = 0.17493661471893618, Recall = 0.9941242305540011, Aging Rate = 0.5908523908523908\n",
      "Test Loss = 0.18553147074958143, Recall = 0.9773462783171522, Aging Rate = 0.5796257796257797, Efficiency = 1.6861676892398978\n",
      "\n",
      "Epoch 91: Train Loss = 0.17385042109633186, Recall = 0.993564633463906, Aging Rate = 0.5896049896049896\n",
      "Epoch 92: Train Loss = 0.1735805231729317, Recall = 0.9916060436485731, Aging Rate = 0.5883575883575883\n",
      "Epoch 93: Train Loss = 0.17333266733424663, Recall = 0.993564633463906, Aging Rate = 0.5889119889119889\n",
      "Epoch 94: Train Loss = 0.17383272613986697, Recall = 0.993564633463906, Aging Rate = 0.5918225918225918\n",
      "Epoch 95: Train Loss = 0.17352950558097346, Recall = 0.9927252378287633, Aging Rate = 0.5880803880803881\n",
      "Test Loss = 0.1843471590050045, Recall = 0.9781553398058253, Aging Rate = 0.5883575883575883, Efficiency = 1.6625184114837044\n",
      "\n",
      "Epoch 96: Train Loss = 0.1746595385643813, Recall = 0.9924454392837158, Aging Rate = 0.5872487872487873\n",
      "Epoch 97: Train Loss = 0.17365801227811467, Recall = 0.9932848349188584, Aging Rate = 0.59002079002079\n",
      "Epoch 98: Train Loss = 0.17352464356442251, Recall = 0.9932848349188584, Aging Rate = 0.59002079002079\n",
      "Epoch 99: Train Loss = 0.17376884363297365, Recall = 0.9930050363738109, Aging Rate = 0.5866943866943867\n",
      "Epoch 100: Train Loss = 0.1730798234728088, Recall = 0.9932848349188584, Aging Rate = 0.5902979902979903\n",
      "Test Loss = 0.18127780498437226, Recall = 0.9838187702265372, Aging Rate = 0.6083160083160083, Efficiency = 1.6172823673952021\n",
      "\n",
      "Epoch 101: Train Loss = 0.17350287422155722, Recall = 0.9918858421936206, Aging Rate = 0.5876645876645876\n",
      "Epoch 102: Train Loss = 0.17312308447028893, Recall = 0.9910464465584778, Aging Rate = 0.585031185031185\n",
      "Epoch 103: Train Loss = 0.17323947516441016, Recall = 0.9930050363738109, Aging Rate = 0.5873873873873874\n",
      "Epoch 104: Train Loss = 0.17354721570708895, Recall = 0.9930050363738109, Aging Rate = 0.5914067914067914\n",
      "Epoch 105: Train Loss = 0.17362380178125056, Recall = 0.9927252378287633, Aging Rate = 0.5893277893277893\n",
      "Test Loss = 0.18199881791077135, Recall = 0.9838187702265372, Aging Rate = 0.6079002079002079, Efficiency = 1.6183885796666049\n",
      "\n",
      "Epoch 106: Train Loss = 0.17321349350594847, Recall = 0.9932848349188584, Aging Rate = 0.5886347886347887\n",
      "Epoch 107: Train Loss = 0.1738245343860453, Recall = 0.993564633463906, Aging Rate = 0.5887733887733888\n",
      "Epoch 108: Train Loss = 0.1725111490785456, Recall = 0.9930050363738109, Aging Rate = 0.5858627858627858\n",
      "Epoch 109: Train Loss = 0.17311022066648685, Recall = 0.9924454392837158, Aging Rate = 0.5880803880803881\n",
      "Epoch 110: Train Loss = 0.17271825697425333, Recall = 0.9944040290990487, Aging Rate = 0.589050589050589\n",
      "Test Loss = 0.18223328973300243, Recall = 0.9854368932038835, Aging Rate = 0.6295218295218296, Efficiency = 1.5653736397015214\n",
      "\n",
      "Epoch 111: Train Loss = 0.17294446752662884, Recall = 0.9932848349188584, Aging Rate = 0.5891891891891892\n",
      "Epoch 112: Train Loss = 0.1733112286401968, Recall = 0.9938444320089536, Aging Rate = 0.5887733887733888\n",
      "Epoch 113: Train Loss = 0.17316994301701122, Recall = 0.993564633463906, Aging Rate = 0.5878031878031879\n",
      "Epoch 114: Train Loss = 0.17326178487935004, Recall = 0.9913262451035255, Aging Rate = 0.5875259875259875\n",
      "Epoch 115: Train Loss = 0.17291070546281065, Recall = 0.9921656407386682, Aging Rate = 0.5884961884961885\n",
      "Test Loss = 0.1837265554188195, Recall = 0.9813915857605178, Aging Rate = 0.5920997920997921, Efficiency = 1.657476631946441\n",
      "\n",
      "Epoch 116: Train Loss = 0.17201249194037807, Recall = 0.9924454392837158, Aging Rate = 0.5875259875259875\n",
      "Epoch 117: Train Loss = 0.17302433774649725, Recall = 0.9924454392837158, Aging Rate = 0.585031185031185\n",
      "Epoch 118: Train Loss = 0.17312386364921958, Recall = 0.9924454392837158, Aging Rate = 0.5883575883575883\n",
      "Epoch 119: Train Loss = 0.17223193982121685, Recall = 0.9927252378287633, Aging Rate = 0.586001386001386\n",
      "Epoch 120: Train Loss = 0.17366497845634848, Recall = 0.9924454392837158, Aging Rate = 0.5878031878031879\n",
      "Test Loss = 0.183643327724661, Recall = 0.9813915857605178, Aging Rate = 0.5920997920997921, Efficiency = 1.657476631946441\n",
      "\n",
      "Epoch 121: Train Loss = 0.1728686591092696, Recall = 0.9924454392837158, Aging Rate = 0.5871101871101871\n",
      "Epoch 122: Train Loss = 0.17214843536819424, Recall = 0.9921656407386682, Aging Rate = 0.5862785862785863\n",
      "Epoch 123: Train Loss = 0.17208829604951525, Recall = 0.9924454392837158, Aging Rate = 0.5875259875259875\n",
      "Epoch 124: Train Loss = 0.17383112630039474, Recall = 0.9924454392837158, Aging Rate = 0.591961191961192\n",
      "Epoch 125: Train Loss = 0.17165985964522623, Recall = 0.9932848349188584, Aging Rate = 0.589050589050589\n",
      "Test Loss = 0.18486573646569202, Recall = 0.9773462783171522, Aging Rate = 0.5783783783783784, Efficiency = 1.6898042837620113\n",
      "\n",
      "Epoch 126: Train Loss = 0.17257527175746026, Recall = 0.9918858421936206, Aging Rate = 0.5858627858627858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127: Train Loss = 0.1723761153361571, Recall = 0.9930050363738109, Aging Rate = 0.59002079002079\n",
      "Epoch 128: Train Loss = 0.1715605863703677, Recall = 0.9930050363738109, Aging Rate = 0.5846153846153846\n",
      "Epoch 129: Train Loss = 0.17214581899277798, Recall = 0.9938444320089536, Aging Rate = 0.5898821898821899\n",
      "Epoch 130: Train Loss = 0.17186511178357025, Recall = 0.9916060436485731, Aging Rate = 0.5835065835065835\n",
      "Test Loss = 0.1818227218491124, Recall = 0.9838187702265372, Aging Rate = 0.6024948024948025, Efficiency = 1.632908283729039\n",
      "\n",
      "Training Finished at epoch 130.\n",
      "Test Loss = 1.6490623274622034, Recall = 0.7837837837837838, Aging Rate = 0.8002913766389936, Efficiency = 0.9793730094677923\n",
      "\n",
      "Starting training Dataset 8:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76242663d6e24c9ba5dc73c65906f003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.29679745637288296, Recall = 0.954912329529641, Aging Rate = 0.6862092862092862\n",
      "Epoch 2: Train Loss = 0.2272547010303948, Recall = 0.9810743111605901, Aging Rate = 0.6401940401940402\n",
      "Epoch 3: Train Loss = 0.21095839153952015, Recall = 0.982187586974673, Aging Rate = 0.6256410256410256\n",
      "Epoch 4: Train Loss = 0.2038819552500547, Recall = 0.9819092680211522, Aging Rate = 0.6188496188496189\n",
      "Epoch 5: Train Loss = 0.19849976372859252, Recall = 0.9816309490676315, Aging Rate = 0.6135828135828136\n",
      "Test Loss = 0.18194651983507953, Recall = 0.9868529170090387, Aging Rate = 0.6365904365904366, Efficiency = 1.550216347566319\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1775847813815436, Recall = 0.9844141386028389, Aging Rate = 0.5922383922383923\n",
      "Epoch 7: Train Loss = 0.1750446965875348, Recall = 0.986362371277484, Aging Rate = 0.5887733887733888\n",
      "Epoch 8: Train Loss = 0.17155189838206197, Recall = 0.985249095463401, Aging Rate = 0.5866943866943867\n",
      "Epoch 9: Train Loss = 0.1696943640461087, Recall = 0.9849707765098803, Aging Rate = 0.5832293832293832\n",
      "Epoch 10: Train Loss = 0.1663699218612203, Recall = 0.985249095463401, Aging Rate = 0.5807345807345807\n",
      "Test Loss = 0.1657795174887671, Recall = 0.9827444535743632, Aging Rate = 0.5970893970893971, Efficiency = 1.6458916234419567\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.16635456737427767, Recall = 0.9855274144169218, Aging Rate = 0.5814275814275814\n",
      "Epoch 12: Train Loss = 0.16437182556153665, Recall = 0.9860840523239632, Aging Rate = 0.5803187803187804\n",
      "Epoch 13: Train Loss = 0.1644531182629816, Recall = 0.9866406902310048, Aging Rate = 0.5796257796257797\n",
      "Epoch 14: Train Loss = 0.16351203974633274, Recall = 0.9858057333704425, Aging Rate = 0.5818433818433818\n",
      "Epoch 15: Train Loss = 0.16246332206291533, Recall = 0.9869190091845255, Aging Rate = 0.577962577962578\n",
      "Test Loss = 0.16316412891147042, Recall = 0.9835661462612982, Aging Rate = 0.5987525987525988, Efficiency = 1.6426920432303322\n",
      "\n",
      "Epoch 16: Train Loss = 0.16210215345531062, Recall = 0.9869190091845255, Aging Rate = 0.58004158004158\n",
      "Epoch 17: Train Loss = 0.16307098750884716, Recall = 0.9849707765098803, Aging Rate = 0.5801801801801801\n",
      "Epoch 18: Train Loss = 0.1605210747226443, Recall = 0.9858057333704425, Aging Rate = 0.5786555786555787\n",
      "Epoch 19: Train Loss = 0.1610187941161775, Recall = 0.9858057333704425, Aging Rate = 0.5787941787941788\n",
      "Epoch 20: Train Loss = 0.16134579605058258, Recall = 0.9866406902310048, Aging Rate = 0.5771309771309772\n",
      "Test Loss = 0.1620364046827919, Recall = 0.9819227608874281, Aging Rate = 0.582952182952183, Efficiency = 1.6843967185624273\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.16110555486016528, Recall = 0.9860840523239632, Aging Rate = 0.5776853776853776\n",
      "Epoch 22: Train Loss = 0.16115377666383507, Recall = 0.9860840523239632, Aging Rate = 0.5792099792099792\n",
      "Epoch 23: Train Loss = 0.16005793020738468, Recall = 0.9860840523239632, Aging Rate = 0.5775467775467775\n",
      "Epoch 24: Train Loss = 0.16103601777256155, Recall = 0.9860840523239632, Aging Rate = 0.5804573804573805\n",
      "Epoch 25: Train Loss = 0.16030117154782295, Recall = 0.9866406902310048, Aging Rate = 0.5767151767151767\n",
      "Test Loss = 0.16234042908446455, Recall = 0.9819227608874281, Aging Rate = 0.5933471933471933, Efficiency = 1.654887316141713\n",
      "\n",
      "Epoch 26: Train Loss = 0.15990117201562234, Recall = 0.9858057333704425, Aging Rate = 0.5781011781011781\n",
      "Epoch 27: Train Loss = 0.1604510659900541, Recall = 0.9844141386028389, Aging Rate = 0.577962577962578\n",
      "Epoch 28: Train Loss = 0.16001655149815072, Recall = 0.9871973281380462, Aging Rate = 0.577962577962578\n",
      "Epoch 29: Train Loss = 0.16087597084908706, Recall = 0.9866406902310048, Aging Rate = 0.5768537768537768\n",
      "Epoch 30: Train Loss = 0.16031056391804563, Recall = 0.9860840523239632, Aging Rate = 0.5776853776853776\n",
      "Test Loss = 0.16135457858226404, Recall = 0.9827444535743632, Aging Rate = 0.5866943866943867, Efficiency = 1.6750534164148185\n",
      "\n",
      "Epoch 31: Train Loss = 0.15996247639857641, Recall = 0.986362371277484, Aging Rate = 0.5782397782397782\n",
      "Epoch 32: Train Loss = 0.1604760918572638, Recall = 0.9869190091845255, Aging Rate = 0.578932778932779\n",
      "Epoch 33: Train Loss = 0.1596799244565924, Recall = 0.9860840523239632, Aging Rate = 0.5764379764379765\n",
      "Epoch 34: Train Loss = 0.1606028862880983, Recall = 0.9860840523239632, Aging Rate = 0.5794871794871795\n",
      "Epoch 35: Train Loss = 0.160365700815712, Recall = 0.9860840523239632, Aging Rate = 0.578932778932779\n",
      "Test Loss = 0.16137376493327088, Recall = 0.9835661462612982, Aging Rate = 0.5966735966735967, Efficiency = 1.648415708790261\n",
      "\n",
      "Epoch 36: Train Loss = 0.15918219735741368, Recall = 0.9860840523239632, Aging Rate = 0.577962577962578\n",
      "Epoch 37: Train Loss = 0.15943041920496703, Recall = 0.985249095463401, Aging Rate = 0.5783783783783784\n",
      "Epoch 38: Train Loss = 0.15983124684725714, Recall = 0.9860840523239632, Aging Rate = 0.576022176022176\n",
      "Epoch 39: Train Loss = 0.15922660515775966, Recall = 0.9858057333704425, Aging Rate = 0.5746361746361747\n",
      "Epoch 40: Train Loss = 0.16078735309925993, Recall = 0.985249095463401, Aging Rate = 0.5793485793485793\n",
      "Test Loss = 0.1616695721159358, Recall = 0.9819227608874281, Aging Rate = 0.585031185031185, Efficiency = 1.6784109449669378\n",
      "\n",
      "Epoch 41: Train Loss = 0.15826056489329823, Recall = 0.9855274144169218, Aging Rate = 0.5767151767151767\n",
      "Epoch 42: Train Loss = 0.1589055832854923, Recall = 0.9858057333704425, Aging Rate = 0.5772695772695773\n",
      "Epoch 43: Train Loss = 0.15872373717655677, Recall = 0.9871973281380462, Aging Rate = 0.5761607761607762\n",
      "Epoch 44: Train Loss = 0.15922608573611874, Recall = 0.986362371277484, Aging Rate = 0.5797643797643798\n",
      "Epoch 45: Train Loss = 0.15923860311012505, Recall = 0.985249095463401, Aging Rate = 0.5776853776853776\n",
      "Test Loss = 0.16327695073927762, Recall = 0.9819227608874281, Aging Rate = 0.5962577962577963, Efficiency = 1.6468090657799905\n",
      "\n",
      "Epoch 46: Train Loss = 0.1591501827382083, Recall = 0.9871973281380462, Aging Rate = 0.5782397782397782\n",
      "Epoch 47: Train Loss = 0.15936017341896302, Recall = 0.9844141386028389, Aging Rate = 0.5761607761607762\n",
      "Epoch 48: Train Loss = 0.15883768272333812, Recall = 0.986362371277484, Aging Rate = 0.5799029799029799\n",
      "Epoch 49: Train Loss = 0.15982110570349464, Recall = 0.9869190091845255, Aging Rate = 0.5761607761607762\n",
      "Epoch 50: Train Loss = 0.157532715887265, Recall = 0.9860840523239632, Aging Rate = 0.5747747747747748\n",
      "Test Loss = 0.16471493556444958, Recall = 0.9769926047658176, Aging Rate = 0.5667359667359667, Efficiency = 1.7238937439487503\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.15782845038468857, Recall = 0.985249095463401, Aging Rate = 0.5758835758835759\n",
      "Epoch 52: Train Loss = 0.15884302735060185, Recall = 0.9866406902310048, Aging Rate = 0.5782397782397782\n",
      "Epoch 53: Train Loss = 0.1581457838302508, Recall = 0.9866406902310048, Aging Rate = 0.5785169785169785\n",
      "Epoch 54: Train Loss = 0.15813356818462196, Recall = 0.985249095463401, Aging Rate = 0.5767151767151767\n",
      "Epoch 55: Train Loss = 0.15904939708532928, Recall = 0.9871973281380462, Aging Rate = 0.5781011781011781\n",
      "Test Loss = 0.16228768339152147, Recall = 0.9819227608874281, Aging Rate = 0.5767151767151767, Efficiency = 1.7026129769188338\n",
      "\n",
      "Epoch 56: Train Loss = 0.15835681503957455, Recall = 0.985249095463401, Aging Rate = 0.5758835758835759\n",
      "Epoch 57: Train Loss = 0.15791700536479408, Recall = 0.987475647091567, Aging Rate = 0.5774081774081774\n",
      "Epoch 58: Train Loss = 0.15884167592680792, Recall = 0.987475647091567, Aging Rate = 0.5785169785169785\n",
      "Epoch 59: Train Loss = 0.15768698140390203, Recall = 0.986362371277484, Aging Rate = 0.5744975744975745\n",
      "Epoch 60: Train Loss = 0.15868017882501095, Recall = 0.9866406902310048, Aging Rate = 0.5753291753291754\n",
      "Test Loss = 0.16233491971933917, Recall = 0.9835661462612982, Aging Rate = 0.6020790020790021, Efficiency = 1.6336163967333892\n",
      "\n",
      "Epoch 61: Train Loss = 0.1575326732604436, Recall = 0.9869190091845255, Aging Rate = 0.5746361746361747\n",
      "Epoch 62: Train Loss = 0.15870291226371164, Recall = 0.9849707765098803, Aging Rate = 0.5772695772695773\n",
      "Epoch 63: Train Loss = 0.1583648492004503, Recall = 0.9866406902310048, Aging Rate = 0.5785169785169785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss = 0.1584087428787229, Recall = 0.9860840523239632, Aging Rate = 0.5774081774081774\n",
      "Epoch 65: Train Loss = 0.15728060508757438, Recall = 0.9869190091845255, Aging Rate = 0.576992376992377\n",
      "Test Loss = 0.16374470887089965, Recall = 0.9819227608874281, Aging Rate = 0.5746361746361747, Efficiency = 1.7087729369307345\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.15838017383963385, Recall = 0.9858057333704425, Aging Rate = 0.5757449757449757\n",
      "Epoch 67: Train Loss = 0.1579641711129468, Recall = 0.9846924575563596, Aging Rate = 0.5754677754677755\n",
      "Epoch 68: Train Loss = 0.1585993979841326, Recall = 0.9866406902310048, Aging Rate = 0.5771309771309772\n",
      "Epoch 69: Train Loss = 0.1578344445032786, Recall = 0.9883106039521291, Aging Rate = 0.576022176022176\n",
      "Epoch 70: Train Loss = 0.1579265057441681, Recall = 0.9877539660450877, Aging Rate = 0.5762993762993763\n",
      "Test Loss = 0.1625763440206492, Recall = 0.981101068200493, Aging Rate = 0.5746361746361747, Efficiency = 1.7073430014186586\n",
      "\n",
      "Epoch 71: Train Loss = 0.15746369610539923, Recall = 0.9877539660450877, Aging Rate = 0.5787941787941788\n",
      "Epoch 72: Train Loss = 0.15793063589684764, Recall = 0.9866406902310048, Aging Rate = 0.5753291753291754\n",
      "Epoch 73: Train Loss = 0.1582502130690087, Recall = 0.9866406902310048, Aging Rate = 0.5793485793485793\n",
      "Epoch 74: Train Loss = 0.15722494450238375, Recall = 0.9869190091845255, Aging Rate = 0.576022176022176\n",
      "Epoch 75: Train Loss = 0.15752405106443046, Recall = 0.9869190091845255, Aging Rate = 0.5740817740817741\n",
      "Test Loss = 0.16245134522414256, Recall = 0.9827444535743632, Aging Rate = 0.5896049896049896, Efficiency = 1.6667844645699414\n",
      "\n",
      "Epoch 76: Train Loss = 0.15787561004845863, Recall = 0.9869190091845255, Aging Rate = 0.576992376992377\n",
      "Epoch 77: Train Loss = 0.1578484340748354, Recall = 0.986362371277484, Aging Rate = 0.5754677754677755\n",
      "Epoch 78: Train Loss = 0.1570692865656881, Recall = 0.9866406902310048, Aging Rate = 0.5782397782397782\n",
      "Epoch 79: Train Loss = 0.1576042395949942, Recall = 0.987475647091567, Aging Rate = 0.5768537768537768\n",
      "Epoch 80: Train Loss = 0.15826938840885255, Recall = 0.9866406902310048, Aging Rate = 0.5776853776853776\n",
      "Test Loss = 0.1692079917246983, Recall = 0.9761709120788825, Aging Rate = 0.5538461538461539, Efficiency = 1.7625307816522875\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.1577920135593381, Recall = 0.9866406902310048, Aging Rate = 0.5774081774081774\n",
      "Epoch 82: Train Loss = 0.15771585769605406, Recall = 0.9877539660450877, Aging Rate = 0.578932778932779\n",
      "Epoch 83: Train Loss = 0.15661503109804722, Recall = 0.987475647091567, Aging Rate = 0.5761607761607762\n",
      "Epoch 84: Train Loss = 0.1566276393866919, Recall = 0.9860840523239632, Aging Rate = 0.5744975744975745\n",
      "Epoch 85: Train Loss = 0.15640488377851658, Recall = 0.9869190091845255, Aging Rate = 0.5758835758835759\n",
      "Test Loss = 0.16220418585932925, Recall = 0.9843878389482333, Aging Rate = 0.6066528066528066, Efficiency = 1.6226543616488442\n",
      "\n",
      "Epoch 86: Train Loss = 0.1569605356917477, Recall = 0.986362371277484, Aging Rate = 0.5751905751905751\n",
      "Epoch 87: Train Loss = 0.15749754019066103, Recall = 0.9869190091845255, Aging Rate = 0.5764379764379765\n",
      "Epoch 88: Train Loss = 0.15746141943102154, Recall = 0.9869190091845255, Aging Rate = 0.5753291753291754\n",
      "Epoch 89: Train Loss = 0.1573617494217819, Recall = 0.986362371277484, Aging Rate = 0.5796257796257797\n",
      "Epoch 90: Train Loss = 0.1572022443171268, Recall = 0.9885889229056499, Aging Rate = 0.5778239778239779\n",
      "Test Loss = 0.1635713327451456, Recall = 0.9835661462612982, Aging Rate = 0.5983367983367983, Efficiency = 1.6438335943184323\n",
      "\n",
      "Epoch 91: Train Loss = 0.15615501584316077, Recall = 0.9860840523239632, Aging Rate = 0.5751905751905751\n",
      "Epoch 92: Train Loss = 0.15669649943285988, Recall = 0.9885889229056499, Aging Rate = 0.5775467775467775\n",
      "Epoch 93: Train Loss = 0.15656057813476543, Recall = 0.9869190091845255, Aging Rate = 0.5753291753291754\n",
      "Epoch 94: Train Loss = 0.15654026374210522, Recall = 0.9871973281380462, Aging Rate = 0.5742203742203742\n",
      "Epoch 95: Train Loss = 0.15700107648586614, Recall = 0.9860840523239632, Aging Rate = 0.5757449757449757\n",
      "Test Loss = 0.16477988559961815, Recall = 0.9802793755135579, Aging Rate = 0.567983367983368, Efficiency = 1.7258944777469578\n",
      "\n",
      "Epoch 96: Train Loss = 0.156975011481069, Recall = 0.987475647091567, Aging Rate = 0.576992376992377\n",
      "Epoch 97: Train Loss = 0.15631815044109967, Recall = 0.9855274144169218, Aging Rate = 0.5753291753291754\n",
      "Epoch 98: Train Loss = 0.15831330099084356, Recall = 0.986362371277484, Aging Rate = 0.5767151767151767\n",
      "Epoch 99: Train Loss = 0.1579697783486511, Recall = 0.9860840523239632, Aging Rate = 0.5772695772695773\n",
      "Epoch 100: Train Loss = 0.1571974822942862, Recall = 0.9866406902310048, Aging Rate = 0.5772695772695773\n",
      "Test Loss = 0.16221889675654888, Recall = 0.9827444535743632, Aging Rate = 0.5817047817047817, Efficiency = 1.6894212796395724\n",
      "\n",
      "Epoch 101: Train Loss = 0.15791747317244753, Recall = 0.9869190091845255, Aging Rate = 0.5781011781011781\n",
      "Epoch 102: Train Loss = 0.15642497573597763, Recall = 0.987475647091567, Aging Rate = 0.5762993762993763\n",
      "Epoch 103: Train Loss = 0.15706661133739738, Recall = 0.9877539660450877, Aging Rate = 0.5778239778239779\n",
      "Epoch 104: Train Loss = 0.15618512272958696, Recall = 0.9869190091845255, Aging Rate = 0.5749133749133749\n",
      "Epoch 105: Train Loss = 0.15673643713474936, Recall = 0.9877539660450877, Aging Rate = 0.5751905751905751\n",
      "Test Loss = 0.1636776857651197, Recall = 0.9802793755135579, Aging Rate = 0.5708939708939709, Efficiency = 1.7170953072206587\n",
      "\n",
      "Epoch 106: Train Loss = 0.15693599891100835, Recall = 0.9869190091845255, Aging Rate = 0.5756063756063756\n",
      "Epoch 107: Train Loss = 0.15725719188536857, Recall = 0.9858057333704425, Aging Rate = 0.5749133749133749\n",
      "Epoch 108: Train Loss = 0.1570548209066781, Recall = 0.9877539660450877, Aging Rate = 0.5753291753291754\n",
      "Epoch 109: Train Loss = 0.1570567923272061, Recall = 0.987475647091567, Aging Rate = 0.5792099792099792\n",
      "Epoch 110: Train Loss = 0.1565945231926912, Recall = 0.9855274144169218, Aging Rate = 0.5761607761607762\n",
      "Test Loss = 0.1618264407972784, Recall = 0.9835661462612982, Aging Rate = 0.5916839916839917, Efficiency = 1.6623166140405534\n",
      "\n",
      "Epoch 111: Train Loss = 0.15630101135267785, Recall = 0.9860840523239632, Aging Rate = 0.5747747747747748\n",
      "Epoch 112: Train Loss = 0.1566378412751464, Recall = 0.9871973281380462, Aging Rate = 0.5757449757449757\n",
      "Epoch 113: Train Loss = 0.15712706755616973, Recall = 0.987475647091567, Aging Rate = 0.5757449757449757\n",
      "Epoch 114: Train Loss = 0.15645948033612053, Recall = 0.9866406902310048, Aging Rate = 0.5738045738045738\n",
      "Epoch 115: Train Loss = 0.15643051271833486, Recall = 0.9866406902310048, Aging Rate = 0.5749133749133749\n",
      "Test Loss = 0.16194273467619058, Recall = 0.9835661462612982, Aging Rate = 0.5916839916839917, Efficiency = 1.6623166140405534\n",
      "\n",
      "Epoch 116: Train Loss = 0.15687957633640398, Recall = 0.986362371277484, Aging Rate = 0.5757449757449757\n",
      "Epoch 117: Train Loss = 0.15673555036592385, Recall = 0.9869190091845255, Aging Rate = 0.5767151767151767\n",
      "Epoch 118: Train Loss = 0.15651793218658602, Recall = 0.9871973281380462, Aging Rate = 0.5775467775467775\n",
      "Epoch 119: Train Loss = 0.1569888985660947, Recall = 0.9880322849986084, Aging Rate = 0.5758835758835759\n",
      "Epoch 120: Train Loss = 0.15711249945606529, Recall = 0.9877539660450877, Aging Rate = 0.5778239778239779\n",
      "Test Loss = 0.16654972202316887, Recall = 0.9761709120788825, Aging Rate = 0.5571725571725572, Efficiency = 1.752008210010384\n",
      "\n",
      "Epoch 121: Train Loss = 0.1565485535978197, Recall = 0.985249095463401, Aging Rate = 0.5725571725571725\n",
      "Epoch 122: Train Loss = 0.15687626793784396, Recall = 0.9880322849986084, Aging Rate = 0.576022176022176\n",
      "Epoch 123: Train Loss = 0.15634269841991194, Recall = 0.9866406902310048, Aging Rate = 0.5778239778239779\n",
      "Epoch 124: Train Loss = 0.15631432498463804, Recall = 0.987475647091567, Aging Rate = 0.5740817740817741\n",
      "Epoch 125: Train Loss = 0.15640866060845157, Recall = 0.987475647091567, Aging Rate = 0.5767151767151767\n",
      "Test Loss = 0.1642593361421831, Recall = 0.981101068200493, Aging Rate = 0.5688149688149688, Efficiency = 1.7248158096055302\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126: Train Loss = 0.15646609634636016, Recall = 0.9877539660450877, Aging Rate = 0.5771309771309772\n",
      "Epoch 127: Train Loss = 0.15803308320061993, Recall = 0.9855274144169218, Aging Rate = 0.5776853776853776\n",
      "Epoch 128: Train Loss = 0.15769633446521256, Recall = 0.9849707765098803, Aging Rate = 0.5785169785169785\n",
      "Epoch 129: Train Loss = 0.15740933115500505, Recall = 0.9880322849986084, Aging Rate = 0.5782397782397782\n",
      "Epoch 130: Train Loss = 0.15709425192158502, Recall = 0.9871973281380462, Aging Rate = 0.5746361746361747\n",
      "Test Loss = 0.16253401159372746, Recall = 0.9835661462612982, Aging Rate = 0.5987525987525988, Efficiency = 1.6426920432303322\n",
      "\n",
      "Training Finished at epoch 130.\n",
      "Test Loss = 1.1268133665629172, Recall = 0.5675675675675675, Aging Rate = 0.5530143607057789, Efficiency = 1.0263161278127642\n",
      "\n",
      "Starting training Dataset 9:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5ccf62904d46eca6b312aff8c9676a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.3563321701461269, Recall = 0.6567567567567567, Aging Rate = 0.1376008064516129\n",
      "Epoch 2: Train Loss = 0.3031419450717588, Recall = 0.7432432432432432, Aging Rate = 0.11970766129032258\n",
      "Epoch 3: Train Loss = 0.28321533578057445, Recall = 0.7594594594594595, Aging Rate = 0.11441532258064516\n",
      "Epoch 4: Train Loss = 0.2711014021788874, Recall = 0.7675675675675676, Aging Rate = 0.10887096774193548\n",
      "Epoch 5: Train Loss = 0.2616311898154597, Recall = 0.7891891891891892, Aging Rate = 0.11088709677419355\n",
      "Test Loss = 0.31219523690094664, Recall = 0.6036036036036037, Aging Rate = 0.07180650037792895, Efficiency = 8.405973224805704\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2296491039376105, Recall = 0.8189189189189189, Aging Rate = 0.10231854838709678\n",
      "Epoch 7: Train Loss = 0.22177541207882664, Recall = 0.8135135135135135, Aging Rate = 0.09702620967741936\n",
      "Epoch 8: Train Loss = 0.21822967512472982, Recall = 0.8135135135135135, Aging Rate = 0.09627016129032258\n",
      "Epoch 9: Train Loss = 0.2154651822582368, Recall = 0.8432432432432433, Aging Rate = 0.1003024193548387\n",
      "Epoch 10: Train Loss = 0.2085284436902692, Recall = 0.8486486486486486, Aging Rate = 0.0985383064516129\n",
      "Test Loss = 0.3038859454829046, Recall = 0.5495495495495496, Aging Rate = 0.05442176870748299, Efficiency = 10.097971117470781\n",
      "\n",
      "Epoch 11: Train Loss = 0.20782909950902384, Recall = 0.8432432432432433, Aging Rate = 0.09551411290322581\n",
      "Epoch 12: Train Loss = 0.20501311555985482, Recall = 0.8486486486486486, Aging Rate = 0.09954637096774194\n",
      "Epoch 13: Train Loss = 0.20180646342135244, Recall = 0.845945945945946, Aging Rate = 0.09753024193548387\n",
      "Epoch 14: Train Loss = 0.19595204293727875, Recall = 0.8702702702702703, Aging Rate = 0.10131048387096774\n",
      "Epoch 15: Train Loss = 0.20015462740294396, Recall = 0.8540540540540541, Aging Rate = 0.09727822580645161\n",
      "Test Loss = 0.2789264011252404, Recall = 0.6846846846846847, Aging Rate = 0.08163265306122448, Efficiency = 8.38738635993256\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.1964570521827667, Recall = 0.8567567567567568, Aging Rate = 0.09753024193548387\n",
      "Epoch 17: Train Loss = 0.19197015223964567, Recall = 0.8675675675675676, Aging Rate = 0.09576612903225806\n",
      "Epoch 18: Train Loss = 0.19333155092693144, Recall = 0.8675675675675676, Aging Rate = 0.09879032258064516\n",
      "Epoch 19: Train Loss = 0.18984143700330489, Recall = 0.8540540540540541, Aging Rate = 0.09576612903225806\n",
      "Epoch 20: Train Loss = 0.19190255131932996, Recall = 0.8648648648648649, Aging Rate = 0.1003024193548387\n",
      "Test Loss = 0.2746721868792539, Recall = 0.6846846846846847, Aging Rate = 0.07860922146636433, Efficiency = 8.709978101969496\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.1888673751344604, Recall = 0.8675675675675676, Aging Rate = 0.09652217741935484\n",
      "Epoch 22: Train Loss = 0.18886930147005665, Recall = 0.8702702702702703, Aging Rate = 0.09601814516129033\n",
      "Epoch 23: Train Loss = 0.1867044468320185, Recall = 0.8756756756756757, Aging Rate = 0.0967741935483871\n",
      "Epoch 24: Train Loss = 0.18675941072644725, Recall = 0.8783783783783784, Aging Rate = 0.09601814516129033\n",
      "Epoch 25: Train Loss = 0.18418122323289995, Recall = 0.8783783783783784, Aging Rate = 0.09450604838709678\n",
      "Test Loss = 0.27037969196126577, Recall = 0.6846846846846847, Aging Rate = 0.07709750566893424, Efficiency = 8.880761964170167\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.18321234468490846, Recall = 0.8756756756756757, Aging Rate = 0.09627016129032258\n",
      "Epoch 27: Train Loss = 0.18220954977216258, Recall = 0.8891891891891892, Aging Rate = 0.09576612903225806\n",
      "Epoch 28: Train Loss = 0.18254438991988858, Recall = 0.8810810810810811, Aging Rate = 0.09601814516129033\n",
      "Epoch 29: Train Loss = 0.18250313148863853, Recall = 0.8918918918918919, Aging Rate = 0.0985383064516129\n",
      "Epoch 30: Train Loss = 0.18170774439650197, Recall = 0.8702702702702703, Aging Rate = 0.09526209677419355\n",
      "Test Loss = 0.27270548271461587, Recall = 0.6936936936936937, Aging Rate = 0.07029478458049887, Efficiency = 9.868350819338445\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.17976071048648126, Recall = 0.9, Aging Rate = 0.09954637096774194\n",
      "Epoch 32: Train Loss = 0.1802092622364721, Recall = 0.8864864864864865, Aging Rate = 0.0950100806451613\n",
      "Epoch 33: Train Loss = 0.18155808254115044, Recall = 0.8837837837837837, Aging Rate = 0.0950100806451613\n",
      "Epoch 34: Train Loss = 0.18034204864694225, Recall = 0.8810810810810811, Aging Rate = 0.09702620967741936\n",
      "Epoch 35: Train Loss = 0.17965698782955447, Recall = 0.8837837837837837, Aging Rate = 0.0967741935483871\n",
      "Test Loss = 0.2659660696825653, Recall = 0.7117117117117117, Aging Rate = 0.0801209372637944, Efficiency = 8.882966764839097\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.1783914090164246, Recall = 0.8972972972972973, Aging Rate = 0.09727822580645161\n",
      "Epoch 37: Train Loss = 0.17912915096648277, Recall = 0.8837837837837837, Aging Rate = 0.09400201612903226\n",
      "Epoch 38: Train Loss = 0.17739691296892782, Recall = 0.8810810810810811, Aging Rate = 0.09526209677419355\n",
      "Epoch 39: Train Loss = 0.17698741800362064, Recall = 0.9027027027027027, Aging Rate = 0.09652217741935484\n",
      "Epoch 40: Train Loss = 0.17728810324784247, Recall = 0.8891891891891892, Aging Rate = 0.09425403225806452\n",
      "Test Loss = 0.27887827211474253, Recall = 0.6486486486486487, Aging Rate = 0.05895691609977324, Efficiency = 11.002077135957457\n",
      "\n",
      "Epoch 41: Train Loss = 0.17852566198956582, Recall = 0.8837837837837837, Aging Rate = 0.09576612903225806\n",
      "Epoch 42: Train Loss = 0.17546160951737436, Recall = 0.8972972972972973, Aging Rate = 0.09627016129032258\n",
      "Epoch 43: Train Loss = 0.17960048130443018, Recall = 0.8837837837837837, Aging Rate = 0.09652217741935484\n",
      "Epoch 44: Train Loss = 0.1774967770663, Recall = 0.8891891891891892, Aging Rate = 0.09450604838709678\n",
      "Epoch 45: Train Loss = 0.1758471655268823, Recall = 0.9, Aging Rate = 0.09551411290322581\n",
      "Test Loss = 0.2668734371819824, Recall = 0.7297297297297297, Aging Rate = 0.08238851095993953, Efficiency = 8.857177204146588\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.17599986253246183, Recall = 0.9081081081081082, Aging Rate = 0.09803427419354839\n",
      "Epoch 47: Train Loss = 0.1750836667995299, Recall = 0.8945945945945946, Aging Rate = 0.09601814516129033\n",
      "Epoch 48: Train Loss = 0.17467156821681606, Recall = 0.8891891891891892, Aging Rate = 0.09526209677419355\n",
      "Epoch 49: Train Loss = 0.17641660511013, Recall = 0.8972972972972973, Aging Rate = 0.09475806451612903\n",
      "Epoch 50: Train Loss = 0.1751904504433755, Recall = 0.8972972972972973, Aging Rate = 0.09778225806451613\n",
      "Test Loss = 0.2679907824965201, Recall = 0.7117117117117117, Aging Rate = 0.07558578987150416, Efficiency = 9.415944700216462\n",
      "\n",
      "Epoch 51: Train Loss = 0.17812512634742644, Recall = 0.8864864864864865, Aging Rate = 0.09551411290322581\n",
      "Epoch 52: Train Loss = 0.17655930475842568, Recall = 0.8864864864864865, Aging Rate = 0.09526209677419355\n",
      "Epoch 53: Train Loss = 0.17482811200522608, Recall = 0.8891891891891892, Aging Rate = 0.0950100806451613\n",
      "Epoch 54: Train Loss = 0.18068823554823477, Recall = 0.8648648648648649, Aging Rate = 0.0950100806451613\n",
      "Epoch 55: Train Loss = 0.17574178391406614, Recall = 0.8972972972972973, Aging Rate = 0.0950100806451613\n",
      "Test Loss = 0.26750913903241075, Recall = 0.6936936936936937, Aging Rate = 0.07029478458049887, Efficiency = 9.868350819338445\n",
      "\n",
      "Epoch 56: Train Loss = 0.17475482941635193, Recall = 0.8918918918918919, Aging Rate = 0.09627016129032258\n",
      "Epoch 57: Train Loss = 0.1728341318666935, Recall = 0.9081081081081082, Aging Rate = 0.09652217741935484\n",
      "Epoch 58: Train Loss = 0.1730432254412482, Recall = 0.9081081081081082, Aging Rate = 0.09627016129032258\n",
      "Epoch 59: Train Loss = 0.17524082309776737, Recall = 0.8918918918918919, Aging Rate = 0.0967741935483871\n",
      "Epoch 60: Train Loss = 0.17493863499933673, Recall = 0.8972972972972973, Aging Rate = 0.09551411290322581\n",
      "Test Loss = 0.26247556946850686, Recall = 0.7117117117117117, Aging Rate = 0.07558578987150416, Efficiency = 9.415944700216462\n",
      "\n",
      "Epoch 61: Train Loss = 0.17400893388736632, Recall = 0.8972972972972973, Aging Rate = 0.09601814516129033\n",
      "Epoch 62: Train Loss = 0.1737398174741576, Recall = 0.9054054054054054, Aging Rate = 0.09576612903225806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: Train Loss = 0.1728679969185783, Recall = 0.9054054054054054, Aging Rate = 0.0967741935483871\n",
      "Epoch 64: Train Loss = 0.17544923482402677, Recall = 0.8918918918918919, Aging Rate = 0.09601814516129033\n",
      "Epoch 65: Train Loss = 0.17264941022280725, Recall = 0.9, Aging Rate = 0.09828629032258064\n",
      "Test Loss = 0.2700996653867417, Recall = 0.6936936936936937, Aging Rate = 0.06953892668178382, Efficiency = 9.975615486732217\n",
      "\n",
      "Epoch 66: Train Loss = 0.17218519530950055, Recall = 0.9027027027027027, Aging Rate = 0.09652217741935484\n",
      "Epoch 67: Train Loss = 0.17307034867905802, Recall = 0.9027027027027027, Aging Rate = 0.09702620967741936\n",
      "Epoch 68: Train Loss = 0.17139766136965445, Recall = 0.8972972972972973, Aging Rate = 0.09526209677419355\n",
      "Epoch 69: Train Loss = 0.1730210254749944, Recall = 0.9081081081081082, Aging Rate = 0.09601814516129033\n",
      "Epoch 70: Train Loss = 0.17245582202749868, Recall = 0.9, Aging Rate = 0.09727822580645161\n",
      "Test Loss = 0.27864286087267526, Recall = 0.6756756756756757, Aging Rate = 0.06273620559334846, Efficiency = 10.770105740125542\n",
      "\n",
      "Epoch 71: Train Loss = 0.17348927067172143, Recall = 0.9, Aging Rate = 0.09627016129032258\n",
      "Epoch 72: Train Loss = 0.1723728878123145, Recall = 0.8972972972972973, Aging Rate = 0.09526209677419355\n",
      "Epoch 73: Train Loss = 0.1741962092778375, Recall = 0.8972972972972973, Aging Rate = 0.09652217741935484\n",
      "Epoch 74: Train Loss = 0.17227662847407402, Recall = 0.9054054054054054, Aging Rate = 0.09652217741935484\n",
      "Epoch 75: Train Loss = 0.1730265796424881, Recall = 0.8945945945945946, Aging Rate = 0.09879032258064516\n",
      "Test Loss = 0.2668995932476681, Recall = 0.7027027027027027, Aging Rate = 0.0710506424792139, Efficiency = 9.890165370519021\n",
      "\n",
      "Epoch 76: Train Loss = 0.17195242703441652, Recall = 0.9027027027027027, Aging Rate = 0.09526209677419355\n",
      "Epoch 77: Train Loss = 0.1714972052122316, Recall = 0.9, Aging Rate = 0.09601814516129033\n",
      "Epoch 78: Train Loss = 0.17027049727978244, Recall = 0.9054054054054054, Aging Rate = 0.09828629032258064\n",
      "Epoch 79: Train Loss = 0.1712827704125835, Recall = 0.9, Aging Rate = 0.09450604838709678\n",
      "Epoch 80: Train Loss = 0.1713861785228214, Recall = 0.9054054054054054, Aging Rate = 0.09627016129032258\n",
      "Test Loss = 0.2707153165872045, Recall = 0.7117117117117117, Aging Rate = 0.06953892668178382, Efficiency = 10.234722382491494\n",
      "\n",
      "Epoch 81: Train Loss = 0.17371711447354285, Recall = 0.9135135135135135, Aging Rate = 0.09828629032258064\n",
      "Epoch 82: Train Loss = 0.17278490191505802, Recall = 0.9, Aging Rate = 0.09551411290322581\n",
      "Epoch 83: Train Loss = 0.1717874294807834, Recall = 0.9054054054054054, Aging Rate = 0.09702620967741936\n",
      "Epoch 84: Train Loss = 0.17184592987741193, Recall = 0.9081081081081082, Aging Rate = 0.09702620967741936\n",
      "Epoch 85: Train Loss = 0.17109365160426787, Recall = 0.8972972972972973, Aging Rate = 0.09526209677419355\n",
      "Test Loss = 0.26447974071075586, Recall = 0.7117117117117117, Aging Rate = 0.07180650037792895, Efficiency = 9.911520668054486\n",
      "\n",
      "Epoch 86: Train Loss = 0.1695998647520619, Recall = 0.9081081081081082, Aging Rate = 0.09652217741935484\n",
      "Epoch 87: Train Loss = 0.17071678741805016, Recall = 0.9108108108108108, Aging Rate = 0.09576612903225806\n",
      "Epoch 88: Train Loss = 0.17010075683074613, Recall = 0.9027027027027027, Aging Rate = 0.09551411290322581\n",
      "Epoch 89: Train Loss = 0.17024176291400386, Recall = 0.8972972972972973, Aging Rate = 0.09400201612903226\n",
      "Epoch 90: Train Loss = 0.17068740377022373, Recall = 0.9135135135135135, Aging Rate = 0.09627016129032258\n",
      "Test Loss = 0.26418909324630135, Recall = 0.7117117117117117, Aging Rate = 0.07936507936507936, Efficiency = 8.967566437654197\n",
      "\n",
      "Epoch 91: Train Loss = 0.16992596404687052, Recall = 0.9108108108108108, Aging Rate = 0.09526209677419355\n",
      "Epoch 92: Train Loss = 0.16930059956446772, Recall = 0.8918918918918919, Aging Rate = 0.09324596774193548\n",
      "Epoch 93: Train Loss = 0.17104430148197758, Recall = 0.9, Aging Rate = 0.09601814516129033\n",
      "Epoch 94: Train Loss = 0.17111925876909687, Recall = 0.9054054054054054, Aging Rate = 0.09652217741935484\n",
      "Epoch 95: Train Loss = 0.16877961188795104, Recall = 0.9054054054054054, Aging Rate = 0.09576612903225806\n",
      "Test Loss = 0.2663078002254348, Recall = 0.7117117117117117, Aging Rate = 0.07029478458049887, Efficiency = 10.124671619840742\n",
      "\n",
      "Training Finished at epoch 95.\n",
      "Test Loss = 0.9176540198490523, Recall = 0.35135135135135137, Aging Rate = 0.4254792683208843, Efficiency = 0.8257778210443711\n"
     ]
    }
   ],
   "source": [
    "runall_modelC = NeuralNetworkC(dim = 133).to(device)\n",
    "runall_optimizerC = torch.optim.Adam(runall_modelC.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "runall_criterionC = nn.CrossEntropyLoss(weight = torch.tensor([0.2, 0.8])).to(device)\n",
    "\n",
    "table_setC, loss_dictC = runall_nn(train_x = run_train_x, \n",
    "                                   train_y = run_train_y, \n",
    "                                   test_x = run_test_x, \n",
    "                                   test_y = run_test_y, \n",
    "                                   n_epoch = 150, \n",
    "                                   batch_size = 64,\n",
    "                                   model = runall_modelC,\n",
    "                                   optimizer = runall_optimizerC, \n",
    "                                   criterion = runall_criterionC, \n",
    "                                   filename = 'runhist_array_m1m6_m7_3criteria_NeuralNetworkC', \n",
    "                                   train_ratio = 0.75, \n",
    "                                   early_stop = 10,\n",
    "                                   mode = 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T09:03:30.278836Z",
     "start_time": "2021-11-07T09:03:29.291590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Aging Rate</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset 0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>43206.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>25305.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17901.0</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.585644</td>\n",
       "      <td>0.922985</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>36576.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6630.0</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.846565</td>\n",
       "      <td>1.021617</td>\n",
       "      <td>0.608776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>34183.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9023.0</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.922412</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 4</th>\n",
       "      <td>19.0</td>\n",
       "      <td>24376.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18830.0</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.564138</td>\n",
       "      <td>0.910263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 5</th>\n",
       "      <td>24.0</td>\n",
       "      <td>28711.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14495.0</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.664501</td>\n",
       "      <td>0.976145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 6</th>\n",
       "      <td>33.0</td>\n",
       "      <td>38263.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4943.0</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>1.007105</td>\n",
       "      <td>0.621375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 7</th>\n",
       "      <td>29.0</td>\n",
       "      <td>34578.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8628.0</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.800291</td>\n",
       "      <td>0.979373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 8</th>\n",
       "      <td>21.0</td>\n",
       "      <td>23893.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19313.0</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.553014</td>\n",
       "      <td>1.026316</td>\n",
       "      <td>0.400827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 9</th>\n",
       "      <td>13.0</td>\n",
       "      <td>18386.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24820.0</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.425479</td>\n",
       "      <td>0.825778</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TP       FP    FN       TN  Precision    Recall  Aging Rate  \\\n",
       "dataset 0   0.0      0.0  37.0  43206.0   0.000000  0.000000    0.000000   \n",
       "dataset 1  20.0  25305.0  17.0  17901.0   0.000790  0.540541    0.585644   \n",
       "dataset 2  32.0  36576.0   5.0   6630.0   0.000874  0.864865    0.846565   \n",
       "dataset 3  27.0  34183.0  10.0   9023.0   0.000789  0.729730    0.791111   \n",
       "dataset 4  19.0  24376.0  18.0  18830.0   0.000779  0.513514    0.564138   \n",
       "dataset 5  24.0  28711.0  13.0  14495.0   0.000835  0.648649    0.664501   \n",
       "dataset 6  33.0  38263.0   4.0   4943.0   0.000862  0.891892    0.885600   \n",
       "dataset 7  29.0  34578.0   8.0   8628.0   0.000838  0.783784    0.800291   \n",
       "dataset 8  21.0  23893.0  16.0  19313.0   0.000878  0.567568    0.553014   \n",
       "dataset 9  13.0  18386.0  24.0  24820.0   0.000707  0.351351    0.425479   \n",
       "\n",
       "           Efficiency     Score  \n",
       "dataset 0    0.000000  0.000000  \n",
       "dataset 1    0.922985  0.000000  \n",
       "dataset 2    1.021617  0.608776  \n",
       "dataset 3    0.922412  0.000000  \n",
       "dataset 4    0.910263  0.000000  \n",
       "dataset 5    0.976145  0.000000  \n",
       "dataset 6    1.007105  0.621375  \n",
       "dataset 7    0.979373  0.000000  \n",
       "dataset 8    1.026316  0.400827  \n",
       "dataset 9    0.825778  0.000000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2376x1296 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot(loss_dictC['train'], loss_dictC['valid'], num_row = 4, num_col = 3)\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T06:36:13.481632Z",
     "start_time": "2021-11-07T06:36:13.285186Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712db7912cec48c5b7c5ba7a20dcdb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training Dataset 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfb72bff4f1481fac9af0ab31baee57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x133 and 114x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fd76f6742c5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                    \u001b[0mtrain_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                    \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                                    mode = 'R')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-6b54ee746a73>\u001b[0m in \u001b[0;36mrunall_nn\u001b[1;34m(train_x, train_y, test_x, test_y, n_epoch, batch_size, model, optimizer, criterion, filename, train_ratio, early_stop, mode)\u001b[0m\n\u001b[0;32m     39\u001b[0m                                                            \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                                                            \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                                                            early_stop = early_stop)\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mtrain_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mvalid_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-cfb30436d66f>\u001b[0m in \u001b[0;36mtrainingR\u001b[1;34m(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aging\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-20aa3d186cad>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aging\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aging\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aging\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aging\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aging\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x133 and 114x64)"
     ]
    }
   ],
   "source": [
    "runall_modelR = NeuralNetworkR().to(device)\n",
    "runall_optimizerR = torch.optim.Adam(runall_modelR.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "runall_criterionR = nn.MSELoss().to(device)\n",
    "\n",
    "table_setR, loss_dictR = runall_nn(train_x = run_train_x, \n",
    "                                   train_y = run_train_y, \n",
    "                                   test_x = run_test_x, \n",
    "                                   test_y = run_test_y, \n",
    "                                   n_epoch = 150, \n",
    "                                   batch_size = 64,\n",
    "                                   model = runall_modelR,\n",
    "                                   optimizer = runall_optimizerR, \n",
    "                                   criterion = runall_criterionR, \n",
    "                                   filename = 'runhist_array_4criteria_NeuralNetworkR', \n",
    "                                   train_ratio = 0.75, \n",
    "                                   early_stop = 10,\n",
    "                                   mode = 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:17:21.986069Z",
     "start_time": "2021-10-31T16:17:20.989691Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_plot(loss_dictR['train'], loss_dictR['valid'], num_row = 4, num_col = 3)\n",
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T08:53:51.483837Z",
     "start_time": "2021-11-07T08:53:51.372729Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20211109'\n",
    "TPE_multi = False\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'NeuralNetwork'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'NeuralNetwork')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
