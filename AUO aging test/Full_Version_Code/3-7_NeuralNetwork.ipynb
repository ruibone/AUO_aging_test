{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:02.186213Z",
     "start_time": "2021-11-02T02:49:00.588450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, AUC, PR_curve, multiple_curve, \\\n",
    "    best_threshold\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:03.546616Z",
     "start_time": "2021-11-02T02:49:03.528665Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "##### GPU ??? #####\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:04.482589Z",
     "start_time": "2021-11-02T02:49:04.463609Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunhistSet(Dataset):\n",
    "    \n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.x = torch.tensor(train_x.values.astype(np.float32))\n",
    "        self.y = torch.tensor(train_y.values.astype(np.float32)).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.x[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:05.452149Z",
     "start_time": "2021-11-02T02:49:05.440181Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkC(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkC, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(114, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class NeuralNetworkR(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkR, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(114, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "#             nn.Linear(32, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:06.454631Z",
     "start_time": "2021-11-02T02:49:06.432827Z"
    }
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.cls = classes\n",
    "        self.dim = dim                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            pred = pred * self.weight.unsqueeze(0)   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:51:17.905629Z",
     "start_time": "2021-11-02T02:51:17.878918Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainingC(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop):\n",
    "    \n",
    "    network.train()\n",
    "    best_model = network\n",
    "    best_objective = 0\n",
    "    stop_trigger = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        \n",
    "        total_loss = 0\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0  \n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)         \n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            TP += torch.dot((predicted == y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "            TN += torch.dot((predicted == y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "            FN += torch.dot((predicted != y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "            FP += torch.dot((predicted != y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "            total_loss += loss.item()*len(y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss.append(total_loss)\n",
    "        recall = TP / (TP + FN)\n",
    "        aging = (TP + FP) / (TP + TN + FP + FN)\n",
    "            \n",
    "        print(f'Epoch {i+1}: Train Loss = {total_loss / (TP + TN + FP + FN)}, Recall = {recall}, Aging Rate = {aging}')\n",
    "        \n",
    "        if ((i+1) % 5 == 0):\n",
    "            five_loss, valid_recall, _ = testingC(network, validloader, criterion)\n",
    "            valid_loss.append(five_loss)\n",
    "            \n",
    "            if valid_recall > best_objective:\n",
    "                best_objective = valid_recall\n",
    "                best_model = network\n",
    "                torch.save(best_model, f'{filename}_NeuralNetworkC_{epoch}.ckpt')\n",
    "                print(f'Model in epoch {i+1} is saved.\\n')\n",
    "                stop_trigger = 0\n",
    "            else:\n",
    "                stop_trigger += 1\n",
    "                print('')\n",
    "                \n",
    "            if stop_trigger == early_stop:\n",
    "                print(f'Training Finished at epoch {i+1}.')\n",
    "                return network, train_loss, valid_loss\n",
    "            \n",
    "    return network, train_loss, valid_loss\n",
    "\n",
    "\n",
    "def PR_matrix(predict):\n",
    "    \n",
    "    Y_new = predict.sort_values(['predict', 'truth'], ascending = [False, True]).reset_index(drop = True)\n",
    "    Y_new.loc[Y_new['truth'] != 1, 'truth'] = 0\n",
    "    \n",
    "    matrix = pd.DataFrame(Y_new.groupby('predict').sum()).rename(columns = {'truth': 'Bad_Count'})\n",
    "    matrix = matrix.sort_index(ascending = False)\n",
    "    matrix['All_Count'] = Y_new.groupby('predict').count()\n",
    "    matrix['Class_Prob'] = matrix.index\n",
    "    \n",
    "    matrix['TP'] = matrix['Bad_Count'].cumsum()\n",
    "    matrix['FP'] = matrix['All_Count'].cumsum() - matrix['TP']\n",
    "    matrix['FN'] = matrix['TP'].values[-1] - matrix['TP']\n",
    "    matrix['TN'] = matrix['FP'].values[-1] - matrix['FP']\n",
    "    \n",
    "    matrix['Precision'] = matrix['TP'] / (matrix['TP'] + matrix['FP'])\n",
    "    matrix['Recall'] = matrix['TP'] / (matrix['TP'] + matrix['FN'])\n",
    "    matrix['Aging Rate'] = (matrix['TP'] + matrix['FP']) / (matrix['TP'] + matrix['FP'] + matrix['FN'] + matrix['TN'])\n",
    "    matrix['Efficiency'] = matrix['Recall'] / matrix['Aging Rate']\n",
    "    matrix['Score'] = score1(matrix['Recall'], matrix['Aging Rate'])            \n",
    "    matrix = matrix.drop(columns = ['Bad_Count', 'All_Count']).reset_index(drop = True)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def trainingR(network, trainloader, validloader, optimizer, criterion, epoch, filename, early_stop):\n",
    "    \n",
    "    network.train()\n",
    "    best_model = network\n",
    "    best_objective = 1\n",
    "    stop_trigger = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        \n",
    "        total_loss = 0\n",
    "        predict_vector = torch.tensor([0])\n",
    "        y_vector = torch.tensor([0])\n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.type(torch.FloatTensor).to(device)\n",
    "            y = y.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()*len(y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            predict_vector = torch.cat((predict_vector, output.data[:,0]), axis = 0)\n",
    "            y_vector = torch.cat((y_vector, y[:,0]), axis = 0)       \n",
    "        result_df = pd.DataFrame(dict(predict = predict_vector, truth = y_vector))\n",
    "        pr_matrix = PR_matrix(result_df.iloc[1:, :])\n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = 'Recall', threshold = 0.7)\n",
    "        auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "        train_loss.append(total_loss)\n",
    "        \n",
    "        recall = best_data[\"Recall\"].values\n",
    "        aging = best_data[\"Aging Rate\"].values\n",
    "        print(f'Epoch {i+1}: Train Loss = {total_loss}, AUC = {auc}, Recall(0.7) = {recall}, Aging Rate = {aging}')\n",
    "        \n",
    "        if ((i+1) % 5 == 0):\n",
    "            five_loss, valid_auc, _ = testingR(network, validloader, criterion)\n",
    "            valid_loss.append(five_loss)\n",
    "            \n",
    "            if valid_auc < best_objective:\n",
    "                best_objective = valid_auc\n",
    "                best_model = network\n",
    "                torch.save(best_model, f'{filename}_NeuralNetworkR_{epoch}.ckpt')\n",
    "                print(f'Model in epoch {i+1} is saved.\\n')\n",
    "                stop_trigger = 0\n",
    "            else:\n",
    "                stop_trigger += 1\n",
    "                print('')\n",
    "                \n",
    "            if stop_trigger == early_stop:\n",
    "                print(f'Training Finished at epoch {i+1}.')\n",
    "                return network, train_loss, valid_loss\n",
    "      \n",
    "    return network, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:51:06.108059Z",
     "start_time": "2021-11-02T02:51:06.077011Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingC(network, dataloader, criterion):\n",
    "    \n",
    "    network.eval()\n",
    "    total_loss = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = network(x)\n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        TP += torch.dot((predicted == y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "        TN += torch.dot((predicted == y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "        FN += torch.dot((predicted != y).to(torch.float32), (y == 1).to(torch.float32)).sum().item()\n",
    "        FP += torch.dot((predicted != y).to(torch.float32), (y == 0).to(torch.float32)).sum().item()\n",
    "        total_loss += loss.item()*len(y)\n",
    "        \n",
    "    recall = TP / (TP + FN)\n",
    "    aging = (TP + FP) / (TP + TN + FP + FN)\n",
    "    if (TP + FP) != 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if aging != 0:\n",
    "        efficiency = recall / aging\n",
    "        score = score1(recall, aging)\n",
    "    else:\n",
    "        efficiency = 0\n",
    "        score = 0\n",
    "        \n",
    "    print(f'Test Loss = {total_loss / (TP + TN + FP + FN)}, Recall = {recall}, Aging Rate = {aging}, Efficiency = {recall / (aging + 1e-8)}')\n",
    "    \n",
    "    valid_objective = recall - 0.5*aging\n",
    "    table = pd.Series({'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN, 'Precision': precision, 'Recall': recall, \n",
    "                       'Aging Rate': aging, 'Efficiency': efficiency, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    return total_loss, valid_objective, table\n",
    "\n",
    "\n",
    "def testingR(network, dataloader, criterion):\n",
    "    \n",
    "    network.eval()   \n",
    "    total_loss = 0\n",
    "    predict_vector = torch.tensor([0])\n",
    "    y_vector = torch.tensor([0])\n",
    "    for x, y in dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y = y.unsqueeze(1)\n",
    "        output = network(x)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.item()*len(y)\n",
    "        \n",
    "        predict_vector = torch.cat((predict_vector, output.data[:,0]), axis = 0)\n",
    "        y_vector = torch.cat((y_vector, y[:,0]), axis = 0)\n",
    "    result_df = pd.DataFrame(dict(predict = predict_vector, truth = y_vector))\n",
    "    pr_matrix = PR_matrix(result_df.iloc[1:, :])\n",
    "    best_data, best_thres = best_threshold(pr_matrix, target = 'Recall', threshold = 0.7)\n",
    "    auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "    recall = best_data['Recall'].values[0]\n",
    "    aging = best_data['Aging Rate'].values[0]\n",
    "    precision = best_data['Precision'].values[0]\n",
    "    efficiency = best_data['Efficiency'].values[0]\n",
    "    score = best_data['Score'].values[0]\n",
    "    TP = best_data['TP'].values[0]\n",
    "    FP = best_data['FP'].values[0]\n",
    "    TN = best_data['TN'].values[0]\n",
    "    FN = best_data['FN'].values[0]\n",
    "        \n",
    "    print(f'Test Loss = {total_loss}, Recall = {recall}, Aging Rate = {aging}, Efficiency = {efficiency}')\n",
    "    \n",
    "    valid_objective = auc\n",
    "    table = pd.Series({'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN, 'Precision': precision, 'Recall': recall,\n",
    "                       'Aging Rate': aging,'Efficiency': efficiency, 'Score': score})\n",
    "    table = pd.DataFrame(table).T\n",
    "    \n",
    "    return total_loss, valid_objective, table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:09.937740Z",
     "start_time": "2021-11-02T02:49:09.913046Z"
    }
   },
   "outputs": [],
   "source": [
    "def runall_nn(train_x, train_y, test_x, test_y, n_epoch, batch_size, model, optimizer, criterion, filename,\n",
    "              train_ratio, early_stop, mode):\n",
    "    \n",
    "    set_name = list(train_x.keys())\n",
    "    result_table = pd.DataFrame()\n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    for num, i in enumerate(tqdm(set_name)):\n",
    "        print(f'\\nStarting training Dataset {num}:')\n",
    "        \n",
    "        # data preparation\n",
    "        train_ratio = train_ratio\n",
    "        train_data = RunhistSet(train_x[i], train_y[i])\n",
    "        test_data = RunhistSet(test_x, test_y)\n",
    "        train_size = int(len(train_data)*train_ratio)\n",
    "        valid_size = len(train_data) - train_size\n",
    "        train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "        valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = False)\n",
    "        test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "        \n",
    "        # training\n",
    "        if mode == 'C':\n",
    "            done_model, train_loss, valid_loss = trainingC(network = model, \n",
    "                                                           trainloader = train_loader, \n",
    "                                                           validloader = valid_loader, \n",
    "                                                           optimizer = optimizer, \n",
    "                                                           criterion = criterion, \n",
    "                                                           epoch = n_epoch, \n",
    "                                                           filename = filename, \n",
    "                                                           early_stop = early_stop)\n",
    "        elif mode == 'R':\n",
    "            done_model, train_loss, valid_loss = trainingR(network = model, \n",
    "                                                           trainloader = train_loader, \n",
    "                                                           validloader = valid_loader, \n",
    "                                                           optimizer = optimizer, \n",
    "                                                           criterion = criterion, \n",
    "                                                           epoch = n_epoch, \n",
    "                                                           filename = filename, \n",
    "                                                           early_stop = early_stop)\n",
    "        train_dict[i] = train_loss\n",
    "        valid_dict[i] = valid_loss\n",
    "        \n",
    "        # testing\n",
    "        if mode == 'C':\n",
    "            _, _, table = testingC(done_model, test_loader, criterion)\n",
    "        elif mode == 'R':\n",
    "            _, _, table = testingR(done_model, test_loader, criterion)\n",
    "        result_table = pd.concat([result_table, table], axis = 0).rename({0: f'dataset {num}'})\n",
    "    loss_dict = dict(train = train_dict, valid = valid_dict)\n",
    "        \n",
    "    return result_table, loss_dict\n",
    "\n",
    "\n",
    "def loss_plot(train_loss, valid_loss, num_row, num_col):\n",
    "    \n",
    "    fig , axes = plt.subplots(num_row, num_col, sharex = False, sharey = False, figsize = (num_row*8 + 1, num_col*6))\n",
    "    plt.suptitle('Training & Validation Loss Curve', y = 0.94, fontsize = 30)\n",
    "    for row in range(num_row):\n",
    "        for col in range(num_col):\n",
    "            \n",
    "            index = num_col*row + col\n",
    "            if index < len(train_loss):\n",
    "                \n",
    "                train = train_loss[f'set{index}']\n",
    "                valid = valid_loss[f'set{index}']\n",
    "                axes[row, col].plot(range(len(train)), train, 'b-', linewidth = 5, label = 'train')\n",
    "                axes[row, col].plot(range(4, len(train)+1, 5), valid, 'r-', linewidth = 5, label = 'valid')\n",
    "                axes[row, col].set_xlabel('Epoch')\n",
    "                axes[row, col].set_ylabel('Total Loss')\n",
    "                axes[row, col].set_title(f'dataset {index}')\n",
    "                axes[row, col].legend(loc = 'upper right', fancybox = True, prop = dict(size = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:13.435997Z",
     "start_time": "2021-11-02T02:49:11.280117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Month 2:\n",
      "\n",
      "Dimension of dataset 0 : (38022, 96)  balance ratio: 863.0\n",
      "Dimension of dataset 1 : (1128, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (962, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1226, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1138, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (883, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (921, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (880, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (880, 96)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (484, 96)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 3:\n",
      "\n",
      "Dimension of dataset 0 : (60607, 107)  balance ratio: 550.0\n",
      "Dimension of dataset 1 : (1800, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2948, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2030, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1814, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2203, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2567, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2200, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2200, 107)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1210, 107)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 4:\n",
      "\n",
      "Dimension of dataset 0 : (57742, 102)  balance ratio: 549.0\n",
      "Dimension of dataset 1 : (1750, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2490, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1934, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1728, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2105, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2299, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2100, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2100, 102)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1155, 102)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Combined training data:\n",
      "\n",
      "Dimension of dataset 0 : (156371, 115)  balance ratio: 603.0\n",
      "Dimension of dataset 1 : (4678, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (6400, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (5190, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (4680, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (5191, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (5787, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (5180, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (5180, 115)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (2849, 115)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      " Dimension of testing data: (48654, 115)\n"
     ]
    }
   ],
   "source": [
    "### training data ### \n",
    "training_month = [2, 3, 4]\n",
    "\n",
    "data_dict, trainset_x, trainset_y = multiple_month(training_month, num_set = 10, filename = 'dataset')\n",
    "\n",
    "print('\\nCombined training data:\\n')\n",
    "run_train = multiple_set(num_set = 10)\n",
    "run_train_x, run_train_y = train_set(run_train, num_set = 10)\n",
    "\n",
    "### testing data ###\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of testing data:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:49:15.452762Z",
     "start_time": "2021-11-02T02:49:15.422734Z"
    }
   },
   "outputs": [],
   "source": [
    "##### data preparation #####\n",
    "train_data = RunhistSet(run_train_x['set7'], run_train_y['set7'])\n",
    "test_data = RunhistSet(run_test_x, run_test_y)\n",
    "train_ratio = 0.75\n",
    "train_size = int(len(train_data)*train_ratio)\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = 64, shuffle = False)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:13:32.498950Z",
     "start_time": "2021-11-02T02:13:25.418192Z"
    }
   },
   "outputs": [],
   "source": [
    "##### model preparation #####\n",
    "# hyperparameter: learning rate, weight decay, weight\n",
    "modelC = NeuralNetworkC().to(device)\n",
    "optimizerC = torch.optim.Adam(modelC.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "criterionC = nn.CrossEntropyLoss(weight = torch.tensor([0.5, 0.5])).to(device)\n",
    "\n",
    "criterionC = LabelSmoothingLoss(classes = 2, smoothing = 0.2)\n",
    "\n",
    "##### training #####\n",
    "done_modelC, train_lossC, valid_lossC = trainingC(network = modelC, \n",
    "                                                  trainloader = train_loader, \n",
    "                                                  validloader = valid_loader, \n",
    "                                                  optimizer = optimizerC, \n",
    "                                                  criterion = criterionC, \n",
    "                                                  epoch = 150, \n",
    "                                                  filename = 'tamama',\n",
    "                                                  early_stop = 10)\n",
    "\n",
    "##### testing #####\n",
    "_, _, result_tableC = testingC(done_modelC, test_loader, criterionC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:51:36.859170Z",
     "start_time": "2021-11-02T02:51:26.514487Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea820ae21ecc4cda888c41afad08269a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.43432676792144775 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.63361662] ,   Aging Rate: [0.54517375]\n",
      "Epoch 1: Train Loss = 846.9388972967863, AUC = 0.3952542841727463, Recall(0.7) = [0.70041752], Aging Rate = [0.54517375]\n",
      "Best Threshold: 0.5488395690917969 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.94908062] ,   Aging Rate: [0.36396396]\n",
      "Epoch 2: Train Loss = 518.1432443782687, AUC = 0.27935565393543216, Recall(0.7) = [0.70041752], Aging Rate = [0.36396396]\n",
      "Best Threshold: 0.6151427626609802 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.98676471] ,   Aging Rate: [0.35006435]\n",
      "Epoch 3: Train Loss = 382.7067674770951, AUC = 0.26039226766551476, Recall(0.7) = [0.70041752], Aging Rate = [0.35006435]\n",
      "Best Threshold: 0.6658647656440735 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.99628805] ,   Aging Rate: [0.34671815]\n",
      "Epoch 4: Train Loss = 303.71694777160883, AUC = 0.2535262898694735, Recall(0.7) = [0.70041752], Aging Rate = [0.34671815]\n",
      "Best Threshold: 0.6969634890556335 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [0.9992554] ,   Aging Rate: [0.34568855]\n",
      "Epoch 5: Train Loss = 252.0662296190858, AUC = 0.24894238948501232, Recall(0.7) = [0.70041752], Aging Rate = [0.34568855]\n",
      "Best Threshold: 0.7454460263252258 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 61.76131880655885, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Best Threshold: 0.8326825499534607 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 6: Train Loss = 113.00535123050213, AUC = 0.2466625958288485, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.8661829233169556 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 7: Train Loss = 83.7673932556063, AUC = 0.24659394711127025, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.8891046047210693 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 8: Train Loss = 66.52354558743536, AUC = 0.2465900512074706, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9067725539207458 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 9: Train Loss = 52.79196160752326, AUC = 0.24658951381444802, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9172561168670654 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 10: Train Loss = 43.28453846462071, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.8625966310501099 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 27.10233525931835, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Best Threshold: 0.9286236763000488 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 11: Train Loss = 35.48507585376501, AUC = 0.2465895138451325, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.936020016670227 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 12: Train Loss = 30.113216229714453, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9405725002288818 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 13: Train Loss = 26.774640945717692, AUC = 0.24658951384513253, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9463627934455872 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 14: Train Loss = 23.633905421942472, AUC = 0.24658951384513256, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9497013688087463 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 15: Train Loss = 20.77750749886036, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9149306416511536 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 18.42299883440137, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Best Threshold: 0.9512624144554138 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 16: Train Loss = 19.11801414284855, AUC = 0.24658951382211913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9547045230865479 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 17: Train Loss = 17.27199205569923, AUC = 0.24658951383746147, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9553464651107788 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 18: Train Loss = 15.861247757915407, AUC = 0.24658951382979033, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9581364393234253 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 19: Train Loss = 15.471164492890239, AUC = 0.2465895138278725, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9609731435775757 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 20: Train Loss = 13.97874150564894, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9355906844139099 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 15.489769376814365, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Best Threshold: 0.962775468826294 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 21: Train Loss = 12.951939071062952, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9628905057907104 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 22: Train Loss = 12.491330340970308, AUC = 0.24658951386047473, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9642407894134521 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 23: Train Loss = 12.697923412546515, AUC = 0.24658951383554364, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9664119482040405 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 24: Train Loss = 11.16673456528224, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9679551720619202 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 25: Train Loss = 10.539420448243618, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9510689973831177 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 13.779261209070683, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9684223532676697 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 26: Train Loss = 9.850985727272928, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.969421923160553 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 27: Train Loss = 9.349297927110456, AUC = 0.24658951382211913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9696387052536011 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 28: Train Loss = 9.763006628141738, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9712596535682678 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 29: Train Loss = 9.336533795576543, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9724233150482178 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 30: Train Loss = 9.217923182761297, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9424140453338623 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 15.832925887778401, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9732487797737122 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 31: Train Loss = 9.04700749879703, AUC = 0.2465895138489681, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.9722369313240051 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 32: Train Loss = 9.031352465739474, AUC = 0.24658951385088584, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9751859903335571 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 33: Train Loss = 8.162652850616723, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9743627309799194 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 34: Train Loss = 8.170216631609946, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9755672216415405 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 35: Train Loss = 8.700050393119454, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9651535153388977 \n",
      "\n",
      "Recall: [0.70178042] ,   Precision: [1.] ,   Aging Rate: [0.36525097]\n",
      "Test Loss = 13.113624626770616, Recall = 0.701780415430267, Aging Rate = 0.36525096525096523, Efficiency = 1.9213649851632049\n",
      "\n",
      "Best Threshold: 0.9758872985839844 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 36: Train Loss = 7.982390616089106, AUC = 0.2465895138393792, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9755007028579712 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 37: Train Loss = 8.199290924705565, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9782941937446594 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 38: Train Loss = 8.40997535130009, AUC = 0.24658951383937913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9764113426208496 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 39: Train Loss = 7.424406706704758, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9778127670288086 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 40: Train Loss = 7.111044444609433, AUC = 0.24658951385472144, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.964168906211853 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 14.232608878985047, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9790011048316956 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 41: Train Loss = 7.259600186836906, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9789282083511353 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 42: Train Loss = 6.919962025131099, AUC = 0.24658951384129696, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9800938963890076 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 43: Train Loss = 6.518845319515094, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9788238406181335 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 44: Train Loss = 7.142630195594393, AUC = 0.24658951381444805, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9802493453025818 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 45: Train Loss = 6.322114697191864, AUC = 0.246589513841297, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.972071647644043 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 12.548936162143946, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9809168577194214 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 46: Train Loss = 6.771576856262982, AUC = 0.24658951382211913, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9804174900054932 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 47: Train Loss = 6.613125075935386, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9801037311553955 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 48: Train Loss = 6.758818219532259, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9798376560211182 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 49: Train Loss = 8.296786832506768, AUC = 0.24658951382787248, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9797123670578003 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 50: Train Loss = 7.790700750192627, AUC = 0.2465895138451325, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9643718004226685 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 12.645020667463541, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9815775156021118 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 51: Train Loss = 7.510928211035207, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9814843535423279 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 52: Train Loss = 6.609729846473783, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9812673926353455 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 53: Train Loss = 7.399489681236446, AUC = 0.24658951384513247, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9807630777359009 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 54: Train Loss = 7.4163997187279165, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9813729524612427 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 55: Train Loss = 6.600580210331827, AUC = 0.246589513814448, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.979337751865387 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 13.42629144154489, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9830928444862366 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 56: Train Loss = 6.32335099845659, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9817944169044495 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 57: Train Loss = 7.089116318617016, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9819741249084473 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 58: Train Loss = 6.890363401733339, AUC = 0.24658951383746142, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.982314944267273 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 59: Train Loss = 7.109322623582557, AUC = 0.24658951385280364, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9832709431648254 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 60: Train Loss = 6.210180180845782, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9742255806922913 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 13.452005784958601, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9822063446044922 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 61: Train Loss = 6.069727279245853, AUC = 0.24658951386047478, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9822067022323608 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 62: Train Loss = 6.2594345971010625, AUC = 0.24658951383170802, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.9831069707870483 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 63: Train Loss = 5.964000734151341, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.982894778251648 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 64: Train Loss = 6.6053667610976845, AUC = 0.24658951382979027, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9815890789031982 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 65: Train Loss = 5.97409304860048, AUC = 0.24658951382787245, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9652794599533081 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 14.8444560226053, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Best Threshold: 0.9824767112731934 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 66: Train Loss = 6.472427227767184, AUC = 0.24658951382211916, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9832783937454224 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 67: Train Loss = 6.716100656194612, AUC = 0.24658951382979025, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9822962284088135 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 68: Train Loss = 6.012331276433542, AUC = 0.24658951383746144, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9817267060279846 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 69: Train Loss = 7.817374554928392, AUC = 0.24658951383362582, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9823286533355713 \n",
      "\n",
      "Recall: [0.70041752] ,   Precision: [1.] ,   Aging Rate: [0.34543115]\n",
      "Epoch 70: Train Loss = 7.8439161505084485, AUC = 0.2465895138374614, Recall(0.7) = [0.70041752], Aging Rate = [0.34543115]\n",
      "Best Threshold: 0.9587316513061523 \n",
      "\n",
      "Recall: [0.70029674] ,   Precision: [1.] ,   Aging Rate: [0.36447876]\n",
      "Test Loss = 15.946332313120365, Recall = 0.7002967359050445, Aging Rate = 0.3644787644787645, Efficiency = 1.9213649851632046\n",
      "\n",
      "Training Finished at epoch 70.\n",
      "Best Threshold: 0.3367293179035187 \n",
      "\n",
      "Recall: [0.71153846] ,   Precision: [0.00133314] ,   Aging Rate: [0.57043614]\n",
      "Test Loss = 13083.386966064572, Recall = 0.7115384615384616, Aging Rate = 0.5704361409133885, Efficiency = 1.2473586620916735\n"
     ]
    }
   ],
   "source": [
    "##### model preparation #####\n",
    "# hyperparameter: learning rate, weight decay, weight\n",
    "modelR = NeuralNetworkR().to(device)\n",
    "optimizerR = torch.optim.Adam(modelR.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterionR = nn.MSELoss().to(device)\n",
    "\n",
    "##### training #####\n",
    "done_modelR, train_lossR, valid_lossR = trainingR(network = modelR, \n",
    "                                                  trainloader = train_loader, \n",
    "                                                  validloader = valid_loader, \n",
    "                                                  optimizer = optimizerR, \n",
    "                                                  criterion = criterionR, \n",
    "                                                  epoch = 150, \n",
    "                                                  filename = 'tamama',\n",
    "                                                  early_stop = 10)\n",
    "\n",
    "##### testing #####\n",
    "_, _, result_tableR = testingR(done_modelR, test_loader, criterionR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:20:47.157284Z",
     "start_time": "2021-10-31T16:17:38.955232Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runall_modelC = NeuralNetworkC().to(device)\n",
    "runall_optimizerC = torch.optim.Adam(runall_modelC.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "runall_criterionC = nn.CrossEntropyLoss(weight = torch.tensor([0.5, 0.5])).to(device)\n",
    "\n",
    "table_setC, loss_dictC = runall_nn(train_x = run_train_x, \n",
    "                                   train_y = run_train_y, \n",
    "                                   test_x = run_test_x, \n",
    "                                   test_y = run_test_y, \n",
    "                                   n_epoch = 150, \n",
    "                                   batch_size = 64,\n",
    "                                   model = runall_modelC,\n",
    "                                   optimizer = runall_optimizerC, \n",
    "                                   criterion = runall_criterionC, \n",
    "                                   filename = 'runhist_array_4criteria_NeuralNetworkC', \n",
    "                                   train_ratio = 0.75, \n",
    "                                   early_stop = 10,\n",
    "                                   mode = 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:20:49.690137Z",
     "start_time": "2021-10-31T16:20:48.814304Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_plot(loss_dictC['train'], loss_dictC['valid'], num_row = 4, num_col = 3)\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:17:19.396922Z",
     "start_time": "2021-10-31T16:08:04.046778Z"
    }
   },
   "outputs": [],
   "source": [
    "runall_modelR = NeuralNetworkR().to(device)\n",
    "runall_optimizerR = torch.optim.Adam(runall_modelR.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "runall_criterionR = nn.MSELoss().to(device)\n",
    "\n",
    "table_setR, loss_dictR = runall_nn(train_x = run_train_x, \n",
    "                                   train_y = run_train_y, \n",
    "                                   test_x = run_test_x, \n",
    "                                   test_y = run_test_y, \n",
    "                                   n_epoch = 150, \n",
    "                                   batch_size = 64,\n",
    "                                   model = runall_modelR,\n",
    "                                   optimizer = runall_optimizerR, \n",
    "                                   criterion = runall_criterionR, \n",
    "                                   filename = 'runhist_array_4criteria_NeuralNetworkR', \n",
    "                                   train_ratio = 0.75, \n",
    "                                   early_stop = 10,\n",
    "                                   mode = 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:17:21.986069Z",
     "start_time": "2021-10-31T16:17:20.989691Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_plot(loss_dictR['train'], loss_dictR['valid'], num_row = 4, num_col = 3)\n",
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T16:20:51.673475Z",
     "start_time": "2021-10-31T16:20:51.361050Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20211102'\n",
    "TPE_multi = True\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setR['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'NeuralNetwork'\n",
    "table_setR['model'] = 'NeuralNetwork'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'NeuralNetwork')\n",
    "with pd.ExcelWriter(f'{savedate}_Regressor.xlsx', mode = 'a') as writer:\n",
    "    table_setR.to_excel(writer, sheet_name = 'NeuralNetwork')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
