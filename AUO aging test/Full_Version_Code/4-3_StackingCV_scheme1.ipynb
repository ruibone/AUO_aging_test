{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T09:05:43.060634Z",
     "start_time": "2021-12-05T09:05:39.530563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor,\\\n",
    "    AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio, train_col\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, cf_matrix, AUC, PR_curve, \\\n",
    "     multiple_curve, PR_matrix, best_threshold, all_optuna, optuna_history, AdaBoost_creator \n",
    "from library.XGBoost import XGBoost_creator\n",
    "from library.LightGBM import LightGBM_creator\n",
    "from library.CatBoost import CatBoost_creator\n",
    "from library.Random_Forest import RandomForest_creator\n",
    "from library.Extra_Trees import ExtraTrees_creator\n",
    "from library.Neural_Network import RunhistSet, NeuralNetworkC, trainingC\n",
    "from library.StackingCV_Scheme3 import stratified_data, runall_LR, runall_RidgeR, stackingCV_creator, vif, \\\n",
    "    correlation_plot, rank_importance\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load hyperparameters from all the base learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T09:05:43.123466Z",
     "start_time": "2021-12-05T09:05:43.108508Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_hyper(num_set, date, model_list, iter_list, filename, mode, TPE_multi) :\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    allset_dict = {}\n",
    "    for j in range(num_set) :\n",
    "        \n",
    "        model_dict = {}\n",
    "        for i, model in enumerate(model_list) :\n",
    "\n",
    "            with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[i]}.data', 'rb') as f:\n",
    "                temp_dict = pickle.load(f)\n",
    "                model_dict[model] = temp_dict[f'set{j}']\n",
    "                \n",
    "        allset_dict[f'set{j}'] = model_dict\n",
    "        \n",
    "    return allset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data by base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T09:05:43.217216Z",
     "start_time": "2021-12-05T09:05:43.170341Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_train(train_data, mode, base_param, cv):\n",
    "    \n",
    "    set_name = list(base_param.keys())\n",
    "    num_set = len(set_name)\n",
    "    model_list = list(base_param[set_name[0]].keys())\n",
    "    set_dict = {}\n",
    "    for i in tqdm(range(num_set)):\n",
    "        \n",
    "        print(f'Dataset {i}:\\n')\n",
    "        train_x_dict, train_y_dict, valid_x_dict, valid_y_dict = stratified_data(train_data[f'set{i}'], cv = cv)\n",
    "        all_cv = pd.DataFrame()\n",
    "        for j in tqdm(range(cv)):\n",
    "\n",
    "            model_predict = pd.DataFrame()\n",
    "            if mode == 'C':\n",
    "                \n",
    "                if 'NeuralNetwork' in model_list:\n",
    "                    temp_train = RunhistSet(train_x_dict[j], train_y_dict[j])\n",
    "                    temp_valid = RunhistSet(valid_x_dict[j], valid_y_dict[j])\n",
    "                    train_loader = DataLoader(temp_train, \n",
    "                                              batch_size = base_param[f'set{i}']['NeuralNetwork']['batch_size'], \n",
    "                                              shuffle = True)\n",
    "                    valid_loader = DataLoader(temp_valid, batch_size = len(valid_x_dict[j]), shuffle = False)\n",
    "                    nn_model = NeuralNetworkC(dim = train_x_dict[j].shape[1])\n",
    "                    optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                                 lr = base_param[f'set{i}']['NeuralNetwork']['learning_rate'], \n",
    "                                                 weight_decay = base_param[f'set{i}']['NeuralNetwork']['weight_decay'])\n",
    "                    criterion = nn.CrossEntropyLoss(\n",
    "                        weight = torch.tensor([1-base_param[f'set{i}']['NeuralNetwork']['bad_weight'], \n",
    "                                               base_param[f'set{i}']['NeuralNetwork']['bad_weight']])).to('cpu')\n",
    "                    network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 200, \n",
    "                                              filename = 'none', early_stop = 10)\n",
    "                    for x, y in valid_loader:\n",
    "                        output = network(x)\n",
    "                        _, predict_y = torch.max(output.data, 1)\n",
    "                    predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                \n",
    "                if 'XGBoost' in model_list:                     \n",
    "                    clf = XGBClassifier(**base_param[f'set{i}']['XGBoost'], n_jobs = -1)\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:                        \n",
    "                    clf = LGBMClassifier(**base_param[f'set{i}']['LightGBM'])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    clf = CatBoostClassifier(**base_param[f'set{i}']['CatBoost'])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeClassifier(\n",
    "                            max_depth = base_param[f'set{i}']['AdaBoost']['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[f'set{i}']['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    clf = AdaBoostClassifier(**boost_param)\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    clf = RandomForestClassifier(**base_param[f'set{i}']['RandomForest'])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    clf = ExtraTreesClassifier(**base_param[f'set{i}']['ExtraTrees'])\n",
    "                    clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            elif mode == 'R':\n",
    "\n",
    "                if 'XGBoost' in model_list:\n",
    "                    reg = XGBRegressor(**base_param[f'set{i}']['XGBoost'], n_jobs = -1)\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'X': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'LightGBM' in model_list:\n",
    "                    reg = LGBMRegressor(**base_param[f'set{i}']['LightGBM'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'L': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'CatBoost' in model_list:\n",
    "                    reg = CatBoostRegressor(**base_param[f'set{i}']['CatBoost'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'C': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'AdaBoost' in model_list:\n",
    "                    tree_param = {\n",
    "                        'base_estimator': DecisionTreeRegressor(\n",
    "                            max_depth = base_param[f'set{i}']['AdaBoost']['max_depth']\n",
    "                        )}\n",
    "                    boost_param = dict(\n",
    "                        (key, base_param[f'set{i}']['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                    )\n",
    "                    boost_param.update(tree_param)\n",
    "                    reg = AdaBoostRegressor(**boost_param)\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'A': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'RandomForest' in model_list:\n",
    "                    reg = RandomForestRegressor(**base_param[f'set{i}']['RandomForest'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'R': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                if 'ExtraTrees' in model_list:\n",
    "                    reg = ExtraTreesRegressor(**base_param[f'set{i}']['ExtraTrees'])\n",
    "                    reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                    predict_y = reg.predict(valid_x_dict[j])\n",
    "                    predict = pd.DataFrame({'E': predict_y})\n",
    "                    model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            test_label = valid_y_dict[j].reset_index(drop = True)\n",
    "            done_cv = pd.concat([model_predict, test_label], axis = 1)\n",
    "            all_cv = pd.concat([all_cv, done_cv], axis = 0)\n",
    "\n",
    "        set_dict[f'set{i}'] = all_cv\n",
    "    \n",
    "    return set_dict\n",
    "\n",
    "\n",
    "def transform_test(train_data, test_data, mode, base_param):\n",
    "    \n",
    "    set_name = list(base_param.keys())\n",
    "    num_set = len(set_name)\n",
    "    model_list = list(base_param[set_name[0]].keys())\n",
    "    test_dict = {}\n",
    "    for i in tqdm(range(num_set)):\n",
    "        \n",
    "        print(f'Dataset {i}:\\n')\n",
    "        train_x, train_y, test_x, test_y = label_divide(train_data[f'set{i}'], test_data, train_only = False)\n",
    "        model_predict = pd.DataFrame()\n",
    "        if mode == 'C':\n",
    "\n",
    "            if 'NeuralNetwork' in model_list:\n",
    "                temp_train = RunhistSet(train_x, train_y)\n",
    "                temp_test = RunhistSet(test_x, test_y)\n",
    "                train_loader = DataLoader(temp_train, \n",
    "                                          batch_size = base_param[f'set{i}']['NeuralNetwork']['batch_size'], \n",
    "                                          shuffle = True)\n",
    "                test_loader = DataLoader(temp_test, batch_size = len(test_x), shuffle = False)\n",
    "                nn_model = NeuralNetworkC(dim = train_x.shape[1])\n",
    "                optimizer = torch.optim.Adam(nn_model.parameters(), \n",
    "                                             lr = base_param[f'set{i}']['NeuralNetwork']['learning_rate'], \n",
    "                                             weight_decay = base_param[f'set{i}']['NeuralNetwork']['weight_decay'])\n",
    "                criterion = nn.CrossEntropyLoss(\n",
    "                    weight = torch.tensor([1-base_param[f'set{i}']['NeuralNetwork']['bad_weight'], \n",
    "                                           base_param[f'set{i}']['NeuralNetwork']['bad_weight']])).to('cpu')\n",
    "                network, _, _ = trainingC(nn_model, train_loader, train_loader, optimizer, criterion, epoch = 200, \n",
    "                                          filename = 'none', early_stop = 20)\n",
    "                for X, Y in test_loader:\n",
    "                    X, Y = X.float(), Y.long()\n",
    "                    output = network(X)\n",
    "                    _, predict_y = torch.max(output.data, 1)\n",
    "                predict = pd.DataFrame({'N': predict_y.numpy()})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "            \n",
    "            if 'XGBoost' in model_list:\n",
    "                clf = XGBClassifier(**base_param[f'set{i}']['XGBoost'], n_jobs = -1)\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'LightGBM' in model_list:\n",
    "                clf = LGBMClassifier(**base_param[f'set{i}']['LightGBM'])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'CatBoost' in model_list:\n",
    "                clf = CatBoostClassifier(**base_param[f'set{i}']['CatBoost'])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'C': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'AdaBoost' in model_list:\n",
    "                tree_param = {\n",
    "                    'base_estimator': DecisionTreeClassifier(\n",
    "                        max_depth = base_param[f'set{i}']['AdaBoost']['max_depth']\n",
    "                    )}\n",
    "                boost_param = dict(\n",
    "                    (key, base_param[f'set{i}']['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                )\n",
    "                boost_param.update(tree_param)\n",
    "                clf = AdaBoostClassifier(**boost_param)\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'A': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'RandomForest' in model_list:\n",
    "                clf = RandomForestClassifier(**base_param[f'set{i}']['RandomForest'])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'R': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'ExtraTrees' in model_list:\n",
    "                clf = ExtraTreesClassifier(**base_param[f'set{i}']['ExtraTrees'])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'E': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "        elif mode == 'R':\n",
    "\n",
    "            if 'XGBoost' in model_list:\n",
    "                reg = XGBRegressor(**base_param[f'set{i}']['XGBoost'], n_jobs = -1)\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'X': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'LightGBM' in model_list:\n",
    "                reg = LGBMRegressor(**base_param[f'set{i}']['LightGBM'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'L': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'CatBoost' in model_list:\n",
    "                reg = CatBoostRegressor(**base_param[f'set{i}']['CatBoost'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'C': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'AdaBoost' in model_list:\n",
    "                tree_param = {\n",
    "                    'base_estimator': DecisionTreeRegressor(\n",
    "                        max_depth = base_param[f'set{i}']['AdaBoost']['max_depth']\n",
    "                    )}\n",
    "                boost_param = dict(\n",
    "                    (key, base_param[f'set{i}']['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators']\n",
    "                )\n",
    "                boost_param.update(tree_param)\n",
    "                reg = AdaBoostRegressor(**boost_param)\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'A': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'RandomForest' in model_list:\n",
    "                reg = RandomForestRegressor(**base_param[f'set{i}']['RandomForest'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'R': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "            if 'ExtraTrees' in model_list:\n",
    "                reg = ExtraTreesRegressor(**base_param[f'set{i}']['ExtraTrees'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'E': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "        model_done = pd.concat([model_predict, test_y], axis = 1)\n",
    "        test_dict[f'set{i}'] = model_done\n",
    "        \n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading training data & testing data & hyperparameters generating from previous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T09:05:48.705546Z",
     "start_time": "2021-12-05T09:05:46.137411Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Month 2:\n",
      "\n",
      "Dimension of dataset 0 : (39009, 85)  balance ratio: 590.0\n",
      "Dimension of dataset 1 : (1296, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2034, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1448, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1320, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (1310, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (1326, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (1320, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (1320, 85)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (726, 85)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 3:\n",
      "\n",
      "Dimension of dataset 0 : (60396, 101)  balance ratio: 574.0\n",
      "Dimension of dataset 1 : (2088, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2568, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2306, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2100, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2101, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2325, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2100, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2100, 101)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1155, 101)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Month 4:\n",
      "\n",
      "Dimension of dataset 0 : (57743, 103)  balance ratio: 524.0\n",
      "Dimension of dataset 1 : (2132, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2948, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (2414, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (2198, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2183, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (2585, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (2200, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2200, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1210, 103)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Combined training data:\n",
      "\n",
      "Dimension of dataset 0 : (157148, 125)  balance ratio: 558.0\n",
      "Dimension of dataset 1 : (5516, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (7550, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (6168, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (5618, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (5594, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 6 : (6236, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 7 : (5620, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (5620, 125)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (3091, 125)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      " Dimension of testing data: (48649, 129)\n"
     ]
    }
   ],
   "source": [
    "### training data ### \n",
    "training_month = range(2, 5)\n",
    "\n",
    "data_dict, trainset_x, trainset_y = multiple_month(training_month, num_set = 10, filename = 'dataset')\n",
    "\n",
    "print('\\nCombined training data:\\n')\n",
    "run_train = multiple_set(num_set = 10)\n",
    "run_train_x, run_train_y = train_set(run_train, num_set = 10)\n",
    "\n",
    "### testing data ###\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of testing data:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T09:06:23.836105Z",
     "start_time": "2021-12-05T09:06:23.823140Z"
    }
   },
   "outputs": [],
   "source": [
    "##### loading hyperparameters #####\n",
    "hyper_info = {\n",
    "    'num_set': 10,\n",
    "    'date': '20211207',\n",
    "    'model_list': ['NeuralNetwork', 'LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'iter_list': [20, 200, 200, 200],\n",
    "    'filename': 'runhist_array_m2m4_m5_3criteria',\n",
    "    'TPE_multi': True\n",
    "}\n",
    "\n",
    "base_paramC = load_hyper(**hyper_info, mode = 'C')\n",
    "# base_paramR = load_hyper(**hyper_info, mode = 'R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transform for scheme 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:50:35.639452Z",
     "start_time": "2021-12-05T09:06:48.715178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71218c9d6ee24e8cbe9bb1412b0068ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1318d3fdbfdb4117a8f2943288633bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b07e729543442feb541cba03a581746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.04646367132580165, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 2: Train Loss = 0.026371365379935726, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.025998948904575585, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.02574090714588478, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.02561247758854133, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02484152645598291, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.02493106199320379, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.024813566596289607, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.024617174215434145, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.024418953684134097, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.024351493424245016, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.0239882318692726, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.02408227768963976, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.023992885546587572, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.0238237880436447, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.02361104005252523, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.02341117473121202, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.023151703238311186, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.02322886533034715, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.02284454920429961, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 18: Train Loss = 0.02274853303187084, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 19: Train Loss = 0.022524970406302372, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 20: Train Loss = 0.02232875517494373, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.021930008981441068, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 21: Train Loss = 0.02207445606194895, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 22: Train Loss = 0.02176990210831003, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 23: Train Loss = 0.021543411679511917, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 24: Train Loss = 0.021299579982304605, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 25: Train Loss = 0.02104922696543695, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.021195426598957956, Recall = 0.022222222222222223, Aging Rate = 4.7725483021659414e-05, precision = 0.8333333333333334\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.020823408020629982, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 27: Train Loss = 0.020574361953711328, Recall = 0.008888888888888889, Aging Rate = 2.3862741510829707e-05, Precision = 0.6666666666666666, f1 = 0.01754385964912281\n",
      "Epoch 28: Train Loss = 0.020299075591838918, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 29: Train Loss = 0.020049519849772116, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 30: Train Loss = 0.019809240357139855, Recall = 0.0044444444444444444, Aging Rate = 7.95424717027657e-06, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.019440570544606718, Recall = 0.022222222222222223, Aging Rate = 4.7725483021659414e-05, precision = 0.8333333333333334\n",
      "\n",
      "Epoch 31: Train Loss = 0.019519566054690983, Recall = 0.017777777777777778, Aging Rate = 3.181698868110628e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.0193521693653767, Recall = 0.0044444444444444444, Aging Rate = 7.95424717027657e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.019121474722789418, Recall = 0.008888888888888889, Aging Rate = 1.590849434055314e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.01882709684951269, Recall = 0.017777777777777778, Aging Rate = 3.181698868110628e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.018707048944323568, Recall = 0.013333333333333334, Aging Rate = 3.181698868110628e-05, Precision = 0.75, f1 = 0.026200873362445417\n",
      "Test Loss = 0.01817071187491127, Recall = 0.022222222222222223, Aging Rate = 3.9771235851382846e-05, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.01839550867593758, Recall = 0.017777777777777778, Aging Rate = 3.181698868110628e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01832519058765113, Recall = 0.022222222222222223, Aging Rate = 4.7725483021659414e-05, Precision = 0.8333333333333334, f1 = 0.043290043290043295\n",
      "Epoch 38: Train Loss = 0.018081122722150414, Recall = 0.022222222222222223, Aging Rate = 4.7725483021659414e-05, Precision = 0.8333333333333334, f1 = 0.043290043290043295\n",
      "Epoch 39: Train Loss = 0.01794022949302227, Recall = 0.017777777777777778, Aging Rate = 3.181698868110628e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01773268892619228, Recall = 0.035555555555555556, Aging Rate = 7.158822453248912e-05, Precision = 0.8888888888888888, f1 = 0.06837606837606838\n",
      "Test Loss = 0.01722144657413162, Recall = 0.022222222222222223, Aging Rate = 3.9771235851382846e-05, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.01754014690807377, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 42: Train Loss = 0.017420564589082917, Recall = 0.022222222222222223, Aging Rate = 3.9771235851382846e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.017231592161363406, Recall = 0.035555555555555556, Aging Rate = 7.158822453248912e-05, Precision = 0.8888888888888888, f1 = 0.06837606837606838\n",
      "Epoch 44: Train Loss = 0.017113882136556367, Recall = 0.02666666666666667, Aging Rate = 4.7725483021659414e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016851472390699954, Recall = 0.044444444444444446, Aging Rate = 7.954247170276569e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01651775905934634, Recall = 0.04888888888888889, Aging Rate = 8.749671887304226e-05, precision = 1.0\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.016703389634090404, Recall = 0.044444444444444446, Aging Rate = 8.749671887304226e-05, Precision = 0.9090909090909091, f1 = 0.08474576271186442\n",
      "Epoch 47: Train Loss = 0.016587059080900628, Recall = 0.06222222222222222, Aging Rate = 0.00011931370755414853, Precision = 0.9333333333333333, f1 = 0.11666666666666667\n",
      "Epoch 48: Train Loss = 0.016460784109209117, Recall = 0.04888888888888889, Aging Rate = 8.749671887304226e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016385379708178303, Recall = 0.04, Aging Rate = 7.158822453248912e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01619848980832214, Recall = 0.06666666666666667, Aging Rate = 0.00012726795472442511, Precision = 0.9375, f1 = 0.12448132780082988\n",
      "Test Loss = 0.015807096838467757, Recall = 0.10666666666666667, Aging Rate = 0.00019885617925691424, precision = 0.96\n",
      "\n",
      "Epoch 51: Train Loss = 0.016091423718610796, Recall = 0.07111111111111111, Aging Rate = 0.00014317644906497825, Precision = 0.8888888888888888, f1 = 0.13168724279835392\n",
      "Epoch 52: Train Loss = 0.016026145504838576, Recall = 0.06222222222222222, Aging Rate = 0.00011931370755414853, Precision = 0.9333333333333333, f1 = 0.11666666666666667\n",
      "Epoch 53: Train Loss = 0.01580913738283185, Recall = 0.10222222222222223, Aging Rate = 0.00019885617925691424, Precision = 0.92, f1 = 0.184\n",
      "Epoch 54: Train Loss = 0.01575477993770714, Recall = 0.08444444444444445, Aging Rate = 0.00016703919057580794, Precision = 0.9047619047619048, f1 = 0.15447154471544716\n",
      "Epoch 55: Train Loss = 0.015654139903481232, Recall = 0.08, Aging Rate = 0.00014317644906497825, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01515012372712849, Recall = 0.11555555555555555, Aging Rate = 0.0002306731679380205, precision = 0.896551724137931\n",
      "\n",
      "Epoch 56: Train Loss = 0.015546901589547083, Recall = 0.08, Aging Rate = 0.0001511306962352548, Precision = 0.9473684210526315, f1 = 0.14754098360655737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Train Loss = 0.015437237418901956, Recall = 0.10666666666666667, Aging Rate = 0.0002068104264271908, Precision = 0.9230769230769231, f1 = 0.1912350597609562\n",
      "Epoch 58: Train Loss = 0.015308832194748911, Recall = 0.09333333333333334, Aging Rate = 0.00017499343774608452, Precision = 0.9545454545454546, f1 = 0.17004048582995954\n",
      "Epoch 59: Train Loss = 0.015237203443053685, Recall = 0.11555555555555555, Aging Rate = 0.0002306731679380205, Precision = 0.896551724137931, f1 = 0.20472440944881887\n",
      "Epoch 60: Train Loss = 0.015188263652479212, Recall = 0.08, Aging Rate = 0.0001511306962352548, Precision = 0.9473684210526315, f1 = 0.14754098360655737\n",
      "Test Loss = 0.014670003718773066, Recall = 0.14666666666666667, Aging Rate = 0.00027839865095967994, precision = 0.9428571428571428\n",
      "\n",
      "Epoch 61: Train Loss = 0.015071493865271925, Recall = 0.10666666666666667, Aging Rate = 0.0002068104264271908, Precision = 0.9230769230769231, f1 = 0.1912350597609562\n",
      "Epoch 62: Train Loss = 0.014978131337814877, Recall = 0.11555555555555555, Aging Rate = 0.0002068104264271908, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.014896412550128534, Recall = 0.08888888888888889, Aging Rate = 0.00016703919057580794, Precision = 0.9523809523809523, f1 = 0.1626016260162602\n",
      "Epoch 64: Train Loss = 0.014795796665751332, Recall = 0.12444444444444444, Aging Rate = 0.00023862741510829706, Precision = 0.9333333333333333, f1 = 0.21960784313725493\n",
      "Epoch 65: Train Loss = 0.014709869622931889, Recall = 0.11555555555555555, Aging Rate = 0.00021476467359746737, Precision = 0.9629629629629629, f1 = 0.20634920634920634\n",
      "Test Loss = 0.014201773689594679, Recall = 0.15555555555555556, Aging Rate = 0.0002863528981299565, precision = 0.9722222222222222\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.014571220041315159, Recall = 0.11555555555555555, Aging Rate = 0.00021476467359746737, Precision = 0.9629629629629629, f1 = 0.20634920634920634\n",
      "Epoch 67: Train Loss = 0.014589153180723558, Recall = 0.1288888888888889, Aging Rate = 0.00025453590944885023, Precision = 0.90625, f1 = 0.22568093385214008\n",
      "Epoch 68: Train Loss = 0.01451693138476967, Recall = 0.1288888888888889, Aging Rate = 0.00025453590944885023, Precision = 0.90625, f1 = 0.22568093385214008\n",
      "Epoch 69: Train Loss = 0.014461369167992207, Recall = 0.1288888888888889, Aging Rate = 0.00023862741510829706, Precision = 0.9666666666666667, f1 = 0.22745098039215683\n",
      "Epoch 70: Train Loss = 0.014352781398288656, Recall = 0.14222222222222222, Aging Rate = 0.00027044440378940334, Precision = 0.9411764705882353, f1 = 0.24710424710424708\n",
      "Test Loss = 0.013902674320556686, Recall = 0.08, Aging Rate = 0.00014317644906497825, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.01429246046471603, Recall = 0.1111111111111111, Aging Rate = 0.00021476467359746737, Precision = 0.9259259259259259, f1 = 0.1984126984126984\n",
      "Epoch 72: Train Loss = 0.014256131701315193, Recall = 0.1511111111111111, Aging Rate = 0.0002863528981299565, Precision = 0.9444444444444444, f1 = 0.26053639846743293\n",
      "Epoch 73: Train Loss = 0.014158448711695937, Recall = 0.13777777777777778, Aging Rate = 0.00025453590944885023, Precision = 0.96875, f1 = 0.2412451361867704\n",
      "Epoch 74: Train Loss = 0.014099524240151163, Recall = 0.1288888888888889, Aging Rate = 0.00025453590944885023, Precision = 0.90625, f1 = 0.22568093385214008\n",
      "Epoch 75: Train Loss = 0.014043043283393472, Recall = 0.13333333333333333, Aging Rate = 0.00025453590944885023, Precision = 0.9375, f1 = 0.23346303501945526\n",
      "Test Loss = 0.01353550694263208, Recall = 0.19555555555555557, Aging Rate = 0.00037384961700299876, precision = 0.9361702127659575\n",
      "\n",
      "Epoch 76: Train Loss = 0.01387929516522953, Recall = 0.16, Aging Rate = 0.0003261241339813393, Precision = 0.8780487804878049, f1 = 0.27067669172932335\n",
      "Epoch 77: Train Loss = 0.013928940506733975, Recall = 0.17777777777777778, Aging Rate = 0.0003420326283218925, Precision = 0.9302325581395349, f1 = 0.2985074626865672\n",
      "Epoch 78: Train Loss = 0.013830002811771028, Recall = 0.1511111111111111, Aging Rate = 0.00029430714530023305, Precision = 0.918918918918919, f1 = 0.25954198473282447\n",
      "Epoch 79: Train Loss = 0.013847952129850596, Recall = 0.15555555555555556, Aging Rate = 0.00029430714530023305, Precision = 0.9459459459459459, f1 = 0.26717557251908397\n",
      "Epoch 80: Train Loss = 0.013778082009765281, Recall = 0.12444444444444444, Aging Rate = 0.00023862741510829706, Precision = 0.9333333333333333, f1 = 0.21960784313725493\n",
      "Test Loss = 0.013327527760711205, Recall = 0.2088888888888889, Aging Rate = 0.00042157510002465814, precision = 0.8867924528301887\n",
      "\n",
      "Epoch 81: Train Loss = 0.013690357243278465, Recall = 0.17333333333333334, Aging Rate = 0.0003340783811516159, Precision = 0.9285714285714286, f1 = 0.29213483146067415\n",
      "Epoch 82: Train Loss = 0.013635150066427761, Recall = 0.18222222222222223, Aging Rate = 0.00037384961700299876, Precision = 0.8723404255319149, f1 = 0.3014705882352941\n",
      "Epoch 83: Train Loss = 0.013625695773549808, Recall = 0.1511111111111111, Aging Rate = 0.0003102156396407862, Precision = 0.8717948717948718, f1 = 0.25757575757575757\n",
      "Epoch 84: Train Loss = 0.013581301928907549, Recall = 0.16444444444444445, Aging Rate = 0.0003102156396407862, Precision = 0.9487179487179487, f1 = 0.28030303030303033\n",
      "Epoch 85: Train Loss = 0.013544421484484462, Recall = 0.1511111111111111, Aging Rate = 0.0003102156396407862, Precision = 0.8717948717948718, f1 = 0.25757575757575757\n",
      "Test Loss = 0.013053129658906294, Recall = 0.2088888888888889, Aging Rate = 0.0003977123585138285, precision = 0.94\n",
      "\n",
      "Epoch 86: Train Loss = 0.013449838692464829, Recall = 0.17333333333333334, Aging Rate = 0.0003261241339813393, Precision = 0.9512195121951219, f1 = 0.29323308270676696\n",
      "Epoch 87: Train Loss = 0.013416817651946742, Recall = 0.14666666666666667, Aging Rate = 0.00029430714530023305, Precision = 0.8918918918918919, f1 = 0.25190839694656486\n",
      "Epoch 88: Train Loss = 0.01337407164891419, Recall = 0.1688888888888889, Aging Rate = 0.00034998687549216904, Precision = 0.8636363636363636, f1 = 0.28252788104089216\n",
      "Epoch 89: Train Loss = 0.013324217501590487, Recall = 0.18222222222222223, Aging Rate = 0.00034998687549216904, Precision = 0.9318181818181818, f1 = 0.3048327137546469\n",
      "Epoch 90: Train Loss = 0.013285314447828218, Recall = 0.18222222222222223, Aging Rate = 0.00034998687549216904, Precision = 0.9318181818181818, f1 = 0.3048327137546469\n",
      "Test Loss = 0.012801176234819894, Recall = 0.2222222222222222, Aging Rate = 0.00042952934719493475, precision = 0.9259259259259259\n",
      "\n",
      "Epoch 91: Train Loss = 0.013215653679544363, Recall = 0.16444444444444445, Aging Rate = 0.0003261241339813393, Precision = 0.9024390243902439, f1 = 0.2781954887218045\n",
      "Epoch 92: Train Loss = 0.013225953514862589, Recall = 0.16, Aging Rate = 0.0003022613924705096, Precision = 0.9473684210526315, f1 = 0.27376425855513303\n",
      "Epoch 93: Train Loss = 0.01313954408424029, Recall = 0.18666666666666668, Aging Rate = 0.0003420326283218925, Precision = 0.9767441860465116, f1 = 0.31343283582089554\n",
      "Epoch 94: Train Loss = 0.013037733563433688, Recall = 0.19555555555555557, Aging Rate = 0.0003818038641732753, Precision = 0.9166666666666666, f1 = 0.32234432234432236\n",
      "Epoch 95: Train Loss = 0.013103485820972358, Recall = 0.1511111111111111, Aging Rate = 0.00029430714530023305, Precision = 0.918918918918919, f1 = 0.25954198473282447\n",
      "Test Loss = 0.012603576384732758, Recall = 0.2311111111111111, Aging Rate = 0.0004374835943652113, precision = 0.9454545454545454\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.01303303373764948, Recall = 0.17777777777777778, Aging Rate = 0.00037384961700299876, Precision = 0.851063829787234, f1 = 0.29411764705882354\n",
      "Epoch 97: Train Loss = 0.012956506064925228, Recall = 0.17333333333333334, Aging Rate = 0.0003420326283218925, Precision = 0.9069767441860465, f1 = 0.291044776119403\n",
      "Epoch 98: Train Loss = 0.012964436603179356, Recall = 0.18222222222222223, Aging Rate = 0.0003658953698327222, Precision = 0.8913043478260869, f1 = 0.30258302583025826\n",
      "Epoch 99: Train Loss = 0.01286041748359787, Recall = 0.17333333333333334, Aging Rate = 0.00031816988681106277, Precision = 0.975, f1 = 0.2943396226415094\n",
      "Epoch 100: Train Loss = 0.012868361246251604, Recall = 0.2, Aging Rate = 0.00042952934719493475, Precision = 0.8333333333333334, f1 = 0.3225806451612903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.012366532695502381, Recall = 0.21777777777777776, Aging Rate = 0.00042157510002465814, precision = 0.9245283018867925\n",
      "\n",
      "Epoch 101: Train Loss = 0.01278073056754481, Recall = 0.19111111111111112, Aging Rate = 0.00037384961700299876, Precision = 0.9148936170212766, f1 = 0.31617647058823534\n",
      "Epoch 102: Train Loss = 0.012844566972925296, Recall = 0.18666666666666668, Aging Rate = 0.0003658953698327222, Precision = 0.9130434782608695, f1 = 0.30996309963099633\n",
      "Epoch 103: Train Loss = 0.012772072722204889, Recall = 0.18222222222222223, Aging Rate = 0.00034998687549216904, Precision = 0.9318181818181818, f1 = 0.3048327137546469\n",
      "Epoch 104: Train Loss = 0.012753700924832908, Recall = 0.2088888888888889, Aging Rate = 0.0003977123585138285, Precision = 0.94, f1 = 0.34181818181818185\n",
      "Epoch 105: Train Loss = 0.012698691198453049, Recall = 0.20444444444444446, Aging Rate = 0.00038975811134355187, Precision = 0.9387755102040817, f1 = 0.3357664233576642\n",
      "Test Loss = 0.012241478534598267, Recall = 0.27555555555555555, Aging Rate = 0.0005408888075788067, precision = 0.9117647058823529\n",
      "\n",
      "Epoch 106: Train Loss = 0.012670217607633222, Recall = 0.20444444444444446, Aging Rate = 0.00042157510002465814, Precision = 0.8679245283018868, f1 = 0.3309352517985612\n",
      "Epoch 107: Train Loss = 0.012650192181183345, Recall = 0.18666666666666668, Aging Rate = 0.0003658953698327222, Precision = 0.9130434782608695, f1 = 0.30996309963099633\n",
      "Epoch 108: Train Loss = 0.012595925571586132, Recall = 0.18222222222222223, Aging Rate = 0.00038975811134355187, Precision = 0.8367346938775511, f1 = 0.2992700729927007\n",
      "Epoch 109: Train Loss = 0.012529210965764967, Recall = 0.2088888888888889, Aging Rate = 0.00040566660568410503, Precision = 0.9215686274509803, f1 = 0.3405797101449275\n",
      "Epoch 110: Train Loss = 0.012570291508329504, Recall = 0.18222222222222223, Aging Rate = 0.00037384961700299876, Precision = 0.8723404255319149, f1 = 0.3014705882352941\n",
      "Test Loss = 0.01211498926755767, Recall = 0.27555555555555555, Aging Rate = 0.0005567973019193599, precision = 0.8857142857142857\n",
      "\n",
      "Epoch 111: Train Loss = 0.012474885834703379, Recall = 0.2311111111111111, Aging Rate = 0.0004693005830463176, Precision = 0.8813559322033898, f1 = 0.3661971830985915\n",
      "Epoch 112: Train Loss = 0.012415918414904609, Recall = 0.2088888888888889, Aging Rate = 0.0004136208528543816, Precision = 0.9038461538461539, f1 = 0.3393501805054151\n",
      "Epoch 113: Train Loss = 0.012395462974688314, Recall = 0.21777777777777776, Aging Rate = 0.0004374835943652113, Precision = 0.8909090909090909, f1 = 0.35\n",
      "Epoch 114: Train Loss = 0.012387861147958841, Recall = 0.2, Aging Rate = 0.0003977123585138285, Precision = 0.9, f1 = 0.32727272727272727\n",
      "Epoch 115: Train Loss = 0.01234001635479738, Recall = 0.20444444444444446, Aging Rate = 0.0003977123585138285, Precision = 0.92, f1 = 0.33454545454545453\n",
      "Test Loss = 0.011862254509197576, Recall = 0.2311111111111111, Aging Rate = 0.0004374835943652113, precision = 0.9454545454545454\n",
      "\n",
      "Epoch 116: Train Loss = 0.012326627034372643, Recall = 0.2088888888888889, Aging Rate = 0.00040566660568410503, Precision = 0.9215686274509803, f1 = 0.3405797101449275\n",
      "Epoch 117: Train Loss = 0.012338707102447612, Recall = 0.20444444444444446, Aging Rate = 0.0003977123585138285, Precision = 0.92, f1 = 0.33454545454545453\n",
      "Epoch 118: Train Loss = 0.012291763981714748, Recall = 0.19555555555555557, Aging Rate = 0.00038975811134355187, Precision = 0.8979591836734694, f1 = 0.32116788321167883\n",
      "Epoch 119: Train Loss = 0.012243842738622833, Recall = 0.21333333333333335, Aging Rate = 0.00042952934719493475, Precision = 0.8888888888888888, f1 = 0.3440860215053763\n",
      "Epoch 120: Train Loss = 0.012241759920883768, Recall = 0.21333333333333335, Aging Rate = 0.00042157510002465814, Precision = 0.9056603773584906, f1 = 0.34532374100719426\n",
      "Test Loss = 0.011952822884113038, Recall = 0.3022222222222222, Aging Rate = 0.000628385526451849, precision = 0.8607594936708861\n",
      "\n",
      "Epoch 121: Train Loss = 0.012164891905145107, Recall = 0.20444444444444446, Aging Rate = 0.0003977123585138285, Precision = 0.92, f1 = 0.33454545454545453\n",
      "Epoch 122: Train Loss = 0.012135935484667576, Recall = 0.19111111111111112, Aging Rate = 0.00037384961700299876, Precision = 0.9148936170212766, f1 = 0.31617647058823534\n",
      "Epoch 123: Train Loss = 0.012127071969694701, Recall = 0.23555555555555555, Aging Rate = 0.00048520907738687074, Precision = 0.8688524590163934, f1 = 0.37062937062937057\n",
      "Epoch 124: Train Loss = 0.01210396950510203, Recall = 0.21333333333333335, Aging Rate = 0.00042157510002465814, Precision = 0.9056603773584906, f1 = 0.34532374100719426\n",
      "Epoch 125: Train Loss = 0.012072831628844134, Recall = 0.20444444444444446, Aging Rate = 0.00038975811134355187, Precision = 0.9387755102040817, f1 = 0.3357664233576642\n",
      "Test Loss = 0.011610847183756734, Recall = 0.24, Aging Rate = 0.0004693005830463176, precision = 0.9152542372881356\n",
      "\n",
      "Epoch 126: Train Loss = 0.012063042195191264, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 127: Train Loss = 0.012062941178917513, Recall = 0.2222222222222222, Aging Rate = 0.0004374835943652113, Precision = 0.9090909090909091, f1 = 0.3571428571428571\n",
      "Epoch 128: Train Loss = 0.012015972601064451, Recall = 0.22666666666666666, Aging Rate = 0.000461346335876041, Precision = 0.8793103448275862, f1 = 0.3604240282685512\n",
      "Epoch 129: Train Loss = 0.012021212545253054, Recall = 0.23555555555555555, Aging Rate = 0.00047725483021659413, Precision = 0.8833333333333333, f1 = 0.3719298245614035\n",
      "Epoch 130: Train Loss = 0.01192954699424432, Recall = 0.2088888888888889, Aging Rate = 0.0004136208528543816, Precision = 0.9038461538461539, f1 = 0.3393501805054151\n",
      "Test Loss = 0.011516244459091975, Recall = 0.2222222222222222, Aging Rate = 0.00042952934719493475, precision = 0.9259259259259259\n",
      "\n",
      "Epoch 131: Train Loss = 0.011951054714143618, Recall = 0.2222222222222222, Aging Rate = 0.00045339208870576447, Precision = 0.8771929824561403, f1 = 0.3546099290780142\n",
      "Epoch 132: Train Loss = 0.011931673528099394, Recall = 0.21777777777777776, Aging Rate = 0.00044543784153548786, Precision = 0.875, f1 = 0.34875444839857644\n",
      "Epoch 133: Train Loss = 0.0119293732561297, Recall = 0.2222222222222222, Aging Rate = 0.00042952934719493475, Precision = 0.9259259259259259, f1 = 0.3584229390681003\n",
      "Epoch 134: Train Loss = 0.011877045930606152, Recall = 0.20444444444444446, Aging Rate = 0.00038975811134355187, Precision = 0.9387755102040817, f1 = 0.3357664233576642\n",
      "Epoch 135: Train Loss = 0.011854351889259804, Recall = 0.2311111111111111, Aging Rate = 0.0004693005830463176, Precision = 0.8813559322033898, f1 = 0.3661971830985915\n",
      "Test Loss = 0.011517168205087535, Recall = 0.1688888888888889, Aging Rate = 0.00031816988681106277, precision = 0.95\n",
      "\n",
      "Epoch 136: Train Loss = 0.01181253943312911, Recall = 0.24, Aging Rate = 0.0004693005830463176, Precision = 0.9152542372881356, f1 = 0.38028169014084506\n",
      "Epoch 137: Train Loss = 0.011804885486404711, Recall = 0.2088888888888889, Aging Rate = 0.00042157510002465814, Precision = 0.8867924528301887, f1 = 0.33812949640287776\n",
      "Epoch 138: Train Loss = 0.011810056713553785, Recall = 0.22666666666666666, Aging Rate = 0.00048520907738687074, Precision = 0.8360655737704918, f1 = 0.35664335664335667\n",
      "Epoch 139: Train Loss = 0.011723004173083905, Recall = 0.24, Aging Rate = 0.000461346335876041, Precision = 0.9310344827586207, f1 = 0.381625441696113\n",
      "Epoch 140: Train Loss = 0.011698225087295141, Recall = 0.21333333333333335, Aging Rate = 0.00045339208870576447, Precision = 0.8421052631578947, f1 = 0.34042553191489366\n",
      "Test Loss = 0.011298512027934521, Recall = 0.2311111111111111, Aging Rate = 0.00042952934719493475, precision = 0.9629629629629629\n",
      "Model in epoch 140 is saved.\n",
      "\n",
      "Epoch 141: Train Loss = 0.011759410924097704, Recall = 0.21777777777777776, Aging Rate = 0.0004374835943652113, Precision = 0.8909090909090909, f1 = 0.35\n",
      "Epoch 142: Train Loss = 0.011731695715382707, Recall = 0.20444444444444446, Aging Rate = 0.0004374835943652113, Precision = 0.8363636363636363, f1 = 0.32857142857142857\n",
      "Epoch 143: Train Loss = 0.011687931015944246, Recall = 0.25333333333333335, Aging Rate = 0.000517026066067977, Precision = 0.8769230769230769, f1 = 0.3931034482758621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144: Train Loss = 0.011694915279289236, Recall = 0.21333333333333335, Aging Rate = 0.00040566660568410503, Precision = 0.9411764705882353, f1 = 0.3478260869565218\n",
      "Epoch 145: Train Loss = 0.011637148617884755, Recall = 0.2311111111111111, Aging Rate = 0.000461346335876041, Precision = 0.896551724137931, f1 = 0.3674911660777385\n",
      "Test Loss = 0.011203329900204392, Recall = 0.22666666666666666, Aging Rate = 0.00042157510002465814, precision = 0.9622641509433962\n",
      "\n",
      "Epoch 146: Train Loss = 0.01161814906474319, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 147: Train Loss = 0.011642360567699373, Recall = 0.2222222222222222, Aging Rate = 0.0004374835943652113, Precision = 0.9090909090909091, f1 = 0.3571428571428571\n",
      "Epoch 148: Train Loss = 0.01156404291778674, Recall = 0.22666666666666666, Aging Rate = 0.00048520907738687074, Precision = 0.8360655737704918, f1 = 0.35664335664335667\n",
      "Epoch 149: Train Loss = 0.011587034891752864, Recall = 0.2311111111111111, Aging Rate = 0.00044543784153548786, Precision = 0.9285714285714286, f1 = 0.37010676156583633\n",
      "Epoch 150: Train Loss = 0.011535645606790934, Recall = 0.24888888888888888, Aging Rate = 0.0004931633245571472, Precision = 0.9032258064516129, f1 = 0.3902439024390244\n",
      "Test Loss = 0.01111572347804885, Recall = 0.29333333333333333, Aging Rate = 0.000572705796259913, precision = 0.9166666666666666\n",
      "\n",
      "Epoch 151: Train Loss = 0.011535572998287376, Recall = 0.24, Aging Rate = 0.00047725483021659413, Precision = 0.9, f1 = 0.3789473684210526\n",
      "Epoch 152: Train Loss = 0.011536480735171866, Recall = 0.24888888888888888, Aging Rate = 0.0005011175717274239, Precision = 0.8888888888888888, f1 = 0.38888888888888884\n",
      "Epoch 153: Train Loss = 0.011567261795023657, Recall = 0.2222222222222222, Aging Rate = 0.0004136208528543816, Precision = 0.9615384615384616, f1 = 0.3610108303249097\n",
      "Epoch 154: Train Loss = 0.011466206255078415, Recall = 0.24, Aging Rate = 0.0004931633245571472, Precision = 0.8709677419354839, f1 = 0.37630662020905925\n",
      "Epoch 155: Train Loss = 0.011450065641214875, Recall = 0.2222222222222222, Aging Rate = 0.0004374835943652113, Precision = 0.9090909090909091, f1 = 0.3571428571428571\n",
      "Test Loss = 0.011096515779739122, Recall = 0.20444444444444446, Aging Rate = 0.0003818038641732753, precision = 0.9583333333333334\n",
      "\n",
      "Epoch 156: Train Loss = 0.01143172938677436, Recall = 0.2222222222222222, Aging Rate = 0.00042952934719493475, Precision = 0.9259259259259259, f1 = 0.3584229390681003\n",
      "Epoch 157: Train Loss = 0.011420197906978917, Recall = 0.23555555555555555, Aging Rate = 0.000461346335876041, Precision = 0.9137931034482759, f1 = 0.37455830388692574\n",
      "Epoch 158: Train Loss = 0.01136970743637716, Recall = 0.2577777777777778, Aging Rate = 0.0005090718188977005, Precision = 0.90625, f1 = 0.40138408304498274\n",
      "Epoch 159: Train Loss = 0.011410857813049392, Recall = 0.23555555555555555, Aging Rate = 0.0004693005830463176, Precision = 0.8983050847457628, f1 = 0.3732394366197183\n",
      "Epoch 160: Train Loss = 0.011418766114116723, Recall = 0.22666666666666666, Aging Rate = 0.00047725483021659413, Precision = 0.85, f1 = 0.35789473684210527\n",
      "Test Loss = 0.01096764652029354, Recall = 0.28444444444444444, Aging Rate = 0.0005806600434301896, precision = 0.8767123287671232\n",
      "\n",
      "Epoch 161: Train Loss = 0.011330845166501752, Recall = 0.24, Aging Rate = 0.0004693005830463176, Precision = 0.9152542372881356, f1 = 0.38028169014084506\n",
      "Epoch 162: Train Loss = 0.011387773789639514, Recall = 0.2311111111111111, Aging Rate = 0.00047725483021659413, Precision = 0.8666666666666667, f1 = 0.3649122807017544\n",
      "Epoch 163: Train Loss = 0.011308572558422087, Recall = 0.24444444444444444, Aging Rate = 0.00048520907738687074, Precision = 0.9016393442622951, f1 = 0.38461538461538464\n",
      "Epoch 164: Train Loss = 0.011340826169226067, Recall = 0.21333333333333335, Aging Rate = 0.00042157510002465814, Precision = 0.9056603773584906, f1 = 0.34532374100719426\n",
      "Epoch 165: Train Loss = 0.011288702227422995, Recall = 0.2577777777777778, Aging Rate = 0.0005408888075788067, Precision = 0.8529411764705882, f1 = 0.39590443686006827\n",
      "Test Loss = 0.010847366730399264, Recall = 0.26222222222222225, Aging Rate = 0.0004931633245571472, precision = 0.9516129032258065\n",
      "Model in epoch 165 is saved.\n",
      "\n",
      "Epoch 166: Train Loss = 0.01128231111879413, Recall = 0.23555555555555555, Aging Rate = 0.00048520907738687074, Precision = 0.8688524590163934, f1 = 0.37062937062937057\n",
      "Epoch 167: Train Loss = 0.011289387832483688, Recall = 0.2311111111111111, Aging Rate = 0.0004693005830463176, Precision = 0.8813559322033898, f1 = 0.3661971830985915\n",
      "Epoch 168: Train Loss = 0.011232748309692217, Recall = 0.26222222222222225, Aging Rate = 0.0005408888075788067, Precision = 0.8676470588235294, f1 = 0.40273037542662116\n",
      "Epoch 169: Train Loss = 0.011211508323173821, Recall = 0.24, Aging Rate = 0.00047725483021659413, Precision = 0.9, f1 = 0.3789473684210526\n",
      "Epoch 170: Train Loss = 0.011201390828489444, Recall = 0.2311111111111111, Aging Rate = 0.0004693005830463176, Precision = 0.8813559322033898, f1 = 0.3661971830985915\n",
      "Test Loss = 0.010825980972204085, Recall = 0.26222222222222225, Aging Rate = 0.0004931633245571472, precision = 0.9516129032258065\n",
      "\n",
      "Epoch 171: Train Loss = 0.011192337513388714, Recall = 0.24, Aging Rate = 0.00048520907738687074, Precision = 0.8852459016393442, f1 = 0.37762237762237766\n",
      "Epoch 172: Train Loss = 0.011253082507265477, Recall = 0.2311111111111111, Aging Rate = 0.00045339208870576447, Precision = 0.9122807017543859, f1 = 0.3687943262411348\n",
      "Epoch 173: Train Loss = 0.011176901976404332, Recall = 0.24444444444444444, Aging Rate = 0.0004693005830463176, Precision = 0.9322033898305084, f1 = 0.3873239436619718\n",
      "Epoch 174: Train Loss = 0.01118510861825762, Recall = 0.2311111111111111, Aging Rate = 0.0004931633245571472, Precision = 0.8387096774193549, f1 = 0.36236933797909404\n",
      "Epoch 175: Train Loss = 0.011136239049693441, Recall = 0.24888888888888888, Aging Rate = 0.0004931633245571472, Precision = 0.9032258064516129, f1 = 0.3902439024390244\n",
      "Test Loss = 0.01071612032706408, Recall = 0.26222222222222225, Aging Rate = 0.0005011175717274239, precision = 0.9365079365079365\n",
      "\n",
      "Epoch 176: Train Loss = 0.011120383210076383, Recall = 0.25333333333333335, Aging Rate = 0.0005090718188977005, Precision = 0.890625, f1 = 0.3944636678200692\n",
      "Epoch 177: Train Loss = 0.011158809279379449, Recall = 0.24444444444444444, Aging Rate = 0.00048520907738687074, Precision = 0.9016393442622951, f1 = 0.38461538461538464\n",
      "Epoch 178: Train Loss = 0.011126365349211888, Recall = 0.2311111111111111, Aging Rate = 0.000461346335876041, Precision = 0.896551724137931, f1 = 0.3674911660777385\n",
      "Epoch 179: Train Loss = 0.011096059215773203, Recall = 0.24444444444444444, Aging Rate = 0.00047725483021659413, Precision = 0.9166666666666666, f1 = 0.3859649122807018\n",
      "Epoch 180: Train Loss = 0.01105441479202014, Recall = 0.26666666666666666, Aging Rate = 0.0005488430547490832, Precision = 0.8695652173913043, f1 = 0.40816326530612246\n",
      "Test Loss = 0.010686307937556353, Recall = 0.24, Aging Rate = 0.00044543784153548786, precision = 0.9642857142857143\n",
      "Model in epoch 180 is saved.\n",
      "\n",
      "Epoch 181: Train Loss = 0.011091779704303114, Recall = 0.2311111111111111, Aging Rate = 0.000461346335876041, Precision = 0.896551724137931, f1 = 0.3674911660777385\n",
      "Epoch 182: Train Loss = 0.0110618091494087, Recall = 0.23555555555555555, Aging Rate = 0.0004931633245571472, Precision = 0.8548387096774194, f1 = 0.3693379790940766\n",
      "Epoch 183: Train Loss = 0.011071619284409515, Recall = 0.25333333333333335, Aging Rate = 0.00048520907738687074, Precision = 0.9344262295081968, f1 = 0.39860139860139865\n",
      "Epoch 184: Train Loss = 0.011055388041734174, Recall = 0.24, Aging Rate = 0.00048520907738687074, Precision = 0.8852459016393442, f1 = 0.37762237762237766\n",
      "Epoch 185: Train Loss = 0.010974035293877494, Recall = 0.24888888888888888, Aging Rate = 0.00048520907738687074, Precision = 0.9180327868852459, f1 = 0.3916083916083916\n",
      "Test Loss = 0.010577548118457162, Recall = 0.27555555555555555, Aging Rate = 0.0005408888075788067, precision = 0.9117647058823529\n",
      "\n",
      "Epoch 186: Train Loss = 0.011021295926213216, Recall = 0.21333333333333335, Aging Rate = 0.0004693005830463176, Precision = 0.8135593220338984, f1 = 0.3380281690140845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187: Train Loss = 0.01102257838453478, Recall = 0.24888888888888888, Aging Rate = 0.00048520907738687074, Precision = 0.9180327868852459, f1 = 0.3916083916083916\n",
      "Epoch 188: Train Loss = 0.011012391246569471, Recall = 0.24888888888888888, Aging Rate = 0.000517026066067977, Precision = 0.8615384615384616, f1 = 0.3862068965517241\n",
      "Epoch 189: Train Loss = 0.010993147218909762, Recall = 0.25333333333333335, Aging Rate = 0.0005090718188977005, Precision = 0.890625, f1 = 0.3944636678200692\n",
      "Epoch 190: Train Loss = 0.010944312849787077, Recall = 0.24444444444444444, Aging Rate = 0.0005090718188977005, Precision = 0.859375, f1 = 0.3806228373702422\n",
      "Test Loss = 0.010596175292552955, Recall = 0.22666666666666666, Aging Rate = 0.00042157510002465814, precision = 0.9622641509433962\n",
      "\n",
      "Epoch 191: Train Loss = 0.010977269153898468, Recall = 0.24, Aging Rate = 0.00047725483021659413, Precision = 0.9, f1 = 0.3789473684210526\n",
      "Epoch 192: Train Loss = 0.010923089614602087, Recall = 0.26666666666666666, Aging Rate = 0.0005488430547490832, Precision = 0.8695652173913043, f1 = 0.40816326530612246\n",
      "Epoch 193: Train Loss = 0.010945393725246215, Recall = 0.26222222222222225, Aging Rate = 0.0005249803132382536, Precision = 0.8939393939393939, f1 = 0.40549828178694164\n",
      "Epoch 194: Train Loss = 0.010892860302077582, Recall = 0.25333333333333335, Aging Rate = 0.0005011175717274239, Precision = 0.9047619047619048, f1 = 0.39583333333333337\n",
      "Epoch 195: Train Loss = 0.010875679170170491, Recall = 0.2311111111111111, Aging Rate = 0.0004374835943652113, Precision = 0.9454545454545454, f1 = 0.3714285714285714\n",
      "Test Loss = 0.010449586905306603, Recall = 0.29333333333333333, Aging Rate = 0.0005647515490896364, precision = 0.9295774647887324\n",
      "\n",
      "Epoch 196: Train Loss = 0.010908846388643666, Recall = 0.27111111111111114, Aging Rate = 0.0005488430547490832, Precision = 0.8840579710144928, f1 = 0.41496598639455784\n",
      "Epoch 197: Train Loss = 0.010887592092673131, Recall = 0.24888888888888888, Aging Rate = 0.0005011175717274239, Precision = 0.8888888888888888, f1 = 0.38888888888888884\n",
      "Epoch 198: Train Loss = 0.010892026700572783, Recall = 0.22666666666666666, Aging Rate = 0.00047725483021659413, Precision = 0.85, f1 = 0.35789473684210527\n",
      "Epoch 199: Train Loss = 0.010815596792761489, Recall = 0.26222222222222225, Aging Rate = 0.000517026066067977, Precision = 0.9076923076923077, f1 = 0.40689655172413797\n",
      "Epoch 200: Train Loss = 0.010853849098580417, Recall = 0.26222222222222225, Aging Rate = 0.000517026066067977, Precision = 0.9076923076923077, f1 = 0.40689655172413797\n",
      "Test Loss = 0.010360989728539223, Recall = 0.28, Aging Rate = 0.0005567973019193599, precision = 0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2820816\ttotal: 188ms\tremaining: 56.4s\n",
      "1:\tlearn: 0.1242346\ttotal: 225ms\tremaining: 33.5s\n",
      "2:\tlearn: 0.0629337\ttotal: 241ms\tremaining: 23.8s\n",
      "3:\tlearn: 0.0369198\ttotal: 256ms\tremaining: 18.9s\n",
      "4:\tlearn: 0.0256879\ttotal: 268ms\tremaining: 15.8s\n",
      "5:\tlearn: 0.0198007\ttotal: 287ms\tremaining: 14.1s\n",
      "6:\tlearn: 0.0166165\ttotal: 333ms\tremaining: 13.9s\n",
      "7:\tlearn: 0.0149295\ttotal: 348ms\tremaining: 12.7s\n",
      "8:\tlearn: 0.0138924\ttotal: 364ms\tremaining: 11.8s\n",
      "9:\tlearn: 0.0132100\ttotal: 379ms\tremaining: 11s\n",
      "10:\tlearn: 0.0127911\ttotal: 426ms\tremaining: 11.2s\n",
      "11:\tlearn: 0.0125379\ttotal: 458ms\tremaining: 11s\n",
      "12:\tlearn: 0.0121141\ttotal: 490ms\tremaining: 10.8s\n",
      "13:\tlearn: 0.0118797\ttotal: 505ms\tremaining: 10.3s\n",
      "14:\tlearn: 0.0117621\ttotal: 519ms\tremaining: 9.87s\n",
      "15:\tlearn: 0.0116817\ttotal: 535ms\tremaining: 9.5s\n",
      "16:\tlearn: 0.0116321\ttotal: 549ms\tremaining: 9.14s\n",
      "17:\tlearn: 0.0115484\ttotal: 566ms\tremaining: 8.87s\n",
      "18:\tlearn: 0.0113475\ttotal: 598ms\tremaining: 8.84s\n",
      "19:\tlearn: 0.0113231\ttotal: 628ms\tremaining: 8.79s\n",
      "20:\tlearn: 0.0111511\ttotal: 661ms\tremaining: 8.78s\n",
      "21:\tlearn: 0.0110669\ttotal: 676ms\tremaining: 8.54s\n",
      "22:\tlearn: 0.0110224\ttotal: 709ms\tremaining: 8.54s\n",
      "23:\tlearn: 0.0109266\ttotal: 738ms\tremaining: 8.49s\n",
      "24:\tlearn: 0.0108592\ttotal: 753ms\tremaining: 8.28s\n",
      "25:\tlearn: 0.0107277\ttotal: 785ms\tremaining: 8.27s\n",
      "26:\tlearn: 0.0106314\ttotal: 816ms\tremaining: 8.25s\n",
      "27:\tlearn: 0.0105492\ttotal: 831ms\tremaining: 8.07s\n",
      "28:\tlearn: 0.0104911\ttotal: 846ms\tremaining: 7.91s\n",
      "29:\tlearn: 0.0104298\ttotal: 878ms\tremaining: 7.91s\n",
      "30:\tlearn: 0.0103694\ttotal: 894ms\tremaining: 7.76s\n",
      "31:\tlearn: 0.0102616\ttotal: 906ms\tremaining: 7.58s\n",
      "32:\tlearn: 0.0101430\ttotal: 927ms\tremaining: 7.5s\n",
      "33:\tlearn: 0.0100946\ttotal: 941ms\tremaining: 7.36s\n",
      "34:\tlearn: 0.0100363\ttotal: 971ms\tremaining: 7.35s\n",
      "35:\tlearn: 0.0099687\ttotal: 1s\tremaining: 7.36s\n",
      "36:\tlearn: 0.0098739\ttotal: 1.02s\tremaining: 7.24s\n",
      "37:\tlearn: 0.0098486\ttotal: 1.05s\tremaining: 7.23s\n",
      "38:\tlearn: 0.0097910\ttotal: 1.08s\tremaining: 7.23s\n",
      "39:\tlearn: 0.0096577\ttotal: 1.11s\tremaining: 7.22s\n",
      "40:\tlearn: 0.0095426\ttotal: 1.13s\tremaining: 7.13s\n",
      "41:\tlearn: 0.0094708\ttotal: 1.14s\tremaining: 7.03s\n",
      "42:\tlearn: 0.0094455\ttotal: 1.16s\tremaining: 6.93s\n",
      "43:\tlearn: 0.0093817\ttotal: 1.17s\tremaining: 6.83s\n",
      "44:\tlearn: 0.0092963\ttotal: 1.19s\tremaining: 6.73s\n",
      "45:\tlearn: 0.0092354\ttotal: 1.22s\tremaining: 6.73s\n",
      "46:\tlearn: 0.0091236\ttotal: 1.24s\tremaining: 6.65s\n",
      "47:\tlearn: 0.0090376\ttotal: 1.25s\tremaining: 6.57s\n",
      "48:\tlearn: 0.0088182\ttotal: 1.28s\tremaining: 6.57s\n",
      "49:\tlearn: 0.0086956\ttotal: 1.3s\tremaining: 6.49s\n",
      "50:\tlearn: 0.0086045\ttotal: 1.31s\tremaining: 6.41s\n",
      "51:\tlearn: 0.0084660\ttotal: 1.33s\tremaining: 6.33s\n",
      "52:\tlearn: 0.0082659\ttotal: 1.34s\tremaining: 6.26s\n",
      "53:\tlearn: 0.0082338\ttotal: 1.36s\tremaining: 6.19s\n",
      "54:\tlearn: 0.0081876\ttotal: 1.37s\tremaining: 6.12s\n",
      "55:\tlearn: 0.0081056\ttotal: 1.41s\tremaining: 6.12s\n",
      "56:\tlearn: 0.0079906\ttotal: 1.44s\tremaining: 6.12s\n",
      "57:\tlearn: 0.0079024\ttotal: 1.47s\tremaining: 6.12s\n",
      "58:\tlearn: 0.0078467\ttotal: 1.48s\tremaining: 6.05s\n",
      "59:\tlearn: 0.0077781\ttotal: 1.5s\tremaining: 5.98s\n",
      "60:\tlearn: 0.0077123\ttotal: 1.54s\tremaining: 6.05s\n",
      "61:\tlearn: 0.0076665\ttotal: 1.57s\tremaining: 6.04s\n",
      "62:\tlearn: 0.0076167\ttotal: 1.59s\tremaining: 5.99s\n",
      "63:\tlearn: 0.0075279\ttotal: 1.61s\tremaining: 5.92s\n",
      "64:\tlearn: 0.0074471\ttotal: 1.62s\tremaining: 5.86s\n",
      "65:\tlearn: 0.0073357\ttotal: 1.64s\tremaining: 5.8s\n",
      "66:\tlearn: 0.0072591\ttotal: 1.65s\tremaining: 5.75s\n",
      "67:\tlearn: 0.0071779\ttotal: 1.67s\tremaining: 5.69s\n",
      "68:\tlearn: 0.0071273\ttotal: 1.68s\tremaining: 5.63s\n",
      "69:\tlearn: 0.0070006\ttotal: 1.7s\tremaining: 5.58s\n",
      "70:\tlearn: 0.0069099\ttotal: 1.71s\tremaining: 5.53s\n",
      "71:\tlearn: 0.0067892\ttotal: 1.75s\tremaining: 5.53s\n",
      "72:\tlearn: 0.0067349\ttotal: 1.76s\tremaining: 5.48s\n",
      "73:\tlearn: 0.0066619\ttotal: 1.78s\tremaining: 5.43s\n",
      "74:\tlearn: 0.0066056\ttotal: 1.79s\tremaining: 5.38s\n",
      "75:\tlearn: 0.0065365\ttotal: 1.81s\tremaining: 5.33s\n",
      "76:\tlearn: 0.0064426\ttotal: 1.84s\tremaining: 5.32s\n",
      "77:\tlearn: 0.0063753\ttotal: 1.85s\tremaining: 5.28s\n",
      "78:\tlearn: 0.0063148\ttotal: 1.87s\tremaining: 5.23s\n",
      "79:\tlearn: 0.0062833\ttotal: 1.9s\tremaining: 5.22s\n",
      "80:\tlearn: 0.0062289\ttotal: 1.93s\tremaining: 5.22s\n",
      "81:\tlearn: 0.0061679\ttotal: 1.95s\tremaining: 5.18s\n",
      "82:\tlearn: 0.0061153\ttotal: 1.98s\tremaining: 5.17s\n",
      "83:\tlearn: 0.0060631\ttotal: 2s\tremaining: 5.13s\n",
      "84:\tlearn: 0.0060293\ttotal: 2.01s\tremaining: 5.09s\n",
      "85:\tlearn: 0.0059829\ttotal: 2.04s\tremaining: 5.08s\n",
      "86:\tlearn: 0.0059035\ttotal: 2.06s\tremaining: 5.04s\n",
      "87:\tlearn: 0.0058817\ttotal: 2.1s\tremaining: 5.07s\n",
      "88:\tlearn: 0.0058123\ttotal: 2.12s\tremaining: 5.03s\n",
      "89:\tlearn: 0.0057543\ttotal: 2.13s\tremaining: 4.98s\n",
      "90:\tlearn: 0.0056550\ttotal: 2.15s\tremaining: 4.93s\n",
      "91:\tlearn: 0.0055917\ttotal: 2.16s\tremaining: 4.89s\n",
      "92:\tlearn: 0.0055419\ttotal: 2.2s\tremaining: 4.89s\n",
      "93:\tlearn: 0.0054813\ttotal: 2.21s\tremaining: 4.85s\n",
      "94:\tlearn: 0.0054167\ttotal: 2.24s\tremaining: 4.84s\n",
      "95:\tlearn: 0.0053532\ttotal: 2.26s\tremaining: 4.8s\n",
      "96:\tlearn: 0.0053094\ttotal: 2.27s\tremaining: 4.76s\n",
      "97:\tlearn: 0.0052694\ttotal: 2.29s\tremaining: 4.72s\n",
      "98:\tlearn: 0.0052377\ttotal: 2.31s\tremaining: 4.68s\n",
      "99:\tlearn: 0.0051681\ttotal: 2.34s\tremaining: 4.67s\n",
      "100:\tlearn: 0.0050518\ttotal: 2.35s\tremaining: 4.63s\n",
      "101:\tlearn: 0.0049750\ttotal: 2.38s\tremaining: 4.63s\n",
      "102:\tlearn: 0.0049218\ttotal: 2.41s\tremaining: 4.62s\n",
      "103:\tlearn: 0.0048646\ttotal: 2.45s\tremaining: 4.61s\n",
      "104:\tlearn: 0.0047988\ttotal: 2.46s\tremaining: 4.57s\n",
      "105:\tlearn: 0.0047544\ttotal: 2.47s\tremaining: 4.53s\n",
      "106:\tlearn: 0.0047068\ttotal: 2.49s\tremaining: 4.49s\n",
      "107:\tlearn: 0.0046378\ttotal: 2.52s\tremaining: 4.48s\n",
      "108:\tlearn: 0.0045727\ttotal: 2.55s\tremaining: 4.47s\n",
      "109:\tlearn: 0.0045369\ttotal: 2.57s\tremaining: 4.44s\n",
      "110:\tlearn: 0.0044792\ttotal: 2.58s\tremaining: 4.4s\n",
      "111:\tlearn: 0.0044207\ttotal: 2.6s\tremaining: 4.36s\n",
      "112:\tlearn: 0.0043891\ttotal: 2.61s\tremaining: 4.33s\n",
      "113:\tlearn: 0.0043183\ttotal: 2.63s\tremaining: 4.29s\n",
      "114:\tlearn: 0.0042674\ttotal: 2.65s\tremaining: 4.25s\n",
      "115:\tlearn: 0.0042049\ttotal: 2.68s\tremaining: 4.25s\n",
      "116:\tlearn: 0.0041660\ttotal: 2.71s\tremaining: 4.23s\n",
      "117:\tlearn: 0.0041128\ttotal: 2.72s\tremaining: 4.2s\n",
      "118:\tlearn: 0.0040792\ttotal: 2.74s\tremaining: 4.17s\n",
      "119:\tlearn: 0.0040049\ttotal: 2.77s\tremaining: 4.16s\n",
      "120:\tlearn: 0.0039640\ttotal: 2.78s\tremaining: 4.12s\n",
      "121:\tlearn: 0.0039138\ttotal: 2.81s\tremaining: 4.11s\n",
      "122:\tlearn: 0.0038904\ttotal: 2.83s\tremaining: 4.08s\n",
      "123:\tlearn: 0.0038297\ttotal: 2.85s\tremaining: 4.04s\n",
      "124:\tlearn: 0.0037712\ttotal: 2.86s\tremaining: 4.01s\n",
      "125:\tlearn: 0.0037214\ttotal: 2.89s\tremaining: 4s\n",
      "126:\tlearn: 0.0036932\ttotal: 2.91s\tremaining: 3.96s\n",
      "127:\tlearn: 0.0036436\ttotal: 2.92s\tremaining: 3.93s\n",
      "128:\tlearn: 0.0036054\ttotal: 2.94s\tremaining: 3.9s\n",
      "129:\tlearn: 0.0035479\ttotal: 2.97s\tremaining: 3.88s\n",
      "130:\tlearn: 0.0034970\ttotal: 2.99s\tremaining: 3.85s\n",
      "131:\tlearn: 0.0034656\ttotal: 3s\tremaining: 3.82s\n",
      "132:\tlearn: 0.0034296\ttotal: 3.03s\tremaining: 3.81s\n",
      "133:\tlearn: 0.0034000\ttotal: 3.06s\tremaining: 3.8s\n",
      "134:\tlearn: 0.0033663\ttotal: 3.08s\tremaining: 3.76s\n",
      "135:\tlearn: 0.0033319\ttotal: 3.11s\tremaining: 3.75s\n",
      "136:\tlearn: 0.0033098\ttotal: 3.13s\tremaining: 3.72s\n",
      "137:\tlearn: 0.0032613\ttotal: 3.16s\tremaining: 3.71s\n",
      "138:\tlearn: 0.0032065\ttotal: 3.17s\tremaining: 3.67s\n",
      "139:\tlearn: 0.0031801\ttotal: 3.19s\tremaining: 3.64s\n",
      "140:\tlearn: 0.0031462\ttotal: 3.2s\tremaining: 3.61s\n",
      "141:\tlearn: 0.0031113\ttotal: 3.23s\tremaining: 3.6s\n",
      "142:\tlearn: 0.0030920\ttotal: 3.25s\tremaining: 3.57s\n",
      "143:\tlearn: 0.0030160\ttotal: 3.27s\tremaining: 3.54s\n",
      "144:\tlearn: 0.0029828\ttotal: 3.28s\tremaining: 3.51s\n",
      "145:\tlearn: 0.0029539\ttotal: 3.29s\tremaining: 3.47s\n",
      "146:\tlearn: 0.0029136\ttotal: 3.31s\tremaining: 3.45s\n",
      "147:\tlearn: 0.0028887\ttotal: 3.34s\tremaining: 3.43s\n",
      "148:\tlearn: 0.0028428\ttotal: 3.37s\tremaining: 3.42s\n",
      "149:\tlearn: 0.0028052\ttotal: 3.39s\tremaining: 3.39s\n",
      "150:\tlearn: 0.0027492\ttotal: 3.41s\tremaining: 3.36s\n",
      "151:\tlearn: 0.0026961\ttotal: 3.42s\tremaining: 3.33s\n",
      "152:\tlearn: 0.0026294\ttotal: 3.44s\tremaining: 3.3s\n",
      "153:\tlearn: 0.0025923\ttotal: 3.45s\tremaining: 3.27s\n",
      "154:\tlearn: 0.0025639\ttotal: 3.5s\tremaining: 3.27s\n",
      "155:\tlearn: 0.0025164\ttotal: 3.53s\tremaining: 3.26s\n",
      "156:\tlearn: 0.0024801\ttotal: 3.54s\tremaining: 3.23s\n",
      "157:\tlearn: 0.0024575\ttotal: 3.56s\tremaining: 3.2s\n",
      "158:\tlearn: 0.0024238\ttotal: 3.59s\tremaining: 3.18s\n",
      "159:\tlearn: 0.0023725\ttotal: 3.62s\tremaining: 3.17s\n",
      "160:\tlearn: 0.0023460\ttotal: 3.64s\tremaining: 3.14s\n",
      "161:\tlearn: 0.0023027\ttotal: 3.65s\tremaining: 3.11s\n",
      "162:\tlearn: 0.0022770\ttotal: 3.67s\tremaining: 3.08s\n",
      "163:\tlearn: 0.0022334\ttotal: 3.69s\tremaining: 3.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164:\tlearn: 0.0021998\ttotal: 3.72s\tremaining: 3.04s\n",
      "165:\tlearn: 0.0021773\ttotal: 3.73s\tremaining: 3.01s\n",
      "166:\tlearn: 0.0021306\ttotal: 3.75s\tremaining: 2.98s\n",
      "167:\tlearn: 0.0021001\ttotal: 3.76s\tremaining: 2.96s\n",
      "168:\tlearn: 0.0020818\ttotal: 3.77s\tremaining: 2.93s\n",
      "169:\tlearn: 0.0020572\ttotal: 3.81s\tremaining: 2.91s\n",
      "170:\tlearn: 0.0020350\ttotal: 3.83s\tremaining: 2.88s\n",
      "171:\tlearn: 0.0020225\ttotal: 3.84s\tremaining: 2.86s\n",
      "172:\tlearn: 0.0019973\ttotal: 3.86s\tremaining: 2.83s\n",
      "173:\tlearn: 0.0019828\ttotal: 3.87s\tremaining: 2.8s\n",
      "174:\tlearn: 0.0019669\ttotal: 3.9s\tremaining: 2.79s\n",
      "175:\tlearn: 0.0019333\ttotal: 3.93s\tremaining: 2.77s\n",
      "176:\tlearn: 0.0019110\ttotal: 3.95s\tremaining: 2.75s\n",
      "177:\tlearn: 0.0018855\ttotal: 3.97s\tremaining: 2.72s\n",
      "178:\tlearn: 0.0018572\ttotal: 3.98s\tremaining: 2.69s\n",
      "179:\tlearn: 0.0018430\ttotal: 4s\tremaining: 2.66s\n",
      "180:\tlearn: 0.0018231\ttotal: 4.03s\tremaining: 2.65s\n",
      "181:\tlearn: 0.0018062\ttotal: 4.04s\tremaining: 2.62s\n",
      "182:\tlearn: 0.0017954\ttotal: 4.06s\tremaining: 2.59s\n",
      "183:\tlearn: 0.0017858\ttotal: 4.07s\tremaining: 2.57s\n",
      "184:\tlearn: 0.0017727\ttotal: 4.09s\tremaining: 2.54s\n",
      "185:\tlearn: 0.0017465\ttotal: 4.1s\tremaining: 2.52s\n",
      "186:\tlearn: 0.0017408\ttotal: 4.12s\tremaining: 2.49s\n",
      "187:\tlearn: 0.0017266\ttotal: 4.13s\tremaining: 2.46s\n",
      "188:\tlearn: 0.0017062\ttotal: 4.17s\tremaining: 2.45s\n",
      "189:\tlearn: 0.0016848\ttotal: 4.2s\tremaining: 2.43s\n",
      "190:\tlearn: 0.0016747\ttotal: 4.21s\tremaining: 2.4s\n",
      "191:\tlearn: 0.0016613\ttotal: 4.23s\tremaining: 2.38s\n",
      "192:\tlearn: 0.0016485\ttotal: 4.26s\tremaining: 2.36s\n",
      "193:\tlearn: 0.0016226\ttotal: 4.28s\tremaining: 2.34s\n",
      "194:\tlearn: 0.0016079\ttotal: 4.29s\tremaining: 2.31s\n",
      "195:\tlearn: 0.0015743\ttotal: 4.31s\tremaining: 2.29s\n",
      "196:\tlearn: 0.0015565\ttotal: 4.32s\tremaining: 2.26s\n",
      "197:\tlearn: 0.0015322\ttotal: 4.35s\tremaining: 2.24s\n",
      "198:\tlearn: 0.0015175\ttotal: 4.37s\tremaining: 2.22s\n",
      "199:\tlearn: 0.0015025\ttotal: 4.4s\tremaining: 2.2s\n",
      "200:\tlearn: 0.0014863\ttotal: 4.41s\tremaining: 2.17s\n",
      "201:\tlearn: 0.0014683\ttotal: 4.43s\tremaining: 2.15s\n",
      "202:\tlearn: 0.0014543\ttotal: 4.45s\tremaining: 2.12s\n",
      "203:\tlearn: 0.0014420\ttotal: 4.46s\tremaining: 2.1s\n",
      "204:\tlearn: 0.0014282\ttotal: 4.48s\tremaining: 2.08s\n",
      "205:\tlearn: 0.0014131\ttotal: 4.49s\tremaining: 2.05s\n",
      "206:\tlearn: 0.0014014\ttotal: 4.52s\tremaining: 2.03s\n",
      "207:\tlearn: 0.0013889\ttotal: 4.54s\tremaining: 2.01s\n",
      "208:\tlearn: 0.0013748\ttotal: 4.56s\tremaining: 1.98s\n",
      "209:\tlearn: 0.0013466\ttotal: 4.57s\tremaining: 1.96s\n",
      "210:\tlearn: 0.0013301\ttotal: 4.6s\tremaining: 1.94s\n",
      "211:\tlearn: 0.0013202\ttotal: 4.62s\tremaining: 1.92s\n",
      "212:\tlearn: 0.0013005\ttotal: 4.63s\tremaining: 1.89s\n",
      "213:\tlearn: 0.0012930\ttotal: 4.65s\tremaining: 1.87s\n",
      "214:\tlearn: 0.0012770\ttotal: 4.68s\tremaining: 1.85s\n",
      "215:\tlearn: 0.0012590\ttotal: 4.69s\tremaining: 1.82s\n",
      "216:\tlearn: 0.0012562\ttotal: 4.71s\tremaining: 1.8s\n",
      "217:\tlearn: 0.0012431\ttotal: 4.76s\tremaining: 1.79s\n",
      "218:\tlearn: 0.0012357\ttotal: 4.77s\tremaining: 1.76s\n",
      "219:\tlearn: 0.0012186\ttotal: 4.8s\tremaining: 1.75s\n",
      "220:\tlearn: 0.0012012\ttotal: 4.82s\tremaining: 1.72s\n",
      "221:\tlearn: 0.0011773\ttotal: 4.83s\tremaining: 1.7s\n",
      "222:\tlearn: 0.0011627\ttotal: 4.85s\tremaining: 1.67s\n",
      "223:\tlearn: 0.0011446\ttotal: 4.87s\tremaining: 1.65s\n",
      "224:\tlearn: 0.0011365\ttotal: 4.9s\tremaining: 1.63s\n",
      "225:\tlearn: 0.0011320\ttotal: 4.94s\tremaining: 1.62s\n",
      "226:\tlearn: 0.0011257\ttotal: 4.96s\tremaining: 1.59s\n",
      "227:\tlearn: 0.0011181\ttotal: 4.97s\tremaining: 1.57s\n",
      "228:\tlearn: 0.0011051\ttotal: 4.99s\tremaining: 1.55s\n",
      "229:\tlearn: 0.0010993\ttotal: 5.01s\tremaining: 1.52s\n",
      "230:\tlearn: 0.0010894\ttotal: 5.02s\tremaining: 1.5s\n",
      "231:\tlearn: 0.0010835\ttotal: 5.04s\tremaining: 1.48s\n",
      "232:\tlearn: 0.0010734\ttotal: 5.05s\tremaining: 1.45s\n",
      "233:\tlearn: 0.0010672\ttotal: 5.06s\tremaining: 1.43s\n",
      "234:\tlearn: 0.0010536\ttotal: 5.08s\tremaining: 1.41s\n",
      "235:\tlearn: 0.0010351\ttotal: 5.11s\tremaining: 1.39s\n",
      "236:\tlearn: 0.0010288\ttotal: 5.13s\tremaining: 1.36s\n",
      "237:\tlearn: 0.0010174\ttotal: 5.14s\tremaining: 1.34s\n",
      "238:\tlearn: 0.0010027\ttotal: 5.16s\tremaining: 1.32s\n",
      "239:\tlearn: 0.0009974\ttotal: 5.17s\tremaining: 1.29s\n",
      "240:\tlearn: 0.0009896\ttotal: 5.19s\tremaining: 1.27s\n",
      "241:\tlearn: 0.0009822\ttotal: 5.21s\tremaining: 1.25s\n",
      "242:\tlearn: 0.0009728\ttotal: 5.22s\tremaining: 1.23s\n",
      "243:\tlearn: 0.0009613\ttotal: 5.24s\tremaining: 1.2s\n",
      "244:\tlearn: 0.0009543\ttotal: 5.25s\tremaining: 1.18s\n",
      "245:\tlearn: 0.0009440\ttotal: 5.27s\tremaining: 1.16s\n",
      "246:\tlearn: 0.0009352\ttotal: 5.29s\tremaining: 1.13s\n",
      "247:\tlearn: 0.0009278\ttotal: 5.3s\tremaining: 1.11s\n",
      "248:\tlearn: 0.0009144\ttotal: 5.32s\tremaining: 1.09s\n",
      "249:\tlearn: 0.0009064\ttotal: 5.33s\tremaining: 1.07s\n",
      "250:\tlearn: 0.0008990\ttotal: 5.34s\tremaining: 1.04s\n",
      "251:\tlearn: 0.0008894\ttotal: 5.38s\tremaining: 1.02s\n",
      "252:\tlearn: 0.0008782\ttotal: 5.39s\tremaining: 1s\n",
      "253:\tlearn: 0.0008706\ttotal: 5.41s\tremaining: 980ms\n",
      "254:\tlearn: 0.0008641\ttotal: 5.44s\tremaining: 960ms\n",
      "255:\tlearn: 0.0008550\ttotal: 5.45s\tremaining: 938ms\n",
      "256:\tlearn: 0.0008417\ttotal: 5.47s\tremaining: 915ms\n",
      "257:\tlearn: 0.0008302\ttotal: 5.49s\tremaining: 893ms\n",
      "258:\tlearn: 0.0008251\ttotal: 5.5s\tremaining: 871ms\n",
      "259:\tlearn: 0.0008196\ttotal: 5.53s\tremaining: 851ms\n",
      "260:\tlearn: 0.0008148\ttotal: 5.55s\tremaining: 829ms\n",
      "261:\tlearn: 0.0008089\ttotal: 5.58s\tremaining: 809ms\n",
      "262:\tlearn: 0.0008018\ttotal: 5.59s\tremaining: 787ms\n",
      "263:\tlearn: 0.0007889\ttotal: 5.63s\tremaining: 767ms\n",
      "264:\tlearn: 0.0007817\ttotal: 5.64s\tremaining: 745ms\n",
      "265:\tlearn: 0.0007766\ttotal: 5.66s\tremaining: 723ms\n",
      "266:\tlearn: 0.0007654\ttotal: 5.69s\tremaining: 703ms\n",
      "267:\tlearn: 0.0007598\ttotal: 5.7s\tremaining: 681ms\n",
      "268:\tlearn: 0.0007545\ttotal: 5.72s\tremaining: 659ms\n",
      "269:\tlearn: 0.0007495\ttotal: 5.74s\tremaining: 637ms\n",
      "270:\tlearn: 0.0007432\ttotal: 5.75s\tremaining: 615ms\n",
      "271:\tlearn: 0.0007376\ttotal: 5.76s\tremaining: 593ms\n",
      "272:\tlearn: 0.0007248\ttotal: 5.8s\tremaining: 573ms\n",
      "273:\tlearn: 0.0007218\ttotal: 5.81s\tremaining: 552ms\n",
      "274:\tlearn: 0.0007173\ttotal: 5.84s\tremaining: 531ms\n",
      "275:\tlearn: 0.0007142\ttotal: 5.86s\tremaining: 509ms\n",
      "276:\tlearn: 0.0007087\ttotal: 5.87s\tremaining: 488ms\n",
      "277:\tlearn: 0.0007048\ttotal: 5.89s\tremaining: 466ms\n",
      "278:\tlearn: 0.0007009\ttotal: 5.91s\tremaining: 444ms\n",
      "279:\tlearn: 0.0006988\ttotal: 5.92s\tremaining: 423ms\n",
      "280:\tlearn: 0.0006936\ttotal: 5.94s\tremaining: 401ms\n",
      "281:\tlearn: 0.0006898\ttotal: 5.95s\tremaining: 380ms\n",
      "282:\tlearn: 0.0006852\ttotal: 5.98s\tremaining: 359ms\n",
      "283:\tlearn: 0.0006826\ttotal: 6s\tremaining: 338ms\n",
      "284:\tlearn: 0.0006792\ttotal: 6.01s\tremaining: 317ms\n",
      "285:\tlearn: 0.0006769\ttotal: 6.03s\tremaining: 295ms\n",
      "286:\tlearn: 0.0006733\ttotal: 6.06s\tremaining: 274ms\n",
      "287:\tlearn: 0.0006686\ttotal: 6.07s\tremaining: 253ms\n",
      "288:\tlearn: 0.0006667\ttotal: 6.11s\tremaining: 232ms\n",
      "289:\tlearn: 0.0006542\ttotal: 6.12s\tremaining: 211ms\n",
      "290:\tlearn: 0.0006508\ttotal: 6.14s\tremaining: 190ms\n",
      "291:\tlearn: 0.0006440\ttotal: 6.15s\tremaining: 169ms\n",
      "292:\tlearn: 0.0006382\ttotal: 6.17s\tremaining: 147ms\n",
      "293:\tlearn: 0.0006347\ttotal: 6.18s\tremaining: 126ms\n",
      "294:\tlearn: 0.0006335\ttotal: 6.2s\tremaining: 105ms\n",
      "295:\tlearn: 0.0006307\ttotal: 6.21s\tremaining: 84ms\n",
      "296:\tlearn: 0.0006299\ttotal: 6.25s\tremaining: 63.1ms\n",
      "297:\tlearn: 0.0006244\ttotal: 6.28s\tremaining: 42.1ms\n",
      "298:\tlearn: 0.0006209\ttotal: 6.29s\tremaining: 21ms\n",
      "299:\tlearn: 0.0006160\ttotal: 6.31s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4f2aca56d248ccb3ab9df1f2a2b2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.048072092554542235, Recall = 0.0, Aging Rate = 0.00025453590944885023, Precision = 0.0, f1 = 0\n",
      "Epoch 2: Train Loss = 0.026509255478812197, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.026074112720549818, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.025882904874943834, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.025312221079349103, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.025024240382199955, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.02487055153453901, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.024713346811578454, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.024484671264369068, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.02435644420053775, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.024153169890167588, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.023720093115660378, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.02385789670743434, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.02369727589587029, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.023485280262038874, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.023205873727651812, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.023057481441265897, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.022508264175518503, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.022746674339284723, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.022425519921015824, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 18: Train Loss = 0.022217176988979907, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 19: Train Loss = 0.022020660953293605, Recall = 0.008888888888888889, Aging Rate = 1.590849434055314e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 20: Train Loss = 0.021678485336867188, Recall = 0.008888888888888889, Aging Rate = 1.590849434055314e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.021207207590684632, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 21: Train Loss = 0.021403238699868814, Recall = 0.008888888888888889, Aging Rate = 2.3862741510829707e-05, Precision = 0.6666666666666666, f1 = 0.01754385964912281\n",
      "Epoch 22: Train Loss = 0.02122116866774038, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 23: Train Loss = 0.020913001519057877, Recall = 0.022222222222222223, Aging Rate = 4.7725483021659414e-05, Precision = 0.8333333333333334, f1 = 0.043290043290043295\n",
      "Epoch 24: Train Loss = 0.020663266420317385, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 25: Train Loss = 0.02040240715202797, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Test Loss = 0.01991031441704702, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, precision = 0.8571428571428571\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.0201054829233991, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 27: Train Loss = 0.019917446180302115, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 28: Train Loss = 0.01965253424729853, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 29: Train Loss = 0.01939727132010808, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 30: Train Loss = 0.019281160797647367, Recall = 0.02666666666666667, Aging Rate = 4.7725483021659414e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.018877565983242778, Recall = 0.04, Aging Rate = 9.545096604331883e-05, precision = 0.75\n",
      "\n",
      "Epoch 31: Train Loss = 0.018956385405612706, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 32: Train Loss = 0.018741614034851112, Recall = 0.02666666666666667, Aging Rate = 6.363397736221256e-05, Precision = 0.75, f1 = 0.05150214592274679\n",
      "Epoch 33: Train Loss = 0.01860170985667025, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, Precision = 0.8571428571428571, f1 = 0.05172413793103449\n",
      "Epoch 34: Train Loss = 0.018338759980027947, Recall = 0.03111111111111111, Aging Rate = 6.363397736221256e-05, Precision = 0.875, f1 = 0.060085836909871244\n",
      "Epoch 35: Train Loss = 0.018213114323403133, Recall = 0.02666666666666667, Aging Rate = 6.363397736221256e-05, Precision = 0.75, f1 = 0.05150214592274679\n",
      "Test Loss = 0.017711674741558093, Recall = 0.02666666666666667, Aging Rate = 5.567973019193598e-05, precision = 0.8571428571428571\n",
      "\n",
      "Epoch 36: Train Loss = 0.017992618238118232, Recall = 0.04, Aging Rate = 8.749671887304226e-05, Precision = 0.8181818181818182, f1 = 0.07627118644067797\n",
      "Epoch 37: Train Loss = 0.01780539255884143, Recall = 0.035555555555555556, Aging Rate = 7.158822453248912e-05, Precision = 0.8888888888888888, f1 = 0.06837606837606838\n",
      "Epoch 38: Train Loss = 0.017643640123723797, Recall = 0.044444444444444446, Aging Rate = 9.545096604331883e-05, Precision = 0.8333333333333334, f1 = 0.08438818565400845\n",
      "Epoch 39: Train Loss = 0.01747440681681639, Recall = 0.02666666666666667, Aging Rate = 6.363397736221256e-05, Precision = 0.75, f1 = 0.05150214592274679\n",
      "Epoch 40: Train Loss = 0.01733563667798738, Recall = 0.035555555555555556, Aging Rate = 9.545096604331883e-05, Precision = 0.6666666666666666, f1 = 0.06751054852320675\n",
      "Test Loss = 0.01676124385101034, Recall = 0.02666666666666667, Aging Rate = 6.363397736221256e-05, precision = 0.75\n",
      "\n",
      "Epoch 41: Train Loss = 0.017141240876224047, Recall = 0.04, Aging Rate = 8.749671887304226e-05, Precision = 0.8181818181818182, f1 = 0.07627118644067797\n",
      "Epoch 42: Train Loss = 0.017001878061403228, Recall = 0.044444444444444446, Aging Rate = 9.545096604331883e-05, Precision = 0.8333333333333334, f1 = 0.08438818565400845\n",
      "Epoch 43: Train Loss = 0.016895104688110923, Recall = 0.04888888888888889, Aging Rate = 0.00011135946038387196, Precision = 0.7857142857142857, f1 = 0.09205020920502092\n",
      "Epoch 44: Train Loss = 0.016669657407784817, Recall = 0.06666666666666667, Aging Rate = 0.00014317644906497825, Precision = 0.8333333333333334, f1 = 0.1234567901234568\n",
      "Epoch 45: Train Loss = 0.016567503761110818, Recall = 0.06666666666666667, Aging Rate = 0.00014317644906497825, Precision = 0.8333333333333334, f1 = 0.1234567901234568\n",
      "Test Loss = 0.016097886579211507, Recall = 0.08888888888888889, Aging Rate = 0.00017499343774608452, precision = 0.9090909090909091\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.016494158402122847, Recall = 0.06666666666666667, Aging Rate = 0.00014317644906497825, Precision = 0.8333333333333334, f1 = 0.1234567901234568\n",
      "Epoch 47: Train Loss = 0.016374111139643644, Recall = 0.08, Aging Rate = 0.00016703919057580794, Precision = 0.8571428571428571, f1 = 0.14634146341463417\n",
      "Epoch 48: Train Loss = 0.01622109016997964, Recall = 0.08444444444444445, Aging Rate = 0.0001829476849163611, Precision = 0.8260869565217391, f1 = 0.1532258064516129\n",
      "Epoch 49: Train Loss = 0.016142809756722463, Recall = 0.07111111111111111, Aging Rate = 0.0001511306962352548, Precision = 0.8421052631578947, f1 = 0.13114754098360656\n",
      "Epoch 50: Train Loss = 0.016011033210096857, Recall = 0.10666666666666667, Aging Rate = 0.00022271892076774393, Precision = 0.8571428571428571, f1 = 0.18972332015810278\n",
      "Test Loss = 0.015741351104085553, Recall = 0.15555555555555556, Aging Rate = 0.00031816988681106277, precision = 0.875\n",
      "\n",
      "Epoch 51: Train Loss = 0.01579798192732751, Recall = 0.1111111111111111, Aging Rate = 0.0002306731679380205, Precision = 0.8620689655172413, f1 = 0.19685039370078738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.015781138946781362, Recall = 0.09333333333333334, Aging Rate = 0.00019885617925691424, Precision = 0.84, f1 = 0.16799999999999998\n",
      "Epoch 53: Train Loss = 0.015681522390257774, Recall = 0.1111111111111111, Aging Rate = 0.00022271892076774393, Precision = 0.8928571428571429, f1 = 0.1976284584980237\n",
      "Epoch 54: Train Loss = 0.015603284296353132, Recall = 0.09777777777777778, Aging Rate = 0.00019885617925691424, Precision = 0.88, f1 = 0.17600000000000002\n",
      "Epoch 55: Train Loss = 0.015487800870340638, Recall = 0.08888888888888889, Aging Rate = 0.00019090193208663766, Precision = 0.8333333333333334, f1 = 0.1606425702811245\n",
      "Test Loss = 0.015024424972469667, Recall = 0.15555555555555556, Aging Rate = 0.0003102156396407862, precision = 0.8974358974358975\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.015384356037051707, Recall = 0.12, Aging Rate = 0.0002306731679380205, Precision = 0.9310344827586207, f1 = 0.21259842519685038\n",
      "Epoch 57: Train Loss = 0.015311405540439534, Recall = 0.11555555555555555, Aging Rate = 0.00023862741510829706, Precision = 0.8666666666666667, f1 = 0.20392156862745098\n",
      "Epoch 58: Train Loss = 0.01520755916692326, Recall = 0.1288888888888889, Aging Rate = 0.00027044440378940334, Precision = 0.8529411764705882, f1 = 0.22393822393822393\n",
      "Epoch 59: Train Loss = 0.01512661336722496, Recall = 0.09777777777777778, Aging Rate = 0.0002068104264271908, Precision = 0.8461538461538461, f1 = 0.17529880478087653\n",
      "Epoch 60: Train Loss = 0.015007362602076456, Recall = 0.11555555555555555, Aging Rate = 0.0002306731679380205, Precision = 0.896551724137931, f1 = 0.20472440944881887\n",
      "Test Loss = 0.015290397449888492, Recall = 0.20444444444444446, Aging Rate = 0.0004693005830463176, precision = 0.7796610169491526\n",
      "\n",
      "Epoch 61: Train Loss = 0.014926553229596497, Recall = 0.1288888888888889, Aging Rate = 0.00027044440378940334, Precision = 0.8529411764705882, f1 = 0.22393822393822393\n",
      "Epoch 62: Train Loss = 0.014865517471731367, Recall = 0.1111111111111111, Aging Rate = 0.00022271892076774393, Precision = 0.8928571428571429, f1 = 0.1976284584980237\n",
      "Epoch 63: Train Loss = 0.014803939106953426, Recall = 0.1288888888888889, Aging Rate = 0.0002465816622785736, Precision = 0.9354838709677419, f1 = 0.22656249999999997\n",
      "Epoch 64: Train Loss = 0.014679688445459886, Recall = 0.1288888888888889, Aging Rate = 0.0002624901566191268, Precision = 0.8787878787878788, f1 = 0.22480620155038758\n",
      "Epoch 65: Train Loss = 0.014692887307832731, Recall = 0.14222222222222222, Aging Rate = 0.00029430714530023305, Precision = 0.8648648648648649, f1 = 0.24427480916030533\n",
      "Test Loss = 0.014195621429791477, Recall = 0.18222222222222223, Aging Rate = 0.0003658953698327222, precision = 0.8913043478260869\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.01453616270884849, Recall = 0.14222222222222222, Aging Rate = 0.00027839865095967994, Precision = 0.9142857142857143, f1 = 0.24615384615384617\n",
      "Epoch 67: Train Loss = 0.014518472876466338, Recall = 0.12444444444444444, Aging Rate = 0.00023862741510829706, Precision = 0.9333333333333333, f1 = 0.21960784313725493\n",
      "Epoch 68: Train Loss = 0.01440656207164596, Recall = 0.14666666666666667, Aging Rate = 0.00029430714530023305, Precision = 0.8918918918918919, f1 = 0.25190839694656486\n",
      "Epoch 69: Train Loss = 0.01433994787420331, Recall = 0.14666666666666667, Aging Rate = 0.00029430714530023305, Precision = 0.8918918918918919, f1 = 0.25190839694656486\n",
      "Epoch 70: Train Loss = 0.014281913208387852, Recall = 0.15555555555555556, Aging Rate = 0.0003102156396407862, Precision = 0.8974358974358975, f1 = 0.26515151515151514\n",
      "Test Loss = 0.013885548017558562, Recall = 0.10666666666666667, Aging Rate = 0.00019885617925691424, precision = 0.96\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.014283589382524997, Recall = 0.13777777777777778, Aging Rate = 0.00027839865095967994, Precision = 0.8857142857142857, f1 = 0.23846153846153845\n",
      "Epoch 72: Train Loss = 0.014186226908594132, Recall = 0.16444444444444445, Aging Rate = 0.0003340783811516159, Precision = 0.8809523809523809, f1 = 0.27715355805243447\n",
      "Epoch 73: Train Loss = 0.01411160355369476, Recall = 0.16, Aging Rate = 0.00031816988681106277, Precision = 0.9, f1 = 0.27169811320754716\n",
      "Epoch 74: Train Loss = 0.014080893745474046, Recall = 0.14222222222222222, Aging Rate = 0.00027839865095967994, Precision = 0.9142857142857143, f1 = 0.24615384615384617\n",
      "Epoch 75: Train Loss = 0.014005384688532909, Recall = 0.14666666666666667, Aging Rate = 0.0003022613924705096, Precision = 0.868421052631579, f1 = 0.2509505703422053\n",
      "Test Loss = 0.013597209530267327, Recall = 0.14222222222222222, Aging Rate = 0.0002624901566191268, precision = 0.9696969696969697\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.013955295316013345, Recall = 0.16444444444444445, Aging Rate = 0.0003102156396407862, Precision = 0.9487179487179487, f1 = 0.28030303030303033\n",
      "Epoch 77: Train Loss = 0.013889045195721685, Recall = 0.16, Aging Rate = 0.0003102156396407862, Precision = 0.9230769230769231, f1 = 0.27272727272727276\n",
      "Epoch 78: Train Loss = 0.013853665131543715, Recall = 0.1688888888888889, Aging Rate = 0.0003340783811516159, Precision = 0.9047619047619048, f1 = 0.2846441947565543\n",
      "Epoch 79: Train Loss = 0.013788796337900927, Recall = 0.16444444444444445, Aging Rate = 0.00034998687549216904, Precision = 0.8409090909090909, f1 = 0.275092936802974\n",
      "Epoch 80: Train Loss = 0.013753028855986567, Recall = 0.17777777777777778, Aging Rate = 0.0003818038641732753, Precision = 0.8333333333333334, f1 = 0.2930402930402931\n",
      "Test Loss = 0.013315233548244912, Recall = 0.15555555555555556, Aging Rate = 0.0003102156396407862, precision = 0.8974358974358975\n",
      "\n",
      "Epoch 81: Train Loss = 0.013736090640013986, Recall = 0.1688888888888889, Aging Rate = 0.0003261241339813393, Precision = 0.926829268292683, f1 = 0.2857142857142857\n",
      "Epoch 82: Train Loss = 0.013648686067619844, Recall = 0.16444444444444445, Aging Rate = 0.0003340783811516159, Precision = 0.8809523809523809, f1 = 0.27715355805243447\n",
      "Epoch 83: Train Loss = 0.013599428126120814, Recall = 0.1511111111111111, Aging Rate = 0.0002863528981299565, Precision = 0.9444444444444444, f1 = 0.26053639846743293\n",
      "Epoch 84: Train Loss = 0.013541627535207386, Recall = 0.1688888888888889, Aging Rate = 0.00034998687549216904, Precision = 0.8636363636363636, f1 = 0.28252788104089216\n",
      "Epoch 85: Train Loss = 0.013504195037339323, Recall = 0.1511111111111111, Aging Rate = 0.0003022613924705096, Precision = 0.8947368421052632, f1 = 0.2585551330798479\n",
      "Test Loss = 0.013157818390972915, Recall = 0.21333333333333335, Aging Rate = 0.00044543784153548786, precision = 0.8571428571428571\n",
      "\n",
      "Epoch 86: Train Loss = 0.013382307895974896, Recall = 0.18222222222222223, Aging Rate = 0.0003658953698327222, Precision = 0.8913043478260869, f1 = 0.30258302583025826\n",
      "Epoch 87: Train Loss = 0.01342211943881211, Recall = 0.16444444444444445, Aging Rate = 0.0003420326283218925, Precision = 0.8604651162790697, f1 = 0.27611940298507465\n",
      "Epoch 88: Train Loss = 0.013357256264959965, Recall = 0.16444444444444445, Aging Rate = 0.0003261241339813393, Precision = 0.9024390243902439, f1 = 0.2781954887218045\n",
      "Epoch 89: Train Loss = 0.013319053802732543, Recall = 0.17333333333333334, Aging Rate = 0.0003579411226624456, Precision = 0.8666666666666667, f1 = 0.2888888888888889\n",
      "Epoch 90: Train Loss = 0.013272932230822557, Recall = 0.17777777777777778, Aging Rate = 0.0003579411226624456, Precision = 0.8888888888888888, f1 = 0.2962962962962963\n",
      "Test Loss = 0.012758191226002535, Recall = 0.17777777777777778, Aging Rate = 0.0003658953698327222, precision = 0.8695652173913043\n",
      "\n",
      "Epoch 91: Train Loss = 0.013230712593089828, Recall = 0.17777777777777778, Aging Rate = 0.0003658953698327222, Precision = 0.8695652173913043, f1 = 0.2952029520295203\n",
      "Epoch 92: Train Loss = 0.013161199807656456, Recall = 0.15555555555555556, Aging Rate = 0.00031816988681106277, Precision = 0.875, f1 = 0.26415094339622647\n",
      "Epoch 93: Train Loss = 0.013150317579644905, Recall = 0.17777777777777778, Aging Rate = 0.00034998687549216904, Precision = 0.9090909090909091, f1 = 0.2973977695167287\n",
      "Epoch 94: Train Loss = 0.013043128503232162, Recall = 0.1511111111111111, Aging Rate = 0.0003102156396407862, Precision = 0.8717948717948718, f1 = 0.25757575757575757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train Loss = 0.0130325292886093, Recall = 0.2, Aging Rate = 0.00044543784153548786, Precision = 0.8035714285714286, f1 = 0.3202846975088968\n",
      "Test Loss = 0.012556866623902258, Recall = 0.19111111111111112, Aging Rate = 0.0003579411226624456, precision = 0.9555555555555556\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.01301221681580985, Recall = 0.1688888888888889, Aging Rate = 0.0003340783811516159, Precision = 0.9047619047619048, f1 = 0.2846441947565543\n",
      "Epoch 97: Train Loss = 0.01295042423931325, Recall = 0.1688888888888889, Aging Rate = 0.0003261241339813393, Precision = 0.926829268292683, f1 = 0.2857142857142857\n",
      "Epoch 98: Train Loss = 0.012939322548015265, Recall = 0.20444444444444446, Aging Rate = 0.0004136208528543816, Precision = 0.8846153846153846, f1 = 0.33212996389891697\n",
      "Epoch 99: Train Loss = 0.01282267347941446, Recall = 0.19111111111111112, Aging Rate = 0.00040566660568410503, Precision = 0.8431372549019608, f1 = 0.3115942028985507\n",
      "Epoch 100: Train Loss = 0.012816827620853751, Recall = 0.19555555555555557, Aging Rate = 0.00038975811134355187, Precision = 0.8979591836734694, f1 = 0.32116788321167883\n",
      "Test Loss = 0.012390187933744719, Recall = 0.2222222222222222, Aging Rate = 0.0004693005830463176, precision = 0.847457627118644\n",
      "\n",
      "Epoch 101: Train Loss = 0.012769860215541582, Recall = 0.2, Aging Rate = 0.0003977123585138285, Precision = 0.9, f1 = 0.32727272727272727\n",
      "Epoch 102: Train Loss = 0.012728338191608317, Recall = 0.17333333333333334, Aging Rate = 0.00034998687549216904, Precision = 0.8863636363636364, f1 = 0.2899628252788104\n",
      "Epoch 103: Train Loss = 0.012729291480625094, Recall = 0.17777777777777778, Aging Rate = 0.00038975811134355187, Precision = 0.8163265306122449, f1 = 0.291970802919708\n",
      "Epoch 104: Train Loss = 0.012626945092723971, Recall = 0.17333333333333334, Aging Rate = 0.0003420326283218925, Precision = 0.9069767441860465, f1 = 0.291044776119403\n",
      "Epoch 105: Train Loss = 0.012592870512794814, Recall = 0.19111111111111112, Aging Rate = 0.0003977123585138285, Precision = 0.86, f1 = 0.31272727272727274\n",
      "Test Loss = 0.012187592479096787, Recall = 0.15555555555555556, Aging Rate = 0.00029430714530023305, precision = 0.9459459459459459\n",
      "\n",
      "Epoch 106: Train Loss = 0.012555939454252266, Recall = 0.20444444444444446, Aging Rate = 0.0003977123585138285, Precision = 0.92, f1 = 0.33454545454545453\n",
      "Epoch 107: Train Loss = 0.012570521809538486, Recall = 0.18666666666666668, Aging Rate = 0.00037384961700299876, Precision = 0.8936170212765957, f1 = 0.3088235294117647\n",
      "Epoch 108: Train Loss = 0.0125029578569826, Recall = 0.17333333333333334, Aging Rate = 0.0003579411226624456, Precision = 0.8666666666666667, f1 = 0.2888888888888889\n",
      "Epoch 109: Train Loss = 0.012482700119953562, Recall = 0.19555555555555557, Aging Rate = 0.0003977123585138285, Precision = 0.88, f1 = 0.32\n",
      "Epoch 110: Train Loss = 0.012479547333860475, Recall = 0.18222222222222223, Aging Rate = 0.0003818038641732753, Precision = 0.8541666666666666, f1 = 0.3003663003663004\n",
      "Test Loss = 0.012025478551142001, Recall = 0.24, Aging Rate = 0.0005090718188977005, precision = 0.84375\n",
      "\n",
      "Epoch 111: Train Loss = 0.012329201376101866, Recall = 0.19111111111111112, Aging Rate = 0.0004136208528543816, Precision = 0.8269230769230769, f1 = 0.3104693140794224\n",
      "Epoch 112: Train Loss = 0.012406801736009753, Recall = 0.18222222222222223, Aging Rate = 0.00038975811134355187, Precision = 0.8367346938775511, f1 = 0.2992700729927007\n",
      "Epoch 113: Train Loss = 0.012347973218499684, Recall = 0.19555555555555557, Aging Rate = 0.00040566660568410503, Precision = 0.8627450980392157, f1 = 0.31884057971014496\n",
      "Epoch 114: Train Loss = 0.012351927026655036, Recall = 0.18222222222222223, Aging Rate = 0.0003977123585138285, Precision = 0.82, f1 = 0.29818181818181816\n",
      "Epoch 115: Train Loss = 0.012324509766448982, Recall = 0.21777777777777776, Aging Rate = 0.0004374835943652113, Precision = 0.8909090909090909, f1 = 0.35\n",
      "Test Loss = 0.011796947720933244, Recall = 0.21333333333333335, Aging Rate = 0.00040566660568410503, precision = 0.9411764705882353\n",
      "\n",
      "Epoch 116: Train Loss = 0.012249411754870878, Recall = 0.18222222222222223, Aging Rate = 0.0003658953698327222, Precision = 0.8913043478260869, f1 = 0.30258302583025826\n",
      "Epoch 117: Train Loss = 0.012243621822438107, Recall = 0.2, Aging Rate = 0.00042157510002465814, Precision = 0.8490566037735849, f1 = 0.32374100719424465\n",
      "Epoch 118: Train Loss = 0.012258493371752268, Recall = 0.2088888888888889, Aging Rate = 0.00042952934719493475, Precision = 0.8703703703703703, f1 = 0.3369175627240143\n",
      "Epoch 119: Train Loss = 0.012076606929781044, Recall = 0.2088888888888889, Aging Rate = 0.0004136208528543816, Precision = 0.9038461538461539, f1 = 0.3393501805054151\n",
      "Epoch 120: Train Loss = 0.012167618339564683, Recall = 0.18666666666666668, Aging Rate = 0.0003818038641732753, Precision = 0.875, f1 = 0.30769230769230765\n",
      "Test Loss = 0.011665156345747328, Recall = 0.2222222222222222, Aging Rate = 0.0004136208528543816, precision = 0.9615384615384616\n",
      "Model in epoch 120 is saved.\n",
      "\n",
      "Epoch 121: Train Loss = 0.012111498367239794, Recall = 0.1688888888888889, Aging Rate = 0.0003420326283218925, Precision = 0.8837209302325582, f1 = 0.2835820895522388\n",
      "Epoch 122: Train Loss = 0.012124817870352559, Recall = 0.18666666666666668, Aging Rate = 0.00038975811134355187, Precision = 0.8571428571428571, f1 = 0.30656934306569344\n",
      "Epoch 123: Train Loss = 0.012095345369550667, Recall = 0.21777777777777776, Aging Rate = 0.0004374835943652113, Precision = 0.8909090909090909, f1 = 0.35\n",
      "Epoch 124: Train Loss = 0.01208399256054118, Recall = 0.18222222222222223, Aging Rate = 0.0003818038641732753, Precision = 0.8541666666666666, f1 = 0.3003663003663004\n",
      "Epoch 125: Train Loss = 0.012062098165502622, Recall = 0.20444444444444446, Aging Rate = 0.0004374835943652113, Precision = 0.8363636363636363, f1 = 0.32857142857142857\n",
      "Test Loss = 0.011522033905975374, Recall = 0.19111111111111112, Aging Rate = 0.0003579411226624456, precision = 0.9555555555555556\n",
      "\n",
      "Epoch 126: Train Loss = 0.011987175659378901, Recall = 0.21333333333333335, Aging Rate = 0.00042952934719493475, Precision = 0.8888888888888888, f1 = 0.3440860215053763\n",
      "Epoch 127: Train Loss = 0.01198194751672352, Recall = 0.2, Aging Rate = 0.00040566660568410503, Precision = 0.8823529411764706, f1 = 0.3260869565217392\n",
      "Epoch 128: Train Loss = 0.011968796340908005, Recall = 0.2, Aging Rate = 0.0004136208528543816, Precision = 0.8653846153846154, f1 = 0.32490974729241884\n",
      "Epoch 129: Train Loss = 0.011924368496525542, Recall = 0.19111111111111112, Aging Rate = 0.0004136208528543816, Precision = 0.8269230769230769, f1 = 0.3104693140794224\n",
      "Epoch 130: Train Loss = 0.0118611561843254, Recall = 0.21777777777777776, Aging Rate = 0.00047725483021659413, Precision = 0.8166666666666667, f1 = 0.343859649122807\n",
      "Test Loss = 0.011418746987864315, Recall = 0.23555555555555555, Aging Rate = 0.00047725483021659413, precision = 0.8833333333333333\n",
      "\n",
      "Epoch 131: Train Loss = 0.011871344454736428, Recall = 0.2088888888888889, Aging Rate = 0.00042157510002465814, Precision = 0.8867924528301887, f1 = 0.33812949640287776\n",
      "Epoch 132: Train Loss = 0.011824728574774868, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 133: Train Loss = 0.011886726085164293, Recall = 0.20444444444444446, Aging Rate = 0.00042157510002465814, Precision = 0.8679245283018868, f1 = 0.3309352517985612\n",
      "Epoch 134: Train Loss = 0.011808920814015302, Recall = 0.2088888888888889, Aging Rate = 0.00040566660568410503, Precision = 0.9215686274509803, f1 = 0.3405797101449275\n",
      "Epoch 135: Train Loss = 0.011797440535608451, Recall = 0.21333333333333335, Aging Rate = 0.00045339208870576447, Precision = 0.8421052631578947, f1 = 0.34042553191489366\n",
      "Test Loss = 0.011294505791762706, Recall = 0.24444444444444444, Aging Rate = 0.0004931633245571472, precision = 0.8870967741935484\n",
      "\n",
      "Epoch 136: Train Loss = 0.011779270572449466, Recall = 0.2088888888888889, Aging Rate = 0.0004136208528543816, Precision = 0.9038461538461539, f1 = 0.3393501805054151\n",
      "Epoch 137: Train Loss = 0.011768125531069287, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138: Train Loss = 0.011705110164604437, Recall = 0.21333333333333335, Aging Rate = 0.00042952934719493475, Precision = 0.8888888888888888, f1 = 0.3440860215053763\n",
      "Epoch 139: Train Loss = 0.011763558471635606, Recall = 0.21333333333333335, Aging Rate = 0.0004374835943652113, Precision = 0.8727272727272727, f1 = 0.3428571428571429\n",
      "Epoch 140: Train Loss = 0.011669582209178313, Recall = 0.22666666666666666, Aging Rate = 0.00044543784153548786, Precision = 0.9107142857142857, f1 = 0.3629893238434163\n",
      "Test Loss = 0.011292481863388174, Recall = 0.20444444444444446, Aging Rate = 0.0004136208528543816, precision = 0.8846153846153846\n",
      "\n",
      "Epoch 141: Train Loss = 0.011710669862303184, Recall = 0.18666666666666668, Aging Rate = 0.0003977123585138285, Precision = 0.84, f1 = 0.3054545454545455\n",
      "Epoch 142: Train Loss = 0.011640258572384557, Recall = 0.2088888888888889, Aging Rate = 0.00044543784153548786, Precision = 0.8392857142857143, f1 = 0.33451957295373674\n",
      "Epoch 143: Train Loss = 0.011616395455734129, Recall = 0.21777777777777776, Aging Rate = 0.000461346335876041, Precision = 0.8448275862068966, f1 = 0.34628975265017664\n",
      "Epoch 144: Train Loss = 0.011566288471822563, Recall = 0.20444444444444446, Aging Rate = 0.000461346335876041, Precision = 0.7931034482758621, f1 = 0.3250883392226149\n",
      "Epoch 145: Train Loss = 0.011577318540339526, Recall = 0.2222222222222222, Aging Rate = 0.00045339208870576447, Precision = 0.8771929824561403, f1 = 0.3546099290780142\n",
      "Test Loss = 0.01116779366593755, Recall = 0.2, Aging Rate = 0.0004136208528543816, precision = 0.8653846153846154\n",
      "\n",
      "Epoch 146: Train Loss = 0.011555262184533477, Recall = 0.25333333333333335, Aging Rate = 0.0005249803132382536, Precision = 0.8636363636363636, f1 = 0.3917525773195877\n",
      "Epoch 147: Train Loss = 0.01149774703469587, Recall = 0.2, Aging Rate = 0.00042952934719493475, Precision = 0.8333333333333334, f1 = 0.3225806451612903\n",
      "Epoch 148: Train Loss = 0.011530343606157945, Recall = 0.2, Aging Rate = 0.00040566660568410503, Precision = 0.8823529411764706, f1 = 0.3260869565217392\n",
      "Epoch 149: Train Loss = 0.011517738942083711, Recall = 0.2222222222222222, Aging Rate = 0.000461346335876041, Precision = 0.8620689655172413, f1 = 0.35335689045936397\n",
      "Epoch 150: Train Loss = 0.011513944985327339, Recall = 0.23555555555555555, Aging Rate = 0.000461346335876041, Precision = 0.9137931034482759, f1 = 0.37455830388692574\n",
      "Test Loss = 0.01102233368084414, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, precision = 0.8545454545454545\n",
      "\n",
      "Epoch 151: Train Loss = 0.011550909448061163, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 152: Train Loss = 0.011448661537197447, Recall = 0.22666666666666666, Aging Rate = 0.00048520907738687074, Precision = 0.8360655737704918, f1 = 0.35664335664335667\n",
      "Epoch 153: Train Loss = 0.011483150423168727, Recall = 0.21777777777777776, Aging Rate = 0.00044543784153548786, Precision = 0.875, f1 = 0.34875444839857644\n",
      "Epoch 154: Train Loss = 0.011390668156411538, Recall = 0.20444444444444446, Aging Rate = 0.00042952934719493475, Precision = 0.8518518518518519, f1 = 0.32974910394265233\n",
      "Epoch 155: Train Loss = 0.011454784096426729, Recall = 0.21777777777777776, Aging Rate = 0.0004374835943652113, Precision = 0.8909090909090909, f1 = 0.35\n",
      "Test Loss = 0.010980751156281394, Recall = 0.21333333333333335, Aging Rate = 0.00040566660568410503, precision = 0.9411764705882353\n",
      "\n",
      "Epoch 156: Train Loss = 0.011407461313555679, Recall = 0.2311111111111111, Aging Rate = 0.00048520907738687074, Precision = 0.8524590163934426, f1 = 0.3636363636363636\n",
      "Epoch 157: Train Loss = 0.011375176177768866, Recall = 0.24, Aging Rate = 0.00048520907738687074, Precision = 0.8852459016393442, f1 = 0.37762237762237766\n",
      "Epoch 158: Train Loss = 0.011318326129654416, Recall = 0.23555555555555555, Aging Rate = 0.00048520907738687074, Precision = 0.8688524590163934, f1 = 0.37062937062937057\n",
      "Epoch 159: Train Loss = 0.011412809905976292, Recall = 0.21777777777777776, Aging Rate = 0.00044543784153548786, Precision = 0.875, f1 = 0.34875444839857644\n",
      "Epoch 160: Train Loss = 0.01139865509676632, Recall = 0.19111111111111112, Aging Rate = 0.0003818038641732753, Precision = 0.8958333333333334, f1 = 0.315018315018315\n",
      "Test Loss = 0.01109320937555466, Recall = 0.29777777777777775, Aging Rate = 0.000628385526451849, precision = 0.8481012658227848\n",
      "\n",
      "Epoch 161: Train Loss = 0.011346593841520763, Recall = 0.2222222222222222, Aging Rate = 0.000461346335876041, Precision = 0.8620689655172413, f1 = 0.35335689045936397\n",
      "Epoch 162: Train Loss = 0.011335611605027732, Recall = 0.22666666666666666, Aging Rate = 0.0005011175717274239, Precision = 0.8095238095238095, f1 = 0.3541666666666667\n",
      "Epoch 163: Train Loss = 0.011306368222266269, Recall = 0.21777777777777776, Aging Rate = 0.0004374835943652113, Precision = 0.8909090909090909, f1 = 0.35\n",
      "Epoch 164: Train Loss = 0.011312091617870905, Recall = 0.2311111111111111, Aging Rate = 0.00047725483021659413, Precision = 0.8666666666666667, f1 = 0.3649122807017544\n",
      "Epoch 165: Train Loss = 0.011240168083479362, Recall = 0.2222222222222222, Aging Rate = 0.000461346335876041, Precision = 0.8620689655172413, f1 = 0.35335689045936397\n",
      "Test Loss = 0.010879683311385072, Recall = 0.3333333333333333, Aging Rate = 0.0006999737509843381, precision = 0.8522727272727273\n",
      "\n",
      "Epoch 166: Train Loss = 0.0112552769152007, Recall = 0.22666666666666666, Aging Rate = 0.00047725483021659413, Precision = 0.85, f1 = 0.35789473684210527\n",
      "Epoch 167: Train Loss = 0.01130235647065803, Recall = 0.21333333333333335, Aging Rate = 0.00042952934719493475, Precision = 0.8888888888888888, f1 = 0.3440860215053763\n",
      "Epoch 168: Train Loss = 0.011182802190338972, Recall = 0.22666666666666666, Aging Rate = 0.000461346335876041, Precision = 0.8793103448275862, f1 = 0.3604240282685512\n",
      "Epoch 169: Train Loss = 0.011246123234484888, Recall = 0.2311111111111111, Aging Rate = 0.00047725483021659413, Precision = 0.8666666666666667, f1 = 0.3649122807017544\n",
      "Epoch 170: Train Loss = 0.01127812423209482, Recall = 0.21777777777777776, Aging Rate = 0.00045339208870576447, Precision = 0.8596491228070176, f1 = 0.34751773049645385\n",
      "Test Loss = 0.010755667719166292, Recall = 0.29333333333333333, Aging Rate = 0.0005965685377707427, precision = 0.88\n",
      "\n",
      "Training Finished at epoch 170.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2804185\ttotal: 39ms\tremaining: 11.7s\n",
      "1:\tlearn: 0.1236108\ttotal: 54.4ms\tremaining: 8.11s\n",
      "2:\tlearn: 0.0628516\ttotal: 86.5ms\tremaining: 8.56s\n",
      "3:\tlearn: 0.0374246\ttotal: 102ms\tremaining: 7.56s\n",
      "4:\tlearn: 0.0258539\ttotal: 117ms\tremaining: 6.91s\n",
      "5:\tlearn: 0.0199787\ttotal: 149ms\tremaining: 7.29s\n",
      "6:\tlearn: 0.0167509\ttotal: 165ms\tremaining: 6.9s\n",
      "7:\tlearn: 0.0149631\ttotal: 195ms\tremaining: 7.12s\n",
      "8:\tlearn: 0.0139457\ttotal: 209ms\tremaining: 6.77s\n",
      "9:\tlearn: 0.0130903\ttotal: 225ms\tremaining: 6.54s\n",
      "10:\tlearn: 0.0126339\ttotal: 239ms\tremaining: 6.28s\n",
      "11:\tlearn: 0.0123684\ttotal: 255ms\tremaining: 6.13s\n",
      "12:\tlearn: 0.0120632\ttotal: 288ms\tremaining: 6.35s\n",
      "13:\tlearn: 0.0117410\ttotal: 319ms\tremaining: 6.52s\n",
      "14:\tlearn: 0.0115875\ttotal: 348ms\tremaining: 6.62s\n",
      "15:\tlearn: 0.0114926\ttotal: 365ms\tremaining: 6.48s\n",
      "16:\tlearn: 0.0114390\ttotal: 397ms\tremaining: 6.6s\n",
      "17:\tlearn: 0.0113957\ttotal: 413ms\tremaining: 6.46s\n",
      "18:\tlearn: 0.0112900\ttotal: 428ms\tremaining: 6.33s\n",
      "19:\tlearn: 0.0111963\ttotal: 460ms\tremaining: 6.43s\n",
      "20:\tlearn: 0.0111492\ttotal: 475ms\tremaining: 6.31s\n",
      "21:\tlearn: 0.0110121\ttotal: 506ms\tremaining: 6.39s\n",
      "22:\tlearn: 0.0109451\ttotal: 538ms\tremaining: 6.49s\n",
      "23:\tlearn: 0.0108923\ttotal: 554ms\tremaining: 6.38s\n",
      "24:\tlearn: 0.0108096\ttotal: 570ms\tremaining: 6.27s\n",
      "25:\tlearn: 0.0106518\ttotal: 602ms\tremaining: 6.35s\n",
      "26:\tlearn: 0.0106033\ttotal: 617ms\tremaining: 6.23s\n",
      "27:\tlearn: 0.0105150\ttotal: 648ms\tremaining: 6.29s\n",
      "28:\tlearn: 0.0103717\ttotal: 663ms\tremaining: 6.2s\n",
      "29:\tlearn: 0.0103327\ttotal: 677ms\tremaining: 6.1s\n",
      "30:\tlearn: 0.0102631\ttotal: 693ms\tremaining: 6.01s\n",
      "31:\tlearn: 0.0102110\ttotal: 724ms\tremaining: 6.07s\n",
      "32:\tlearn: 0.0100214\ttotal: 739ms\tremaining: 5.98s\n",
      "33:\tlearn: 0.0099701\ttotal: 755ms\tremaining: 5.91s\n",
      "34:\tlearn: 0.0098469\ttotal: 787ms\tremaining: 5.96s\n",
      "35:\tlearn: 0.0097410\ttotal: 819ms\tremaining: 6.01s\n",
      "36:\tlearn: 0.0096953\ttotal: 867ms\tremaining: 6.16s\n",
      "37:\tlearn: 0.0096214\ttotal: 899ms\tremaining: 6.2s\n",
      "38:\tlearn: 0.0095734\ttotal: 930ms\tremaining: 6.23s\n",
      "39:\tlearn: 0.0095411\ttotal: 960ms\tremaining: 6.24s\n",
      "40:\tlearn: 0.0094084\ttotal: 992ms\tremaining: 6.27s\n",
      "41:\tlearn: 0.0093579\ttotal: 1.01s\tremaining: 6.19s\n",
      "42:\tlearn: 0.0092402\ttotal: 1.02s\tremaining: 6.11s\n",
      "43:\tlearn: 0.0091594\ttotal: 1.05s\tremaining: 6.13s\n",
      "44:\tlearn: 0.0090974\ttotal: 1.07s\tremaining: 6.07s\n",
      "45:\tlearn: 0.0090195\ttotal: 1.09s\tremaining: 6s\n",
      "46:\tlearn: 0.0089508\ttotal: 1.1s\tremaining: 5.94s\n",
      "47:\tlearn: 0.0088351\ttotal: 1.12s\tremaining: 5.87s\n",
      "48:\tlearn: 0.0087276\ttotal: 1.13s\tremaining: 5.81s\n",
      "49:\tlearn: 0.0086715\ttotal: 1.15s\tremaining: 5.75s\n",
      "50:\tlearn: 0.0085341\ttotal: 1.17s\tremaining: 5.69s\n",
      "51:\tlearn: 0.0084938\ttotal: 1.18s\tremaining: 5.63s\n",
      "52:\tlearn: 0.0084455\ttotal: 1.2s\tremaining: 5.58s\n",
      "53:\tlearn: 0.0083769\ttotal: 1.21s\tremaining: 5.52s\n",
      "54:\tlearn: 0.0082688\ttotal: 1.23s\tremaining: 5.47s\n",
      "55:\tlearn: 0.0082321\ttotal: 1.24s\tremaining: 5.41s\n",
      "56:\tlearn: 0.0080841\ttotal: 1.26s\tremaining: 5.36s\n",
      "57:\tlearn: 0.0080368\ttotal: 1.27s\tremaining: 5.31s\n",
      "58:\tlearn: 0.0079598\ttotal: 1.29s\tremaining: 5.26s\n",
      "59:\tlearn: 0.0078612\ttotal: 1.32s\tremaining: 5.28s\n",
      "60:\tlearn: 0.0077847\ttotal: 1.33s\tremaining: 5.23s\n",
      "61:\tlearn: 0.0077219\ttotal: 1.35s\tremaining: 5.19s\n",
      "62:\tlearn: 0.0076527\ttotal: 1.37s\tremaining: 5.14s\n",
      "63:\tlearn: 0.0075513\ttotal: 1.38s\tremaining: 5.1s\n",
      "64:\tlearn: 0.0074897\ttotal: 1.4s\tremaining: 5.05s\n",
      "65:\tlearn: 0.0074590\ttotal: 1.41s\tremaining: 5.01s\n",
      "66:\tlearn: 0.0073756\ttotal: 1.43s\tremaining: 4.97s\n",
      "67:\tlearn: 0.0072944\ttotal: 1.45s\tremaining: 4.93s\n",
      "68:\tlearn: 0.0072143\ttotal: 1.49s\tremaining: 5s\n",
      "69:\tlearn: 0.0071659\ttotal: 1.52s\tremaining: 5.01s\n",
      "70:\tlearn: 0.0071203\ttotal: 1.54s\tremaining: 4.97s\n",
      "71:\tlearn: 0.0070228\ttotal: 1.56s\tremaining: 4.93s\n",
      "72:\tlearn: 0.0069091\ttotal: 1.59s\tremaining: 4.94s\n",
      "73:\tlearn: 0.0068188\ttotal: 1.6s\tremaining: 4.9s\n",
      "74:\tlearn: 0.0067701\ttotal: 1.62s\tremaining: 4.86s\n",
      "75:\tlearn: 0.0067232\ttotal: 1.64s\tremaining: 4.82s\n",
      "76:\tlearn: 0.0066563\ttotal: 1.65s\tremaining: 4.78s\n",
      "77:\tlearn: 0.0065955\ttotal: 1.68s\tremaining: 4.79s\n",
      "78:\tlearn: 0.0065149\ttotal: 1.7s\tremaining: 4.75s\n",
      "79:\tlearn: 0.0064576\ttotal: 1.71s\tremaining: 4.71s\n",
      "80:\tlearn: 0.0063745\ttotal: 1.73s\tremaining: 4.68s\n",
      "81:\tlearn: 0.0063007\ttotal: 1.76s\tremaining: 4.68s\n",
      "82:\tlearn: 0.0062563\ttotal: 1.78s\tremaining: 4.65s\n",
      "83:\tlearn: 0.0061401\ttotal: 1.79s\tremaining: 4.61s\n",
      "84:\tlearn: 0.0060696\ttotal: 1.82s\tremaining: 4.62s\n",
      "85:\tlearn: 0.0059944\ttotal: 1.84s\tremaining: 4.58s\n",
      "86:\tlearn: 0.0059691\ttotal: 1.86s\tremaining: 4.54s\n",
      "87:\tlearn: 0.0058600\ttotal: 1.89s\tremaining: 4.55s\n",
      "88:\tlearn: 0.0057682\ttotal: 1.9s\tremaining: 4.51s\n",
      "89:\tlearn: 0.0057278\ttotal: 1.94s\tremaining: 4.52s\n",
      "90:\tlearn: 0.0056711\ttotal: 1.95s\tremaining: 4.48s\n",
      "91:\tlearn: 0.0056153\ttotal: 1.97s\tremaining: 4.45s\n",
      "92:\tlearn: 0.0055351\ttotal: 2s\tremaining: 4.45s\n",
      "93:\tlearn: 0.0054807\ttotal: 2.02s\tremaining: 4.42s\n",
      "94:\tlearn: 0.0054354\ttotal: 2.03s\tremaining: 4.38s\n",
      "95:\tlearn: 0.0053690\ttotal: 2.06s\tremaining: 4.38s\n",
      "96:\tlearn: 0.0053196\ttotal: 2.09s\tremaining: 4.38s\n",
      "97:\tlearn: 0.0052916\ttotal: 2.11s\tremaining: 4.34s\n",
      "98:\tlearn: 0.0052336\ttotal: 2.12s\tremaining: 4.31s\n",
      "99:\tlearn: 0.0051796\ttotal: 2.14s\tremaining: 4.28s\n",
      "100:\tlearn: 0.0051560\ttotal: 2.17s\tremaining: 4.28s\n",
      "101:\tlearn: 0.0051197\ttotal: 2.2s\tremaining: 4.27s\n",
      "102:\tlearn: 0.0050594\ttotal: 2.22s\tremaining: 4.24s\n",
      "103:\tlearn: 0.0049797\ttotal: 2.26s\tremaining: 4.27s\n",
      "104:\tlearn: 0.0049383\ttotal: 2.3s\tremaining: 4.26s\n",
      "105:\tlearn: 0.0048948\ttotal: 2.31s\tremaining: 4.23s\n",
      "106:\tlearn: 0.0048612\ttotal: 2.33s\tremaining: 4.2s\n",
      "107:\tlearn: 0.0048189\ttotal: 2.34s\tremaining: 4.17s\n",
      "108:\tlearn: 0.0047702\ttotal: 2.38s\tremaining: 4.16s\n",
      "109:\tlearn: 0.0047389\ttotal: 2.39s\tremaining: 4.13s\n",
      "110:\tlearn: 0.0047204\ttotal: 2.42s\tremaining: 4.12s\n",
      "111:\tlearn: 0.0046488\ttotal: 2.45s\tremaining: 4.12s\n",
      "112:\tlearn: 0.0045741\ttotal: 2.47s\tremaining: 4.09s\n",
      "113:\tlearn: 0.0045218\ttotal: 2.5s\tremaining: 4.08s\n",
      "114:\tlearn: 0.0044526\ttotal: 2.53s\tremaining: 4.08s\n",
      "115:\tlearn: 0.0044008\ttotal: 2.55s\tremaining: 4.04s\n",
      "116:\tlearn: 0.0043188\ttotal: 2.56s\tremaining: 4.01s\n",
      "117:\tlearn: 0.0042892\ttotal: 2.58s\tremaining: 3.98s\n",
      "118:\tlearn: 0.0042476\ttotal: 2.6s\tremaining: 3.95s\n",
      "119:\tlearn: 0.0041860\ttotal: 2.63s\tremaining: 3.94s\n",
      "120:\tlearn: 0.0041408\ttotal: 2.66s\tremaining: 3.93s\n",
      "121:\tlearn: 0.0040364\ttotal: 2.67s\tremaining: 3.9s\n",
      "122:\tlearn: 0.0040036\ttotal: 2.69s\tremaining: 3.87s\n",
      "123:\tlearn: 0.0039030\ttotal: 2.72s\tremaining: 3.86s\n",
      "124:\tlearn: 0.0038335\ttotal: 2.77s\tremaining: 3.87s\n",
      "125:\tlearn: 0.0037679\ttotal: 2.78s\tremaining: 3.84s\n",
      "126:\tlearn: 0.0037264\ttotal: 2.8s\tremaining: 3.81s\n",
      "127:\tlearn: 0.0036672\ttotal: 2.81s\tremaining: 3.78s\n",
      "128:\tlearn: 0.0036225\ttotal: 2.86s\tremaining: 3.79s\n",
      "129:\tlearn: 0.0036025\ttotal: 2.89s\tremaining: 3.78s\n",
      "130:\tlearn: 0.0035647\ttotal: 2.9s\tremaining: 3.75s\n",
      "131:\tlearn: 0.0035445\ttotal: 2.92s\tremaining: 3.72s\n",
      "132:\tlearn: 0.0035199\ttotal: 2.94s\tremaining: 3.69s\n",
      "133:\tlearn: 0.0034568\ttotal: 2.97s\tremaining: 3.68s\n",
      "134:\tlearn: 0.0033860\ttotal: 3s\tremaining: 3.67s\n",
      "135:\tlearn: 0.0033505\ttotal: 3.02s\tremaining: 3.64s\n",
      "136:\tlearn: 0.0032976\ttotal: 3.05s\tremaining: 3.63s\n",
      "137:\tlearn: 0.0032376\ttotal: 3.06s\tremaining: 3.6s\n",
      "138:\tlearn: 0.0032090\ttotal: 3.08s\tremaining: 3.57s\n",
      "139:\tlearn: 0.0031716\ttotal: 3.1s\tremaining: 3.54s\n",
      "140:\tlearn: 0.0031172\ttotal: 3.13s\tremaining: 3.53s\n",
      "141:\tlearn: 0.0030741\ttotal: 3.14s\tremaining: 3.5s\n",
      "142:\tlearn: 0.0030322\ttotal: 3.17s\tremaining: 3.48s\n",
      "143:\tlearn: 0.0029981\ttotal: 3.19s\tremaining: 3.46s\n",
      "144:\tlearn: 0.0028915\ttotal: 3.22s\tremaining: 3.44s\n",
      "145:\tlearn: 0.0028533\ttotal: 3.24s\tremaining: 3.42s\n",
      "146:\tlearn: 0.0028084\ttotal: 3.27s\tremaining: 3.4s\n",
      "147:\tlearn: 0.0027768\ttotal: 3.29s\tremaining: 3.37s\n",
      "148:\tlearn: 0.0027538\ttotal: 3.3s\tremaining: 3.35s\n",
      "149:\tlearn: 0.0027338\ttotal: 3.32s\tremaining: 3.32s\n",
      "150:\tlearn: 0.0027005\ttotal: 3.33s\tremaining: 3.29s\n",
      "151:\tlearn: 0.0026728\ttotal: 3.36s\tremaining: 3.27s\n",
      "152:\tlearn: 0.0026146\ttotal: 3.38s\tremaining: 3.25s\n",
      "153:\tlearn: 0.0025858\ttotal: 3.39s\tremaining: 3.22s\n",
      "154:\tlearn: 0.0025450\ttotal: 3.42s\tremaining: 3.2s\n",
      "155:\tlearn: 0.0025188\ttotal: 3.46s\tremaining: 3.19s\n",
      "156:\tlearn: 0.0024957\ttotal: 3.49s\tremaining: 3.18s\n",
      "157:\tlearn: 0.0024614\ttotal: 3.5s\tremaining: 3.15s\n",
      "158:\tlearn: 0.0024434\ttotal: 3.52s\tremaining: 3.12s\n",
      "159:\tlearn: 0.0024171\ttotal: 3.55s\tremaining: 3.11s\n",
      "160:\tlearn: 0.0023817\ttotal: 3.56s\tremaining: 3.08s\n",
      "161:\tlearn: 0.0023411\ttotal: 3.6s\tremaining: 3.06s\n",
      "162:\tlearn: 0.0023142\ttotal: 3.63s\tremaining: 3.05s\n",
      "163:\tlearn: 0.0022995\ttotal: 3.66s\tremaining: 3.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164:\tlearn: 0.0022812\ttotal: 3.69s\tremaining: 3.02s\n",
      "165:\tlearn: 0.0022546\ttotal: 3.71s\tremaining: 2.99s\n",
      "166:\tlearn: 0.0022149\ttotal: 3.72s\tremaining: 2.96s\n",
      "167:\tlearn: 0.0021843\ttotal: 3.74s\tremaining: 2.94s\n",
      "168:\tlearn: 0.0021588\ttotal: 3.77s\tremaining: 2.92s\n",
      "169:\tlearn: 0.0021218\ttotal: 3.8s\tremaining: 2.91s\n",
      "170:\tlearn: 0.0020989\ttotal: 3.83s\tremaining: 2.89s\n",
      "171:\tlearn: 0.0020784\ttotal: 3.85s\tremaining: 2.86s\n",
      "172:\tlearn: 0.0020254\ttotal: 3.86s\tremaining: 2.84s\n",
      "173:\tlearn: 0.0019874\ttotal: 3.9s\tremaining: 2.82s\n",
      "174:\tlearn: 0.0019564\ttotal: 3.91s\tremaining: 2.79s\n",
      "175:\tlearn: 0.0019260\ttotal: 3.93s\tremaining: 2.77s\n",
      "176:\tlearn: 0.0019028\ttotal: 3.96s\tremaining: 2.75s\n",
      "177:\tlearn: 0.0018756\ttotal: 3.98s\tremaining: 2.73s\n",
      "178:\tlearn: 0.0018539\ttotal: 3.99s\tremaining: 2.7s\n",
      "179:\tlearn: 0.0018411\ttotal: 4.01s\tremaining: 2.67s\n",
      "180:\tlearn: 0.0018158\ttotal: 4.02s\tremaining: 2.64s\n",
      "181:\tlearn: 0.0017998\ttotal: 4.04s\tremaining: 2.62s\n",
      "182:\tlearn: 0.0017862\ttotal: 4.07s\tremaining: 2.6s\n",
      "183:\tlearn: 0.0017737\ttotal: 4.09s\tremaining: 2.58s\n",
      "184:\tlearn: 0.0017540\ttotal: 4.12s\tremaining: 2.56s\n",
      "185:\tlearn: 0.0017406\ttotal: 4.15s\tremaining: 2.54s\n",
      "186:\tlearn: 0.0017182\ttotal: 4.16s\tremaining: 2.52s\n",
      "187:\tlearn: 0.0016965\ttotal: 4.18s\tremaining: 2.49s\n",
      "188:\tlearn: 0.0016775\ttotal: 4.21s\tremaining: 2.47s\n",
      "189:\tlearn: 0.0016652\ttotal: 4.24s\tremaining: 2.45s\n",
      "190:\tlearn: 0.0016350\ttotal: 4.27s\tremaining: 2.44s\n",
      "191:\tlearn: 0.0016221\ttotal: 4.3s\tremaining: 2.42s\n",
      "192:\tlearn: 0.0016028\ttotal: 4.32s\tremaining: 2.39s\n",
      "193:\tlearn: 0.0015943\ttotal: 4.33s\tremaining: 2.37s\n",
      "194:\tlearn: 0.0015720\ttotal: 4.36s\tremaining: 2.35s\n",
      "195:\tlearn: 0.0015577\ttotal: 4.39s\tremaining: 2.33s\n",
      "196:\tlearn: 0.0015429\ttotal: 4.42s\tremaining: 2.31s\n",
      "197:\tlearn: 0.0015264\ttotal: 4.44s\tremaining: 2.29s\n",
      "198:\tlearn: 0.0015189\ttotal: 4.47s\tremaining: 2.27s\n",
      "199:\tlearn: 0.0014943\ttotal: 4.5s\tremaining: 2.25s\n",
      "200:\tlearn: 0.0014863\ttotal: 4.54s\tremaining: 2.23s\n",
      "201:\tlearn: 0.0014569\ttotal: 4.55s\tremaining: 2.21s\n",
      "202:\tlearn: 0.0014356\ttotal: 4.57s\tremaining: 2.18s\n",
      "203:\tlearn: 0.0014263\ttotal: 4.6s\tremaining: 2.16s\n",
      "204:\tlearn: 0.0014037\ttotal: 4.62s\tremaining: 2.14s\n",
      "205:\tlearn: 0.0013905\ttotal: 4.63s\tremaining: 2.11s\n",
      "206:\tlearn: 0.0013772\ttotal: 4.65s\tremaining: 2.09s\n",
      "207:\tlearn: 0.0013431\ttotal: 4.66s\tremaining: 2.06s\n",
      "208:\tlearn: 0.0013367\ttotal: 4.68s\tremaining: 2.04s\n",
      "209:\tlearn: 0.0013095\ttotal: 4.71s\tremaining: 2.02s\n",
      "210:\tlearn: 0.0012791\ttotal: 4.74s\tremaining: 2s\n",
      "211:\tlearn: 0.0012703\ttotal: 4.76s\tremaining: 1.97s\n",
      "212:\tlearn: 0.0012579\ttotal: 4.77s\tremaining: 1.95s\n",
      "213:\tlearn: 0.0012509\ttotal: 4.79s\tremaining: 1.92s\n",
      "214:\tlearn: 0.0012306\ttotal: 4.82s\tremaining: 1.91s\n",
      "215:\tlearn: 0.0012209\ttotal: 4.84s\tremaining: 1.88s\n",
      "216:\tlearn: 0.0012106\ttotal: 4.85s\tremaining: 1.85s\n",
      "217:\tlearn: 0.0011947\ttotal: 4.9s\tremaining: 1.84s\n",
      "218:\tlearn: 0.0011859\ttotal: 4.92s\tremaining: 1.82s\n",
      "219:\tlearn: 0.0011598\ttotal: 4.95s\tremaining: 1.8s\n",
      "220:\tlearn: 0.0011455\ttotal: 4.98s\tremaining: 1.78s\n",
      "221:\tlearn: 0.0011265\ttotal: 5s\tremaining: 1.75s\n",
      "222:\tlearn: 0.0011052\ttotal: 5.01s\tremaining: 1.73s\n",
      "223:\tlearn: 0.0010971\ttotal: 5.03s\tremaining: 1.71s\n",
      "224:\tlearn: 0.0010835\ttotal: 5.06s\tremaining: 1.69s\n",
      "225:\tlearn: 0.0010712\ttotal: 5.07s\tremaining: 1.66s\n",
      "226:\tlearn: 0.0010643\ttotal: 5.09s\tremaining: 1.64s\n",
      "227:\tlearn: 0.0010552\ttotal: 5.11s\tremaining: 1.61s\n",
      "228:\tlearn: 0.0010334\ttotal: 5.14s\tremaining: 1.59s\n",
      "229:\tlearn: 0.0010144\ttotal: 5.15s\tremaining: 1.57s\n",
      "230:\tlearn: 0.0010033\ttotal: 5.17s\tremaining: 1.54s\n",
      "231:\tlearn: 0.0009911\ttotal: 5.18s\tremaining: 1.52s\n",
      "232:\tlearn: 0.0009878\ttotal: 5.2s\tremaining: 1.5s\n",
      "233:\tlearn: 0.0009824\ttotal: 5.22s\tremaining: 1.47s\n",
      "234:\tlearn: 0.0009710\ttotal: 5.23s\tremaining: 1.45s\n",
      "235:\tlearn: 0.0009623\ttotal: 5.25s\tremaining: 1.42s\n",
      "236:\tlearn: 0.0009517\ttotal: 5.26s\tremaining: 1.4s\n",
      "237:\tlearn: 0.0009481\ttotal: 5.28s\tremaining: 1.38s\n",
      "238:\tlearn: 0.0009332\ttotal: 5.29s\tremaining: 1.35s\n",
      "239:\tlearn: 0.0009221\ttotal: 5.31s\tremaining: 1.33s\n",
      "240:\tlearn: 0.0009125\ttotal: 5.33s\tremaining: 1.3s\n",
      "241:\tlearn: 0.0009100\ttotal: 5.36s\tremaining: 1.28s\n",
      "242:\tlearn: 0.0009045\ttotal: 5.37s\tremaining: 1.26s\n",
      "243:\tlearn: 0.0008945\ttotal: 5.39s\tremaining: 1.24s\n",
      "244:\tlearn: 0.0008864\ttotal: 5.41s\tremaining: 1.21s\n",
      "245:\tlearn: 0.0008819\ttotal: 5.42s\tremaining: 1.19s\n",
      "246:\tlearn: 0.0008660\ttotal: 5.44s\tremaining: 1.17s\n",
      "247:\tlearn: 0.0008583\ttotal: 5.45s\tremaining: 1.14s\n",
      "248:\tlearn: 0.0008510\ttotal: 5.48s\tremaining: 1.12s\n",
      "249:\tlearn: 0.0008440\ttotal: 5.51s\tremaining: 1.1s\n",
      "250:\tlearn: 0.0008333\ttotal: 5.55s\tremaining: 1.08s\n",
      "251:\tlearn: 0.0008295\ttotal: 5.56s\tremaining: 1.06s\n",
      "252:\tlearn: 0.0008208\ttotal: 5.59s\tremaining: 1.04s\n",
      "253:\tlearn: 0.0008146\ttotal: 5.64s\tremaining: 1.02s\n",
      "254:\tlearn: 0.0008056\ttotal: 5.66s\tremaining: 999ms\n",
      "255:\tlearn: 0.0008016\ttotal: 5.67s\tremaining: 975ms\n",
      "256:\tlearn: 0.0007924\ttotal: 5.69s\tremaining: 952ms\n",
      "257:\tlearn: 0.0007848\ttotal: 5.71s\tremaining: 929ms\n",
      "258:\tlearn: 0.0007711\ttotal: 5.74s\tremaining: 908ms\n",
      "259:\tlearn: 0.0007613\ttotal: 5.77s\tremaining: 888ms\n",
      "260:\tlearn: 0.0007561\ttotal: 5.79s\tremaining: 864ms\n",
      "261:\tlearn: 0.0007477\ttotal: 5.8s\tremaining: 841ms\n",
      "262:\tlearn: 0.0007348\ttotal: 5.83s\tremaining: 821ms\n",
      "263:\tlearn: 0.0007269\ttotal: 5.85s\tremaining: 797ms\n",
      "264:\tlearn: 0.0007204\ttotal: 5.86s\tremaining: 775ms\n",
      "265:\tlearn: 0.0007101\ttotal: 5.88s\tremaining: 752ms\n",
      "266:\tlearn: 0.0007051\ttotal: 5.91s\tremaining: 731ms\n",
      "267:\tlearn: 0.0006923\ttotal: 5.93s\tremaining: 708ms\n",
      "268:\tlearn: 0.0006850\ttotal: 5.94s\tremaining: 685ms\n",
      "269:\tlearn: 0.0006798\ttotal: 5.97s\tremaining: 664ms\n",
      "270:\tlearn: 0.0006784\ttotal: 5.99s\tremaining: 641ms\n",
      "271:\tlearn: 0.0006689\ttotal: 6.02s\tremaining: 620ms\n",
      "272:\tlearn: 0.0006631\ttotal: 6.04s\tremaining: 597ms\n",
      "273:\tlearn: 0.0006604\ttotal: 6.05s\tremaining: 574ms\n",
      "274:\tlearn: 0.0006536\ttotal: 6.07s\tremaining: 552ms\n",
      "275:\tlearn: 0.0006468\ttotal: 6.09s\tremaining: 529ms\n",
      "276:\tlearn: 0.0006416\ttotal: 6.1s\tremaining: 507ms\n",
      "277:\tlearn: 0.0006376\ttotal: 6.12s\tremaining: 484ms\n",
      "278:\tlearn: 0.0006345\ttotal: 6.15s\tremaining: 463ms\n",
      "279:\tlearn: 0.0006231\ttotal: 6.16s\tremaining: 440ms\n",
      "280:\tlearn: 0.0006154\ttotal: 6.18s\tremaining: 418ms\n",
      "281:\tlearn: 0.0006095\ttotal: 6.21s\tremaining: 396ms\n",
      "282:\tlearn: 0.0006074\ttotal: 6.22s\tremaining: 374ms\n",
      "283:\tlearn: 0.0006046\ttotal: 6.24s\tremaining: 352ms\n",
      "284:\tlearn: 0.0006014\ttotal: 6.26s\tremaining: 329ms\n",
      "285:\tlearn: 0.0005943\ttotal: 6.27s\tremaining: 307ms\n",
      "286:\tlearn: 0.0005849\ttotal: 6.29s\tremaining: 285ms\n",
      "287:\tlearn: 0.0005797\ttotal: 6.3s\tremaining: 263ms\n",
      "288:\tlearn: 0.0005713\ttotal: 6.32s\tremaining: 241ms\n",
      "289:\tlearn: 0.0005646\ttotal: 6.35s\tremaining: 219ms\n",
      "290:\tlearn: 0.0005600\ttotal: 6.37s\tremaining: 197ms\n",
      "291:\tlearn: 0.0005594\ttotal: 6.4s\tremaining: 175ms\n",
      "292:\tlearn: 0.0005557\ttotal: 6.43s\tremaining: 154ms\n",
      "293:\tlearn: 0.0005532\ttotal: 6.46s\tremaining: 132ms\n",
      "294:\tlearn: 0.0005487\ttotal: 6.48s\tremaining: 110ms\n",
      "295:\tlearn: 0.0005471\ttotal: 6.49s\tremaining: 87.8ms\n",
      "296:\tlearn: 0.0005417\ttotal: 6.53s\tremaining: 65.9ms\n",
      "297:\tlearn: 0.0005333\ttotal: 6.54s\tremaining: 43.9ms\n",
      "298:\tlearn: 0.0005299\ttotal: 6.56s\tremaining: 21.9ms\n",
      "299:\tlearn: 0.0005252\ttotal: 6.57s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552163a1360248eb9d4a0ec3db48c3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.05735931596081712, Recall = 0.0, Aging Rate = 0.005218027649183092, Precision = 0.0, f1 = 0\n",
      "Epoch 2: Train Loss = 0.026783882615337123, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.02630024416806646, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.02588054063438525, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.025812737786256604, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.024846224509417154, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.024977033900200844, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.024725997853025213, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.02453850627783981, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.02424697982053906, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.024108837558496236, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.023699224211278718, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.023807609994264903, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.023615065875987485, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.023426012742633746, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.02316077883526904, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.022933142703872766, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.022450516896372626, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.022635905024274185, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.022368094427613474, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 18: Train Loss = 0.02216704884886799, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 19: Train Loss = 0.02187638509372222, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 20: Train Loss = 0.021595833155902935, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.021140121405844783, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 21: Train Loss = 0.02136775623983592, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 22: Train Loss = 0.02109873285700297, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 23: Train Loss = 0.020867581939867805, Recall = 0.0044444444444444444, Aging Rate = 7.954310440827884e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.020617511652752104, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 25: Train Loss = 0.020356973600893346, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.019832441017917254, Recall = 0.022222222222222223, Aging Rate = 3.9771552204139425e-05, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.020122987945936286, Recall = 0.008888888888888889, Aging Rate = 1.5908620881655768e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.019857290742482693, Recall = 0.008888888888888889, Aging Rate = 1.5908620881655768e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.01967358343663619, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 29: Train Loss = 0.01950284281862333, Recall = 0.013333333333333334, Aging Rate = 3.1817241763311536e-05, Precision = 0.75, f1 = 0.026200873362445417\n",
      "Epoch 30: Train Loss = 0.01923719599686301, Recall = 0.017777777777777778, Aging Rate = 3.1817241763311536e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.018755944810674687, Recall = 0.035555555555555556, Aging Rate = 6.363448352662307e-05, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.01902735264675442, Recall = 0.04, Aging Rate = 7.954310440827885e-05, Precision = 0.9, f1 = 0.07659574468085106\n",
      "Epoch 32: Train Loss = 0.01881700915119152, Recall = 0.035555555555555556, Aging Rate = 7.158879396745097e-05, Precision = 0.8888888888888888, f1 = 0.06837606837606838\n",
      "Epoch 33: Train Loss = 0.01859979363849055, Recall = 0.03111111111111111, Aging Rate = 5.568017308579519e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.018410771974113834, Recall = 0.04, Aging Rate = 7.954310440827885e-05, Precision = 0.9, f1 = 0.07659574468085106\n",
      "Epoch 35: Train Loss = 0.018242581967120343, Recall = 0.05333333333333334, Aging Rate = 0.0001034060357307625, Precision = 0.9230769230769231, f1 = 0.10084033613445378\n",
      "Test Loss = 0.017699322271733257, Recall = 0.05333333333333334, Aging Rate = 0.0001034060357307625, precision = 0.9230769230769231\n",
      "\n",
      "Epoch 36: Train Loss = 0.018059600096398443, Recall = 0.044444444444444446, Aging Rate = 7.954310440827885e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.017839001619995125, Recall = 0.057777777777777775, Aging Rate = 0.0001034060357307625, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.01767746026591983, Recall = 0.035555555555555556, Aging Rate = 7.158879396745097e-05, Precision = 0.8888888888888888, f1 = 0.06837606837606838\n",
      "Epoch 39: Train Loss = 0.017501766955132305, Recall = 0.057777777777777775, Aging Rate = 0.00011136034617159038, Precision = 0.9285714285714286, f1 = 0.10878661087866108\n",
      "Epoch 40: Train Loss = 0.017367694742928905, Recall = 0.05333333333333334, Aging Rate = 0.0001034060357307625, Precision = 0.9230769230769231, f1 = 0.10084033613445378\n",
      "Test Loss = 0.016848889646033562, Recall = 0.09333333333333334, Aging Rate = 0.00016704051925738557, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.01717400068457743, Recall = 0.057777777777777775, Aging Rate = 0.00011931465661241828, Precision = 0.8666666666666667, f1 = 0.10833333333333334\n",
      "Epoch 42: Train Loss = 0.01704231249799235, Recall = 0.05333333333333334, Aging Rate = 0.0001034060357307625, Precision = 0.9230769230769231, f1 = 0.10084033613445378\n",
      "Epoch 43: Train Loss = 0.0169158115345867, Recall = 0.08, Aging Rate = 0.0001590862088165577, Precision = 0.9, f1 = 0.1469387755102041\n",
      "Epoch 44: Train Loss = 0.016681464725194765, Recall = 0.05333333333333334, Aging Rate = 0.00011136034617159038, Precision = 0.8571428571428571, f1 = 0.100418410041841\n",
      "Epoch 45: Train Loss = 0.016604930420236624, Recall = 0.08, Aging Rate = 0.0001590862088165577, Precision = 0.9, f1 = 0.1469387755102041\n",
      "Test Loss = 0.01610543260831649, Recall = 0.09333333333333334, Aging Rate = 0.00017499482969821347, precision = 0.9545454545454546\n",
      "\n",
      "Epoch 46: Train Loss = 0.016418498409232377, Recall = 0.057777777777777775, Aging Rate = 0.00011931465661241828, Precision = 0.8666666666666667, f1 = 0.10833333333333334\n",
      "Epoch 47: Train Loss = 0.016364301635498783, Recall = 0.07555555555555556, Aging Rate = 0.00016704051925738557, Precision = 0.8095238095238095, f1 = 0.13821138211382114\n",
      "Epoch 48: Train Loss = 0.016258536284326382, Recall = 0.08, Aging Rate = 0.0001590862088165577, Precision = 0.9, f1 = 0.1469387755102041\n",
      "Epoch 49: Train Loss = 0.01610571252025802, Recall = 0.08444444444444445, Aging Rate = 0.0001590862088165577, Precision = 0.95, f1 = 0.15510204081632653\n",
      "Epoch 50: Train Loss = 0.015996217201709604, Recall = 0.08, Aging Rate = 0.0001590862088165577, Precision = 0.9, f1 = 0.1469387755102041\n",
      "Test Loss = 0.015458697964732279, Recall = 0.09333333333333334, Aging Rate = 0.00016704051925738557, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.015866629249724037, Recall = 0.08, Aging Rate = 0.0001511318983757298, Precision = 0.9473684210526315, f1 = 0.14754098360655737\n",
      "Epoch 52: Train Loss = 0.01574466152094106, Recall = 0.08444444444444445, Aging Rate = 0.00017499482969821347, Precision = 0.8636363636363636, f1 = 0.15384615384615385\n",
      "Epoch 53: Train Loss = 0.015600663289212864, Recall = 0.08, Aging Rate = 0.00017499482969821347, Precision = 0.8181818181818182, f1 = 0.14574898785425103\n",
      "Epoch 54: Train Loss = 0.015510490332430401, Recall = 0.08888888888888889, Aging Rate = 0.00019090345057986923, Precision = 0.8333333333333334, f1 = 0.1606425702811245\n",
      "Epoch 55: Train Loss = 0.015438160960964323, Recall = 0.07555555555555556, Aging Rate = 0.0001590862088165577, Precision = 0.85, f1 = 0.13877551020408163\n",
      "Test Loss = 0.014909801146022902, Recall = 0.09333333333333334, Aging Rate = 0.00019090345057986923, precision = 0.875\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Train Loss = 0.0153301277052263, Recall = 0.08888888888888889, Aging Rate = 0.00018294914013904133, Precision = 0.8695652173913043, f1 = 0.16129032258064516\n",
      "Epoch 57: Train Loss = 0.015260354270426366, Recall = 0.09333333333333334, Aging Rate = 0.00018294914013904133, Precision = 0.9130434782608695, f1 = 0.16935483870967744\n",
      "Epoch 58: Train Loss = 0.015102037653464719, Recall = 0.07555555555555556, Aging Rate = 0.0001590862088165577, Precision = 0.85, f1 = 0.13877551020408163\n",
      "Epoch 59: Train Loss = 0.015070825092132738, Recall = 0.09333333333333334, Aging Rate = 0.00019090345057986923, Precision = 0.875, f1 = 0.1686746987951807\n",
      "Epoch 60: Train Loss = 0.014907975156333944, Recall = 0.09333333333333334, Aging Rate = 0.00019885776102069713, Precision = 0.84, f1 = 0.16799999999999998\n",
      "Test Loss = 0.014540771823328413, Recall = 0.1288888888888889, Aging Rate = 0.0002624922445473202, precision = 0.8787878787878788\n",
      "\n",
      "Epoch 61: Train Loss = 0.014823377797078938, Recall = 0.10666666666666667, Aging Rate = 0.00023862931322483655, Precision = 0.8, f1 = 0.18823529411764706\n",
      "Epoch 62: Train Loss = 0.014774492220114014, Recall = 0.10222222222222223, Aging Rate = 0.00019885776102069713, Precision = 0.92, f1 = 0.184\n",
      "Epoch 63: Train Loss = 0.014691353627129885, Recall = 0.08888888888888889, Aging Rate = 0.000206812071461525, Precision = 0.7692307692307693, f1 = 0.1593625498007968\n",
      "Epoch 64: Train Loss = 0.014509851822395359, Recall = 0.09777777777777778, Aging Rate = 0.0002147663819023529, Precision = 0.8148148148148148, f1 = 0.17460317460317462\n",
      "Epoch 65: Train Loss = 0.014457505347590065, Recall = 0.1111111111111111, Aging Rate = 0.00023862931322483655, Precision = 0.8333333333333334, f1 = 0.19607843137254902\n",
      "Test Loss = 0.01417844125920927, Recall = 0.09333333333333334, Aging Rate = 0.00016704051925738557, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.014450527128816643, Recall = 0.1111111111111111, Aging Rate = 0.00022272069234318076, Precision = 0.8928571428571429, f1 = 0.1976284584980237\n",
      "Epoch 67: Train Loss = 0.01444136510715895, Recall = 0.11555555555555555, Aging Rate = 0.00023067500278400866, Precision = 0.896551724137931, f1 = 0.20472440944881887\n",
      "Epoch 68: Train Loss = 0.014285345339368913, Recall = 0.08888888888888889, Aging Rate = 0.00019090345057986923, Precision = 0.8333333333333334, f1 = 0.1606425702811245\n",
      "Epoch 69: Train Loss = 0.014180050829347996, Recall = 0.11555555555555555, Aging Rate = 0.00024658362366566445, Precision = 0.8387096774193549, f1 = 0.203125\n",
      "Epoch 70: Train Loss = 0.014183148755604754, Recall = 0.1111111111111111, Aging Rate = 0.00023067500278400866, Precision = 0.8620689655172413, f1 = 0.19685039370078738\n",
      "Test Loss = 0.013645900478829093, Recall = 0.13333333333333333, Aging Rate = 0.0002704465549881481, precision = 0.8823529411764706\n",
      "\n",
      "Epoch 71: Train Loss = 0.014111943190994446, Recall = 0.12, Aging Rate = 0.00023862931322483655, Precision = 0.9, f1 = 0.21176470588235294\n",
      "Epoch 72: Train Loss = 0.014053066620261338, Recall = 0.1111111111111111, Aging Rate = 0.00024658362366566445, Precision = 0.8064516129032258, f1 = 0.19531249999999997\n",
      "Epoch 73: Train Loss = 0.013977878842563712, Recall = 0.12444444444444444, Aging Rate = 0.0002545379341064923, Precision = 0.875, f1 = 0.21789883268482488\n",
      "Epoch 74: Train Loss = 0.013908527158150663, Recall = 0.10222222222222223, Aging Rate = 0.000206812071461525, Precision = 0.8846153846153846, f1 = 0.18326693227091634\n",
      "Epoch 75: Train Loss = 0.013844516042627767, Recall = 0.10666666666666667, Aging Rate = 0.00023862931322483655, Precision = 0.8, f1 = 0.18823529411764706\n",
      "Test Loss = 0.01330022844725292, Recall = 0.1511111111111111, Aging Rate = 0.0003022637967514596, precision = 0.8947368421052632\n",
      "\n",
      "Epoch 76: Train Loss = 0.013716568042001878, Recall = 0.1288888888888889, Aging Rate = 0.000278400865428976, Precision = 0.8285714285714286, f1 = 0.22307692307692306\n",
      "Epoch 77: Train Loss = 0.013760378622439283, Recall = 0.12, Aging Rate = 0.00023862931322483655, Precision = 0.9, f1 = 0.21176470588235294\n",
      "Epoch 78: Train Loss = 0.01369142555780969, Recall = 0.14666666666666667, Aging Rate = 0.0002863551758698039, Precision = 0.9166666666666666, f1 = 0.25287356321839083\n",
      "Epoch 79: Train Loss = 0.013633954810880103, Recall = 0.12, Aging Rate = 0.0002545379341064923, Precision = 0.84375, f1 = 0.2101167315175097\n",
      "Epoch 80: Train Loss = 0.013547182115121455, Recall = 0.1288888888888889, Aging Rate = 0.0002704465549881481, Precision = 0.8529411764705882, f1 = 0.22393822393822393\n",
      "Test Loss = 0.013004041413437318, Recall = 0.14222222222222222, Aging Rate = 0.0002863551758698039, precision = 0.8888888888888888\n",
      "\n",
      "Epoch 81: Train Loss = 0.013496059198943974, Recall = 0.1511111111111111, Aging Rate = 0.0002943094863106317, Precision = 0.918918918918919, f1 = 0.25954198473282447\n",
      "Epoch 82: Train Loss = 0.01346850171600545, Recall = 0.1288888888888889, Aging Rate = 0.0002624922445473202, Precision = 0.8787878787878788, f1 = 0.22480620155038758\n",
      "Epoch 83: Train Loss = 0.013367799280635988, Recall = 0.13777777777777778, Aging Rate = 0.0002863551758698039, Precision = 0.8611111111111112, f1 = 0.23754789272030652\n",
      "Epoch 84: Train Loss = 0.013364992855522626, Recall = 0.14666666666666667, Aging Rate = 0.0003181724176331154, Precision = 0.825, f1 = 0.2490566037735849\n",
      "Epoch 85: Train Loss = 0.013297947218402833, Recall = 0.15555555555555556, Aging Rate = 0.00033408103851477114, Precision = 0.8333333333333334, f1 = 0.2621722846441948\n",
      "Test Loss = 0.012908827741321757, Recall = 0.09333333333333334, Aging Rate = 0.00019090345057986923, precision = 0.875\n",
      "\n",
      "Epoch 86: Train Loss = 0.013273992537664822, Recall = 0.13333333333333333, Aging Rate = 0.0002704465549881481, Precision = 0.8823529411764706, f1 = 0.23166023166023167\n",
      "Epoch 87: Train Loss = 0.013177733553852653, Recall = 0.14222222222222222, Aging Rate = 0.000278400865428976, Precision = 0.9142857142857143, f1 = 0.24615384615384617\n",
      "Epoch 88: Train Loss = 0.013199042151473099, Recall = 0.14666666666666667, Aging Rate = 0.0002943094863106317, Precision = 0.8918918918918919, f1 = 0.25190839694656486\n",
      "Epoch 89: Train Loss = 0.013103509057327368, Recall = 0.1511111111111111, Aging Rate = 0.0003022637967514596, Precision = 0.8947368421052632, f1 = 0.2585551330798479\n",
      "Epoch 90: Train Loss = 0.01304442970081186, Recall = 0.1511111111111111, Aging Rate = 0.0002863551758698039, Precision = 0.9444444444444444, f1 = 0.26053639846743293\n",
      "Test Loss = 0.012558418879629987, Recall = 0.1688888888888889, Aging Rate = 0.00033408103851477114, precision = 0.9047619047619048\n",
      "\n",
      "Training Finished at epoch 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2812295\ttotal: 23.1ms\tremaining: 6.9s\n",
      "1:\tlearn: 0.1237327\ttotal: 37.3ms\tremaining: 5.56s\n",
      "2:\tlearn: 0.0624066\ttotal: 52.8ms\tremaining: 5.23s\n",
      "3:\tlearn: 0.0372676\ttotal: 67ms\tremaining: 4.96s\n",
      "4:\tlearn: 0.0253510\ttotal: 98.9ms\tremaining: 5.83s\n",
      "5:\tlearn: 0.0195684\ttotal: 115ms\tremaining: 5.66s\n",
      "6:\tlearn: 0.0166187\ttotal: 130ms\tremaining: 5.45s\n",
      "7:\tlearn: 0.0147675\ttotal: 147ms\tremaining: 5.35s\n",
      "8:\tlearn: 0.0136082\ttotal: 176ms\tremaining: 5.69s\n",
      "9:\tlearn: 0.0129875\ttotal: 191ms\tremaining: 5.54s\n",
      "10:\tlearn: 0.0125588\ttotal: 206ms\tremaining: 5.42s\n",
      "11:\tlearn: 0.0122636\ttotal: 239ms\tremaining: 5.74s\n",
      "12:\tlearn: 0.0121187\ttotal: 254ms\tremaining: 5.62s\n",
      "13:\tlearn: 0.0119306\ttotal: 286ms\tremaining: 5.85s\n",
      "14:\tlearn: 0.0118166\ttotal: 302ms\tremaining: 5.74s\n",
      "15:\tlearn: 0.0116999\ttotal: 318ms\tremaining: 5.64s\n",
      "16:\tlearn: 0.0115993\ttotal: 334ms\tremaining: 5.56s\n",
      "17:\tlearn: 0.0115505\ttotal: 348ms\tremaining: 5.46s\n",
      "18:\tlearn: 0.0114607\ttotal: 365ms\tremaining: 5.4s\n",
      "19:\tlearn: 0.0113749\ttotal: 380ms\tremaining: 5.32s\n",
      "20:\tlearn: 0.0112767\ttotal: 397ms\tremaining: 5.27s\n",
      "21:\tlearn: 0.0111426\ttotal: 412ms\tremaining: 5.2s\n",
      "22:\tlearn: 0.0110954\ttotal: 428ms\tremaining: 5.16s\n",
      "23:\tlearn: 0.0110274\ttotal: 440ms\tremaining: 5.06s\n",
      "24:\tlearn: 0.0109669\ttotal: 476ms\tremaining: 5.23s\n",
      "25:\tlearn: 0.0108715\ttotal: 506ms\tremaining: 5.33s\n",
      "26:\tlearn: 0.0108506\ttotal: 523ms\tremaining: 5.29s\n",
      "27:\tlearn: 0.0107510\ttotal: 555ms\tremaining: 5.39s\n",
      "28:\tlearn: 0.0106190\ttotal: 587ms\tremaining: 5.48s\n",
      "29:\tlearn: 0.0105365\ttotal: 602ms\tremaining: 5.42s\n",
      "30:\tlearn: 0.0104117\ttotal: 618ms\tremaining: 5.37s\n",
      "31:\tlearn: 0.0103568\ttotal: 650ms\tremaining: 5.44s\n",
      "32:\tlearn: 0.0103093\ttotal: 682ms\tremaining: 5.52s\n",
      "33:\tlearn: 0.0102093\ttotal: 697ms\tremaining: 5.45s\n",
      "34:\tlearn: 0.0101293\ttotal: 713ms\tremaining: 5.4s\n",
      "35:\tlearn: 0.0100824\ttotal: 729ms\tremaining: 5.34s\n",
      "36:\tlearn: 0.0100028\ttotal: 761ms\tremaining: 5.41s\n",
      "37:\tlearn: 0.0099297\ttotal: 771ms\tremaining: 5.32s\n",
      "38:\tlearn: 0.0098347\ttotal: 809ms\tremaining: 5.41s\n",
      "39:\tlearn: 0.0097691\ttotal: 824ms\tremaining: 5.36s\n",
      "40:\tlearn: 0.0096749\ttotal: 839ms\tremaining: 5.3s\n",
      "41:\tlearn: 0.0095520\ttotal: 856ms\tremaining: 5.26s\n",
      "42:\tlearn: 0.0094659\ttotal: 886ms\tremaining: 5.29s\n",
      "43:\tlearn: 0.0093713\ttotal: 917ms\tremaining: 5.34s\n",
      "44:\tlearn: 0.0091613\ttotal: 949ms\tremaining: 5.38s\n",
      "45:\tlearn: 0.0090566\ttotal: 965ms\tremaining: 5.33s\n",
      "46:\tlearn: 0.0089612\ttotal: 980ms\tremaining: 5.28s\n",
      "47:\tlearn: 0.0088266\ttotal: 997ms\tremaining: 5.23s\n",
      "48:\tlearn: 0.0087630\ttotal: 1.03s\tremaining: 5.26s\n",
      "49:\tlearn: 0.0086461\ttotal: 1.04s\tremaining: 5.21s\n",
      "50:\tlearn: 0.0085798\ttotal: 1.06s\tremaining: 5.17s\n",
      "51:\tlearn: 0.0084907\ttotal: 1.07s\tremaining: 5.13s\n",
      "52:\tlearn: 0.0084251\ttotal: 1.09s\tremaining: 5.08s\n",
      "53:\tlearn: 0.0083471\ttotal: 1.12s\tremaining: 5.11s\n",
      "54:\tlearn: 0.0082003\ttotal: 1.15s\tremaining: 5.14s\n",
      "55:\tlearn: 0.0081477\ttotal: 1.18s\tremaining: 5.16s\n",
      "56:\tlearn: 0.0079709\ttotal: 1.2s\tremaining: 5.11s\n",
      "57:\tlearn: 0.0078984\ttotal: 1.22s\tremaining: 5.07s\n",
      "58:\tlearn: 0.0077755\ttotal: 1.23s\tremaining: 5.03s\n",
      "59:\tlearn: 0.0076547\ttotal: 1.25s\tremaining: 4.99s\n",
      "60:\tlearn: 0.0075386\ttotal: 1.28s\tremaining: 5.01s\n",
      "61:\tlearn: 0.0074975\ttotal: 1.29s\tremaining: 4.97s\n",
      "62:\tlearn: 0.0073997\ttotal: 1.31s\tremaining: 4.91s\n",
      "63:\tlearn: 0.0073385\ttotal: 1.32s\tremaining: 4.89s\n",
      "64:\tlearn: 0.0072055\ttotal: 1.34s\tremaining: 4.85s\n",
      "65:\tlearn: 0.0070809\ttotal: 1.36s\tremaining: 4.81s\n",
      "66:\tlearn: 0.0069768\ttotal: 1.39s\tremaining: 4.83s\n",
      "67:\tlearn: 0.0068757\ttotal: 1.41s\tremaining: 4.79s\n",
      "68:\tlearn: 0.0068325\ttotal: 1.44s\tremaining: 4.81s\n",
      "69:\tlearn: 0.0067469\ttotal: 1.45s\tremaining: 4.77s\n",
      "70:\tlearn: 0.0066584\ttotal: 1.47s\tremaining: 4.74s\n",
      "71:\tlearn: 0.0065789\ttotal: 1.5s\tremaining: 4.75s\n",
      "72:\tlearn: 0.0065147\ttotal: 1.53s\tremaining: 4.76s\n",
      "73:\tlearn: 0.0064153\ttotal: 1.54s\tremaining: 4.72s\n",
      "74:\tlearn: 0.0063604\ttotal: 1.56s\tremaining: 4.68s\n",
      "75:\tlearn: 0.0063062\ttotal: 1.59s\tremaining: 4.7s\n",
      "76:\tlearn: 0.0062424\ttotal: 1.61s\tremaining: 4.66s\n",
      "77:\tlearn: 0.0061895\ttotal: 1.62s\tremaining: 4.61s\n",
      "78:\tlearn: 0.0061267\ttotal: 1.65s\tremaining: 4.63s\n",
      "79:\tlearn: 0.0060625\ttotal: 1.69s\tremaining: 4.63s\n",
      "80:\tlearn: 0.0059862\ttotal: 1.71s\tremaining: 4.64s\n",
      "81:\tlearn: 0.0059239\ttotal: 1.73s\tremaining: 4.6s\n",
      "82:\tlearn: 0.0057814\ttotal: 1.75s\tremaining: 4.56s\n",
      "83:\tlearn: 0.0057244\ttotal: 1.78s\tremaining: 4.57s\n",
      "84:\tlearn: 0.0056642\ttotal: 1.79s\tremaining: 4.53s\n",
      "85:\tlearn: 0.0056145\ttotal: 1.81s\tremaining: 4.5s\n",
      "86:\tlearn: 0.0055622\ttotal: 1.82s\tremaining: 4.47s\n",
      "87:\tlearn: 0.0055214\ttotal: 1.84s\tremaining: 4.43s\n",
      "88:\tlearn: 0.0054903\ttotal: 1.86s\tremaining: 4.4s\n",
      "89:\tlearn: 0.0054188\ttotal: 1.87s\tremaining: 4.37s\n",
      "90:\tlearn: 0.0053633\ttotal: 1.89s\tremaining: 4.33s\n",
      "91:\tlearn: 0.0052971\ttotal: 1.9s\tremaining: 4.3s\n",
      "92:\tlearn: 0.0052536\ttotal: 1.92s\tremaining: 4.27s\n",
      "93:\tlearn: 0.0052320\ttotal: 1.93s\tremaining: 4.24s\n",
      "94:\tlearn: 0.0051282\ttotal: 1.95s\tremaining: 4.21s\n",
      "95:\tlearn: 0.0050735\ttotal: 1.96s\tremaining: 4.17s\n",
      "96:\tlearn: 0.0050320\ttotal: 1.98s\tremaining: 4.14s\n",
      "97:\tlearn: 0.0049872\ttotal: 2s\tremaining: 4.11s\n",
      "98:\tlearn: 0.0049443\ttotal: 2.01s\tremaining: 4.08s\n",
      "99:\tlearn: 0.0048671\ttotal: 2.03s\tremaining: 4.06s\n",
      "100:\tlearn: 0.0048144\ttotal: 2.04s\tremaining: 4.03s\n",
      "101:\tlearn: 0.0047715\ttotal: 2.06s\tremaining: 4s\n",
      "102:\tlearn: 0.0047273\ttotal: 2.07s\tremaining: 3.97s\n",
      "103:\tlearn: 0.0046843\ttotal: 2.11s\tremaining: 3.97s\n",
      "104:\tlearn: 0.0046383\ttotal: 2.12s\tremaining: 3.94s\n",
      "105:\tlearn: 0.0045987\ttotal: 2.14s\tremaining: 3.91s\n",
      "106:\tlearn: 0.0044211\ttotal: 2.15s\tremaining: 3.88s\n",
      "107:\tlearn: 0.0043901\ttotal: 2.17s\tremaining: 3.86s\n",
      "108:\tlearn: 0.0043518\ttotal: 2.18s\tremaining: 3.83s\n",
      "109:\tlearn: 0.0043119\ttotal: 2.22s\tremaining: 3.83s\n",
      "110:\tlearn: 0.0042482\ttotal: 2.23s\tremaining: 3.8s\n",
      "111:\tlearn: 0.0041960\ttotal: 2.25s\tremaining: 3.77s\n",
      "112:\tlearn: 0.0041532\ttotal: 2.26s\tremaining: 3.75s\n",
      "113:\tlearn: 0.0040939\ttotal: 2.28s\tremaining: 3.72s\n",
      "114:\tlearn: 0.0040593\ttotal: 2.29s\tremaining: 3.69s\n",
      "115:\tlearn: 0.0040195\ttotal: 2.31s\tremaining: 3.67s\n",
      "116:\tlearn: 0.0039601\ttotal: 2.34s\tremaining: 3.67s\n",
      "117:\tlearn: 0.0039117\ttotal: 2.36s\tremaining: 3.64s\n",
      "118:\tlearn: 0.0038923\ttotal: 2.37s\tremaining: 3.61s\n",
      "119:\tlearn: 0.0038139\ttotal: 2.39s\tremaining: 3.59s\n",
      "120:\tlearn: 0.0037725\ttotal: 2.41s\tremaining: 3.56s\n",
      "121:\tlearn: 0.0037292\ttotal: 2.42s\tremaining: 3.53s\n",
      "122:\tlearn: 0.0036961\ttotal: 2.44s\tremaining: 3.51s\n",
      "123:\tlearn: 0.0036276\ttotal: 2.45s\tremaining: 3.48s\n",
      "124:\tlearn: 0.0035845\ttotal: 2.46s\tremaining: 3.45s\n",
      "125:\tlearn: 0.0035487\ttotal: 2.48s\tremaining: 3.43s\n",
      "126:\tlearn: 0.0035165\ttotal: 2.52s\tremaining: 3.43s\n",
      "127:\tlearn: 0.0034970\ttotal: 2.53s\tremaining: 3.4s\n",
      "128:\tlearn: 0.0034550\ttotal: 2.55s\tremaining: 3.38s\n",
      "129:\tlearn: 0.0034177\ttotal: 2.56s\tremaining: 3.35s\n",
      "130:\tlearn: 0.0033655\ttotal: 2.58s\tremaining: 3.33s\n",
      "131:\tlearn: 0.0032782\ttotal: 2.59s\tremaining: 3.3s\n",
      "132:\tlearn: 0.0032517\ttotal: 2.6s\tremaining: 3.27s\n",
      "133:\tlearn: 0.0031919\ttotal: 2.63s\tremaining: 3.25s\n",
      "134:\tlearn: 0.0031672\ttotal: 2.64s\tremaining: 3.23s\n",
      "135:\tlearn: 0.0031346\ttotal: 2.66s\tremaining: 3.2s\n",
      "136:\tlearn: 0.0030830\ttotal: 2.67s\tremaining: 3.17s\n",
      "137:\tlearn: 0.0030480\ttotal: 2.69s\tremaining: 3.15s\n",
      "138:\tlearn: 0.0030124\ttotal: 2.7s\tremaining: 3.13s\n",
      "139:\tlearn: 0.0029533\ttotal: 2.73s\tremaining: 3.13s\n",
      "140:\tlearn: 0.0029153\ttotal: 2.77s\tremaining: 3.12s\n",
      "141:\tlearn: 0.0028776\ttotal: 2.8s\tremaining: 3.11s\n",
      "142:\tlearn: 0.0028138\ttotal: 2.81s\tremaining: 3.09s\n",
      "143:\tlearn: 0.0027766\ttotal: 2.84s\tremaining: 3.08s\n",
      "144:\tlearn: 0.0027119\ttotal: 2.86s\tremaining: 3.06s\n",
      "145:\tlearn: 0.0026886\ttotal: 2.89s\tremaining: 3.05s\n",
      "146:\tlearn: 0.0026570\ttotal: 2.91s\tremaining: 3.03s\n",
      "147:\tlearn: 0.0026128\ttotal: 2.92s\tremaining: 3s\n",
      "148:\tlearn: 0.0025702\ttotal: 2.94s\tremaining: 2.98s\n",
      "149:\tlearn: 0.0025339\ttotal: 2.96s\tremaining: 2.96s\n",
      "150:\tlearn: 0.0025089\ttotal: 2.97s\tremaining: 2.93s\n",
      "151:\tlearn: 0.0024362\ttotal: 2.98s\tremaining: 2.91s\n",
      "152:\tlearn: 0.0024159\ttotal: 3s\tremaining: 2.88s\n",
      "153:\tlearn: 0.0023645\ttotal: 3.02s\tremaining: 2.86s\n",
      "154:\tlearn: 0.0023342\ttotal: 3.03s\tremaining: 2.84s\n",
      "155:\tlearn: 0.0023064\ttotal: 3.04s\tremaining: 2.81s\n",
      "156:\tlearn: 0.0022831\ttotal: 3.06s\tremaining: 2.78s\n",
      "157:\tlearn: 0.0022481\ttotal: 3.08s\tremaining: 2.77s\n",
      "158:\tlearn: 0.0022168\ttotal: 3.1s\tremaining: 2.74s\n",
      "159:\tlearn: 0.0022034\ttotal: 3.11s\tremaining: 2.72s\n",
      "160:\tlearn: 0.0021828\ttotal: 3.13s\tremaining: 2.7s\n",
      "161:\tlearn: 0.0021448\ttotal: 3.14s\tremaining: 2.68s\n",
      "162:\tlearn: 0.0021151\ttotal: 3.16s\tremaining: 2.65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163:\tlearn: 0.0020977\ttotal: 3.19s\tremaining: 2.65s\n",
      "164:\tlearn: 0.0020792\ttotal: 3.2s\tremaining: 2.62s\n",
      "165:\tlearn: 0.0020497\ttotal: 3.22s\tremaining: 2.6s\n",
      "166:\tlearn: 0.0020179\ttotal: 3.23s\tremaining: 2.58s\n",
      "167:\tlearn: 0.0019872\ttotal: 3.25s\tremaining: 2.55s\n",
      "168:\tlearn: 0.0019626\ttotal: 3.28s\tremaining: 2.54s\n",
      "169:\tlearn: 0.0019103\ttotal: 3.31s\tremaining: 2.53s\n",
      "170:\tlearn: 0.0018620\ttotal: 3.33s\tremaining: 2.51s\n",
      "171:\tlearn: 0.0018166\ttotal: 3.34s\tremaining: 2.49s\n",
      "172:\tlearn: 0.0017912\ttotal: 3.36s\tremaining: 2.47s\n",
      "173:\tlearn: 0.0017662\ttotal: 3.39s\tremaining: 2.46s\n",
      "174:\tlearn: 0.0017516\ttotal: 3.42s\tremaining: 2.44s\n",
      "175:\tlearn: 0.0017116\ttotal: 3.44s\tremaining: 2.42s\n",
      "176:\tlearn: 0.0016878\ttotal: 3.45s\tremaining: 2.4s\n",
      "177:\tlearn: 0.0016557\ttotal: 3.47s\tremaining: 2.38s\n",
      "178:\tlearn: 0.0016232\ttotal: 3.5s\tremaining: 2.37s\n",
      "179:\tlearn: 0.0016089\ttotal: 3.52s\tremaining: 2.35s\n",
      "180:\tlearn: 0.0015714\ttotal: 3.55s\tremaining: 2.33s\n",
      "181:\tlearn: 0.0015596\ttotal: 3.58s\tremaining: 2.32s\n",
      "182:\tlearn: 0.0015389\ttotal: 3.6s\tremaining: 2.3s\n",
      "183:\tlearn: 0.0015259\ttotal: 3.63s\tremaining: 2.29s\n",
      "184:\tlearn: 0.0015113\ttotal: 3.66s\tremaining: 2.27s\n",
      "185:\tlearn: 0.0014965\ttotal: 3.69s\tremaining: 2.26s\n",
      "186:\tlearn: 0.0014768\ttotal: 3.71s\tremaining: 2.24s\n",
      "187:\tlearn: 0.0014693\ttotal: 3.72s\tremaining: 2.22s\n",
      "188:\tlearn: 0.0014271\ttotal: 3.74s\tremaining: 2.2s\n",
      "189:\tlearn: 0.0014158\ttotal: 3.75s\tremaining: 2.17s\n",
      "190:\tlearn: 0.0014046\ttotal: 3.77s\tremaining: 2.15s\n",
      "191:\tlearn: 0.0013898\ttotal: 3.79s\tremaining: 2.13s\n",
      "192:\tlearn: 0.0013688\ttotal: 3.8s\tremaining: 2.11s\n",
      "193:\tlearn: 0.0013535\ttotal: 3.82s\tremaining: 2.08s\n",
      "194:\tlearn: 0.0013474\ttotal: 3.83s\tremaining: 2.06s\n",
      "195:\tlearn: 0.0013299\ttotal: 3.86s\tremaining: 2.05s\n",
      "196:\tlearn: 0.0013237\ttotal: 3.88s\tremaining: 2.03s\n",
      "197:\tlearn: 0.0013136\ttotal: 3.9s\tremaining: 2.01s\n",
      "198:\tlearn: 0.0012903\ttotal: 3.93s\tremaining: 1.99s\n",
      "199:\tlearn: 0.0012784\ttotal: 3.94s\tremaining: 1.97s\n",
      "200:\tlearn: 0.0012665\ttotal: 3.96s\tremaining: 1.95s\n",
      "201:\tlearn: 0.0012576\ttotal: 3.97s\tremaining: 1.93s\n",
      "202:\tlearn: 0.0012506\ttotal: 3.99s\tremaining: 1.91s\n",
      "203:\tlearn: 0.0012388\ttotal: 4s\tremaining: 1.88s\n",
      "204:\tlearn: 0.0012208\ttotal: 4.04s\tremaining: 1.87s\n",
      "205:\tlearn: 0.0012029\ttotal: 4.05s\tremaining: 1.85s\n",
      "206:\tlearn: 0.0011949\ttotal: 4.07s\tremaining: 1.83s\n",
      "207:\tlearn: 0.0011712\ttotal: 4.1s\tremaining: 1.81s\n",
      "208:\tlearn: 0.0011514\ttotal: 4.11s\tremaining: 1.79s\n",
      "209:\tlearn: 0.0011353\ttotal: 4.13s\tremaining: 1.77s\n",
      "210:\tlearn: 0.0011242\ttotal: 4.14s\tremaining: 1.75s\n",
      "211:\tlearn: 0.0011165\ttotal: 4.18s\tremaining: 1.73s\n",
      "212:\tlearn: 0.0011066\ttotal: 4.21s\tremaining: 1.72s\n",
      "213:\tlearn: 0.0010944\ttotal: 4.22s\tremaining: 1.7s\n",
      "214:\tlearn: 0.0010873\ttotal: 4.25s\tremaining: 1.68s\n",
      "215:\tlearn: 0.0010821\ttotal: 4.29s\tremaining: 1.67s\n",
      "216:\tlearn: 0.0010702\ttotal: 4.32s\tremaining: 1.65s\n",
      "217:\tlearn: 0.0010617\ttotal: 4.33s\tremaining: 1.63s\n",
      "218:\tlearn: 0.0010526\ttotal: 4.35s\tremaining: 1.61s\n",
      "219:\tlearn: 0.0010451\ttotal: 4.37s\tremaining: 1.59s\n",
      "220:\tlearn: 0.0010343\ttotal: 4.38s\tremaining: 1.57s\n",
      "221:\tlearn: 0.0010286\ttotal: 4.4s\tremaining: 1.54s\n",
      "222:\tlearn: 0.0010209\ttotal: 4.41s\tremaining: 1.52s\n",
      "223:\tlearn: 0.0010096\ttotal: 4.43s\tremaining: 1.5s\n",
      "224:\tlearn: 0.0009980\ttotal: 4.46s\tremaining: 1.49s\n",
      "225:\tlearn: 0.0009807\ttotal: 4.49s\tremaining: 1.47s\n",
      "226:\tlearn: 0.0009719\ttotal: 4.52s\tremaining: 1.45s\n",
      "227:\tlearn: 0.0009653\ttotal: 4.55s\tremaining: 1.44s\n",
      "228:\tlearn: 0.0009543\ttotal: 4.57s\tremaining: 1.42s\n",
      "229:\tlearn: 0.0009346\ttotal: 4.59s\tremaining: 1.4s\n",
      "230:\tlearn: 0.0009276\ttotal: 4.6s\tremaining: 1.37s\n",
      "231:\tlearn: 0.0009195\ttotal: 4.63s\tremaining: 1.36s\n",
      "232:\tlearn: 0.0009089\ttotal: 4.65s\tremaining: 1.34s\n",
      "233:\tlearn: 0.0008992\ttotal: 4.66s\tremaining: 1.31s\n",
      "234:\tlearn: 0.0008940\ttotal: 4.68s\tremaining: 1.29s\n",
      "235:\tlearn: 0.0008832\ttotal: 4.71s\tremaining: 1.28s\n",
      "236:\tlearn: 0.0008720\ttotal: 4.74s\tremaining: 1.26s\n",
      "237:\tlearn: 0.0008595\ttotal: 4.76s\tremaining: 1.24s\n",
      "238:\tlearn: 0.0008414\ttotal: 4.79s\tremaining: 1.22s\n",
      "239:\tlearn: 0.0008260\ttotal: 4.82s\tremaining: 1.21s\n",
      "240:\tlearn: 0.0008177\ttotal: 4.84s\tremaining: 1.18s\n",
      "241:\tlearn: 0.0008140\ttotal: 4.85s\tremaining: 1.16s\n",
      "242:\tlearn: 0.0008020\ttotal: 4.87s\tremaining: 1.14s\n",
      "243:\tlearn: 0.0007959\ttotal: 4.88s\tremaining: 1.12s\n",
      "244:\tlearn: 0.0007876\ttotal: 4.92s\tremaining: 1.1s\n",
      "245:\tlearn: 0.0007841\ttotal: 4.93s\tremaining: 1.08s\n",
      "246:\tlearn: 0.0007734\ttotal: 4.96s\tremaining: 1.06s\n",
      "247:\tlearn: 0.0007647\ttotal: 4.98s\tremaining: 1.04s\n",
      "248:\tlearn: 0.0007579\ttotal: 4.99s\tremaining: 1.02s\n",
      "249:\tlearn: 0.0007546\ttotal: 5.03s\tremaining: 1s\n",
      "250:\tlearn: 0.0007422\ttotal: 5.04s\tremaining: 984ms\n",
      "251:\tlearn: 0.0007385\ttotal: 5.06s\tremaining: 963ms\n",
      "252:\tlearn: 0.0007206\ttotal: 5.09s\tremaining: 945ms\n",
      "253:\tlearn: 0.0007164\ttotal: 5.1s\tremaining: 924ms\n",
      "254:\tlearn: 0.0007096\ttotal: 5.11s\tremaining: 903ms\n",
      "255:\tlearn: 0.0007003\ttotal: 5.15s\tremaining: 885ms\n",
      "256:\tlearn: 0.0006934\ttotal: 5.17s\tremaining: 864ms\n",
      "257:\tlearn: 0.0006876\ttotal: 5.18s\tremaining: 843ms\n",
      "258:\tlearn: 0.0006746\ttotal: 5.21s\tremaining: 825ms\n",
      "259:\tlearn: 0.0006686\ttotal: 5.24s\tremaining: 807ms\n",
      "260:\tlearn: 0.0006624\ttotal: 5.26s\tremaining: 786ms\n",
      "261:\tlearn: 0.0006584\ttotal: 5.27s\tremaining: 765ms\n",
      "262:\tlearn: 0.0006550\ttotal: 5.29s\tremaining: 744ms\n",
      "263:\tlearn: 0.0006489\ttotal: 5.31s\tremaining: 724ms\n",
      "264:\tlearn: 0.0006464\ttotal: 5.32s\tremaining: 703ms\n",
      "265:\tlearn: 0.0006414\ttotal: 5.34s\tremaining: 682ms\n",
      "266:\tlearn: 0.0006387\ttotal: 5.37s\tremaining: 664ms\n",
      "267:\tlearn: 0.0006338\ttotal: 5.38s\tremaining: 643ms\n",
      "268:\tlearn: 0.0006258\ttotal: 5.4s\tremaining: 622ms\n",
      "269:\tlearn: 0.0006194\ttotal: 5.43s\tremaining: 603ms\n",
      "270:\tlearn: 0.0006162\ttotal: 5.45s\tremaining: 583ms\n",
      "271:\tlearn: 0.0006110\ttotal: 5.46s\tremaining: 562ms\n",
      "272:\tlearn: 0.0006087\ttotal: 5.48s\tremaining: 542ms\n",
      "273:\tlearn: 0.0006029\ttotal: 5.49s\tremaining: 521ms\n",
      "274:\tlearn: 0.0005998\ttotal: 5.51s\tremaining: 501ms\n",
      "275:\tlearn: 0.0005913\ttotal: 5.52s\tremaining: 480ms\n",
      "276:\tlearn: 0.0005867\ttotal: 5.55s\tremaining: 461ms\n",
      "277:\tlearn: 0.0005797\ttotal: 5.57s\tremaining: 441ms\n",
      "278:\tlearn: 0.0005714\ttotal: 5.6s\tremaining: 422ms\n",
      "279:\tlearn: 0.0005664\ttotal: 5.62s\tremaining: 401ms\n",
      "280:\tlearn: 0.0005592\ttotal: 5.65s\tremaining: 382ms\n",
      "281:\tlearn: 0.0005526\ttotal: 5.68s\tremaining: 363ms\n",
      "282:\tlearn: 0.0005513\ttotal: 5.71s\tremaining: 343ms\n",
      "283:\tlearn: 0.0005479\ttotal: 5.73s\tremaining: 323ms\n",
      "284:\tlearn: 0.0005461\ttotal: 5.76s\tremaining: 303ms\n",
      "285:\tlearn: 0.0005420\ttotal: 5.78s\tremaining: 283ms\n",
      "286:\tlearn: 0.0005372\ttotal: 5.81s\tremaining: 263ms\n",
      "287:\tlearn: 0.0005337\ttotal: 5.82s\tremaining: 243ms\n",
      "288:\tlearn: 0.0005288\ttotal: 5.84s\tremaining: 222ms\n",
      "289:\tlearn: 0.0005253\ttotal: 5.85s\tremaining: 202ms\n",
      "290:\tlearn: 0.0005227\ttotal: 5.88s\tremaining: 182ms\n",
      "291:\tlearn: 0.0005199\ttotal: 5.9s\tremaining: 162ms\n",
      "292:\tlearn: 0.0005169\ttotal: 5.93s\tremaining: 142ms\n",
      "293:\tlearn: 0.0005133\ttotal: 5.95s\tremaining: 121ms\n",
      "294:\tlearn: 0.0005109\ttotal: 5.96s\tremaining: 101ms\n",
      "295:\tlearn: 0.0005081\ttotal: 6s\tremaining: 81ms\n",
      "296:\tlearn: 0.0005059\ttotal: 6.03s\tremaining: 60.9ms\n",
      "297:\tlearn: 0.0004993\ttotal: 6.04s\tremaining: 40.6ms\n",
      "298:\tlearn: 0.0004953\ttotal: 6.06s\tremaining: 20.3ms\n",
      "299:\tlearn: 0.0004920\ttotal: 6.09s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce90babce4f40e3b25f8628f78336d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.05408094227677801, Recall = 0.0044444444444444444, Aging Rate = 0.0011533658396901026, Precision = 0.006896551724137931, f1 = 0.005405405405405405\n",
      "Epoch 2: Train Loss = 0.026632566065355374, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.026096953775561014, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.026016514865203018, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.025642839855722327, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02483594966640013, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.024977231951433507, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.024806601688240348, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.024617360569076208, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.024450506414581297, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.024225365742751387, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.023858072716327876, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.024042735861457283, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.023811912463451834, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.023624757929668422, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.023395023731996877, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.023101246857145798, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.022714543394401304, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.022890687955098386, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.022599082731934096, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 18: Train Loss = 0.022344415869337223, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 19: Train Loss = 0.022059240075212698, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 20: Train Loss = 0.021791805014489348, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02130382836221433, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 21: Train Loss = 0.021458306427969875, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 22: Train Loss = 0.021230689785391946, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 23: Train Loss = 0.020978584547790096, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 24: Train Loss = 0.020720483968851115, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 25: Train Loss = 0.0204040578703302, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.019961058691213313, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 26: Train Loss = 0.020181067899279443, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 27: Train Loss = 0.01996569253075518, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 28: Train Loss = 0.019697316761098135, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 29: Train Loss = 0.01945947561426969, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 30: Train Loss = 0.019190745839800606, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.018951320057445535, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 31: Train Loss = 0.01900986881126647, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 32: Train Loss = 0.01873208711554787, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 33: Train Loss = 0.018532501929770556, Recall = 0.0044444444444444444, Aging Rate = 7.95424717027657e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.018347995706066707, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 35: Train Loss = 0.018191998474476542, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.017590417612094764, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 36: Train Loss = 0.01786828199291545, Recall = 0.008888888888888889, Aging Rate = 1.590849434055314e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01778200240027871, Recall = 0.0044444444444444444, Aging Rate = 7.95424717027657e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.017581838464404132, Recall = 0.022222222222222223, Aging Rate = 3.9771235851382846e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.01738287118167876, Recall = 0.017777777777777778, Aging Rate = 3.181698868110628e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.017214241896596965, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.016887395582533982, Recall = 0.04, Aging Rate = 0.0001034052132135954, precision = 0.6923076923076923\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.017125697752873478, Recall = 0.008888888888888889, Aging Rate = 1.590849434055314e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.016894787318158633, Recall = 0.0044444444444444444, Aging Rate = 7.95424717027657e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.016769933643280877, Recall = 0.017777777777777778, Aging Rate = 3.9771235851382846e-05, Precision = 0.8, f1 = 0.034782608695652174\n",
      "Epoch 44: Train Loss = 0.016606422473545508, Recall = 0.017777777777777778, Aging Rate = 3.181698868110628e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.01644209464530214, Recall = 0.035555555555555556, Aging Rate = 7.158822453248912e-05, Precision = 0.8888888888888888, f1 = 0.06837606837606838\n",
      "Test Loss = 0.016174217301009333, Recall = 0.07111111111111111, Aging Rate = 0.00019090193208663766, precision = 0.6666666666666666\n",
      "\n",
      "Epoch 46: Train Loss = 0.0162583523593713, Recall = 0.022222222222222223, Aging Rate = 4.7725483021659414e-05, Precision = 0.8333333333333334, f1 = 0.043290043290043295\n",
      "Epoch 47: Train Loss = 0.016138571071035935, Recall = 0.03111111111111111, Aging Rate = 5.567973019193598e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016034462688317652, Recall = 0.05333333333333334, Aging Rate = 9.545096604331883e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.015923608319952418, Recall = 0.044444444444444446, Aging Rate = 7.954247170276569e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01581859498188031, Recall = 0.044444444444444446, Aging Rate = 8.749671887304226e-05, Precision = 0.9090909090909091, f1 = 0.08474576271186442\n",
      "Test Loss = 0.015393070470752725, Recall = 0.07111111111111111, Aging Rate = 0.00014317644906497825, precision = 0.8888888888888888\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.01567572256653823, Recall = 0.03111111111111111, Aging Rate = 6.363397736221256e-05, Precision = 0.875, f1 = 0.060085836909871244\n",
      "Epoch 52: Train Loss = 0.015474202992544215, Recall = 0.04, Aging Rate = 9.545096604331883e-05, Precision = 0.75, f1 = 0.07594936708860758\n",
      "Epoch 53: Train Loss = 0.015467650753062731, Recall = 0.05333333333333334, Aging Rate = 0.0001034052132135954, Precision = 0.9230769230769231, f1 = 0.10084033613445378\n",
      "Epoch 54: Train Loss = 0.01524431965076456, Recall = 0.08, Aging Rate = 0.00019090193208663766, Precision = 0.75, f1 = 0.14457831325301204\n",
      "Epoch 55: Train Loss = 0.015231875756883614, Recall = 0.04, Aging Rate = 7.954247170276569e-05, Precision = 0.9, f1 = 0.07659574468085106\n",
      "Test Loss = 0.015017982943515681, Recall = 0.17333333333333334, Aging Rate = 0.00038975811134355187, precision = 0.7959183673469388\n",
      "\n",
      "Epoch 56: Train Loss = 0.015091898303418004, Recall = 0.07555555555555556, Aging Rate = 0.0001829476849163611, Precision = 0.7391304347826086, f1 = 0.13709677419354838\n",
      "Epoch 57: Train Loss = 0.015024235005480993, Recall = 0.07555555555555556, Aging Rate = 0.00014317644906497825, Precision = 0.9444444444444444, f1 = 0.139917695473251\n",
      "Epoch 58: Train Loss = 0.014853749749869787, Recall = 0.07555555555555556, Aging Rate = 0.00016703919057580794, Precision = 0.8095238095238095, f1 = 0.13821138211382114\n",
      "Epoch 59: Train Loss = 0.01481415161737834, Recall = 0.06666666666666667, Aging Rate = 0.00015908494340553139, Precision = 0.75, f1 = 0.12244897959183675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train Loss = 0.014655811037544115, Recall = 0.06666666666666667, Aging Rate = 0.00015908494340553139, Precision = 0.75, f1 = 0.12244897959183675\n",
      "Test Loss = 0.014309140060925471, Recall = 0.022222222222222223, Aging Rate = 3.9771235851382846e-05, precision = 1.0\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.014599728448362192, Recall = 0.08, Aging Rate = 0.00019090193208663766, Precision = 0.75, f1 = 0.14457831325301204\n",
      "Epoch 62: Train Loss = 0.014504374558348038, Recall = 0.08888888888888889, Aging Rate = 0.00019090193208663766, Precision = 0.8333333333333334, f1 = 0.1606425702811245\n",
      "Epoch 63: Train Loss = 0.014416283403218036, Recall = 0.09777777777777778, Aging Rate = 0.0002068104264271908, Precision = 0.8461538461538461, f1 = 0.17529880478087653\n",
      "Epoch 64: Train Loss = 0.014326250400174304, Recall = 0.08444444444444445, Aging Rate = 0.0002068104264271908, Precision = 0.7307692307692307, f1 = 0.15139442231075698\n",
      "Epoch 65: Train Loss = 0.014247262862949183, Recall = 0.08, Aging Rate = 0.0001829476849163611, Precision = 0.782608695652174, f1 = 0.14516129032258066\n",
      "Test Loss = 0.013825702509592506, Recall = 0.16444444444444445, Aging Rate = 0.0003579411226624456, precision = 0.8222222222222222\n",
      "\n",
      "Epoch 66: Train Loss = 0.014156190349816218, Recall = 0.09333333333333334, Aging Rate = 0.00022271892076774393, Precision = 0.75, f1 = 0.16600790513833993\n",
      "Epoch 67: Train Loss = 0.014016503272782692, Recall = 0.12, Aging Rate = 0.00025453590944885023, Precision = 0.84375, f1 = 0.2101167315175097\n",
      "Epoch 68: Train Loss = 0.014008836967662414, Recall = 0.09777777777777778, Aging Rate = 0.00019885617925691424, Precision = 0.88, f1 = 0.17600000000000002\n",
      "Epoch 69: Train Loss = 0.013846781746472183, Recall = 0.1111111111111111, Aging Rate = 0.00025453590944885023, Precision = 0.78125, f1 = 0.19455252918287935\n",
      "Epoch 70: Train Loss = 0.013832260025577454, Recall = 0.10666666666666667, Aging Rate = 0.0002306731679380205, Precision = 0.8275862068965517, f1 = 0.18897637795275593\n",
      "Test Loss = 0.01329757991785446, Recall = 0.1111111111111111, Aging Rate = 0.0002306731679380205, precision = 0.8620689655172413\n",
      "\n",
      "Epoch 71: Train Loss = 0.013718074822047514, Recall = 0.10222222222222223, Aging Rate = 0.0002306731679380205, Precision = 0.7931034482758621, f1 = 0.18110236220472442\n",
      "Epoch 72: Train Loss = 0.013658862083462321, Recall = 0.12444444444444444, Aging Rate = 0.0002863528981299565, Precision = 0.7777777777777778, f1 = 0.21455938697318008\n",
      "Epoch 73: Train Loss = 0.013571102874268793, Recall = 0.11555555555555555, Aging Rate = 0.0002465816622785736, Precision = 0.8387096774193549, f1 = 0.203125\n",
      "Epoch 74: Train Loss = 0.013498179008125642, Recall = 0.12, Aging Rate = 0.00023862741510829706, Precision = 0.9, f1 = 0.21176470588235294\n",
      "Epoch 75: Train Loss = 0.013510075993063387, Recall = 0.11555555555555555, Aging Rate = 0.00025453590944885023, Precision = 0.8125, f1 = 0.20233463035019456\n",
      "Test Loss = 0.012932241034983942, Recall = 0.13333333333333333, Aging Rate = 0.00027044440378940334, precision = 0.8823529411764706\n",
      "\n",
      "Epoch 76: Train Loss = 0.01345883024257988, Recall = 0.11555555555555555, Aging Rate = 0.00025453590944885023, Precision = 0.8125, f1 = 0.20233463035019456\n",
      "Epoch 77: Train Loss = 0.013335064048149867, Recall = 0.14666666666666667, Aging Rate = 0.0003340783811516159, Precision = 0.7857142857142857, f1 = 0.24719101123595508\n",
      "Epoch 78: Train Loss = 0.013288872225146822, Recall = 0.1288888888888889, Aging Rate = 0.00027839865095967994, Precision = 0.8285714285714286, f1 = 0.22307692307692306\n",
      "Epoch 79: Train Loss = 0.013244027907540996, Recall = 0.12444444444444444, Aging Rate = 0.0002624901566191268, Precision = 0.8484848484848485, f1 = 0.21705426356589147\n",
      "Epoch 80: Train Loss = 0.013133404377138927, Recall = 0.13777777777777778, Aging Rate = 0.0002863528981299565, Precision = 0.8611111111111112, f1 = 0.23754789272030652\n",
      "Test Loss = 0.012790498732080696, Recall = 0.09777777777777778, Aging Rate = 0.00019090193208663766, precision = 0.9166666666666666\n",
      "\n",
      "Epoch 81: Train Loss = 0.013044937397300569, Recall = 0.1288888888888889, Aging Rate = 0.00027839865095967994, Precision = 0.8285714285714286, f1 = 0.22307692307692306\n",
      "Epoch 82: Train Loss = 0.012969254704134032, Recall = 0.14666666666666667, Aging Rate = 0.0003340783811516159, Precision = 0.7857142857142857, f1 = 0.24719101123595508\n",
      "Epoch 83: Train Loss = 0.012931626329517408, Recall = 0.14666666666666667, Aging Rate = 0.00031816988681106277, Precision = 0.825, f1 = 0.2490566037735849\n",
      "Epoch 84: Train Loss = 0.012812498617587393, Recall = 0.1688888888888889, Aging Rate = 0.0003420326283218925, Precision = 0.8837209302325582, f1 = 0.2835820895522388\n",
      "Epoch 85: Train Loss = 0.012835243896577402, Recall = 0.14666666666666667, Aging Rate = 0.00029430714530023305, Precision = 0.8918918918918919, f1 = 0.25190839694656486\n",
      "Test Loss = 0.012415319366777061, Recall = 0.20444444444444446, Aging Rate = 0.000461346335876041, precision = 0.7931034482758621\n",
      "\n",
      "Epoch 86: Train Loss = 0.012737710616169354, Recall = 0.15555555555555556, Aging Rate = 0.0003420326283218925, Precision = 0.813953488372093, f1 = 0.26119402985074625\n",
      "Epoch 87: Train Loss = 0.012760324145357327, Recall = 0.1288888888888889, Aging Rate = 0.00027839865095967994, Precision = 0.8285714285714286, f1 = 0.22307692307692306\n",
      "Epoch 88: Train Loss = 0.012651922886975404, Recall = 0.16, Aging Rate = 0.0003340783811516159, Precision = 0.8571428571428571, f1 = 0.2696629213483146\n",
      "Epoch 89: Train Loss = 0.012650292764589966, Recall = 0.13333333333333333, Aging Rate = 0.00027839865095967994, Precision = 0.8571428571428571, f1 = 0.23076923076923078\n",
      "Epoch 90: Train Loss = 0.01257957760063557, Recall = 0.16444444444444445, Aging Rate = 0.0003658953698327222, Precision = 0.8043478260869565, f1 = 0.2730627306273063\n",
      "Test Loss = 0.012055806469885019, Recall = 0.20444444444444446, Aging Rate = 0.0003977123585138285, precision = 0.92\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.01248403845017163, Recall = 0.17777777777777778, Aging Rate = 0.0003658953698327222, Precision = 0.8695652173913043, f1 = 0.2952029520295203\n",
      "Epoch 92: Train Loss = 0.012494301753775899, Recall = 0.1688888888888889, Aging Rate = 0.00037384961700299876, Precision = 0.8085106382978723, f1 = 0.27941176470588236\n",
      "Epoch 93: Train Loss = 0.012377205768611685, Recall = 0.1511111111111111, Aging Rate = 0.00031816988681106277, Precision = 0.85, f1 = 0.25660377358490566\n",
      "Epoch 94: Train Loss = 0.012325959378507384, Recall = 0.17333333333333334, Aging Rate = 0.0003658953698327222, Precision = 0.8478260869565217, f1 = 0.2878228782287823\n",
      "Epoch 95: Train Loss = 0.012394288359293161, Recall = 0.13777777777777778, Aging Rate = 0.0003102156396407862, Precision = 0.7948717948717948, f1 = 0.23484848484848483\n",
      "Test Loss = 0.011821241316145571, Recall = 0.19111111111111112, Aging Rate = 0.00037384961700299876, precision = 0.9148936170212766\n",
      "\n",
      "Epoch 96: Train Loss = 0.012314063450404525, Recall = 0.1688888888888889, Aging Rate = 0.0003420326283218925, Precision = 0.8837209302325582, f1 = 0.2835820895522388\n",
      "Epoch 97: Train Loss = 0.012194899852003067, Recall = 0.18666666666666668, Aging Rate = 0.0003977123585138285, Precision = 0.84, f1 = 0.3054545454545455\n",
      "Epoch 98: Train Loss = 0.012228986990063641, Recall = 0.14222222222222222, Aging Rate = 0.00031816988681106277, Precision = 0.8, f1 = 0.24150943396226415\n",
      "Epoch 99: Train Loss = 0.012136972924222724, Recall = 0.19111111111111112, Aging Rate = 0.0003977123585138285, Precision = 0.86, f1 = 0.31272727272727274\n",
      "Epoch 100: Train Loss = 0.012107740207109152, Recall = 0.1688888888888889, Aging Rate = 0.00037384961700299876, Precision = 0.8085106382978723, f1 = 0.27941176470588236\n",
      "Test Loss = 0.011712240335187663, Recall = 0.2088888888888889, Aging Rate = 0.00040566660568410503, precision = 0.9215686274509803\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.01211829067975657, Recall = 0.1688888888888889, Aging Rate = 0.00034998687549216904, Precision = 0.8636363636363636, f1 = 0.28252788104089216\n",
      "Epoch 102: Train Loss = 0.011991067420020637, Recall = 0.16444444444444445, Aging Rate = 0.0003420326283218925, Precision = 0.8604651162790697, f1 = 0.27611940298507465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103: Train Loss = 0.012019862619172011, Recall = 0.18666666666666668, Aging Rate = 0.0003658953698327222, Precision = 0.9130434782608695, f1 = 0.30996309963099633\n",
      "Epoch 104: Train Loss = 0.01198782114837937, Recall = 0.15555555555555556, Aging Rate = 0.0003579411226624456, Precision = 0.7777777777777778, f1 = 0.25925925925925924\n",
      "Epoch 105: Train Loss = 0.011954049223801772, Recall = 0.17333333333333334, Aging Rate = 0.00037384961700299876, Precision = 0.8297872340425532, f1 = 0.2867647058823529\n",
      "Test Loss = 0.011412451805417114, Recall = 0.2311111111111111, Aging Rate = 0.0004931633245571472, precision = 0.8387096774193549\n",
      "\n",
      "Epoch 106: Train Loss = 0.01187890089607776, Recall = 0.17777777777777778, Aging Rate = 0.00042157510002465814, Precision = 0.7547169811320755, f1 = 0.28776978417266186\n",
      "Epoch 107: Train Loss = 0.01187920934844731, Recall = 0.17777777777777778, Aging Rate = 0.00037384961700299876, Precision = 0.851063829787234, f1 = 0.29411764705882354\n",
      "Epoch 108: Train Loss = 0.01178444329583531, Recall = 0.19111111111111112, Aging Rate = 0.0003977123585138285, Precision = 0.86, f1 = 0.31272727272727274\n",
      "Epoch 109: Train Loss = 0.011804894948616772, Recall = 0.19555555555555557, Aging Rate = 0.0004136208528543816, Precision = 0.8461538461538461, f1 = 0.3176895306859206\n",
      "Epoch 110: Train Loss = 0.011787117020545735, Recall = 0.16, Aging Rate = 0.0003340783811516159, Precision = 0.8571428571428571, f1 = 0.2696629213483146\n",
      "Test Loss = 0.011264761291519147, Recall = 0.24888888888888888, Aging Rate = 0.0005567973019193599, precision = 0.8\n",
      "\n",
      "Epoch 111: Train Loss = 0.011726335019518908, Recall = 0.18222222222222223, Aging Rate = 0.00040566660568410503, Precision = 0.803921568627451, f1 = 0.2971014492753623\n",
      "Epoch 112: Train Loss = 0.011689093725304725, Recall = 0.18222222222222223, Aging Rate = 0.00042157510002465814, Precision = 0.7735849056603774, f1 = 0.2949640287769784\n",
      "Epoch 113: Train Loss = 0.011736374893147094, Recall = 0.19111111111111112, Aging Rate = 0.00042157510002465814, Precision = 0.8113207547169812, f1 = 0.3093525179856115\n",
      "Epoch 114: Train Loss = 0.01163167788209808, Recall = 0.20444444444444446, Aging Rate = 0.0003977123585138285, Precision = 0.92, f1 = 0.33454545454545453\n",
      "Epoch 115: Train Loss = 0.01161609250800111, Recall = 0.17333333333333334, Aging Rate = 0.0003818038641732753, Precision = 0.8125, f1 = 0.2857142857142857\n",
      "Test Loss = 0.011093536616563943, Recall = 0.21333333333333335, Aging Rate = 0.00044543784153548786, precision = 0.8571428571428571\n",
      "\n",
      "Epoch 116: Train Loss = 0.011576452281790002, Recall = 0.19111111111111112, Aging Rate = 0.00042157510002465814, Precision = 0.8113207547169812, f1 = 0.3093525179856115\n",
      "Epoch 117: Train Loss = 0.011549650692555282, Recall = 0.19555555555555557, Aging Rate = 0.00040566660568410503, Precision = 0.8627450980392157, f1 = 0.31884057971014496\n",
      "Epoch 118: Train Loss = 0.01149174414090652, Recall = 0.16444444444444445, Aging Rate = 0.0003579411226624456, Precision = 0.8222222222222222, f1 = 0.2740740740740741\n",
      "Epoch 119: Train Loss = 0.01149239139590869, Recall = 0.16, Aging Rate = 0.0003818038641732753, Precision = 0.75, f1 = 0.26373626373626374\n",
      "Epoch 120: Train Loss = 0.011506514804197102, Recall = 0.19555555555555557, Aging Rate = 0.00042157510002465814, Precision = 0.8301886792452831, f1 = 0.3165467625899281\n",
      "Test Loss = 0.011134332216171968, Recall = 0.27555555555555555, Aging Rate = 0.0006363397736221255, precision = 0.775\n",
      "\n",
      "Epoch 121: Train Loss = 0.011392032704049208, Recall = 0.20444444444444446, Aging Rate = 0.00044543784153548786, Precision = 0.8214285714285714, f1 = 0.3274021352313167\n",
      "Epoch 122: Train Loss = 0.011432120604714146, Recall = 0.2088888888888889, Aging Rate = 0.00045339208870576447, Precision = 0.8245614035087719, f1 = 0.3333333333333333\n",
      "Epoch 123: Train Loss = 0.011335455351343622, Recall = 0.18666666666666668, Aging Rate = 0.00038975811134355187, Precision = 0.8571428571428571, f1 = 0.30656934306569344\n",
      "Epoch 124: Train Loss = 0.01137540109200581, Recall = 0.18666666666666668, Aging Rate = 0.00038975811134355187, Precision = 0.8571428571428571, f1 = 0.30656934306569344\n",
      "Epoch 125: Train Loss = 0.011337664664442195, Recall = 0.2, Aging Rate = 0.0004374835943652113, Precision = 0.8181818181818182, f1 = 0.32142857142857145\n",
      "Test Loss = 0.010841946313394807, Recall = 0.2577777777777778, Aging Rate = 0.0005965685377707427, precision = 0.7733333333333333\n",
      "\n",
      "Epoch 126: Train Loss = 0.011340674588535705, Recall = 0.20444444444444446, Aging Rate = 0.00042952934719493475, Precision = 0.8518518518518519, f1 = 0.32974910394265233\n",
      "Epoch 127: Train Loss = 0.011297263647818379, Recall = 0.2, Aging Rate = 0.00040566660568410503, Precision = 0.8823529411764706, f1 = 0.3260869565217392\n",
      "Epoch 128: Train Loss = 0.011263646167847694, Recall = 0.2, Aging Rate = 0.000461346335876041, Precision = 0.7758620689655172, f1 = 0.31802120141342755\n",
      "Epoch 129: Train Loss = 0.01119745928663143, Recall = 0.2, Aging Rate = 0.00044543784153548786, Precision = 0.8035714285714286, f1 = 0.3202846975088968\n",
      "Epoch 130: Train Loss = 0.011216615353789837, Recall = 0.21333333333333335, Aging Rate = 0.000461346335876041, Precision = 0.8275862068965517, f1 = 0.3392226148409895\n",
      "Test Loss = 0.010712722653465468, Recall = 0.26666666666666666, Aging Rate = 0.0005329345604085301, precision = 0.8955223880597015\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.011215138012388687, Recall = 0.18666666666666668, Aging Rate = 0.0004136208528543816, Precision = 0.8076923076923077, f1 = 0.30324909747292417\n",
      "Epoch 132: Train Loss = 0.011150798306136692, Recall = 0.2088888888888889, Aging Rate = 0.00045339208870576447, Precision = 0.8245614035087719, f1 = 0.3333333333333333\n",
      "Epoch 133: Train Loss = 0.011131968891163689, Recall = 0.2, Aging Rate = 0.0004374835943652113, Precision = 0.8181818181818182, f1 = 0.32142857142857145\n",
      "Epoch 134: Train Loss = 0.011119793183488543, Recall = 0.21333333333333335, Aging Rate = 0.000461346335876041, Precision = 0.8275862068965517, f1 = 0.3392226148409895\n",
      "Epoch 135: Train Loss = 0.011122741163707789, Recall = 0.19555555555555557, Aging Rate = 0.00040566660568410503, Precision = 0.8627450980392157, f1 = 0.31884057971014496\n",
      "Test Loss = 0.010610516850716026, Recall = 0.26666666666666666, Aging Rate = 0.0005567973019193599, precision = 0.8571428571428571\n",
      "\n",
      "Epoch 136: Train Loss = 0.011052948752117944, Recall = 0.20444444444444446, Aging Rate = 0.00042952934719493475, Precision = 0.8518518518518519, f1 = 0.32974910394265233\n",
      "Epoch 137: Train Loss = 0.011054149755037284, Recall = 0.2, Aging Rate = 0.00042157510002465814, Precision = 0.8490566037735849, f1 = 0.32374100719424465\n",
      "Epoch 138: Train Loss = 0.011041912236173413, Recall = 0.21777777777777776, Aging Rate = 0.00045339208870576447, Precision = 0.8596491228070176, f1 = 0.34751773049645385\n",
      "Epoch 139: Train Loss = 0.011071684829403535, Recall = 0.2, Aging Rate = 0.0004374835943652113, Precision = 0.8181818181818182, f1 = 0.32142857142857145\n",
      "Epoch 140: Train Loss = 0.011029572529270884, Recall = 0.2088888888888889, Aging Rate = 0.00044543784153548786, Precision = 0.8392857142857143, f1 = 0.33451957295373674\n",
      "Test Loss = 0.010581195814033606, Recall = 0.2577777777777778, Aging Rate = 0.0005249803132382536, precision = 0.8787878787878788\n",
      "\n",
      "Epoch 141: Train Loss = 0.010973325323835787, Recall = 0.21777777777777776, Aging Rate = 0.0004693005830463176, Precision = 0.8305084745762712, f1 = 0.34507042253521125\n",
      "Epoch 142: Train Loss = 0.010995924604667926, Recall = 0.2088888888888889, Aging Rate = 0.000461346335876041, Precision = 0.8103448275862069, f1 = 0.33215547703180215\n",
      "Epoch 143: Train Loss = 0.011014105396539036, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 144: Train Loss = 0.010894750098222864, Recall = 0.21333333333333335, Aging Rate = 0.00042952934719493475, Precision = 0.8888888888888888, f1 = 0.3440860215053763\n",
      "Epoch 145: Train Loss = 0.010885673160095841, Recall = 0.21777777777777776, Aging Rate = 0.0004693005830463176, Precision = 0.8305084745762712, f1 = 0.34507042253521125\n",
      "Test Loss = 0.010413158720912419, Recall = 0.24888888888888888, Aging Rate = 0.00048520907738687074, precision = 0.9180327868852459\n",
      "Model in epoch 145 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146: Train Loss = 0.010952631067816939, Recall = 0.20444444444444446, Aging Rate = 0.00042952934719493475, Precision = 0.8518518518518519, f1 = 0.32974910394265233\n",
      "Epoch 147: Train Loss = 0.010885160326937508, Recall = 0.21333333333333335, Aging Rate = 0.00044543784153548786, Precision = 0.8571428571428571, f1 = 0.3416370106761566\n",
      "Epoch 148: Train Loss = 0.010890424769016062, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 149: Train Loss = 0.010860939988378886, Recall = 0.18666666666666668, Aging Rate = 0.0003977123585138285, Precision = 0.84, f1 = 0.3054545454545455\n",
      "Epoch 150: Train Loss = 0.010858214081266018, Recall = 0.21777777777777776, Aging Rate = 0.00042952934719493475, Precision = 0.9074074074074074, f1 = 0.35125448028673834\n",
      "Test Loss = 0.010436323147753895, Recall = 0.19111111111111112, Aging Rate = 0.00037384961700299876, precision = 0.9148936170212766\n",
      "\n",
      "Epoch 151: Train Loss = 0.01091380442759625, Recall = 0.20444444444444446, Aging Rate = 0.00042952934719493475, Precision = 0.8518518518518519, f1 = 0.32974910394265233\n",
      "Epoch 152: Train Loss = 0.010794476720498448, Recall = 0.2222222222222222, Aging Rate = 0.000461346335876041, Precision = 0.8620689655172413, f1 = 0.35335689045936397\n",
      "Epoch 153: Train Loss = 0.010785290831655334, Recall = 0.2311111111111111, Aging Rate = 0.000517026066067977, Precision = 0.8, f1 = 0.3586206896551724\n",
      "Epoch 154: Train Loss = 0.010801750912507258, Recall = 0.19111111111111112, Aging Rate = 0.0004136208528543816, Precision = 0.8269230769230769, f1 = 0.3104693140794224\n",
      "Epoch 155: Train Loss = 0.010795043172967412, Recall = 0.21777777777777776, Aging Rate = 0.0004931633245571472, Precision = 0.7903225806451613, f1 = 0.3414634146341463\n",
      "Test Loss = 0.010255647672950806, Recall = 0.2222222222222222, Aging Rate = 0.00045339208870576447, precision = 0.8771929824561403\n",
      "\n",
      "Epoch 156: Train Loss = 0.010792450911206081, Recall = 0.2088888888888889, Aging Rate = 0.00042952934719493475, Precision = 0.8703703703703703, f1 = 0.3369175627240143\n",
      "Epoch 157: Train Loss = 0.010729875875979102, Recall = 0.21333333333333335, Aging Rate = 0.00045339208870576447, Precision = 0.8421052631578947, f1 = 0.34042553191489366\n",
      "Epoch 158: Train Loss = 0.010784879910246159, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 159: Train Loss = 0.01071183367779085, Recall = 0.19111111111111112, Aging Rate = 0.00042157510002465814, Precision = 0.8113207547169812, f1 = 0.3093525179856115\n",
      "Epoch 160: Train Loss = 0.010719662473013262, Recall = 0.2222222222222222, Aging Rate = 0.0004693005830463176, Precision = 0.847457627118644, f1 = 0.352112676056338\n",
      "Test Loss = 0.010189368508285517, Recall = 0.24888888888888888, Aging Rate = 0.00048520907738687074, precision = 0.9180327868852459\n",
      "\n",
      "Epoch 161: Train Loss = 0.010657625438564128, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 162: Train Loss = 0.010644629440779744, Recall = 0.21777777777777776, Aging Rate = 0.0004693005830463176, Precision = 0.8305084745762712, f1 = 0.34507042253521125\n",
      "Epoch 163: Train Loss = 0.010586433499075653, Recall = 0.2088888888888889, Aging Rate = 0.000461346335876041, Precision = 0.8103448275862069, f1 = 0.33215547703180215\n",
      "Epoch 164: Train Loss = 0.010698659699025208, Recall = 0.24, Aging Rate = 0.0005011175717274239, Precision = 0.8571428571428571, f1 = 0.375\n",
      "Epoch 165: Train Loss = 0.010671858436809765, Recall = 0.2, Aging Rate = 0.0004374835943652113, Precision = 0.8181818181818182, f1 = 0.32142857142857145\n",
      "Test Loss = 0.01013462685996204, Recall = 0.26666666666666666, Aging Rate = 0.0005965685377707427, precision = 0.8\n",
      "\n",
      "Epoch 166: Train Loss = 0.010664490795804626, Recall = 0.23555555555555555, Aging Rate = 0.000517026066067977, Precision = 0.8153846153846154, f1 = 0.36551724137931035\n",
      "Epoch 167: Train Loss = 0.010615099006203476, Recall = 0.2311111111111111, Aging Rate = 0.0004931633245571472, Precision = 0.8387096774193549, f1 = 0.36236933797909404\n",
      "Epoch 168: Train Loss = 0.010590520258729853, Recall = 0.2088888888888889, Aging Rate = 0.00044543784153548786, Precision = 0.8392857142857143, f1 = 0.33451957295373674\n",
      "Epoch 169: Train Loss = 0.010596947108771923, Recall = 0.2311111111111111, Aging Rate = 0.0005011175717274239, Precision = 0.8253968253968254, f1 = 0.3611111111111111\n",
      "Epoch 170: Train Loss = 0.010594958519799512, Recall = 0.2222222222222222, Aging Rate = 0.0004693005830463176, Precision = 0.847457627118644, f1 = 0.352112676056338\n",
      "Test Loss = 0.010172732468767017, Recall = 0.29333333333333333, Aging Rate = 0.0006522482679626787, precision = 0.8048780487804879\n",
      "\n",
      "Epoch 171: Train Loss = 0.010498977485091343, Recall = 0.2, Aging Rate = 0.00042952934719493475, Precision = 0.8333333333333334, f1 = 0.3225806451612903\n",
      "Epoch 172: Train Loss = 0.010592719050925785, Recall = 0.21777777777777776, Aging Rate = 0.0004931633245571472, Precision = 0.7903225806451613, f1 = 0.3414634146341463\n",
      "Epoch 173: Train Loss = 0.010559361324907146, Recall = 0.19111111111111112, Aging Rate = 0.0003977123585138285, Precision = 0.86, f1 = 0.31272727272727274\n",
      "Epoch 174: Train Loss = 0.010516058588787678, Recall = 0.24, Aging Rate = 0.0005090718188977005, Precision = 0.84375, f1 = 0.37370242214532867\n",
      "Epoch 175: Train Loss = 0.010560813166912665, Recall = 0.2, Aging Rate = 0.0004136208528543816, Precision = 0.8653846153846154, f1 = 0.32490974729241884\n",
      "Test Loss = 0.010091075753490624, Recall = 0.28444444444444444, Aging Rate = 0.0006363397736221255, precision = 0.8\n",
      "\n",
      "Epoch 176: Train Loss = 0.010490995386587113, Recall = 0.2222222222222222, Aging Rate = 0.0005090718188977005, Precision = 0.78125, f1 = 0.3460207612456747\n",
      "Epoch 177: Train Loss = 0.010482102615487956, Recall = 0.21777777777777776, Aging Rate = 0.00047725483021659413, Precision = 0.8166666666666667, f1 = 0.343859649122807\n",
      "Epoch 178: Train Loss = 0.01052465134492884, Recall = 0.2088888888888889, Aging Rate = 0.00045339208870576447, Precision = 0.8245614035087719, f1 = 0.3333333333333333\n",
      "Epoch 179: Train Loss = 0.01044368558587042, Recall = 0.22666666666666666, Aging Rate = 0.0005011175717274239, Precision = 0.8095238095238095, f1 = 0.3541666666666667\n",
      "Epoch 180: Train Loss = 0.010473889200516628, Recall = 0.24888888888888888, Aging Rate = 0.0005488430547490832, Precision = 0.8115942028985508, f1 = 0.38095238095238093\n",
      "Test Loss = 0.010069864318655431, Recall = 0.2088888888888889, Aging Rate = 0.00042157510002465814, precision = 0.8867924528301887\n",
      "\n",
      "Epoch 181: Train Loss = 0.010528059298509945, Recall = 0.21777777777777776, Aging Rate = 0.000461346335876041, Precision = 0.8448275862068966, f1 = 0.34628975265017664\n",
      "Epoch 182: Train Loss = 0.010489066762961867, Recall = 0.2222222222222222, Aging Rate = 0.00048520907738687074, Precision = 0.819672131147541, f1 = 0.3496503496503496\n",
      "Epoch 183: Train Loss = 0.01048336301277777, Recall = 0.22666666666666666, Aging Rate = 0.0005011175717274239, Precision = 0.8095238095238095, f1 = 0.3541666666666667\n",
      "Epoch 184: Train Loss = 0.01045014393992984, Recall = 0.21333333333333335, Aging Rate = 0.0004693005830463176, Precision = 0.8135593220338984, f1 = 0.3380281690140845\n",
      "Epoch 185: Train Loss = 0.010461870788186636, Recall = 0.24, Aging Rate = 0.00048520907738687074, Precision = 0.8852459016393442, f1 = 0.37762237762237766\n",
      "Test Loss = 0.010199799367935877, Recall = 0.3244444444444444, Aging Rate = 0.000739744986835721, precision = 0.7849462365591398\n",
      "\n",
      "Epoch 186: Train Loss = 0.01045016965457793, Recall = 0.21333333333333335, Aging Rate = 0.00045339208870576447, Precision = 0.8421052631578947, f1 = 0.34042553191489366\n",
      "Epoch 187: Train Loss = 0.010427040621466774, Recall = 0.2088888888888889, Aging Rate = 0.00044543784153548786, Precision = 0.8392857142857143, f1 = 0.33451957295373674\n",
      "Epoch 188: Train Loss = 0.010483770772066265, Recall = 0.2088888888888889, Aging Rate = 0.0004374835943652113, Precision = 0.8545454545454545, f1 = 0.33571428571428574\n",
      "Epoch 189: Train Loss = 0.01036852240742423, Recall = 0.25333333333333335, Aging Rate = 0.0005329345604085301, Precision = 0.8507462686567164, f1 = 0.3904109589041096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190: Train Loss = 0.010451102189196437, Recall = 0.20444444444444446, Aging Rate = 0.0004693005830463176, Precision = 0.7796610169491526, f1 = 0.323943661971831\n",
      "Test Loss = 0.010043635447287681, Recall = 0.3111111111111111, Aging Rate = 0.000684065256643785, precision = 0.813953488372093\n",
      "\n",
      "Epoch 191: Train Loss = 0.010410592411698723, Recall = 0.2222222222222222, Aging Rate = 0.000461346335876041, Precision = 0.8620689655172413, f1 = 0.35335689045936397\n",
      "Epoch 192: Train Loss = 0.01040420202056416, Recall = 0.21333333333333335, Aging Rate = 0.00047725483021659413, Precision = 0.8, f1 = 0.3368421052631579\n",
      "Epoch 193: Train Loss = 0.010365432351353067, Recall = 0.21777777777777776, Aging Rate = 0.00048520907738687074, Precision = 0.8032786885245902, f1 = 0.3426573426573426\n",
      "Epoch 194: Train Loss = 0.010369718630211132, Recall = 0.23555555555555555, Aging Rate = 0.000517026066067977, Precision = 0.8153846153846154, f1 = 0.36551724137931035\n",
      "Epoch 195: Train Loss = 0.01038433976774425, Recall = 0.2311111111111111, Aging Rate = 0.00047725483021659413, Precision = 0.8666666666666667, f1 = 0.3649122807017544\n",
      "Test Loss = 0.00985762608759982, Recall = 0.2311111111111111, Aging Rate = 0.00045339208870576447, precision = 0.9122807017543859\n",
      "\n",
      "Training Finished at epoch 195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2839703\ttotal: 29.2ms\tremaining: 8.73s\n",
      "1:\tlearn: 0.1253167\ttotal: 59.1ms\tremaining: 8.8s\n",
      "2:\tlearn: 0.0635610\ttotal: 89.5ms\tremaining: 8.86s\n",
      "3:\tlearn: 0.0374853\ttotal: 106ms\tremaining: 7.82s\n",
      "4:\tlearn: 0.0257752\ttotal: 137ms\tremaining: 8.1s\n",
      "5:\tlearn: 0.0197062\ttotal: 153ms\tremaining: 7.51s\n",
      "6:\tlearn: 0.0164905\ttotal: 168ms\tremaining: 7.02s\n",
      "7:\tlearn: 0.0150808\ttotal: 213ms\tremaining: 7.76s\n",
      "8:\tlearn: 0.0140792\ttotal: 244ms\tremaining: 7.88s\n",
      "9:\tlearn: 0.0133380\ttotal: 262ms\tremaining: 7.6s\n",
      "10:\tlearn: 0.0127018\ttotal: 294ms\tremaining: 7.71s\n",
      "11:\tlearn: 0.0124006\ttotal: 309ms\tremaining: 7.42s\n",
      "12:\tlearn: 0.0121845\ttotal: 325ms\tremaining: 7.17s\n",
      "13:\tlearn: 0.0120232\ttotal: 341ms\tremaining: 6.96s\n",
      "14:\tlearn: 0.0119284\ttotal: 357ms\tremaining: 6.79s\n",
      "15:\tlearn: 0.0118438\ttotal: 373ms\tremaining: 6.62s\n",
      "16:\tlearn: 0.0116231\ttotal: 404ms\tremaining: 6.73s\n",
      "17:\tlearn: 0.0115415\ttotal: 420ms\tremaining: 6.58s\n",
      "18:\tlearn: 0.0114586\ttotal: 451ms\tremaining: 6.67s\n",
      "19:\tlearn: 0.0114029\ttotal: 467ms\tremaining: 6.54s\n",
      "20:\tlearn: 0.0113786\ttotal: 483ms\tremaining: 6.42s\n",
      "21:\tlearn: 0.0112493\ttotal: 499ms\tremaining: 6.3s\n",
      "22:\tlearn: 0.0111206\ttotal: 528ms\tremaining: 6.36s\n",
      "23:\tlearn: 0.0110914\ttotal: 544ms\tremaining: 6.26s\n",
      "24:\tlearn: 0.0109947\ttotal: 560ms\tremaining: 6.16s\n",
      "25:\tlearn: 0.0109331\ttotal: 592ms\tremaining: 6.24s\n",
      "26:\tlearn: 0.0108670\ttotal: 624ms\tremaining: 6.3s\n",
      "27:\tlearn: 0.0107996\ttotal: 655ms\tremaining: 6.37s\n",
      "28:\tlearn: 0.0107106\ttotal: 670ms\tremaining: 6.26s\n",
      "29:\tlearn: 0.0106288\ttotal: 687ms\tremaining: 6.18s\n",
      "30:\tlearn: 0.0105260\ttotal: 703ms\tremaining: 6.1s\n",
      "31:\tlearn: 0.0104165\ttotal: 718ms\tremaining: 6.01s\n",
      "32:\tlearn: 0.0102928\ttotal: 749ms\tremaining: 6.06s\n",
      "33:\tlearn: 0.0100975\ttotal: 782ms\tremaining: 6.12s\n",
      "34:\tlearn: 0.0098854\ttotal: 814ms\tremaining: 6.16s\n",
      "35:\tlearn: 0.0097721\ttotal: 830ms\tremaining: 6.09s\n",
      "36:\tlearn: 0.0096806\ttotal: 860ms\tremaining: 6.11s\n",
      "37:\tlearn: 0.0096024\ttotal: 891ms\tremaining: 6.14s\n",
      "38:\tlearn: 0.0095696\ttotal: 922ms\tremaining: 6.17s\n",
      "39:\tlearn: 0.0094762\ttotal: 954ms\tremaining: 6.2s\n",
      "40:\tlearn: 0.0093987\ttotal: 986ms\tremaining: 6.23s\n",
      "41:\tlearn: 0.0092702\ttotal: 1.02s\tremaining: 6.25s\n",
      "42:\tlearn: 0.0092240\ttotal: 1.03s\tremaining: 6.17s\n",
      "43:\tlearn: 0.0091023\ttotal: 1.06s\tremaining: 6.19s\n",
      "44:\tlearn: 0.0089983\ttotal: 1.08s\tremaining: 6.13s\n",
      "45:\tlearn: 0.0088661\ttotal: 1.1s\tremaining: 6.05s\n",
      "46:\tlearn: 0.0088007\ttotal: 1.13s\tremaining: 6.07s\n",
      "47:\tlearn: 0.0086658\ttotal: 1.14s\tremaining: 6s\n",
      "48:\tlearn: 0.0086354\ttotal: 1.16s\tremaining: 5.94s\n",
      "49:\tlearn: 0.0085621\ttotal: 1.17s\tremaining: 5.87s\n",
      "50:\tlearn: 0.0085083\ttotal: 1.19s\tremaining: 5.81s\n",
      "51:\tlearn: 0.0083778\ttotal: 1.21s\tremaining: 5.76s\n",
      "52:\tlearn: 0.0083133\ttotal: 1.22s\tremaining: 5.7s\n",
      "53:\tlearn: 0.0082430\ttotal: 1.25s\tremaining: 5.71s\n",
      "54:\tlearn: 0.0081559\ttotal: 1.27s\tremaining: 5.66s\n",
      "55:\tlearn: 0.0080585\ttotal: 1.29s\tremaining: 5.6s\n",
      "56:\tlearn: 0.0078987\ttotal: 1.3s\tremaining: 5.55s\n",
      "57:\tlearn: 0.0078110\ttotal: 1.33s\tremaining: 5.56s\n",
      "58:\tlearn: 0.0077431\ttotal: 1.35s\tremaining: 5.51s\n",
      "59:\tlearn: 0.0076395\ttotal: 1.36s\tremaining: 5.46s\n",
      "60:\tlearn: 0.0075406\ttotal: 1.38s\tremaining: 5.41s\n",
      "61:\tlearn: 0.0074311\ttotal: 1.41s\tremaining: 5.42s\n",
      "62:\tlearn: 0.0073701\ttotal: 1.43s\tremaining: 5.37s\n",
      "63:\tlearn: 0.0072966\ttotal: 1.46s\tremaining: 5.38s\n",
      "64:\tlearn: 0.0072160\ttotal: 1.48s\tremaining: 5.33s\n",
      "65:\tlearn: 0.0071007\ttotal: 1.51s\tremaining: 5.34s\n",
      "66:\tlearn: 0.0070155\ttotal: 1.52s\tremaining: 5.3s\n",
      "67:\tlearn: 0.0069626\ttotal: 1.55s\tremaining: 5.3s\n",
      "68:\tlearn: 0.0068998\ttotal: 1.57s\tremaining: 5.25s\n",
      "69:\tlearn: 0.0068175\ttotal: 1.6s\tremaining: 5.26s\n",
      "70:\tlearn: 0.0067497\ttotal: 1.62s\tremaining: 5.21s\n",
      "71:\tlearn: 0.0067070\ttotal: 1.65s\tremaining: 5.22s\n",
      "72:\tlearn: 0.0066145\ttotal: 1.66s\tremaining: 5.17s\n",
      "73:\tlearn: 0.0064766\ttotal: 1.7s\tremaining: 5.18s\n",
      "74:\tlearn: 0.0063957\ttotal: 1.71s\tremaining: 5.13s\n",
      "75:\tlearn: 0.0063242\ttotal: 1.73s\tremaining: 5.09s\n",
      "76:\tlearn: 0.0062648\ttotal: 1.74s\tremaining: 5.05s\n",
      "77:\tlearn: 0.0061866\ttotal: 1.76s\tremaining: 5s\n",
      "78:\tlearn: 0.0060965\ttotal: 1.79s\tremaining: 5.01s\n",
      "79:\tlearn: 0.0060409\ttotal: 1.8s\tremaining: 4.96s\n",
      "80:\tlearn: 0.0059896\ttotal: 1.82s\tremaining: 4.92s\n",
      "81:\tlearn: 0.0059252\ttotal: 1.84s\tremaining: 4.88s\n",
      "82:\tlearn: 0.0058509\ttotal: 1.85s\tremaining: 4.84s\n",
      "83:\tlearn: 0.0058043\ttotal: 1.9s\tremaining: 4.89s\n",
      "84:\tlearn: 0.0057160\ttotal: 1.92s\tremaining: 4.85s\n",
      "85:\tlearn: 0.0056525\ttotal: 1.93s\tremaining: 4.81s\n",
      "86:\tlearn: 0.0055870\ttotal: 1.95s\tremaining: 4.77s\n",
      "87:\tlearn: 0.0055135\ttotal: 1.96s\tremaining: 4.73s\n",
      "88:\tlearn: 0.0054631\ttotal: 1.98s\tremaining: 4.69s\n",
      "89:\tlearn: 0.0053705\ttotal: 2s\tremaining: 4.66s\n",
      "90:\tlearn: 0.0053074\ttotal: 2.01s\tremaining: 4.62s\n",
      "91:\tlearn: 0.0052488\ttotal: 2.03s\tremaining: 4.58s\n",
      "92:\tlearn: 0.0051847\ttotal: 2.04s\tremaining: 4.55s\n",
      "93:\tlearn: 0.0051272\ttotal: 2.07s\tremaining: 4.54s\n",
      "94:\tlearn: 0.0050822\ttotal: 2.09s\tremaining: 4.5s\n",
      "95:\tlearn: 0.0050374\ttotal: 2.1s\tremaining: 4.46s\n",
      "96:\tlearn: 0.0049827\ttotal: 2.12s\tremaining: 4.43s\n",
      "97:\tlearn: 0.0049111\ttotal: 2.13s\tremaining: 4.4s\n",
      "98:\tlearn: 0.0048707\ttotal: 2.15s\tremaining: 4.36s\n",
      "99:\tlearn: 0.0047473\ttotal: 2.16s\tremaining: 4.33s\n",
      "100:\tlearn: 0.0046966\ttotal: 2.18s\tremaining: 4.29s\n",
      "101:\tlearn: 0.0046362\ttotal: 2.21s\tremaining: 4.29s\n",
      "102:\tlearn: 0.0045877\ttotal: 2.24s\tremaining: 4.29s\n",
      "103:\tlearn: 0.0045439\ttotal: 2.26s\tremaining: 4.25s\n",
      "104:\tlearn: 0.0044993\ttotal: 2.29s\tremaining: 4.25s\n",
      "105:\tlearn: 0.0044466\ttotal: 2.3s\tremaining: 4.21s\n",
      "106:\tlearn: 0.0043858\ttotal: 2.35s\tremaining: 4.24s\n",
      "107:\tlearn: 0.0043171\ttotal: 2.37s\tremaining: 4.21s\n",
      "108:\tlearn: 0.0042703\ttotal: 2.38s\tremaining: 4.17s\n",
      "109:\tlearn: 0.0042078\ttotal: 2.4s\tremaining: 4.14s\n",
      "110:\tlearn: 0.0041725\ttotal: 2.43s\tremaining: 4.14s\n",
      "111:\tlearn: 0.0041379\ttotal: 2.46s\tremaining: 4.13s\n",
      "112:\tlearn: 0.0041157\ttotal: 2.48s\tremaining: 4.1s\n",
      "113:\tlearn: 0.0040822\ttotal: 2.51s\tremaining: 4.09s\n",
      "114:\tlearn: 0.0040530\ttotal: 2.52s\tremaining: 4.06s\n",
      "115:\tlearn: 0.0040120\ttotal: 2.55s\tremaining: 4.05s\n",
      "116:\tlearn: 0.0039729\ttotal: 2.57s\tremaining: 4.02s\n",
      "117:\tlearn: 0.0039044\ttotal: 2.59s\tremaining: 3.99s\n",
      "118:\tlearn: 0.0038572\ttotal: 2.63s\tremaining: 4s\n",
      "119:\tlearn: 0.0037752\ttotal: 2.66s\tremaining: 4s\n",
      "120:\tlearn: 0.0037385\ttotal: 2.68s\tremaining: 3.96s\n",
      "121:\tlearn: 0.0036870\ttotal: 2.71s\tremaining: 3.95s\n",
      "122:\tlearn: 0.0036566\ttotal: 2.73s\tremaining: 3.92s\n",
      "123:\tlearn: 0.0036161\ttotal: 2.74s\tremaining: 3.89s\n",
      "124:\tlearn: 0.0035996\ttotal: 2.77s\tremaining: 3.88s\n",
      "125:\tlearn: 0.0035352\ttotal: 2.79s\tremaining: 3.85s\n",
      "126:\tlearn: 0.0034941\ttotal: 2.82s\tremaining: 3.84s\n",
      "127:\tlearn: 0.0034616\ttotal: 2.85s\tremaining: 3.83s\n",
      "128:\tlearn: 0.0034344\ttotal: 2.88s\tremaining: 3.82s\n",
      "129:\tlearn: 0.0033921\ttotal: 2.91s\tremaining: 3.81s\n",
      "130:\tlearn: 0.0033641\ttotal: 2.95s\tremaining: 3.8s\n",
      "131:\tlearn: 0.0033030\ttotal: 2.96s\tremaining: 3.77s\n",
      "132:\tlearn: 0.0032767\ttotal: 2.98s\tremaining: 3.74s\n",
      "133:\tlearn: 0.0032172\ttotal: 2.99s\tremaining: 3.71s\n",
      "134:\tlearn: 0.0032018\ttotal: 3.01s\tremaining: 3.68s\n",
      "135:\tlearn: 0.0031635\ttotal: 3.04s\tremaining: 3.67s\n",
      "136:\tlearn: 0.0031226\ttotal: 3.07s\tremaining: 3.65s\n",
      "137:\tlearn: 0.0030904\ttotal: 3.09s\tremaining: 3.62s\n",
      "138:\tlearn: 0.0030617\ttotal: 3.1s\tremaining: 3.59s\n",
      "139:\tlearn: 0.0030063\ttotal: 3.12s\tremaining: 3.56s\n",
      "140:\tlearn: 0.0029749\ttotal: 3.13s\tremaining: 3.53s\n",
      "141:\tlearn: 0.0029266\ttotal: 3.15s\tremaining: 3.5s\n",
      "142:\tlearn: 0.0029009\ttotal: 3.17s\tremaining: 3.48s\n",
      "143:\tlearn: 0.0028638\ttotal: 3.18s\tremaining: 3.45s\n",
      "144:\tlearn: 0.0028258\ttotal: 3.21s\tremaining: 3.44s\n",
      "145:\tlearn: 0.0027837\ttotal: 3.23s\tremaining: 3.4s\n",
      "146:\tlearn: 0.0027549\ttotal: 3.26s\tremaining: 3.39s\n",
      "147:\tlearn: 0.0027259\ttotal: 3.28s\tremaining: 3.37s\n",
      "148:\tlearn: 0.0027003\ttotal: 3.29s\tremaining: 3.34s\n",
      "149:\tlearn: 0.0026805\ttotal: 3.32s\tremaining: 3.32s\n",
      "150:\tlearn: 0.0026294\ttotal: 3.34s\tremaining: 3.29s\n",
      "151:\tlearn: 0.0025982\ttotal: 3.35s\tremaining: 3.27s\n",
      "152:\tlearn: 0.0025498\ttotal: 3.37s\tremaining: 3.23s\n",
      "153:\tlearn: 0.0025266\ttotal: 3.39s\tremaining: 3.21s\n",
      "154:\tlearn: 0.0024748\ttotal: 3.4s\tremaining: 3.18s\n",
      "155:\tlearn: 0.0024353\ttotal: 3.42s\tremaining: 3.16s\n",
      "156:\tlearn: 0.0024117\ttotal: 3.45s\tremaining: 3.14s\n",
      "157:\tlearn: 0.0023483\ttotal: 3.47s\tremaining: 3.12s\n",
      "158:\tlearn: 0.0023163\ttotal: 3.5s\tremaining: 3.1s\n",
      "159:\tlearn: 0.0022912\ttotal: 3.53s\tremaining: 3.09s\n",
      "160:\tlearn: 0.0022617\ttotal: 3.56s\tremaining: 3.07s\n",
      "161:\tlearn: 0.0022395\ttotal: 3.58s\tremaining: 3.05s\n",
      "162:\tlearn: 0.0022198\ttotal: 3.59s\tremaining: 3.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163:\tlearn: 0.0021821\ttotal: 3.61s\tremaining: 2.99s\n",
      "164:\tlearn: 0.0021510\ttotal: 3.63s\tremaining: 2.97s\n",
      "165:\tlearn: 0.0021409\ttotal: 3.65s\tremaining: 2.95s\n",
      "166:\tlearn: 0.0021223\ttotal: 3.68s\tremaining: 2.93s\n",
      "167:\tlearn: 0.0020887\ttotal: 3.69s\tremaining: 2.9s\n",
      "168:\tlearn: 0.0020617\ttotal: 3.73s\tremaining: 2.89s\n",
      "169:\tlearn: 0.0020335\ttotal: 3.74s\tremaining: 2.86s\n",
      "170:\tlearn: 0.0020022\ttotal: 3.78s\tremaining: 2.85s\n",
      "171:\tlearn: 0.0019801\ttotal: 3.79s\tremaining: 2.82s\n",
      "172:\tlearn: 0.0019555\ttotal: 3.82s\tremaining: 2.81s\n",
      "173:\tlearn: 0.0019321\ttotal: 3.83s\tremaining: 2.77s\n",
      "174:\tlearn: 0.0019047\ttotal: 3.87s\tremaining: 2.76s\n",
      "175:\tlearn: 0.0018850\ttotal: 3.9s\tremaining: 2.75s\n",
      "176:\tlearn: 0.0018648\ttotal: 3.95s\tremaining: 2.74s\n",
      "177:\tlearn: 0.0018482\ttotal: 3.96s\tremaining: 2.71s\n",
      "178:\tlearn: 0.0018300\ttotal: 3.99s\tremaining: 2.7s\n",
      "179:\tlearn: 0.0018152\ttotal: 4.01s\tremaining: 2.67s\n",
      "180:\tlearn: 0.0017948\ttotal: 4.04s\tremaining: 2.66s\n",
      "181:\tlearn: 0.0017737\ttotal: 4.06s\tremaining: 2.63s\n",
      "182:\tlearn: 0.0017476\ttotal: 4.07s\tremaining: 2.6s\n",
      "183:\tlearn: 0.0017380\ttotal: 4.09s\tremaining: 2.58s\n",
      "184:\tlearn: 0.0017259\ttotal: 4.1s\tremaining: 2.55s\n",
      "185:\tlearn: 0.0017178\ttotal: 4.14s\tremaining: 2.54s\n",
      "186:\tlearn: 0.0016972\ttotal: 4.15s\tremaining: 2.51s\n",
      "187:\tlearn: 0.0016906\ttotal: 4.17s\tremaining: 2.48s\n",
      "188:\tlearn: 0.0016707\ttotal: 4.18s\tremaining: 2.46s\n",
      "189:\tlearn: 0.0016579\ttotal: 4.2s\tremaining: 2.43s\n",
      "190:\tlearn: 0.0016458\ttotal: 4.21s\tremaining: 2.4s\n",
      "191:\tlearn: 0.0016361\ttotal: 4.25s\tremaining: 2.39s\n",
      "192:\tlearn: 0.0016156\ttotal: 4.28s\tremaining: 2.37s\n",
      "193:\tlearn: 0.0015910\ttotal: 4.31s\tremaining: 2.35s\n",
      "194:\tlearn: 0.0015670\ttotal: 4.32s\tremaining: 2.33s\n",
      "195:\tlearn: 0.0015496\ttotal: 4.34s\tremaining: 2.3s\n",
      "196:\tlearn: 0.0015168\ttotal: 4.35s\tremaining: 2.28s\n",
      "197:\tlearn: 0.0014943\ttotal: 4.39s\tremaining: 2.26s\n",
      "198:\tlearn: 0.0014618\ttotal: 4.42s\tremaining: 2.24s\n",
      "199:\tlearn: 0.0014338\ttotal: 4.43s\tremaining: 2.22s\n",
      "200:\tlearn: 0.0014241\ttotal: 4.45s\tremaining: 2.19s\n",
      "201:\tlearn: 0.0014001\ttotal: 4.47s\tremaining: 2.17s\n",
      "202:\tlearn: 0.0013850\ttotal: 4.48s\tremaining: 2.14s\n",
      "203:\tlearn: 0.0013779\ttotal: 4.53s\tremaining: 2.13s\n",
      "204:\tlearn: 0.0013695\ttotal: 4.56s\tremaining: 2.11s\n",
      "205:\tlearn: 0.0013475\ttotal: 4.58s\tremaining: 2.09s\n",
      "206:\tlearn: 0.0013189\ttotal: 4.59s\tremaining: 2.06s\n",
      "207:\tlearn: 0.0012962\ttotal: 4.63s\tremaining: 2.04s\n",
      "208:\tlearn: 0.0012780\ttotal: 4.64s\tremaining: 2.02s\n",
      "209:\tlearn: 0.0012606\ttotal: 4.66s\tremaining: 2s\n",
      "210:\tlearn: 0.0012425\ttotal: 4.69s\tremaining: 1.98s\n",
      "211:\tlearn: 0.0012249\ttotal: 4.72s\tremaining: 1.96s\n",
      "212:\tlearn: 0.0012074\ttotal: 4.74s\tremaining: 1.93s\n",
      "213:\tlearn: 0.0011891\ttotal: 4.77s\tremaining: 1.92s\n",
      "214:\tlearn: 0.0011661\ttotal: 4.8s\tremaining: 1.9s\n",
      "215:\tlearn: 0.0011596\ttotal: 4.81s\tremaining: 1.87s\n",
      "216:\tlearn: 0.0011493\ttotal: 4.86s\tremaining: 1.86s\n",
      "217:\tlearn: 0.0011367\ttotal: 4.88s\tremaining: 1.83s\n",
      "218:\tlearn: 0.0011180\ttotal: 4.89s\tremaining: 1.81s\n",
      "219:\tlearn: 0.0011065\ttotal: 4.92s\tremaining: 1.79s\n",
      "220:\tlearn: 0.0010899\ttotal: 4.94s\tremaining: 1.76s\n",
      "221:\tlearn: 0.0010619\ttotal: 4.96s\tremaining: 1.74s\n",
      "222:\tlearn: 0.0010540\ttotal: 4.97s\tremaining: 1.72s\n",
      "223:\tlearn: 0.0010388\ttotal: 4.99s\tremaining: 1.69s\n",
      "224:\tlearn: 0.0010268\ttotal: 5.02s\tremaining: 1.67s\n",
      "225:\tlearn: 0.0010091\ttotal: 5.05s\tremaining: 1.65s\n",
      "226:\tlearn: 0.0009876\ttotal: 5.06s\tremaining: 1.63s\n",
      "227:\tlearn: 0.0009711\ttotal: 5.08s\tremaining: 1.6s\n",
      "228:\tlearn: 0.0009568\ttotal: 5.11s\tremaining: 1.58s\n",
      "229:\tlearn: 0.0009340\ttotal: 5.13s\tremaining: 1.56s\n",
      "230:\tlearn: 0.0009256\ttotal: 5.14s\tremaining: 1.53s\n",
      "231:\tlearn: 0.0009134\ttotal: 5.18s\tremaining: 1.52s\n",
      "232:\tlearn: 0.0009075\ttotal: 5.21s\tremaining: 1.5s\n",
      "233:\tlearn: 0.0008955\ttotal: 5.24s\tremaining: 1.48s\n",
      "234:\tlearn: 0.0008881\ttotal: 5.25s\tremaining: 1.45s\n",
      "235:\tlearn: 0.0008756\ttotal: 5.27s\tremaining: 1.43s\n",
      "236:\tlearn: 0.0008661\ttotal: 5.3s\tremaining: 1.41s\n",
      "237:\tlearn: 0.0008621\ttotal: 5.33s\tremaining: 1.39s\n",
      "238:\tlearn: 0.0008568\ttotal: 5.38s\tremaining: 1.37s\n",
      "239:\tlearn: 0.0008427\ttotal: 5.4s\tremaining: 1.35s\n",
      "240:\tlearn: 0.0008360\ttotal: 5.41s\tremaining: 1.32s\n",
      "241:\tlearn: 0.0008305\ttotal: 5.43s\tremaining: 1.3s\n",
      "242:\tlearn: 0.0008153\ttotal: 5.44s\tremaining: 1.28s\n",
      "243:\tlearn: 0.0008126\ttotal: 5.47s\tremaining: 1.26s\n",
      "244:\tlearn: 0.0008037\ttotal: 5.49s\tremaining: 1.23s\n",
      "245:\tlearn: 0.0008003\ttotal: 5.5s\tremaining: 1.21s\n",
      "246:\tlearn: 0.0007960\ttotal: 5.52s\tremaining: 1.18s\n",
      "247:\tlearn: 0.0007857\ttotal: 5.55s\tremaining: 1.16s\n",
      "248:\tlearn: 0.0007760\ttotal: 5.58s\tremaining: 1.14s\n",
      "249:\tlearn: 0.0007605\ttotal: 5.6s\tremaining: 1.12s\n",
      "250:\tlearn: 0.0007548\ttotal: 5.61s\tremaining: 1.09s\n",
      "251:\tlearn: 0.0007406\ttotal: 5.63s\tremaining: 1.07s\n",
      "252:\tlearn: 0.0007340\ttotal: 5.66s\tremaining: 1.05s\n",
      "253:\tlearn: 0.0007213\ttotal: 5.67s\tremaining: 1.03s\n",
      "254:\tlearn: 0.0007128\ttotal: 5.69s\tremaining: 1s\n",
      "255:\tlearn: 0.0007011\ttotal: 5.71s\tremaining: 981ms\n",
      "256:\tlearn: 0.0006929\ttotal: 5.74s\tremaining: 960ms\n",
      "257:\tlearn: 0.0006775\ttotal: 5.79s\tremaining: 942ms\n",
      "258:\tlearn: 0.0006716\ttotal: 5.82s\tremaining: 921ms\n",
      "259:\tlearn: 0.0006629\ttotal: 5.85s\tremaining: 900ms\n",
      "260:\tlearn: 0.0006572\ttotal: 5.86s\tremaining: 876ms\n",
      "261:\tlearn: 0.0006492\ttotal: 5.88s\tremaining: 853ms\n",
      "262:\tlearn: 0.0006423\ttotal: 5.89s\tremaining: 829ms\n",
      "263:\tlearn: 0.0006353\ttotal: 5.92s\tremaining: 808ms\n",
      "264:\tlearn: 0.0006258\ttotal: 5.94s\tremaining: 785ms\n",
      "265:\tlearn: 0.0006241\ttotal: 5.96s\tremaining: 761ms\n",
      "266:\tlearn: 0.0006210\ttotal: 5.99s\tremaining: 740ms\n",
      "267:\tlearn: 0.0006135\ttotal: 6.02s\tremaining: 719ms\n",
      "268:\tlearn: 0.0006087\ttotal: 6.03s\tremaining: 695ms\n",
      "269:\tlearn: 0.0006052\ttotal: 6.05s\tremaining: 672ms\n",
      "270:\tlearn: 0.0006030\ttotal: 6.08s\tremaining: 651ms\n",
      "271:\tlearn: 0.0005992\ttotal: 6.1s\tremaining: 628ms\n",
      "272:\tlearn: 0.0005960\ttotal: 6.11s\tremaining: 604ms\n",
      "273:\tlearn: 0.0005899\ttotal: 6.14s\tremaining: 583ms\n",
      "274:\tlearn: 0.0005849\ttotal: 6.16s\tremaining: 560ms\n",
      "275:\tlearn: 0.0005775\ttotal: 6.19s\tremaining: 538ms\n",
      "276:\tlearn: 0.0005734\ttotal: 6.21s\tremaining: 515ms\n",
      "277:\tlearn: 0.0005676\ttotal: 6.22s\tremaining: 492ms\n",
      "278:\tlearn: 0.0005628\ttotal: 6.25s\tremaining: 471ms\n",
      "279:\tlearn: 0.0005548\ttotal: 6.28s\tremaining: 449ms\n",
      "280:\tlearn: 0.0005517\ttotal: 6.32s\tremaining: 427ms\n",
      "281:\tlearn: 0.0005438\ttotal: 6.36s\tremaining: 406ms\n",
      "282:\tlearn: 0.0005393\ttotal: 6.38s\tremaining: 383ms\n",
      "283:\tlearn: 0.0005338\ttotal: 6.41s\tremaining: 361ms\n",
      "284:\tlearn: 0.0005286\ttotal: 6.44s\tremaining: 339ms\n",
      "285:\tlearn: 0.0005252\ttotal: 6.49s\tremaining: 318ms\n",
      "286:\tlearn: 0.0005146\ttotal: 6.5s\tremaining: 295ms\n",
      "287:\tlearn: 0.0005110\ttotal: 6.54s\tremaining: 272ms\n",
      "288:\tlearn: 0.0005086\ttotal: 6.55s\tremaining: 249ms\n",
      "289:\tlearn: 0.0005071\ttotal: 6.57s\tremaining: 226ms\n",
      "290:\tlearn: 0.0005011\ttotal: 6.58s\tremaining: 204ms\n",
      "291:\tlearn: 0.0004964\ttotal: 6.6s\tremaining: 181ms\n",
      "292:\tlearn: 0.0004941\ttotal: 6.61s\tremaining: 158ms\n",
      "293:\tlearn: 0.0004895\ttotal: 6.63s\tremaining: 135ms\n",
      "294:\tlearn: 0.0004864\ttotal: 6.66s\tremaining: 113ms\n",
      "295:\tlearn: 0.0004841\ttotal: 6.71s\tremaining: 90.7ms\n",
      "296:\tlearn: 0.0004770\ttotal: 6.74s\tremaining: 68.1ms\n",
      "297:\tlearn: 0.0004725\ttotal: 6.77s\tremaining: 45.5ms\n",
      "298:\tlearn: 0.0004689\ttotal: 6.8s\tremaining: 22.8ms\n",
      "299:\tlearn: 0.0004642\ttotal: 6.84s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0820fd1b904fea97a180888ca64b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.05912794780659286, Recall = 0.0, Aging Rate = 0.00794641933867337, Precision = 0.0, f1 = 0\n",
      "Epoch 2: Train Loss = 0.02606626348471758, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.025818084012809314, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.025416218106980037, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.024974245542521423, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02416044616606016, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.02421694114867592, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.02395635609382903, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.023701484058540435, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.023451304533036694, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.023169394081404712, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.022820646486496888, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.02290503328862598, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.022559854588002384, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.022369941528827736, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.0219589371938621, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.021679253033715585, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.021295989577585276, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.021315758830056975, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.0210419008077248, Recall = 0.017857142857142856, Aging Rate = 3.181749484954302e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 18: Train Loss = 0.02070504945898654, Recall = 0.004464285714285714, Aging Rate = 7.954373712385755e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 19: Train Loss = 0.020384947014022627, Recall = 0.008928571428571428, Aging Rate = 1.590874742477151e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 20: Train Loss = 0.020057727089738905, Recall = 0.022321428571428572, Aging Rate = 3.9771868561928776e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01950771049952291, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.019730673104436634, Recall = 0.022321428571428572, Aging Rate = 3.9771868561928776e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 22: Train Loss = 0.019408989247460835, Recall = 0.013392857142857142, Aging Rate = 2.3863121137157266e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.01906521652695876, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.018807904148502842, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.018466934894985965, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.018126062687452673, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.018172512553859006, Recall = 0.03125, Aging Rate = 5.568061598670029e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.017869890621795767, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.017631829811655446, Recall = 0.044642857142857144, Aging Rate = 7.954373712385755e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.017370744160516333, Recall = 0.04017857142857143, Aging Rate = 7.15893634114718e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.017043117817684614, Recall = 0.05803571428571429, Aging Rate = 0.00010340685826101482, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016579460645537978, Recall = 0.026785714285714284, Aging Rate = 4.772624227431453e-05, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.0167893836841347, Recall = 0.044642857142857144, Aging Rate = 7.954373712385755e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.016546917375655352, Recall = 0.05803571428571429, Aging Rate = 0.00010340685826101482, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.016252196274607226, Recall = 0.06696428571428571, Aging Rate = 0.00011931560568578633, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.016071906605293618, Recall = 0.06696428571428571, Aging Rate = 0.00012726997939817208, Precision = 0.9375, f1 = 0.12499999999999997\n",
      "Epoch 35: Train Loss = 0.015796367429507293, Recall = 0.07142857142857142, Aging Rate = 0.00013522435311055783, Precision = 0.9411764705882353, f1 = 0.13278008298755187\n",
      "Test Loss = 0.015241472047238037, Recall = 0.07142857142857142, Aging Rate = 0.00012726997939817208, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.015591856214877702, Recall = 0.07589285714285714, Aging Rate = 0.00015113310053532935, Precision = 0.8947368421052632, f1 = 0.139917695473251\n",
      "Epoch 37: Train Loss = 0.015359959193042956, Recall = 0.07142857142857142, Aging Rate = 0.00012726997939817208, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.015111333661342131, Recall = 0.07589285714285714, Aging Rate = 0.00013522435311055783, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.014977697735737802, Recall = 0.08928571428571429, Aging Rate = 0.0001590874742477151, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.014714498691818548, Recall = 0.08482142857142858, Aging Rate = 0.00015113310053532935, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014154179990263407, Recall = 0.08928571428571429, Aging Rate = 0.0001590874742477151, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.014538966296377884, Recall = 0.09821428571428571, Aging Rate = 0.00018295059538487237, Precision = 0.9565217391304348, f1 = 0.1781376518218623\n",
      "Epoch 42: Train Loss = 0.01441607152850185, Recall = 0.10267857142857142, Aging Rate = 0.00018295059538487237, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01419740426027784, Recall = 0.11160714285714286, Aging Rate = 0.0001988593428096439, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.014015118266063074, Recall = 0.11160714285714286, Aging Rate = 0.0001988593428096439, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.01377701616567118, Recall = 0.08928571428571429, Aging Rate = 0.00017499622167248662, Precision = 0.9090909090909091, f1 = 0.16260162601626016\n",
      "Test Loss = 0.01340589349445905, Recall = 0.09821428571428571, Aging Rate = 0.00017499622167248662, precision = 1.0\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.01374772855342755, Recall = 0.09821428571428571, Aging Rate = 0.00018295059538487237, Precision = 0.9565217391304348, f1 = 0.1781376518218623\n",
      "Epoch 47: Train Loss = 0.013553068723790015, Recall = 0.11607142857142858, Aging Rate = 0.00022272246394680116, Precision = 0.9285714285714286, f1 = 0.20634920634920634\n",
      "Epoch 48: Train Loss = 0.013419422853979766, Recall = 0.125, Aging Rate = 0.0002465855850839584, Precision = 0.9032258064516129, f1 = 0.21960784313725487\n",
      "Epoch 49: Train Loss = 0.013235064239715752, Recall = 0.14732142857142858, Aging Rate = 0.0002624943325087299, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01316518236324926, Recall = 0.10714285714285714, Aging Rate = 0.00019090496909725813, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012606129850713525, Recall = 0.19642857142857142, Aging Rate = 0.00038180993819451626, precision = 0.9166666666666666\n",
      "\n",
      "Epoch 51: Train Loss = 0.013037695695849318, Recall = 0.13839285714285715, Aging Rate = 0.00027044870622111567, Precision = 0.9117647058823529, f1 = 0.24031007751937986\n",
      "Epoch 52: Train Loss = 0.012838172834839403, Recall = 0.14732142857142858, Aging Rate = 0.0003022662010706587, Precision = 0.868421052631579, f1 = 0.25190839694656486\n",
      "Epoch 53: Train Loss = 0.012726928271801864, Recall = 0.13839285714285715, Aging Rate = 0.0002624943325087299, Precision = 0.9393939393939394, f1 = 0.2412451361867704\n",
      "Epoch 54: Train Loss = 0.01259788455510356, Recall = 0.17410714285714285, Aging Rate = 0.00034999244334497323, Precision = 0.8863636363636364, f1 = 0.291044776119403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Train Loss = 0.012491934153844952, Recall = 0.14285714285714285, Aging Rate = 0.0002863574536458872, Precision = 0.8888888888888888, f1 = 0.24615384615384617\n",
      "Test Loss = 0.011917577635753056, Recall = 0.16071428571428573, Aging Rate = 0.00029431182735827294, precision = 0.972972972972973\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.012373243741309834, Recall = 0.15178571428571427, Aging Rate = 0.00029431182735827294, Precision = 0.918918918918919, f1 = 0.26053639846743293\n",
      "Epoch 57: Train Loss = 0.01221987146169241, Recall = 0.16071428571428573, Aging Rate = 0.00031022057478304445, Precision = 0.9230769230769231, f1 = 0.2737642585551331\n",
      "Epoch 58: Train Loss = 0.012135680465232331, Recall = 0.16517857142857142, Aging Rate = 0.0003022662010706587, Precision = 0.9736842105263158, f1 = 0.282442748091603\n",
      "Epoch 59: Train Loss = 0.012077553303143782, Recall = 0.1875, Aging Rate = 0.00036590119076974475, Precision = 0.9130434782608695, f1 = 0.31111111111111106\n",
      "Epoch 60: Train Loss = 0.011970168684581313, Recall = 0.15178571428571427, Aging Rate = 0.0003181749484954302, Precision = 0.85, f1 = 0.25757575757575757\n",
      "Test Loss = 0.011409562384776448, Recall = 0.16517857142857142, Aging Rate = 0.0003022662010706587, precision = 0.9736842105263158\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.011839354914885152, Recall = 0.15625, Aging Rate = 0.0003022662010706587, Precision = 0.9210526315789473, f1 = 0.26717557251908397\n",
      "Epoch 62: Train Loss = 0.01168633504440151, Recall = 0.17857142857142858, Aging Rate = 0.000357946817057359, Precision = 0.8888888888888888, f1 = 0.29739776951672864\n",
      "Epoch 63: Train Loss = 0.011655764694506524, Recall = 0.17857142857142858, Aging Rate = 0.00034999244334497323, Precision = 0.9090909090909091, f1 = 0.2985074626865672\n",
      "Epoch 64: Train Loss = 0.011531255248144714, Recall = 0.19642857142857142, Aging Rate = 0.0003977186856192878, Precision = 0.88, f1 = 0.3211678832116788\n",
      "Epoch 65: Train Loss = 0.011474747094965595, Recall = 0.1875, Aging Rate = 0.0003738555644821305, Precision = 0.8936170212765957, f1 = 0.30996309963099633\n",
      "Test Loss = 0.010912169547842458, Recall = 0.19642857142857142, Aging Rate = 0.00036590119076974475, precision = 0.9565217391304348\n",
      "\n",
      "Epoch 66: Train Loss = 0.01132994207150184, Recall = 0.1875, Aging Rate = 0.000389764311906902, Precision = 0.8571428571428571, f1 = 0.30769230769230765\n",
      "Epoch 67: Train Loss = 0.011261174599522394, Recall = 0.22321428571428573, Aging Rate = 0.0004136274330440593, Precision = 0.9615384615384616, f1 = 0.36231884057971014\n",
      "Epoch 68: Train Loss = 0.011165727757923525, Recall = 0.20089285714285715, Aging Rate = 0.0003977186856192878, Precision = 0.9, f1 = 0.3284671532846716\n",
      "Epoch 69: Train Loss = 0.011123309256364192, Recall = 0.24107142857142858, Aging Rate = 0.0004693080490307596, Precision = 0.9152542372881356, f1 = 0.38162544169611307\n",
      "Epoch 70: Train Loss = 0.011092135978306992, Recall = 0.20089285714285715, Aging Rate = 0.00042158180675644504, Precision = 0.8490566037735849, f1 = 0.3249097472924188\n",
      "Test Loss = 0.010550391938626419, Recall = 0.21428571428571427, Aging Rate = 0.000389764311906902, precision = 0.9795918367346939\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.010980602278111718, Recall = 0.20982142857142858, Aging Rate = 0.00042158180675644504, Precision = 0.8867924528301887, f1 = 0.33935018050541516\n",
      "Epoch 72: Train Loss = 0.01087750382430953, Recall = 0.24107142857142858, Aging Rate = 0.0004613536753183738, Precision = 0.9310344827586207, f1 = 0.3829787234042554\n",
      "Epoch 73: Train Loss = 0.010811829599947856, Recall = 0.26785714285714285, Aging Rate = 0.0005249886650174598, Precision = 0.9090909090909091, f1 = 0.41379310344827586\n",
      "Epoch 74: Train Loss = 0.01083448323952392, Recall = 0.21875, Aging Rate = 0.00042158180675644504, Precision = 0.9245283018867925, f1 = 0.35379061371841153\n",
      "Epoch 75: Train Loss = 0.010708004372831545, Recall = 0.25892857142857145, Aging Rate = 0.0005249886650174598, Precision = 0.8787878787878788, f1 = 0.4\n",
      "Test Loss = 0.010209857889157367, Recall = 0.2767857142857143, Aging Rate = 0.0005408974124422313, precision = 0.9117647058823529\n",
      "\n",
      "Epoch 76: Train Loss = 0.01068121732671097, Recall = 0.23214285714285715, Aging Rate = 0.0004693080490307596, Precision = 0.8813559322033898, f1 = 0.3674911660777385\n",
      "Epoch 77: Train Loss = 0.010629718365798792, Recall = 0.25, Aging Rate = 0.0005249886650174598, Precision = 0.8484848484848485, f1 = 0.3862068965517241\n",
      "Epoch 78: Train Loss = 0.010537383528057033, Recall = 0.28125, Aging Rate = 0.0005647605335793887, Precision = 0.8873239436619719, f1 = 0.4271186440677966\n",
      "Epoch 79: Train Loss = 0.010464306884087535, Recall = 0.2857142857142857, Aging Rate = 0.0005408974124422313, Precision = 0.9411764705882353, f1 = 0.4383561643835616\n",
      "Epoch 80: Train Loss = 0.01041655991165833, Recall = 0.24553571428571427, Aging Rate = 0.00047726242274314534, Precision = 0.9166666666666666, f1 = 0.3873239436619718\n",
      "Test Loss = 0.009903156030591135, Recall = 0.3482142857142857, Aging Rate = 0.0006999848866899465, precision = 0.8863636363636364\n",
      "\n",
      "Epoch 81: Train Loss = 0.010291107161431662, Recall = 0.27232142857142855, Aging Rate = 0.0005727149072917744, Precision = 0.8472222222222222, f1 = 0.41216216216216217\n",
      "Epoch 82: Train Loss = 0.010344944280998223, Recall = 0.27232142857142855, Aging Rate = 0.0005249886650174598, Precision = 0.9242424242424242, f1 = 0.4206896551724138\n",
      "Epoch 83: Train Loss = 0.010223139919952386, Recall = 0.27232142857142855, Aging Rate = 0.0005488517861546171, Precision = 0.8840579710144928, f1 = 0.41638225255972694\n",
      "Epoch 84: Train Loss = 0.010207139245939221, Recall = 0.27232142857142855, Aging Rate = 0.0005488517861546171, Precision = 0.8840579710144928, f1 = 0.41638225255972694\n",
      "Epoch 85: Train Loss = 0.010132240930477246, Recall = 0.29017857142857145, Aging Rate = 0.0005806692810041602, Precision = 0.8904109589041096, f1 = 0.43771043771043777\n",
      "Test Loss = 0.009625939091505548, Recall = 0.3080357142857143, Aging Rate = 0.0006045324021413174, precision = 0.9078947368421053\n",
      "\n",
      "Epoch 86: Train Loss = 0.010070187049151977, Recall = 0.2767857142857143, Aging Rate = 0.0005568061598670028, Precision = 0.8857142857142857, f1 = 0.42176870748299317\n",
      "Epoch 87: Train Loss = 0.009938820728031579, Recall = 0.29017857142857145, Aging Rate = 0.0005886236547165459, Precision = 0.8783783783783784, f1 = 0.436241610738255\n",
      "Epoch 88: Train Loss = 0.00998198248195235, Recall = 0.29017857142857145, Aging Rate = 0.0005886236547165459, Precision = 0.8783783783783784, f1 = 0.436241610738255\n",
      "Epoch 89: Train Loss = 0.009964474125300844, Recall = 0.29464285714285715, Aging Rate = 0.0005965780284289317, Precision = 0.88, f1 = 0.44147157190635455\n",
      "Epoch 90: Train Loss = 0.00989918866266732, Recall = 0.29464285714285715, Aging Rate = 0.0005806692810041602, Precision = 0.9041095890410958, f1 = 0.4444444444444444\n",
      "Test Loss = 0.009485600639160433, Recall = 0.38839285714285715, Aging Rate = 0.0007556655026766467, precision = 0.9157894736842105\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.009843298961067835, Recall = 0.29464285714285715, Aging Rate = 0.0005727149072917744, Precision = 0.9166666666666666, f1 = 0.44594594594594594\n",
      "Epoch 92: Train Loss = 0.009758307815792405, Recall = 0.30357142857142855, Aging Rate = 0.0006045324021413174, Precision = 0.8947368421052632, f1 = 0.4533333333333333\n",
      "Epoch 93: Train Loss = 0.009817543339471786, Recall = 0.29464285714285715, Aging Rate = 0.0005727149072917744, Precision = 0.9166666666666666, f1 = 0.44594594594594594\n",
      "Epoch 94: Train Loss = 0.009745491056231277, Recall = 0.3125, Aging Rate = 0.0006204411495660889, Precision = 0.8974358974358975, f1 = 0.4635761589403974\n",
      "Epoch 95: Train Loss = 0.009729478469815681, Recall = 0.3169642857142857, Aging Rate = 0.0006443042707032462, Precision = 0.8765432098765432, f1 = 0.4655737704918032\n",
      "Test Loss = 0.009204587484438153, Recall = 0.38839285714285715, Aging Rate = 0.0007636198763890325, precision = 0.90625\n",
      "\n",
      "Epoch 96: Train Loss = 0.009640787852305184, Recall = 0.3169642857142857, Aging Rate = 0.0006283955232784747, Precision = 0.8987341772151899, f1 = 0.4686468646864686\n",
      "Epoch 97: Train Loss = 0.009628900904004404, Recall = 0.3169642857142857, Aging Rate = 0.0006283955232784747, Precision = 0.8987341772151899, f1 = 0.4686468646864686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: Train Loss = 0.009584226161488517, Recall = 0.33035714285714285, Aging Rate = 0.0006443042707032462, Precision = 0.9135802469135802, f1 = 0.4852459016393442\n",
      "Epoch 99: Train Loss = 0.00953113489748823, Recall = 0.3080357142857143, Aging Rate = 0.0005886236547165459, Precision = 0.9324324324324325, f1 = 0.46308724832214765\n",
      "Epoch 100: Train Loss = 0.009531047144050713, Recall = 0.36607142857142855, Aging Rate = 0.0007238480078271037, Precision = 0.9010989010989011, f1 = 0.5206349206349207\n",
      "Test Loss = 0.009008672443680223, Recall = 0.33035714285714285, Aging Rate = 0.0006204411495660889, precision = 0.9487179487179487\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.009510537913717396, Recall = 0.33482142857142855, Aging Rate = 0.0006522586444156319, Precision = 0.9146341463414634, f1 = 0.49019607843137253\n",
      "Epoch 102: Train Loss = 0.009431409684245768, Recall = 0.33482142857142855, Aging Rate = 0.0006761217655527893, Precision = 0.8823529411764706, f1 = 0.4854368932038834\n",
      "Epoch 103: Train Loss = 0.009482886812564763, Recall = 0.3080357142857143, Aging Rate = 0.0005965780284289317, Precision = 0.92, f1 = 0.46153846153846156\n",
      "Epoch 104: Train Loss = 0.00931356711813967, Recall = 0.33482142857142855, Aging Rate = 0.0006443042707032462, Precision = 0.9259259259259259, f1 = 0.4918032786885245\n",
      "Epoch 105: Train Loss = 0.009426704407818913, Recall = 0.33035714285714285, Aging Rate = 0.0006443042707032462, Precision = 0.9135802469135802, f1 = 0.4852459016393442\n",
      "Test Loss = 0.008920425491014544, Recall = 0.41517857142857145, Aging Rate = 0.0008033917449509612, precision = 0.9207920792079208\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.009298188125190716, Recall = 0.35714285714285715, Aging Rate = 0.0006999848866899465, Precision = 0.9090909090909091, f1 = 0.5128205128205129\n",
      "Epoch 107: Train Loss = 0.009323949778721325, Recall = 0.33035714285714285, Aging Rate = 0.0006443042707032462, Precision = 0.9135802469135802, f1 = 0.4852459016393442\n",
      "Epoch 108: Train Loss = 0.009240368381396344, Recall = 0.3392857142857143, Aging Rate = 0.000684076139265175, Precision = 0.8837209302325582, f1 = 0.4903225806451613\n",
      "Epoch 109: Train Loss = 0.00920249809120948, Recall = 0.33035714285714285, Aging Rate = 0.0006602130181280177, Precision = 0.891566265060241, f1 = 0.4820846905537459\n",
      "Epoch 110: Train Loss = 0.009212450585699077, Recall = 0.36160714285714285, Aging Rate = 0.000715893634114718, Precision = 0.9, f1 = 0.5159235668789809\n",
      "Test Loss = 0.008753151815817763, Recall = 0.35267857142857145, Aging Rate = 0.0006920305129775608, precision = 0.9080459770114943\n",
      "\n",
      "Epoch 111: Train Loss = 0.009170070473726758, Recall = 0.32589285714285715, Aging Rate = 0.0006283955232784747, Precision = 0.9240506329113924, f1 = 0.48184818481848185\n",
      "Epoch 112: Train Loss = 0.009163875356735457, Recall = 0.3169642857142857, Aging Rate = 0.0006443042707032462, Precision = 0.8765432098765432, f1 = 0.4655737704918032\n",
      "Epoch 113: Train Loss = 0.00906935537084724, Recall = 0.34375, Aging Rate = 0.0007079392604023322, Precision = 0.8651685393258427, f1 = 0.49201277955271566\n",
      "Epoch 114: Train Loss = 0.009125250704798289, Recall = 0.34375, Aging Rate = 0.0006681673918404034, Precision = 0.9166666666666666, f1 = 0.5\n",
      "Epoch 115: Train Loss = 0.009066851894206404, Recall = 0.3392857142857143, Aging Rate = 0.0006681673918404034, Precision = 0.9047619047619048, f1 = 0.49350649350649356\n",
      "Test Loss = 0.00894105897765466, Recall = 0.49107142857142855, Aging Rate = 0.0009704335929110622, precision = 0.9016393442622951\n",
      "Model in epoch 115 is saved.\n",
      "\n",
      "Epoch 116: Train Loss = 0.009055774074583231, Recall = 0.35714285714285715, Aging Rate = 0.0006999848866899465, Precision = 0.9090909090909091, f1 = 0.5128205128205129\n",
      "Epoch 117: Train Loss = 0.00903873770852535, Recall = 0.32142857142857145, Aging Rate = 0.0006204411495660889, Precision = 0.9230769230769231, f1 = 0.47682119205298024\n",
      "Epoch 118: Train Loss = 0.009004928720210983, Recall = 0.3482142857142857, Aging Rate = 0.0006761217655527893, Precision = 0.9176470588235294, f1 = 0.5048543689320388\n",
      "Epoch 119: Train Loss = 0.00898641109216452, Recall = 0.33035714285714285, Aging Rate = 0.0006522586444156319, Precision = 0.9024390243902439, f1 = 0.4836601307189542\n",
      "Epoch 120: Train Loss = 0.008930615315850888, Recall = 0.3392857142857143, Aging Rate = 0.0006761217655527893, Precision = 0.8941176470588236, f1 = 0.49190938511326865\n",
      "Test Loss = 0.008447951836278597, Recall = 0.4107142857142857, Aging Rate = 0.0008033917449509612, precision = 0.9108910891089109\n",
      "\n",
      "Epoch 121: Train Loss = 0.008892349608524605, Recall = 0.36160714285714285, Aging Rate = 0.000747711128964261, Precision = 0.8617021276595744, f1 = 0.5094339622641509\n",
      "Epoch 122: Train Loss = 0.008932468256200445, Recall = 0.35714285714285715, Aging Rate = 0.0006999848866899465, Precision = 0.9090909090909091, f1 = 0.5128205128205129\n",
      "Epoch 123: Train Loss = 0.00889945734994911, Recall = 0.35267857142857145, Aging Rate = 0.0006920305129775608, Precision = 0.9080459770114943, f1 = 0.5080385852090032\n",
      "Epoch 124: Train Loss = 0.00884718413359143, Recall = 0.34375, Aging Rate = 0.0006602130181280177, Precision = 0.927710843373494, f1 = 0.501628664495114\n",
      "Epoch 125: Train Loss = 0.00885878690996042, Recall = 0.34375, Aging Rate = 0.0006681673918404034, Precision = 0.9166666666666666, f1 = 0.5\n",
      "Test Loss = 0.008467293048010851, Recall = 0.4375, Aging Rate = 0.0008908898557872046, precision = 0.875\n",
      "\n",
      "Epoch 126: Train Loss = 0.008878243187452586, Recall = 0.3705357142857143, Aging Rate = 0.0007318023815394895, Precision = 0.9021739130434783, f1 = 0.5253164556962026\n",
      "Epoch 127: Train Loss = 0.008828954881987925, Recall = 0.35714285714285715, Aging Rate = 0.0006920305129775608, Precision = 0.9195402298850575, f1 = 0.5144694533762058\n",
      "Epoch 128: Train Loss = 0.008781848379715284, Recall = 0.33035714285714285, Aging Rate = 0.0006443042707032462, Precision = 0.9135802469135802, f1 = 0.4852459016393442\n",
      "Epoch 129: Train Loss = 0.008730035819794035, Recall = 0.3482142857142857, Aging Rate = 0.0007079392604023322, Precision = 0.8764044943820225, f1 = 0.49840255591054317\n",
      "Epoch 130: Train Loss = 0.00877030101034211, Recall = 0.35714285714285715, Aging Rate = 0.000715893634114718, Precision = 0.8888888888888888, f1 = 0.5095541401273885\n",
      "Test Loss = 0.008252151723704056, Recall = 0.40625, Aging Rate = 0.0007874829975261897, precision = 0.9191919191919192\n",
      "\n",
      "Epoch 131: Train Loss = 0.008698999234751629, Recall = 0.3794642857142857, Aging Rate = 0.0007318023815394895, Precision = 0.9239130434782609, f1 = 0.5379746835443038\n",
      "Epoch 132: Train Loss = 0.008690355431585072, Recall = 0.3705357142857143, Aging Rate = 0.0007318023815394895, Precision = 0.9021739130434783, f1 = 0.5253164556962026\n",
      "Epoch 133: Train Loss = 0.00869982695950618, Recall = 0.3482142857142857, Aging Rate = 0.0006920305129775608, Precision = 0.896551724137931, f1 = 0.5016077170418006\n",
      "Epoch 134: Train Loss = 0.008653327942681447, Recall = 0.36160714285714285, Aging Rate = 0.0007238480078271037, Precision = 0.8901098901098901, f1 = 0.5142857142857143\n",
      "Epoch 135: Train Loss = 0.008669536736612221, Recall = 0.36607142857142855, Aging Rate = 0.000715893634114718, Precision = 0.9111111111111111, f1 = 0.5222929936305732\n",
      "Test Loss = 0.008202264963762142, Recall = 0.35714285714285715, Aging Rate = 0.0006761217655527893, precision = 0.9411764705882353\n",
      "\n",
      "Epoch 136: Train Loss = 0.00864886433021811, Recall = 0.36160714285714285, Aging Rate = 0.000715893634114718, Precision = 0.9, f1 = 0.5159235668789809\n",
      "Epoch 137: Train Loss = 0.008609889037970499, Recall = 0.36160714285714285, Aging Rate = 0.0006920305129775608, Precision = 0.9310344827586207, f1 = 0.5209003215434083\n",
      "Epoch 138: Train Loss = 0.008595295774128717, Recall = 0.38839285714285715, Aging Rate = 0.0007556655026766467, Precision = 0.9157894736842105, f1 = 0.5454545454545454\n",
      "Epoch 139: Train Loss = 0.00856104256907996, Recall = 0.39285714285714285, Aging Rate = 0.0007556655026766467, Precision = 0.9263157894736842, f1 = 0.5517241379310346\n",
      "Epoch 140: Train Loss = 0.008609528413720162, Recall = 0.3705357142857143, Aging Rate = 0.0007079392604023322, Precision = 0.9325842696629213, f1 = 0.5303514376996805\n",
      "Test Loss = 0.008123596009709829, Recall = 0.4375, Aging Rate = 0.0008511179872252758, precision = 0.9158878504672897\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141: Train Loss = 0.008540758650503424, Recall = 0.38839285714285715, Aging Rate = 0.000779528623813804, Precision = 0.8877551020408163, f1 = 0.5403726708074534\n",
      "Epoch 142: Train Loss = 0.008594557212230482, Recall = 0.3392857142857143, Aging Rate = 0.0006761217655527893, Precision = 0.8941176470588236, f1 = 0.49190938511326865\n",
      "Epoch 143: Train Loss = 0.008547793275407894, Recall = 0.3705357142857143, Aging Rate = 0.0007397567552518752, Precision = 0.8924731182795699, f1 = 0.5236593059936909\n",
      "Epoch 144: Train Loss = 0.008464068901238202, Recall = 0.38839285714285715, Aging Rate = 0.0007636198763890325, Precision = 0.90625, f1 = 0.54375\n",
      "Epoch 145: Train Loss = 0.008568969421440102, Recall = 0.3705357142857143, Aging Rate = 0.0007238480078271037, Precision = 0.9120879120879121, f1 = 0.526984126984127\n",
      "Test Loss = 0.008002190474981805, Recall = 0.41517857142857145, Aging Rate = 0.0008272548660881186, precision = 0.8942307692307693\n",
      "\n",
      "Epoch 146: Train Loss = 0.00846070763248914, Recall = 0.38839285714285715, Aging Rate = 0.0007715742501014182, Precision = 0.8969072164948454, f1 = 0.5420560747663552\n",
      "Epoch 147: Train Loss = 0.008467536139209851, Recall = 0.3705357142857143, Aging Rate = 0.0007318023815394895, Precision = 0.9021739130434783, f1 = 0.5253164556962026\n",
      "Epoch 148: Train Loss = 0.008444257804469599, Recall = 0.38392857142857145, Aging Rate = 0.000747711128964261, Precision = 0.9148936170212766, f1 = 0.5408805031446541\n",
      "Epoch 149: Train Loss = 0.00842374264300033, Recall = 0.38392857142857145, Aging Rate = 0.0007636198763890325, Precision = 0.8958333333333334, f1 = 0.5375\n",
      "Epoch 150: Train Loss = 0.00842418431336613, Recall = 0.3794642857142857, Aging Rate = 0.0007636198763890325, Precision = 0.8854166666666666, f1 = 0.53125\n",
      "Test Loss = 0.007972485196029312, Recall = 0.35714285714285715, Aging Rate = 0.0006443042707032462, precision = 0.9876543209876543\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Epoch 151: Train Loss = 0.008400521842417467, Recall = 0.38392857142857145, Aging Rate = 0.0007636198763890325, Precision = 0.8958333333333334, f1 = 0.5375\n",
      "Epoch 152: Train Loss = 0.008466836291728463, Recall = 0.36607142857142855, Aging Rate = 0.0007318023815394895, Precision = 0.8913043478260869, f1 = 0.5189873417721518\n",
      "Epoch 153: Train Loss = 0.008410801206294678, Recall = 0.39732142857142855, Aging Rate = 0.0007874829975261897, Precision = 0.898989898989899, f1 = 0.5510835913312693\n",
      "Epoch 154: Train Loss = 0.008377036175400001, Recall = 0.375, Aging Rate = 0.000747711128964261, Precision = 0.8936170212765957, f1 = 0.5283018867924528\n",
      "Epoch 155: Train Loss = 0.00837525591845385, Recall = 0.4107142857142857, Aging Rate = 0.0007874829975261897, Precision = 0.9292929292929293, f1 = 0.5696594427244581\n",
      "Test Loss = 0.008143215087929473, Recall = 0.5178571428571429, Aging Rate = 0.001042022956322534, precision = 0.8854961832061069\n",
      "\n",
      "Epoch 156: Train Loss = 0.008350068471232671, Recall = 0.38839285714285715, Aging Rate = 0.0007715742501014182, Precision = 0.8969072164948454, f1 = 0.5420560747663552\n",
      "Epoch 157: Train Loss = 0.008307760140448487, Recall = 0.3794642857142857, Aging Rate = 0.0007556655026766467, Precision = 0.8947368421052632, f1 = 0.5329153605015674\n",
      "Epoch 158: Train Loss = 0.008252162165401965, Recall = 0.4017857142857143, Aging Rate = 0.000779528623813804, Precision = 0.9183673469387755, f1 = 0.5590062111801243\n",
      "Epoch 159: Train Loss = 0.008308701887639812, Recall = 0.39285714285714285, Aging Rate = 0.0007715742501014182, Precision = 0.9072164948453608, f1 = 0.5482866043613707\n",
      "Epoch 160: Train Loss = 0.00827205541686609, Recall = 0.4017857142857143, Aging Rate = 0.0007715742501014182, Precision = 0.9278350515463918, f1 = 0.5607476635514019\n",
      "Test Loss = 0.00798065265711647, Recall = 0.3080357142857143, Aging Rate = 0.0005647605335793887, precision = 0.971830985915493\n",
      "\n",
      "Epoch 161: Train Loss = 0.008308452397627186, Recall = 0.38392857142857145, Aging Rate = 0.0007556655026766467, Precision = 0.9052631578947369, f1 = 0.5391849529780564\n",
      "Epoch 162: Train Loss = 0.008285254641650674, Recall = 0.38392857142857145, Aging Rate = 0.0007318023815394895, Precision = 0.9347826086956522, f1 = 0.5443037974683544\n",
      "Epoch 163: Train Loss = 0.0082662881333702, Recall = 0.38392857142857145, Aging Rate = 0.0007636198763890325, Precision = 0.8958333333333334, f1 = 0.5375\n",
      "Epoch 164: Train Loss = 0.00820183207368379, Recall = 0.3794642857142857, Aging Rate = 0.0007636198763890325, Precision = 0.8854166666666666, f1 = 0.53125\n",
      "Epoch 165: Train Loss = 0.008248183303067815, Recall = 0.38392857142857145, Aging Rate = 0.000779528623813804, Precision = 0.8775510204081632, f1 = 0.5341614906832298\n",
      "Test Loss = 0.007797654856167749, Recall = 0.45982142857142855, Aging Rate = 0.0009227073506367477, precision = 0.8879310344827587\n",
      "\n",
      "Epoch 166: Train Loss = 0.008193266738778338, Recall = 0.3794642857142857, Aging Rate = 0.0007636198763890325, Precision = 0.8854166666666666, f1 = 0.53125\n",
      "Epoch 167: Train Loss = 0.008208994720066797, Recall = 0.39732142857142855, Aging Rate = 0.000779528623813804, Precision = 0.9081632653061225, f1 = 0.5527950310559006\n",
      "Epoch 168: Train Loss = 0.00818035198888578, Recall = 0.41517857142857145, Aging Rate = 0.0008113461186633471, Precision = 0.9117647058823529, f1 = 0.5705521472392637\n",
      "Epoch 169: Train Loss = 0.008132846893220885, Recall = 0.39732142857142855, Aging Rate = 0.0007874829975261897, Precision = 0.898989898989899, f1 = 0.5510835913312693\n",
      "Epoch 170: Train Loss = 0.008212802118358342, Recall = 0.39732142857142855, Aging Rate = 0.0007715742501014182, Precision = 0.9175257731958762, f1 = 0.5545171339563864\n",
      "Test Loss = 0.007721098688123182, Recall = 0.42857142857142855, Aging Rate = 0.0008431636135128901, precision = 0.9056603773584906\n",
      "\n",
      "Epoch 171: Train Loss = 0.008200429696601523, Recall = 0.39285714285714285, Aging Rate = 0.0007556655026766467, Precision = 0.9263157894736842, f1 = 0.5517241379310346\n",
      "Epoch 172: Train Loss = 0.008157597966365943, Recall = 0.40625, Aging Rate = 0.0008272548660881186, Precision = 0.875, f1 = 0.5548780487804879\n",
      "Epoch 173: Train Loss = 0.008161339036645666, Recall = 0.3794642857142857, Aging Rate = 0.000747711128964261, Precision = 0.9042553191489362, f1 = 0.5345911949685535\n",
      "Epoch 174: Train Loss = 0.008151609324704165, Recall = 0.4017857142857143, Aging Rate = 0.0007954373712385755, Precision = 0.9, f1 = 0.5555555555555556\n",
      "Epoch 175: Train Loss = 0.008116364604478543, Recall = 0.39285714285714285, Aging Rate = 0.0007636198763890325, Precision = 0.9166666666666666, f1 = 0.55\n",
      "Test Loss = 0.007654928431569961, Recall = 0.4330357142857143, Aging Rate = 0.0008352092398005043, precision = 0.9238095238095239\n",
      "\n",
      "Epoch 176: Train Loss = 0.008133044871889475, Recall = 0.40625, Aging Rate = 0.0007874829975261897, Precision = 0.9191919191919192, f1 = 0.563467492260062\n",
      "Epoch 177: Train Loss = 0.008141155962556684, Recall = 0.39285714285714285, Aging Rate = 0.0007636198763890325, Precision = 0.9166666666666666, f1 = 0.55\n",
      "Epoch 178: Train Loss = 0.008137992662927187, Recall = 0.38839285714285715, Aging Rate = 0.0007556655026766467, Precision = 0.9157894736842105, f1 = 0.5454545454545454\n",
      "Epoch 179: Train Loss = 0.008119600518748417, Recall = 0.4017857142857143, Aging Rate = 0.0007874829975261897, Precision = 0.9090909090909091, f1 = 0.5572755417956656\n",
      "Epoch 180: Train Loss = 0.00810221604774698, Recall = 0.38839285714285715, Aging Rate = 0.000747711128964261, Precision = 0.925531914893617, f1 = 0.5471698113207547\n",
      "Test Loss = 0.007628711555216322, Recall = 0.45535714285714285, Aging Rate = 0.0008908898557872046, precision = 0.9107142857142857\n",
      "\n",
      "Epoch 181: Train Loss = 0.008067943327798926, Recall = 0.41517857142857145, Aging Rate = 0.0007954373712385755, Precision = 0.93, f1 = 0.5740740740740741\n",
      "Epoch 182: Train Loss = 0.008097158563751617, Recall = 0.39732142857142855, Aging Rate = 0.000779528623813804, Precision = 0.9081632653061225, f1 = 0.5527950310559006\n",
      "Epoch 183: Train Loss = 0.008075111235395708, Recall = 0.4107142857142857, Aging Rate = 0.0008033917449509612, Precision = 0.9108910891089109, f1 = 0.5661538461538461\n",
      "Epoch 184: Train Loss = 0.008061474774683624, Recall = 0.4017857142857143, Aging Rate = 0.0008033917449509612, Precision = 0.8910891089108911, f1 = 0.5538461538461539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185: Train Loss = 0.008069485309416171, Recall = 0.4017857142857143, Aging Rate = 0.0008113461186633471, Precision = 0.8823529411764706, f1 = 0.5521472392638037\n",
      "Test Loss = 0.0076171051106081085, Recall = 0.40625, Aging Rate = 0.0008113461186633471, precision = 0.8921568627450981\n",
      "\n",
      "Epoch 186: Train Loss = 0.008071320387155688, Recall = 0.39732142857142855, Aging Rate = 0.0007715742501014182, Precision = 0.9175257731958762, f1 = 0.5545171339563864\n",
      "Epoch 187: Train Loss = 0.008052792052719412, Recall = 0.40625, Aging Rate = 0.0007954373712385755, Precision = 0.91, f1 = 0.5617283950617283\n",
      "Epoch 188: Train Loss = 0.007976660046219115, Recall = 0.4419642857142857, Aging Rate = 0.0008829354820748188, Precision = 0.8918918918918919, f1 = 0.591044776119403\n",
      "Epoch 189: Train Loss = 0.008043180766308904, Recall = 0.40625, Aging Rate = 0.0008033917449509612, Precision = 0.900990099009901, f1 = 0.56\n",
      "Epoch 190: Train Loss = 0.008077019506146546, Recall = 0.41517857142857145, Aging Rate = 0.0008193004923757328, Precision = 0.9029126213592233, f1 = 0.5688073394495413\n",
      "Test Loss = 0.007594481570738227, Recall = 0.4642857142857143, Aging Rate = 0.0009227073506367477, precision = 0.896551724137931\n",
      "\n",
      "Epoch 191: Train Loss = 0.008056924383593367, Recall = 0.39285714285714285, Aging Rate = 0.000779528623813804, Precision = 0.8979591836734694, f1 = 0.5465838509316769\n",
      "Epoch 192: Train Loss = 0.008069446881321917, Recall = 0.41964285714285715, Aging Rate = 0.0008431636135128901, Precision = 0.8867924528301887, f1 = 0.5696969696969697\n",
      "Epoch 193: Train Loss = 0.00803478867736759, Recall = 0.39732142857142855, Aging Rate = 0.0008033917449509612, Precision = 0.8811881188118812, f1 = 0.5476923076923077\n",
      "Epoch 194: Train Loss = 0.008017767361465141, Recall = 0.4017857142857143, Aging Rate = 0.0008193004923757328, Precision = 0.8737864077669902, f1 = 0.5504587155963303\n",
      "Epoch 195: Train Loss = 0.007974028499992082, Recall = 0.4017857142857143, Aging Rate = 0.0008033917449509612, Precision = 0.8910891089108911, f1 = 0.5538461538461539\n",
      "Test Loss = 0.007533999117295813, Recall = 0.42410714285714285, Aging Rate = 0.0008113461186633471, precision = 0.9313725490196079\n",
      "\n",
      "Epoch 196: Train Loss = 0.007999572554547902, Recall = 0.4330357142857143, Aging Rate = 0.0008352092398005043, Precision = 0.9238095238095239, f1 = 0.5896656534954409\n",
      "Epoch 197: Train Loss = 0.007948756740073015, Recall = 0.4330357142857143, Aging Rate = 0.0008670267346500473, Precision = 0.8899082568807339, f1 = 0.5825825825825827\n",
      "Epoch 198: Train Loss = 0.007967873312849167, Recall = 0.4107142857142857, Aging Rate = 0.0008113461186633471, Precision = 0.9019607843137255, f1 = 0.5644171779141105\n",
      "Epoch 199: Train Loss = 0.007995679775515466, Recall = 0.4017857142857143, Aging Rate = 0.0007874829975261897, Precision = 0.9090909090909091, f1 = 0.5572755417956656\n",
      "Epoch 200: Train Loss = 0.007988475835438902, Recall = 0.4419642857142857, Aging Rate = 0.0008590723609376616, Precision = 0.9166666666666666, f1 = 0.5963855421686747\n",
      "Test Loss = 0.007546047223950081, Recall = 0.38839285714285715, Aging Rate = 0.0006999848866899465, precision = 0.9886363636363636\n",
      "Model in epoch 200 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2831338\ttotal: 35ms\tremaining: 10.5s\n",
      "1:\tlearn: 0.1249738\ttotal: 65.2ms\tremaining: 9.72s\n",
      "2:\tlearn: 0.0636165\ttotal: 97.5ms\tremaining: 9.65s\n",
      "3:\tlearn: 0.0377362\ttotal: 145ms\tremaining: 10.8s\n",
      "4:\tlearn: 0.0259190\ttotal: 161ms\tremaining: 9.47s\n",
      "5:\tlearn: 0.0198104\ttotal: 176ms\tremaining: 8.63s\n",
      "6:\tlearn: 0.0165347\ttotal: 192ms\tremaining: 8.02s\n",
      "7:\tlearn: 0.0147917\ttotal: 208ms\tremaining: 7.59s\n",
      "8:\tlearn: 0.0137893\ttotal: 224ms\tremaining: 7.25s\n",
      "9:\tlearn: 0.0131079\ttotal: 239ms\tremaining: 6.94s\n",
      "10:\tlearn: 0.0126421\ttotal: 255ms\tremaining: 6.7s\n",
      "11:\tlearn: 0.0123834\ttotal: 287ms\tremaining: 6.88s\n",
      "12:\tlearn: 0.0119085\ttotal: 302ms\tremaining: 6.67s\n",
      "13:\tlearn: 0.0117345\ttotal: 318ms\tremaining: 6.5s\n",
      "14:\tlearn: 0.0115478\ttotal: 333ms\tremaining: 6.33s\n",
      "15:\tlearn: 0.0114726\ttotal: 366ms\tremaining: 6.49s\n",
      "16:\tlearn: 0.0113157\ttotal: 397ms\tremaining: 6.61s\n",
      "17:\tlearn: 0.0112164\ttotal: 413ms\tremaining: 6.47s\n",
      "18:\tlearn: 0.0111799\ttotal: 445ms\tremaining: 6.58s\n",
      "19:\tlearn: 0.0110930\ttotal: 461ms\tremaining: 6.45s\n",
      "20:\tlearn: 0.0110598\ttotal: 476ms\tremaining: 6.33s\n",
      "21:\tlearn: 0.0110268\ttotal: 491ms\tremaining: 6.21s\n",
      "22:\tlearn: 0.0109125\ttotal: 502ms\tremaining: 6.04s\n",
      "23:\tlearn: 0.0108352\ttotal: 513ms\tremaining: 5.9s\n",
      "24:\tlearn: 0.0107196\ttotal: 553ms\tremaining: 6.08s\n",
      "25:\tlearn: 0.0106270\ttotal: 568ms\tremaining: 5.99s\n",
      "26:\tlearn: 0.0105600\ttotal: 584ms\tremaining: 5.9s\n",
      "27:\tlearn: 0.0104430\ttotal: 595ms\tremaining: 5.78s\n",
      "28:\tlearn: 0.0103621\ttotal: 631ms\tremaining: 5.89s\n",
      "29:\tlearn: 0.0102328\ttotal: 646ms\tremaining: 5.81s\n",
      "30:\tlearn: 0.0101904\ttotal: 677ms\tremaining: 5.87s\n",
      "31:\tlearn: 0.0100921\ttotal: 691ms\tremaining: 5.79s\n",
      "32:\tlearn: 0.0100110\ttotal: 708ms\tremaining: 5.72s\n",
      "33:\tlearn: 0.0098935\ttotal: 739ms\tremaining: 5.78s\n",
      "34:\tlearn: 0.0097457\ttotal: 769ms\tremaining: 5.82s\n",
      "35:\tlearn: 0.0096422\ttotal: 785ms\tremaining: 5.76s\n",
      "36:\tlearn: 0.0095196\ttotal: 816ms\tremaining: 5.8s\n",
      "37:\tlearn: 0.0094708\ttotal: 848ms\tremaining: 5.84s\n",
      "38:\tlearn: 0.0093408\ttotal: 865ms\tremaining: 5.79s\n",
      "39:\tlearn: 0.0093044\ttotal: 879ms\tremaining: 5.72s\n",
      "40:\tlearn: 0.0091358\ttotal: 895ms\tremaining: 5.65s\n",
      "41:\tlearn: 0.0090193\ttotal: 911ms\tremaining: 5.59s\n",
      "42:\tlearn: 0.0089168\ttotal: 927ms\tremaining: 5.54s\n",
      "43:\tlearn: 0.0088778\ttotal: 943ms\tremaining: 5.48s\n",
      "44:\tlearn: 0.0088494\ttotal: 958ms\tremaining: 5.43s\n",
      "45:\tlearn: 0.0088044\ttotal: 972ms\tremaining: 5.37s\n",
      "46:\tlearn: 0.0087138\ttotal: 983ms\tremaining: 5.29s\n",
      "47:\tlearn: 0.0086161\ttotal: 1s\tremaining: 5.27s\n",
      "48:\tlearn: 0.0085226\ttotal: 1.02s\tremaining: 5.22s\n",
      "49:\tlearn: 0.0084235\ttotal: 1.03s\tremaining: 5.17s\n",
      "50:\tlearn: 0.0083376\ttotal: 1.05s\tremaining: 5.13s\n",
      "51:\tlearn: 0.0082703\ttotal: 1.07s\tremaining: 5.09s\n",
      "52:\tlearn: 0.0082010\ttotal: 1.1s\tremaining: 5.12s\n",
      "53:\tlearn: 0.0079206\ttotal: 1.13s\tremaining: 5.15s\n",
      "54:\tlearn: 0.0078288\ttotal: 1.15s\tremaining: 5.11s\n",
      "55:\tlearn: 0.0077247\ttotal: 1.16s\tremaining: 5.06s\n",
      "56:\tlearn: 0.0076438\ttotal: 1.18s\tremaining: 5.02s\n",
      "57:\tlearn: 0.0075342\ttotal: 1.19s\tremaining: 4.97s\n",
      "58:\tlearn: 0.0074344\ttotal: 1.21s\tremaining: 4.94s\n",
      "59:\tlearn: 0.0073493\ttotal: 1.22s\tremaining: 4.9s\n",
      "60:\tlearn: 0.0072659\ttotal: 1.24s\tremaining: 4.86s\n",
      "61:\tlearn: 0.0071841\ttotal: 1.27s\tremaining: 4.88s\n",
      "62:\tlearn: 0.0071157\ttotal: 1.29s\tremaining: 4.84s\n",
      "63:\tlearn: 0.0070271\ttotal: 1.3s\tremaining: 4.81s\n",
      "64:\tlearn: 0.0069212\ttotal: 1.32s\tremaining: 4.77s\n",
      "65:\tlearn: 0.0067915\ttotal: 1.35s\tremaining: 4.78s\n",
      "66:\tlearn: 0.0066856\ttotal: 1.36s\tremaining: 4.75s\n",
      "67:\tlearn: 0.0065844\ttotal: 1.38s\tremaining: 4.71s\n",
      "68:\tlearn: 0.0065004\ttotal: 1.4s\tremaining: 4.67s\n",
      "69:\tlearn: 0.0064306\ttotal: 1.41s\tremaining: 4.63s\n",
      "70:\tlearn: 0.0063856\ttotal: 1.43s\tremaining: 4.6s\n",
      "71:\tlearn: 0.0063242\ttotal: 1.44s\tremaining: 4.57s\n",
      "72:\tlearn: 0.0062568\ttotal: 1.46s\tremaining: 4.53s\n",
      "73:\tlearn: 0.0061900\ttotal: 1.49s\tremaining: 4.54s\n",
      "74:\tlearn: 0.0061433\ttotal: 1.53s\tremaining: 4.6s\n",
      "75:\tlearn: 0.0060720\ttotal: 1.55s\tremaining: 4.57s\n",
      "76:\tlearn: 0.0060255\ttotal: 1.58s\tremaining: 4.58s\n",
      "77:\tlearn: 0.0059505\ttotal: 1.6s\tremaining: 4.55s\n",
      "78:\tlearn: 0.0059200\ttotal: 1.61s\tremaining: 4.52s\n",
      "79:\tlearn: 0.0058743\ttotal: 1.63s\tremaining: 4.48s\n",
      "80:\tlearn: 0.0058414\ttotal: 1.65s\tremaining: 4.45s\n",
      "81:\tlearn: 0.0057926\ttotal: 1.66s\tremaining: 4.42s\n",
      "82:\tlearn: 0.0056680\ttotal: 1.68s\tremaining: 4.38s\n",
      "83:\tlearn: 0.0055179\ttotal: 1.69s\tremaining: 4.36s\n",
      "84:\tlearn: 0.0054527\ttotal: 1.71s\tremaining: 4.32s\n",
      "85:\tlearn: 0.0053643\ttotal: 1.72s\tremaining: 4.29s\n",
      "86:\tlearn: 0.0052380\ttotal: 1.74s\tremaining: 4.26s\n",
      "87:\tlearn: 0.0051140\ttotal: 1.76s\tremaining: 4.23s\n",
      "88:\tlearn: 0.0050282\ttotal: 1.77s\tremaining: 4.2s\n",
      "89:\tlearn: 0.0049780\ttotal: 1.78s\tremaining: 4.16s\n",
      "90:\tlearn: 0.0049338\ttotal: 1.8s\tremaining: 4.14s\n",
      "91:\tlearn: 0.0049094\ttotal: 1.83s\tremaining: 4.15s\n",
      "92:\tlearn: 0.0048743\ttotal: 1.85s\tremaining: 4.12s\n",
      "93:\tlearn: 0.0047915\ttotal: 1.86s\tremaining: 4.09s\n",
      "94:\tlearn: 0.0047077\ttotal: 1.88s\tremaining: 4.06s\n",
      "95:\tlearn: 0.0046519\ttotal: 1.9s\tremaining: 4.03s\n",
      "96:\tlearn: 0.0046144\ttotal: 1.91s\tremaining: 4s\n",
      "97:\tlearn: 0.0045424\ttotal: 1.93s\tremaining: 3.98s\n",
      "98:\tlearn: 0.0044944\ttotal: 1.95s\tremaining: 3.95s\n",
      "99:\tlearn: 0.0044517\ttotal: 1.96s\tremaining: 3.92s\n",
      "100:\tlearn: 0.0044108\ttotal: 1.98s\tremaining: 3.89s\n",
      "101:\tlearn: 0.0043704\ttotal: 2.01s\tremaining: 3.89s\n",
      "102:\tlearn: 0.0043089\ttotal: 2.04s\tremaining: 3.9s\n",
      "103:\tlearn: 0.0042609\ttotal: 2.05s\tremaining: 3.87s\n",
      "104:\tlearn: 0.0041929\ttotal: 2.07s\tremaining: 3.84s\n",
      "105:\tlearn: 0.0041574\ttotal: 2.08s\tremaining: 3.81s\n",
      "106:\tlearn: 0.0040751\ttotal: 2.1s\tremaining: 3.79s\n",
      "107:\tlearn: 0.0040288\ttotal: 2.12s\tremaining: 3.76s\n",
      "108:\tlearn: 0.0039891\ttotal: 2.13s\tremaining: 3.73s\n",
      "109:\tlearn: 0.0039017\ttotal: 2.15s\tremaining: 3.71s\n",
      "110:\tlearn: 0.0038470\ttotal: 2.16s\tremaining: 3.68s\n",
      "111:\tlearn: 0.0038033\ttotal: 2.18s\tremaining: 3.65s\n",
      "112:\tlearn: 0.0037305\ttotal: 2.19s\tremaining: 3.63s\n",
      "113:\tlearn: 0.0036850\ttotal: 2.22s\tremaining: 3.63s\n",
      "114:\tlearn: 0.0035789\ttotal: 2.24s\tremaining: 3.6s\n",
      "115:\tlearn: 0.0035240\ttotal: 2.25s\tremaining: 3.58s\n",
      "116:\tlearn: 0.0034454\ttotal: 2.29s\tremaining: 3.57s\n",
      "117:\tlearn: 0.0033794\ttotal: 2.3s\tremaining: 3.55s\n",
      "118:\tlearn: 0.0033308\ttotal: 2.33s\tremaining: 3.55s\n",
      "119:\tlearn: 0.0032885\ttotal: 2.35s\tremaining: 3.52s\n",
      "120:\tlearn: 0.0032508\ttotal: 2.38s\tremaining: 3.52s\n",
      "121:\tlearn: 0.0032039\ttotal: 2.4s\tremaining: 3.5s\n",
      "122:\tlearn: 0.0031800\ttotal: 2.43s\tremaining: 3.49s\n",
      "123:\tlearn: 0.0031438\ttotal: 2.46s\tremaining: 3.49s\n",
      "124:\tlearn: 0.0030972\ttotal: 2.49s\tremaining: 3.48s\n",
      "125:\tlearn: 0.0030818\ttotal: 2.5s\tremaining: 3.46s\n",
      "126:\tlearn: 0.0030064\ttotal: 2.55s\tremaining: 3.48s\n",
      "127:\tlearn: 0.0029764\ttotal: 2.57s\tremaining: 3.45s\n",
      "128:\tlearn: 0.0029256\ttotal: 2.6s\tremaining: 3.45s\n",
      "129:\tlearn: 0.0028846\ttotal: 2.63s\tremaining: 3.44s\n",
      "130:\tlearn: 0.0028647\ttotal: 2.65s\tremaining: 3.42s\n",
      "131:\tlearn: 0.0028403\ttotal: 2.68s\tremaining: 3.41s\n",
      "132:\tlearn: 0.0028096\ttotal: 2.71s\tremaining: 3.4s\n",
      "133:\tlearn: 0.0027672\ttotal: 2.73s\tremaining: 3.38s\n",
      "134:\tlearn: 0.0027401\ttotal: 2.74s\tremaining: 3.35s\n",
      "135:\tlearn: 0.0027079\ttotal: 2.77s\tremaining: 3.34s\n",
      "136:\tlearn: 0.0026474\ttotal: 2.8s\tremaining: 3.33s\n",
      "137:\tlearn: 0.0026262\ttotal: 2.83s\tremaining: 3.33s\n",
      "138:\tlearn: 0.0025679\ttotal: 2.85s\tremaining: 3.3s\n",
      "139:\tlearn: 0.0024859\ttotal: 2.87s\tremaining: 3.28s\n",
      "140:\tlearn: 0.0024612\ttotal: 2.9s\tremaining: 3.27s\n",
      "141:\tlearn: 0.0024138\ttotal: 2.91s\tremaining: 3.24s\n",
      "142:\tlearn: 0.0023719\ttotal: 2.93s\tremaining: 3.21s\n",
      "143:\tlearn: 0.0023371\ttotal: 2.94s\tremaining: 3.19s\n",
      "144:\tlearn: 0.0023131\ttotal: 2.98s\tremaining: 3.18s\n",
      "145:\tlearn: 0.0022610\ttotal: 2.99s\tremaining: 3.15s\n",
      "146:\tlearn: 0.0022221\ttotal: 3s\tremaining: 3.13s\n",
      "147:\tlearn: 0.0021839\ttotal: 3.02s\tremaining: 3.1s\n",
      "148:\tlearn: 0.0021413\ttotal: 3.03s\tremaining: 3.07s\n",
      "149:\tlearn: 0.0021111\ttotal: 3.07s\tremaining: 3.07s\n",
      "150:\tlearn: 0.0020857\ttotal: 3.1s\tremaining: 3.06s\n",
      "151:\tlearn: 0.0020611\ttotal: 3.12s\tremaining: 3.03s\n",
      "152:\tlearn: 0.0020226\ttotal: 3.13s\tremaining: 3.01s\n",
      "153:\tlearn: 0.0019964\ttotal: 3.16s\tremaining: 3s\n",
      "154:\tlearn: 0.0019801\ttotal: 3.19s\tremaining: 2.99s\n",
      "155:\tlearn: 0.0019513\ttotal: 3.21s\tremaining: 2.96s\n",
      "156:\tlearn: 0.0019135\ttotal: 3.23s\tremaining: 2.94s\n",
      "157:\tlearn: 0.0018872\ttotal: 3.26s\tremaining: 2.93s\n",
      "158:\tlearn: 0.0018698\ttotal: 3.29s\tremaining: 2.92s\n",
      "159:\tlearn: 0.0018407\ttotal: 3.31s\tremaining: 2.89s\n",
      "160:\tlearn: 0.0018130\ttotal: 3.32s\tremaining: 2.87s\n",
      "161:\tlearn: 0.0017978\ttotal: 3.35s\tremaining: 2.86s\n",
      "162:\tlearn: 0.0017847\ttotal: 3.38s\tremaining: 2.85s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163:\tlearn: 0.0017540\ttotal: 3.42s\tremaining: 2.83s\n",
      "164:\tlearn: 0.0017253\ttotal: 3.43s\tremaining: 2.81s\n",
      "165:\tlearn: 0.0016985\ttotal: 3.45s\tremaining: 2.78s\n",
      "166:\tlearn: 0.0016758\ttotal: 3.46s\tremaining: 2.76s\n",
      "167:\tlearn: 0.0016632\ttotal: 3.48s\tremaining: 2.73s\n",
      "168:\tlearn: 0.0016512\ttotal: 3.51s\tremaining: 2.72s\n",
      "169:\tlearn: 0.0016340\ttotal: 3.54s\tremaining: 2.71s\n",
      "170:\tlearn: 0.0016112\ttotal: 3.57s\tremaining: 2.69s\n",
      "171:\tlearn: 0.0015834\ttotal: 3.59s\tremaining: 2.67s\n",
      "172:\tlearn: 0.0015562\ttotal: 3.6s\tremaining: 2.64s\n",
      "173:\tlearn: 0.0015495\ttotal: 3.62s\tremaining: 2.62s\n",
      "174:\tlearn: 0.0015319\ttotal: 3.63s\tremaining: 2.6s\n",
      "175:\tlearn: 0.0015150\ttotal: 3.65s\tremaining: 2.57s\n",
      "176:\tlearn: 0.0014732\ttotal: 3.66s\tremaining: 2.55s\n",
      "177:\tlearn: 0.0014504\ttotal: 3.68s\tremaining: 2.52s\n",
      "178:\tlearn: 0.0014391\ttotal: 3.69s\tremaining: 2.5s\n",
      "179:\tlearn: 0.0014123\ttotal: 3.71s\tremaining: 2.47s\n",
      "180:\tlearn: 0.0013898\ttotal: 3.74s\tremaining: 2.46s\n",
      "181:\tlearn: 0.0013764\ttotal: 3.77s\tremaining: 2.44s\n",
      "182:\tlearn: 0.0013539\ttotal: 3.79s\tremaining: 2.42s\n",
      "183:\tlearn: 0.0013335\ttotal: 3.8s\tremaining: 2.4s\n",
      "184:\tlearn: 0.0013166\ttotal: 3.84s\tremaining: 2.38s\n",
      "185:\tlearn: 0.0013033\ttotal: 3.85s\tremaining: 2.36s\n",
      "186:\tlearn: 0.0012824\ttotal: 3.87s\tremaining: 2.34s\n",
      "187:\tlearn: 0.0012680\ttotal: 3.9s\tremaining: 2.32s\n",
      "188:\tlearn: 0.0012528\ttotal: 3.96s\tremaining: 2.33s\n",
      "189:\tlearn: 0.0012349\ttotal: 3.98s\tremaining: 2.3s\n",
      "190:\tlearn: 0.0012255\ttotal: 4.01s\tremaining: 2.29s\n",
      "191:\tlearn: 0.0012146\ttotal: 4.03s\tremaining: 2.26s\n",
      "192:\tlearn: 0.0011984\ttotal: 4.06s\tremaining: 2.25s\n",
      "193:\tlearn: 0.0011854\ttotal: 4.07s\tremaining: 2.23s\n",
      "194:\tlearn: 0.0011740\ttotal: 4.09s\tremaining: 2.2s\n",
      "195:\tlearn: 0.0011535\ttotal: 4.11s\tremaining: 2.18s\n",
      "196:\tlearn: 0.0011385\ttotal: 4.12s\tremaining: 2.15s\n",
      "197:\tlearn: 0.0011323\ttotal: 4.15s\tremaining: 2.14s\n",
      "198:\tlearn: 0.0011272\ttotal: 4.17s\tremaining: 2.12s\n",
      "199:\tlearn: 0.0011089\ttotal: 4.2s\tremaining: 2.1s\n",
      "200:\tlearn: 0.0010827\ttotal: 4.22s\tremaining: 2.08s\n",
      "201:\tlearn: 0.0010729\ttotal: 4.23s\tremaining: 2.05s\n",
      "202:\tlearn: 0.0010655\ttotal: 4.25s\tremaining: 2.03s\n",
      "203:\tlearn: 0.0010599\ttotal: 4.28s\tremaining: 2.01s\n",
      "204:\tlearn: 0.0010380\ttotal: 4.31s\tremaining: 2s\n",
      "205:\tlearn: 0.0010228\ttotal: 4.32s\tremaining: 1.97s\n",
      "206:\tlearn: 0.0010136\ttotal: 4.34s\tremaining: 1.95s\n",
      "207:\tlearn: 0.0010062\ttotal: 4.37s\tremaining: 1.93s\n",
      "208:\tlearn: 0.0009946\ttotal: 4.39s\tremaining: 1.91s\n",
      "209:\tlearn: 0.0009834\ttotal: 4.42s\tremaining: 1.89s\n",
      "210:\tlearn: 0.0009693\ttotal: 4.43s\tremaining: 1.87s\n",
      "211:\tlearn: 0.0009655\ttotal: 4.47s\tremaining: 1.85s\n",
      "212:\tlearn: 0.0009487\ttotal: 4.5s\tremaining: 1.84s\n",
      "213:\tlearn: 0.0009440\ttotal: 4.51s\tremaining: 1.81s\n",
      "214:\tlearn: 0.0009367\ttotal: 4.53s\tremaining: 1.79s\n",
      "215:\tlearn: 0.0009291\ttotal: 4.54s\tremaining: 1.77s\n",
      "216:\tlearn: 0.0009208\ttotal: 4.58s\tremaining: 1.75s\n",
      "217:\tlearn: 0.0009122\ttotal: 4.59s\tremaining: 1.73s\n",
      "218:\tlearn: 0.0008919\ttotal: 4.61s\tremaining: 1.7s\n",
      "219:\tlearn: 0.0008793\ttotal: 4.62s\tremaining: 1.68s\n",
      "220:\tlearn: 0.0008750\ttotal: 4.64s\tremaining: 1.66s\n",
      "221:\tlearn: 0.0008651\ttotal: 4.67s\tremaining: 1.64s\n",
      "222:\tlearn: 0.0008476\ttotal: 4.68s\tremaining: 1.62s\n",
      "223:\tlearn: 0.0008347\ttotal: 4.7s\tremaining: 1.59s\n",
      "224:\tlearn: 0.0008298\ttotal: 4.72s\tremaining: 1.57s\n",
      "225:\tlearn: 0.0008157\ttotal: 4.73s\tremaining: 1.55s\n",
      "226:\tlearn: 0.0008114\ttotal: 4.75s\tremaining: 1.53s\n",
      "227:\tlearn: 0.0007987\ttotal: 4.76s\tremaining: 1.5s\n",
      "228:\tlearn: 0.0007923\ttotal: 4.78s\tremaining: 1.48s\n",
      "229:\tlearn: 0.0007822\ttotal: 4.79s\tremaining: 1.46s\n",
      "230:\tlearn: 0.0007673\ttotal: 4.81s\tremaining: 1.44s\n",
      "231:\tlearn: 0.0007597\ttotal: 4.83s\tremaining: 1.41s\n",
      "232:\tlearn: 0.0007539\ttotal: 4.86s\tremaining: 1.4s\n",
      "233:\tlearn: 0.0007495\ttotal: 4.87s\tremaining: 1.37s\n",
      "234:\tlearn: 0.0007447\ttotal: 4.89s\tremaining: 1.35s\n",
      "235:\tlearn: 0.0007403\ttotal: 4.9s\tremaining: 1.33s\n",
      "236:\tlearn: 0.0007359\ttotal: 4.92s\tremaining: 1.31s\n",
      "237:\tlearn: 0.0007302\ttotal: 4.93s\tremaining: 1.28s\n",
      "238:\tlearn: 0.0007211\ttotal: 4.95s\tremaining: 1.26s\n",
      "239:\tlearn: 0.0007169\ttotal: 4.97s\tremaining: 1.24s\n",
      "240:\tlearn: 0.0007147\ttotal: 4.98s\tremaining: 1.22s\n",
      "241:\tlearn: 0.0007086\ttotal: 5.01s\tremaining: 1.2s\n",
      "242:\tlearn: 0.0007029\ttotal: 5.03s\tremaining: 1.18s\n",
      "243:\tlearn: 0.0006780\ttotal: 5.04s\tremaining: 1.16s\n",
      "244:\tlearn: 0.0006555\ttotal: 5.06s\tremaining: 1.14s\n",
      "245:\tlearn: 0.0006507\ttotal: 5.08s\tremaining: 1.11s\n",
      "246:\tlearn: 0.0006352\ttotal: 5.09s\tremaining: 1.09s\n",
      "247:\tlearn: 0.0006241\ttotal: 5.11s\tremaining: 1.07s\n",
      "248:\tlearn: 0.0006162\ttotal: 5.14s\tremaining: 1.05s\n",
      "249:\tlearn: 0.0006058\ttotal: 5.16s\tremaining: 1.03s\n",
      "250:\tlearn: 0.0006005\ttotal: 5.17s\tremaining: 1.01s\n",
      "251:\tlearn: 0.0005910\ttotal: 5.19s\tremaining: 988ms\n",
      "252:\tlearn: 0.0005868\ttotal: 5.22s\tremaining: 970ms\n",
      "253:\tlearn: 0.0005827\ttotal: 5.23s\tremaining: 948ms\n",
      "254:\tlearn: 0.0005771\ttotal: 5.25s\tremaining: 927ms\n",
      "255:\tlearn: 0.0005740\ttotal: 5.28s\tremaining: 908ms\n",
      "256:\tlearn: 0.0005672\ttotal: 5.3s\tremaining: 887ms\n",
      "257:\tlearn: 0.0005625\ttotal: 5.31s\tremaining: 865ms\n",
      "258:\tlearn: 0.0005588\ttotal: 5.34s\tremaining: 846ms\n",
      "259:\tlearn: 0.0005544\ttotal: 5.36s\tremaining: 825ms\n",
      "260:\tlearn: 0.0005491\ttotal: 5.38s\tremaining: 804ms\n",
      "261:\tlearn: 0.0005419\ttotal: 5.39s\tremaining: 782ms\n",
      "262:\tlearn: 0.0005373\ttotal: 5.41s\tremaining: 761ms\n",
      "263:\tlearn: 0.0005316\ttotal: 5.44s\tremaining: 742ms\n",
      "264:\tlearn: 0.0005250\ttotal: 5.47s\tremaining: 723ms\n",
      "265:\tlearn: 0.0005218\ttotal: 5.49s\tremaining: 701ms\n",
      "266:\tlearn: 0.0005166\ttotal: 5.5s\tremaining: 680ms\n",
      "267:\tlearn: 0.0005152\ttotal: 5.52s\tremaining: 659ms\n",
      "268:\tlearn: 0.0005104\ttotal: 5.53s\tremaining: 638ms\n",
      "269:\tlearn: 0.0005056\ttotal: 5.55s\tremaining: 617ms\n",
      "270:\tlearn: 0.0005035\ttotal: 5.56s\tremaining: 596ms\n",
      "271:\tlearn: 0.0004986\ttotal: 5.58s\tremaining: 575ms\n",
      "272:\tlearn: 0.0004932\ttotal: 5.6s\tremaining: 554ms\n",
      "273:\tlearn: 0.0004911\ttotal: 5.63s\tremaining: 534ms\n",
      "274:\tlearn: 0.0004887\ttotal: 5.64s\tremaining: 513ms\n",
      "275:\tlearn: 0.0004851\ttotal: 5.66s\tremaining: 492ms\n",
      "276:\tlearn: 0.0004825\ttotal: 5.67s\tremaining: 471ms\n",
      "277:\tlearn: 0.0004784\ttotal: 5.71s\tremaining: 452ms\n",
      "278:\tlearn: 0.0004754\ttotal: 5.72s\tremaining: 431ms\n",
      "279:\tlearn: 0.0004719\ttotal: 5.77s\tremaining: 412ms\n",
      "280:\tlearn: 0.0004676\ttotal: 5.8s\tremaining: 392ms\n",
      "281:\tlearn: 0.0004628\ttotal: 5.82s\tremaining: 371ms\n",
      "282:\tlearn: 0.0004600\ttotal: 5.83s\tremaining: 350ms\n",
      "283:\tlearn: 0.0004563\ttotal: 5.86s\tremaining: 330ms\n",
      "284:\tlearn: 0.0004504\ttotal: 5.89s\tremaining: 310ms\n",
      "285:\tlearn: 0.0004461\ttotal: 5.91s\tremaining: 289ms\n",
      "286:\tlearn: 0.0004442\ttotal: 5.93s\tremaining: 268ms\n",
      "287:\tlearn: 0.0004413\ttotal: 5.96s\tremaining: 248ms\n",
      "288:\tlearn: 0.0004375\ttotal: 6.01s\tremaining: 229ms\n",
      "289:\tlearn: 0.0004338\ttotal: 6.02s\tremaining: 208ms\n",
      "290:\tlearn: 0.0004305\ttotal: 6.04s\tremaining: 187ms\n",
      "291:\tlearn: 0.0004279\ttotal: 6.05s\tremaining: 166ms\n",
      "292:\tlearn: 0.0004267\ttotal: 6.07s\tremaining: 145ms\n",
      "293:\tlearn: 0.0004231\ttotal: 6.08s\tremaining: 124ms\n",
      "294:\tlearn: 0.0004202\ttotal: 6.1s\tremaining: 103ms\n",
      "295:\tlearn: 0.0004169\ttotal: 6.12s\tremaining: 82.7ms\n",
      "296:\tlearn: 0.0004134\ttotal: 6.15s\tremaining: 62.1ms\n",
      "297:\tlearn: 0.0004113\ttotal: 6.18s\tremaining: 41.5ms\n",
      "298:\tlearn: 0.0004100\ttotal: 6.2s\tremaining: 20.7ms\n",
      "299:\tlearn: 0.0004092\ttotal: 6.21s\tremaining: 0us\n",
      "Dataset 1:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659428a6ac454a17848e3106941df0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be7bf2b35c1422b8f798cc2d1df74d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5762503287562105, Recall = 0.9130040779338469, Aging Rate = 0.7301767104666969, Precision = 0.6251939187092771, f1 = 0.7421731123388583\n",
      "Epoch 2: Train Loss = 0.3680853390645052, Recall = 0.8812868146805618, Aging Rate = 0.5303579519710013, Precision = 0.8308415207176421, f1 = 0.855321020228672\n",
      "Epoch 3: Train Loss = 0.27875596432054967, Recall = 0.9143633892161305, Aging Rate = 0.5215224286361577, Precision = 0.8766290182450044, f1 = 0.8950986915058773\n",
      "Epoch 4: Train Loss = 0.21741259123034534, Recall = 0.940643407340281, Aging Rate = 0.520163117353874, Precision = 0.9041811846689896, f1 = 0.922051965356429\n",
      "Epoch 5: Train Loss = 0.17436902329670018, Recall = 0.9569551427276847, Aging Rate = 0.5172179429089262, Precision = 0.9250985545335085, f1 = 0.9407572383073497\n",
      "Test Loss = 0.13947001371682904, Recall = 0.966017217942909, Aging Rate = 0.501132759401903, precision = 0.9638336347197106\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.12726366163434408, Recall = 0.978704123244223, Aging Rate = 0.5095151789759855, Precision = 0.9604268563806136, f1 = 0.9694793536804309\n",
      "Epoch 7: Train Loss = 0.09723746482203544, Recall = 0.9882193022202084, Aging Rate = 0.50747621205256, Precision = 0.9736607142857143, f1 = 0.9808859905554306\n",
      "Epoch 8: Train Loss = 0.07696896333181594, Recall = 0.9918441323062981, Aging Rate = 0.5049841413683733, Precision = 0.9820547330641544, f1 = 0.9869251577998197\n",
      "Epoch 9: Train Loss = 0.061889144302926885, Recall = 0.9945627548708654, Aging Rate = 0.5043044857272315, Precision = 0.9860736747529201, f1 = 0.9903000225580871\n",
      "Epoch 10: Train Loss = 0.04922410829721072, Recall = 0.9954689623923878, Aging Rate = 0.5024920706841867, Precision = 0.9905320108205591, f1 = 0.9929943502824858\n",
      "Test Loss = 0.04152417240650015, Recall = 0.9986406887177164, Aging Rate = 0.5033982782057091, precision = 0.991899189918992\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04068942662239021, Recall = 0.9977344811961939, Aging Rate = 0.5024920706841867, Precision = 0.9927862939585211, f1 = 0.9952542372881354\n",
      "Epoch 12: Train Loss = 0.03358337736009579, Recall = 0.9977344811961939, Aging Rate = 0.5006796556411418, Precision = 0.9963800904977376, f1 = 0.997056825899932\n",
      "Epoch 13: Train Loss = 0.028097296144609058, Recall = 0.9986406887177164, Aging Rate = 0.5015858631626643, Precision = 0.995483288166215, f1 = 0.9970594888034382\n",
      "Epoch 14: Train Loss = 0.023740687392783694, Recall = 0.9995468962392388, Aging Rate = 0.5013593112822836, Precision = 0.9968368730230457, f1 = 0.9981900452488688\n",
      "Epoch 15: Train Loss = 0.01987284143410174, Recall = 0.9995468962392388, Aging Rate = 0.501132759401903, Precision = 0.9972875226039783, f1 = 0.9984159312061552\n",
      "Test Loss = 0.017329445245916696, Recall = 1.0, Aging Rate = 0.5006796556411418, precision = 0.9986425339366516\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.017615727908102483, Recall = 0.9995468962392388, Aging Rate = 0.5006796556411418, Precision = 0.9981900452488688, f1 = 0.9988680099615123\n",
      "Epoch 17: Train Loss = 0.015103340816085269, Recall = 0.9995468962392388, Aging Rate = 0.5006796556411418, Precision = 0.9981900452488688, f1 = 0.9988680099615123\n",
      "Epoch 18: Train Loss = 0.013056362498472274, Recall = 1.0, Aging Rate = 0.5009062075215225, Precision = 0.9981908638625057, f1 = 0.9990946129470348\n",
      "Epoch 19: Train Loss = 0.011317511366812768, Recall = 1.0, Aging Rate = 0.5002265518803806, Precision = 0.9995471014492754, f1 = 0.9997734994337486\n",
      "Epoch 20: Train Loss = 0.010544452190443056, Recall = 1.0, Aging Rate = 0.5004531037607612, Precision = 0.9990946129470348, f1 = 0.9995471014492754\n",
      "Test Loss = 0.009403729476254074, Recall = 1.0, Aging Rate = 0.5002265518803806, precision = 0.9995471014492754\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.009051137537356613, Recall = 1.0, Aging Rate = 0.5004531037607612, Precision = 0.9990946129470348, f1 = 0.9995471014492754\n",
      "Epoch 22: Train Loss = 0.00801820783062716, Recall = 1.0, Aging Rate = 0.5002265518803806, Precision = 0.9995471014492754, f1 = 0.9997734994337486\n",
      "Epoch 23: Train Loss = 0.00718342095691969, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.0065042932925464805, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.0058945817276827, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0054905947355201575, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.005371991546755369, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.005021379708840356, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.004561077364824634, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.004274432205898484, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.003934995878186527, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0035143577913392047, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.003667162987540717, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.003452174750371491, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.0031699443583709653, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0031219744410996265, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.0029765571591325733, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002613789274930738, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.0028340527059879167, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.002608504531821502, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.002592085570765395, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0024412514308090254, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.002400338488973181, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0024414048257980566, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.0023834632616152535, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.002585475557056683, Recall = 1.0, Aging Rate = 0.5002265518803806, Precision = 0.9995471014492754, f1 = 0.9997734994337486\n",
      "Epoch 43: Train Loss = 0.0022101938003133165, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0021131515635222226, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0020066112392562166, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018529846721405175, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.001986799005559959, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0019261678566841393, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0021239839442851595, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0019042412767302384, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.001865608881140743, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016506107741465525, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.001904144397291967, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0019066575104936174, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0017759451647063336, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0020427394648410437, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0017689632488803715, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001681286030785296, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.001734343854404309, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0018185007766329721, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0018130527604282693, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: Train Loss = 0.0017312021442238742, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0017434217507972484, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015085495213058486, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0016710419162131397, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0018125560051729076, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0017427075604684937, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.003979192087937384, Recall = 0.9995468962392388, Aging Rate = 0.5, Precision = 0.9995468962392388, f1 = 0.9995468962392388\n",
      "Epoch 65: Train Loss = 0.0017492154748014444, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001243233673867829, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0013515350993724366, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0014108743360188023, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.001402124381007219, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.001486394689253801, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0014766599177722585, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016384816397906471, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0015909769571790119, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.001605643283349589, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0016659818221293778, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0019792632354270643, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.001729880109411214, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017695969048618263, Recall = 1.0, Aging Rate = 0.5002265518803806, precision = 0.9995471014492754\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6247613\ttotal: 12.6ms\tremaining: 2.51s\n",
      "1:\tlearn: 0.5654511\ttotal: 25.3ms\tremaining: 2.5s\n",
      "2:\tlearn: 0.5196859\ttotal: 37.9ms\tremaining: 2.48s\n",
      "3:\tlearn: 0.4769736\ttotal: 50.4ms\tremaining: 2.47s\n",
      "4:\tlearn: 0.4432012\ttotal: 63.1ms\tremaining: 2.46s\n",
      "5:\tlearn: 0.4056793\ttotal: 75.2ms\tremaining: 2.43s\n",
      "6:\tlearn: 0.3696595\ttotal: 87ms\tremaining: 2.4s\n",
      "7:\tlearn: 0.3441038\ttotal: 98ms\tremaining: 2.35s\n",
      "8:\tlearn: 0.3186199\ttotal: 109ms\tremaining: 2.31s\n",
      "9:\tlearn: 0.3019559\ttotal: 121ms\tremaining: 2.29s\n",
      "10:\tlearn: 0.2799736\ttotal: 132ms\tremaining: 2.26s\n",
      "11:\tlearn: 0.2671142\ttotal: 148ms\tremaining: 2.32s\n",
      "12:\tlearn: 0.2558919\ttotal: 160ms\tremaining: 2.29s\n",
      "13:\tlearn: 0.2383306\ttotal: 171ms\tremaining: 2.27s\n",
      "14:\tlearn: 0.2232573\ttotal: 182ms\tremaining: 2.24s\n",
      "15:\tlearn: 0.2118596\ttotal: 192ms\tremaining: 2.21s\n",
      "16:\tlearn: 0.2043252\ttotal: 204ms\tremaining: 2.19s\n",
      "17:\tlearn: 0.1968181\ttotal: 215ms\tremaining: 2.17s\n",
      "18:\tlearn: 0.1841178\ttotal: 226ms\tremaining: 2.16s\n",
      "19:\tlearn: 0.1745763\ttotal: 237ms\tremaining: 2.13s\n",
      "20:\tlearn: 0.1681151\ttotal: 248ms\tremaining: 2.12s\n",
      "21:\tlearn: 0.1600692\ttotal: 259ms\tremaining: 2.1s\n",
      "22:\tlearn: 0.1532749\ttotal: 270ms\tremaining: 2.08s\n",
      "23:\tlearn: 0.1467333\ttotal: 281ms\tremaining: 2.06s\n",
      "24:\tlearn: 0.1395669\ttotal: 292ms\tremaining: 2.04s\n",
      "25:\tlearn: 0.1302042\ttotal: 303ms\tremaining: 2.03s\n",
      "26:\tlearn: 0.1228163\ttotal: 314ms\tremaining: 2.01s\n",
      "27:\tlearn: 0.1171025\ttotal: 325ms\tremaining: 2s\n",
      "28:\tlearn: 0.1107163\ttotal: 336ms\tremaining: 1.98s\n",
      "29:\tlearn: 0.1077148\ttotal: 346ms\tremaining: 1.96s\n",
      "30:\tlearn: 0.1027524\ttotal: 357ms\tremaining: 1.95s\n",
      "31:\tlearn: 0.0982164\ttotal: 368ms\tremaining: 1.93s\n",
      "32:\tlearn: 0.0946939\ttotal: 380ms\tremaining: 1.92s\n",
      "33:\tlearn: 0.0916524\ttotal: 391ms\tremaining: 1.91s\n",
      "34:\tlearn: 0.0885638\ttotal: 402ms\tremaining: 1.89s\n",
      "35:\tlearn: 0.0857004\ttotal: 414ms\tremaining: 1.88s\n",
      "36:\tlearn: 0.0823418\ttotal: 425ms\tremaining: 1.87s\n",
      "37:\tlearn: 0.0786257\ttotal: 436ms\tremaining: 1.86s\n",
      "38:\tlearn: 0.0759734\ttotal: 447ms\tremaining: 1.85s\n",
      "39:\tlearn: 0.0733972\ttotal: 458ms\tremaining: 1.83s\n",
      "40:\tlearn: 0.0719689\ttotal: 469ms\tremaining: 1.82s\n",
      "41:\tlearn: 0.0707559\ttotal: 481ms\tremaining: 1.81s\n",
      "42:\tlearn: 0.0683631\ttotal: 492ms\tremaining: 1.79s\n",
      "43:\tlearn: 0.0674062\ttotal: 503ms\tremaining: 1.78s\n",
      "44:\tlearn: 0.0650098\ttotal: 514ms\tremaining: 1.77s\n",
      "45:\tlearn: 0.0623068\ttotal: 524ms\tremaining: 1.75s\n",
      "46:\tlearn: 0.0599653\ttotal: 536ms\tremaining: 1.75s\n",
      "47:\tlearn: 0.0575242\ttotal: 547ms\tremaining: 1.73s\n",
      "48:\tlearn: 0.0552949\ttotal: 559ms\tremaining: 1.72s\n",
      "49:\tlearn: 0.0540784\ttotal: 569ms\tremaining: 1.71s\n",
      "50:\tlearn: 0.0527414\ttotal: 580ms\tremaining: 1.69s\n",
      "51:\tlearn: 0.0516373\ttotal: 591ms\tremaining: 1.68s\n",
      "52:\tlearn: 0.0510418\ttotal: 602ms\tremaining: 1.67s\n",
      "53:\tlearn: 0.0495030\ttotal: 613ms\tremaining: 1.66s\n",
      "54:\tlearn: 0.0485611\ttotal: 624ms\tremaining: 1.64s\n",
      "55:\tlearn: 0.0473864\ttotal: 635ms\tremaining: 1.63s\n",
      "56:\tlearn: 0.0461040\ttotal: 646ms\tremaining: 1.62s\n",
      "57:\tlearn: 0.0447447\ttotal: 657ms\tremaining: 1.61s\n",
      "58:\tlearn: 0.0424162\ttotal: 668ms\tremaining: 1.6s\n",
      "59:\tlearn: 0.0417363\ttotal: 679ms\tremaining: 1.58s\n",
      "60:\tlearn: 0.0404907\ttotal: 690ms\tremaining: 1.57s\n",
      "61:\tlearn: 0.0393916\ttotal: 701ms\tremaining: 1.56s\n",
      "62:\tlearn: 0.0380339\ttotal: 712ms\tremaining: 1.55s\n",
      "63:\tlearn: 0.0367689\ttotal: 723ms\tremaining: 1.54s\n",
      "64:\tlearn: 0.0360210\ttotal: 735ms\tremaining: 1.52s\n",
      "65:\tlearn: 0.0350457\ttotal: 745ms\tremaining: 1.51s\n",
      "66:\tlearn: 0.0342515\ttotal: 756ms\tremaining: 1.5s\n",
      "67:\tlearn: 0.0334969\ttotal: 767ms\tremaining: 1.49s\n",
      "68:\tlearn: 0.0330929\ttotal: 778ms\tremaining: 1.48s\n",
      "69:\tlearn: 0.0320004\ttotal: 790ms\tremaining: 1.47s\n",
      "70:\tlearn: 0.0303386\ttotal: 801ms\tremaining: 1.45s\n",
      "71:\tlearn: 0.0295470\ttotal: 812ms\tremaining: 1.44s\n",
      "72:\tlearn: 0.0287032\ttotal: 823ms\tremaining: 1.43s\n",
      "73:\tlearn: 0.0278428\ttotal: 834ms\tremaining: 1.42s\n",
      "74:\tlearn: 0.0269422\ttotal: 844ms\tremaining: 1.41s\n",
      "75:\tlearn: 0.0262734\ttotal: 855ms\tremaining: 1.4s\n",
      "76:\tlearn: 0.0257654\ttotal: 867ms\tremaining: 1.38s\n",
      "77:\tlearn: 0.0251319\ttotal: 878ms\tremaining: 1.37s\n",
      "78:\tlearn: 0.0247611\ttotal: 889ms\tremaining: 1.36s\n",
      "79:\tlearn: 0.0241602\ttotal: 900ms\tremaining: 1.35s\n",
      "80:\tlearn: 0.0234754\ttotal: 911ms\tremaining: 1.34s\n",
      "81:\tlearn: 0.0229253\ttotal: 922ms\tremaining: 1.33s\n",
      "82:\tlearn: 0.0222362\ttotal: 933ms\tremaining: 1.31s\n",
      "83:\tlearn: 0.0214193\ttotal: 943ms\tremaining: 1.3s\n",
      "84:\tlearn: 0.0208809\ttotal: 954ms\tremaining: 1.29s\n",
      "85:\tlearn: 0.0204117\ttotal: 965ms\tremaining: 1.28s\n",
      "86:\tlearn: 0.0196648\ttotal: 976ms\tremaining: 1.27s\n",
      "87:\tlearn: 0.0193666\ttotal: 987ms\tremaining: 1.26s\n",
      "88:\tlearn: 0.0189324\ttotal: 999ms\tremaining: 1.25s\n",
      "89:\tlearn: 0.0186904\ttotal: 1.01s\tremaining: 1.24s\n",
      "90:\tlearn: 0.0179386\ttotal: 1.02s\tremaining: 1.22s\n",
      "91:\tlearn: 0.0176274\ttotal: 1.03s\tremaining: 1.21s\n",
      "92:\tlearn: 0.0169851\ttotal: 1.04s\tremaining: 1.2s\n",
      "93:\tlearn: 0.0165528\ttotal: 1.05s\tremaining: 1.19s\n",
      "94:\tlearn: 0.0163405\ttotal: 1.06s\tremaining: 1.18s\n",
      "95:\tlearn: 0.0156159\ttotal: 1.08s\tremaining: 1.17s\n",
      "96:\tlearn: 0.0153538\ttotal: 1.09s\tremaining: 1.16s\n",
      "97:\tlearn: 0.0150431\ttotal: 1.1s\tremaining: 1.14s\n",
      "98:\tlearn: 0.0147014\ttotal: 1.11s\tremaining: 1.13s\n",
      "99:\tlearn: 0.0144777\ttotal: 1.13s\tremaining: 1.13s\n",
      "100:\tlearn: 0.0144573\ttotal: 1.13s\tremaining: 1.1s\n",
      "101:\tlearn: 0.0141703\ttotal: 1.14s\tremaining: 1.1s\n",
      "102:\tlearn: 0.0138749\ttotal: 1.16s\tremaining: 1.09s\n",
      "103:\tlearn: 0.0134438\ttotal: 1.18s\tremaining: 1.09s\n",
      "104:\tlearn: 0.0130810\ttotal: 1.19s\tremaining: 1.08s\n",
      "105:\tlearn: 0.0128063\ttotal: 1.21s\tremaining: 1.07s\n",
      "106:\tlearn: 0.0125334\ttotal: 1.22s\tremaining: 1.06s\n",
      "107:\tlearn: 0.0123384\ttotal: 1.24s\tremaining: 1.05s\n",
      "108:\tlearn: 0.0121227\ttotal: 1.25s\tremaining: 1.04s\n",
      "109:\tlearn: 0.0119648\ttotal: 1.26s\tremaining: 1.03s\n",
      "110:\tlearn: 0.0115203\ttotal: 1.27s\tremaining: 1.02s\n",
      "111:\tlearn: 0.0113049\ttotal: 1.28s\tremaining: 1.01s\n",
      "112:\tlearn: 0.0111599\ttotal: 1.29s\tremaining: 994ms\n",
      "113:\tlearn: 0.0108200\ttotal: 1.3s\tremaining: 983ms\n",
      "114:\tlearn: 0.0106349\ttotal: 1.31s\tremaining: 971ms\n",
      "115:\tlearn: 0.0104783\ttotal: 1.32s\tremaining: 959ms\n",
      "116:\tlearn: 0.0101831\ttotal: 1.33s\tremaining: 947ms\n",
      "117:\tlearn: 0.0099970\ttotal: 1.35s\tremaining: 935ms\n",
      "118:\tlearn: 0.0097835\ttotal: 1.36s\tremaining: 924ms\n",
      "119:\tlearn: 0.0095742\ttotal: 1.37s\tremaining: 912ms\n",
      "120:\tlearn: 0.0094723\ttotal: 1.37s\tremaining: 897ms\n",
      "121:\tlearn: 0.0092619\ttotal: 1.39s\tremaining: 886ms\n",
      "122:\tlearn: 0.0090783\ttotal: 1.4s\tremaining: 874ms\n",
      "123:\tlearn: 0.0089100\ttotal: 1.41s\tremaining: 863ms\n",
      "124:\tlearn: 0.0087452\ttotal: 1.42s\tremaining: 851ms\n",
      "125:\tlearn: 0.0085556\ttotal: 1.43s\tremaining: 840ms\n",
      "126:\tlearn: 0.0084450\ttotal: 1.44s\tremaining: 828ms\n",
      "127:\tlearn: 0.0082032\ttotal: 1.45s\tremaining: 817ms\n",
      "128:\tlearn: 0.0080194\ttotal: 1.46s\tremaining: 805ms\n",
      "129:\tlearn: 0.0078610\ttotal: 1.47s\tremaining: 794ms\n",
      "130:\tlearn: 0.0077618\ttotal: 1.49s\tremaining: 782ms\n",
      "131:\tlearn: 0.0076880\ttotal: 1.5s\tremaining: 771ms\n",
      "132:\tlearn: 0.0074904\ttotal: 1.51s\tremaining: 759ms\n",
      "133:\tlearn: 0.0072162\ttotal: 1.52s\tremaining: 748ms\n",
      "134:\tlearn: 0.0070858\ttotal: 1.53s\tremaining: 737ms\n",
      "135:\tlearn: 0.0068237\ttotal: 1.54s\tremaining: 725ms\n",
      "136:\tlearn: 0.0067247\ttotal: 1.55s\tremaining: 714ms\n",
      "137:\tlearn: 0.0065518\ttotal: 1.56s\tremaining: 702ms\n",
      "138:\tlearn: 0.0063553\ttotal: 1.57s\tremaining: 691ms\n",
      "139:\tlearn: 0.0062026\ttotal: 1.58s\tremaining: 679ms\n",
      "140:\tlearn: 0.0060327\ttotal: 1.6s\tremaining: 668ms\n",
      "141:\tlearn: 0.0059357\ttotal: 1.61s\tremaining: 657ms\n",
      "142:\tlearn: 0.0058234\ttotal: 1.62s\tremaining: 645ms\n",
      "143:\tlearn: 0.0057390\ttotal: 1.63s\tremaining: 634ms\n",
      "144:\tlearn: 0.0056583\ttotal: 1.64s\tremaining: 623ms\n",
      "145:\tlearn: 0.0055340\ttotal: 1.65s\tremaining: 611ms\n",
      "146:\tlearn: 0.0054079\ttotal: 1.66s\tremaining: 600ms\n",
      "147:\tlearn: 0.0053411\ttotal: 1.68s\tremaining: 589ms\n",
      "148:\tlearn: 0.0052484\ttotal: 1.69s\tremaining: 577ms\n",
      "149:\tlearn: 0.0051396\ttotal: 1.7s\tremaining: 566ms\n",
      "150:\tlearn: 0.0050695\ttotal: 1.71s\tremaining: 554ms\n",
      "151:\tlearn: 0.0049841\ttotal: 1.72s\tremaining: 543ms\n",
      "152:\tlearn: 0.0048799\ttotal: 1.73s\tremaining: 531ms\n",
      "153:\tlearn: 0.0047952\ttotal: 1.74s\tremaining: 520ms\n",
      "154:\tlearn: 0.0047130\ttotal: 1.75s\tremaining: 508ms\n",
      "155:\tlearn: 0.0046429\ttotal: 1.76s\tremaining: 497ms\n",
      "156:\tlearn: 0.0045405\ttotal: 1.77s\tremaining: 486ms\n",
      "157:\tlearn: 0.0044172\ttotal: 1.78s\tremaining: 474ms\n",
      "158:\tlearn: 0.0043487\ttotal: 1.79s\tremaining: 463ms\n",
      "159:\tlearn: 0.0042766\ttotal: 1.8s\tremaining: 451ms\n",
      "160:\tlearn: 0.0041756\ttotal: 1.82s\tremaining: 440ms\n",
      "161:\tlearn: 0.0040822\ttotal: 1.83s\tremaining: 429ms\n",
      "162:\tlearn: 0.0039769\ttotal: 1.84s\tremaining: 418ms\n",
      "163:\tlearn: 0.0038495\ttotal: 1.85s\tremaining: 406ms\n",
      "164:\tlearn: 0.0037729\ttotal: 1.86s\tremaining: 395ms\n",
      "165:\tlearn: 0.0036989\ttotal: 1.87s\tremaining: 384ms\n",
      "166:\tlearn: 0.0036587\ttotal: 1.88s\tremaining: 372ms\n",
      "167:\tlearn: 0.0036260\ttotal: 1.9s\tremaining: 361ms\n",
      "168:\tlearn: 0.0035379\ttotal: 1.91s\tremaining: 350ms\n",
      "169:\tlearn: 0.0034918\ttotal: 1.92s\tremaining: 338ms\n",
      "170:\tlearn: 0.0034361\ttotal: 1.93s\tremaining: 327ms\n",
      "171:\tlearn: 0.0033082\ttotal: 1.94s\tremaining: 316ms\n",
      "172:\tlearn: 0.0032706\ttotal: 1.95s\tremaining: 304ms\n",
      "173:\tlearn: 0.0032375\ttotal: 1.96s\tremaining: 293ms\n",
      "174:\tlearn: 0.0031937\ttotal: 1.97s\tremaining: 282ms\n",
      "175:\tlearn: 0.0031662\ttotal: 1.98s\tremaining: 271ms\n",
      "176:\tlearn: 0.0031310\ttotal: 2s\tremaining: 259ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177:\tlearn: 0.0030127\ttotal: 2.01s\tremaining: 248ms\n",
      "178:\tlearn: 0.0029571\ttotal: 2.02s\tremaining: 237ms\n",
      "179:\tlearn: 0.0029209\ttotal: 2.03s\tremaining: 225ms\n",
      "180:\tlearn: 0.0028356\ttotal: 2.04s\tremaining: 214ms\n",
      "181:\tlearn: 0.0028181\ttotal: 2.05s\tremaining: 202ms\n",
      "182:\tlearn: 0.0027812\ttotal: 2.06s\tremaining: 191ms\n",
      "183:\tlearn: 0.0027390\ttotal: 2.07s\tremaining: 180ms\n",
      "184:\tlearn: 0.0026785\ttotal: 2.08s\tremaining: 169ms\n",
      "185:\tlearn: 0.0026319\ttotal: 2.09s\tremaining: 157ms\n",
      "186:\tlearn: 0.0025985\ttotal: 2.1s\tremaining: 146ms\n",
      "187:\tlearn: 0.0025235\ttotal: 2.12s\tremaining: 135ms\n",
      "188:\tlearn: 0.0024900\ttotal: 2.14s\tremaining: 124ms\n",
      "189:\tlearn: 0.0024602\ttotal: 2.15s\tremaining: 113ms\n",
      "190:\tlearn: 0.0024428\ttotal: 2.17s\tremaining: 102ms\n",
      "191:\tlearn: 0.0023834\ttotal: 2.18s\tremaining: 91ms\n",
      "192:\tlearn: 0.0023507\ttotal: 2.2s\tremaining: 79.7ms\n",
      "193:\tlearn: 0.0022907\ttotal: 2.21s\tremaining: 68.3ms\n",
      "194:\tlearn: 0.0022521\ttotal: 2.22s\tremaining: 57ms\n",
      "195:\tlearn: 0.0022274\ttotal: 2.23s\tremaining: 45.6ms\n",
      "196:\tlearn: 0.0021836\ttotal: 2.24s\tremaining: 34.2ms\n",
      "197:\tlearn: 0.0021241\ttotal: 2.25s\tremaining: 22.8ms\n",
      "198:\tlearn: 0.0020596\ttotal: 2.27s\tremaining: 11.4ms\n",
      "199:\tlearn: 0.0020261\ttotal: 2.28s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ef2139e3014471b2a7e013a6f517ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5689190467540934, Recall = 0.8735267452402539, Aging Rate = 0.6434723481414325, Precision = 0.6787601268052131, f1 = 0.7639246778989099\n",
      "Epoch 2: Train Loss = 0.36771960193097103, Recall = 0.8957388939256573, Aging Rate = 0.5512239347234814, Precision = 0.8125, f1 = 0.8520914187149634\n",
      "Epoch 3: Train Loss = 0.27401787361445906, Recall = 0.9242973708068903, Aging Rate = 0.5337715321849501, Precision = 0.8658174097664544, f1 = 0.894102170576628\n",
      "Epoch 4: Train Loss = 0.21565271084511808, Recall = 0.9392565729827742, Aging Rate = 0.5213055303717135, Precision = 0.9008695652173913, f1 = 0.9196626719928983\n",
      "Epoch 5: Train Loss = 0.16771178309580248, Recall = 0.9678150498640072, Aging Rate = 0.5194922937443336, Precision = 0.9315008726003491, f1 = 0.9493108048021343\n",
      "Test Loss = 0.1322064742594552, Recall = 0.9922937443336355, Aging Rate = 0.5310516772438804, precision = 0.9342723004694836\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.11733588788418584, Recall = 0.9873073436083409, Aging Rate = 0.513599274705349, Precision = 0.9611650485436893, f1 = 0.9740608228980323\n",
      "Epoch 7: Train Loss = 0.09180368568177452, Recall = 0.9913871260199456, Aging Rate = 0.5088395285584769, Precision = 0.9741648106904232, f1 = 0.9827005167378117\n",
      "Epoch 8: Train Loss = 0.07104508526089617, Recall = 0.9918404351767905, Aging Rate = 0.5047597461468721, Precision = 0.9824876515491693, f1 = 0.9871418903676967\n",
      "Epoch 9: Train Loss = 0.056985219989515716, Recall = 0.9950135992747053, Aging Rate = 0.5031731640979148, Precision = 0.9887387387387387, f1 = 0.9918662449164031\n",
      "Epoch 10: Train Loss = 0.04594460775837613, Recall = 0.9954669084315503, Aging Rate = 0.5029465095194923, Precision = 0.9896349707075259, f1 = 0.9925423728813559\n",
      "Test Loss = 0.03865232482444451, Recall = 0.9963735267452403, Aging Rate = 0.5006799637352675, precision = 0.9950203712086917\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.03786592818425119, Recall = 0.9963735267452403, Aging Rate = 0.5013599274705349, Precision = 0.9936708860759493, f1 = 0.9950203712086917\n",
      "Epoch 12: Train Loss = 0.030967585164935398, Recall = 0.9995466908431551, Aging Rate = 0.5024932003626473, Precision = 0.9945872801082544, f1 = 0.9970608184490165\n",
      "Epoch 13: Train Loss = 0.026160715548921097, Recall = 0.99909338168631, Aging Rate = 0.5022665457842248, Precision = 0.9945848375451264, f1 = 0.9968340117593849\n",
      "Epoch 14: Train Loss = 0.022320924743888384, Recall = 0.99909338168631, Aging Rate = 0.5004533091568449, Precision = 0.9981884057971014, f1 = 0.9986406887177163\n",
      "Epoch 15: Train Loss = 0.018798197574021864, Recall = 0.9995466908431551, Aging Rate = 0.50090661831369, Precision = 0.997737556561086, f1 = 0.998641304347826\n",
      "Test Loss = 0.016446034207007933, Recall = 1.0, Aging Rate = 0.50090661831369, precision = 0.9981900452488688\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.016210222192925967, Recall = 0.9995466908431551, Aging Rate = 0.5004533091568449, Precision = 0.998641304347826, f1 = 0.9990937924784776\n",
      "Epoch 17: Train Loss = 0.01413351275340898, Recall = 1.0, Aging Rate = 0.50090661831369, Precision = 0.9981900452488688, f1 = 0.9990942028985508\n",
      "Epoch 18: Train Loss = 0.012141074378175132, Recall = 1.0, Aging Rate = 0.5004533091568449, Precision = 0.9990942028985508, f1 = 0.9995468962392388\n",
      "Epoch 19: Train Loss = 0.010561204386211371, Recall = 1.0, Aging Rate = 0.5004533091568449, Precision = 0.9990942028985508, f1 = 0.9995468962392388\n",
      "Epoch 20: Train Loss = 0.00944596158504378, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Test Loss = 0.00816831295821792, Recall = 1.0, Aging Rate = 0.5002266545784225, precision = 0.9995468962392388\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.00836142716956336, Recall = 1.0, Aging Rate = 0.5004533091568449, Precision = 0.9990942028985508, f1 = 0.9995468962392388\n",
      "Epoch 22: Train Loss = 0.0073433674775665175, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 23: Train Loss = 0.0066361945934749035, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.006065743463662734, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.005430754639313017, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004900772051070078, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.0049853605817300915, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.004621907661467156, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.004124112829886012, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.003901673458492956, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.003692893495239446, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0032244912242721207, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.00334156173976275, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.0032419958965623193, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.00302306555738421, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0028883532577904376, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.0027896049791306986, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002446853730520225, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.0025731751717228512, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.002441184502559379, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.002371146628617943, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.002415077227606005, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.0022831601306039532, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019467571421618408, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.002115098981808192, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0020167623084639675, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.001980433230822803, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0020230657540106383, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0020596803297269297, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017449696486434162, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.0018894947030524962, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0018793902429984249, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0018484154103977346, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0017914137895263778, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0018697792294976433, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016411118111628042, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0017551344081401447, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0018487087970621486, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0018720166789999372, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.00496752976732462, Recall = 0.9995466908431551, Aging Rate = 0.5, Precision = 0.9995466908431551, f1 = 0.9995466908431551\n",
      "Epoch 55: Train Loss = 0.0015400238036442336, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014476136857278963, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0013869161409298877, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.001393299966770649, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0014124593975280808, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: Train Loss = 0.0014741772055205428, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0014992766910809298, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013181218617373668, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0015141787307196763, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0015629184615341498, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0016405237529381219, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0016730745183261735, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0015744427782639603, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013509312272451798, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0015195756139535987, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0015524041527377294, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0016042679311949945, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.001656698960775437, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.001590566740245478, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013840484675839003, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0016135245140267034, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0017742146691434079, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0015871911752945384, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0015429736470511473, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0015656110742053393, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013273749259344436, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6282463\ttotal: 13.6ms\tremaining: 2.71s\n",
      "1:\tlearn: 0.5664245\ttotal: 27ms\tremaining: 2.67s\n",
      "2:\tlearn: 0.5179865\ttotal: 40ms\tremaining: 2.63s\n",
      "3:\tlearn: 0.4715740\ttotal: 53.4ms\tremaining: 2.62s\n",
      "4:\tlearn: 0.4336932\ttotal: 66.7ms\tremaining: 2.6s\n",
      "5:\tlearn: 0.4137783\ttotal: 79.1ms\tremaining: 2.56s\n",
      "6:\tlearn: 0.3811509\ttotal: 92.7ms\tremaining: 2.55s\n",
      "7:\tlearn: 0.3540033\ttotal: 107ms\tremaining: 2.56s\n",
      "8:\tlearn: 0.3312574\ttotal: 119ms\tremaining: 2.52s\n",
      "9:\tlearn: 0.3069746\ttotal: 131ms\tremaining: 2.49s\n",
      "10:\tlearn: 0.2940318\ttotal: 143ms\tremaining: 2.46s\n",
      "11:\tlearn: 0.2763260\ttotal: 155ms\tremaining: 2.42s\n",
      "12:\tlearn: 0.2595115\ttotal: 166ms\tremaining: 2.39s\n",
      "13:\tlearn: 0.2467195\ttotal: 178ms\tremaining: 2.36s\n",
      "14:\tlearn: 0.2271952\ttotal: 190ms\tremaining: 2.34s\n",
      "15:\tlearn: 0.2208473\ttotal: 202ms\tremaining: 2.32s\n",
      "16:\tlearn: 0.2089641\ttotal: 213ms\tremaining: 2.3s\n",
      "17:\tlearn: 0.1971199\ttotal: 225ms\tremaining: 2.27s\n",
      "18:\tlearn: 0.1870742\ttotal: 237ms\tremaining: 2.26s\n",
      "19:\tlearn: 0.1798212\ttotal: 249ms\tremaining: 2.24s\n",
      "20:\tlearn: 0.1697921\ttotal: 261ms\tremaining: 2.22s\n",
      "21:\tlearn: 0.1614158\ttotal: 273ms\tremaining: 2.21s\n",
      "22:\tlearn: 0.1544679\ttotal: 285ms\tremaining: 2.19s\n",
      "23:\tlearn: 0.1472669\ttotal: 297ms\tremaining: 2.18s\n",
      "24:\tlearn: 0.1418770\ttotal: 309ms\tremaining: 2.16s\n",
      "25:\tlearn: 0.1380352\ttotal: 321ms\tremaining: 2.15s\n",
      "26:\tlearn: 0.1321017\ttotal: 332ms\tremaining: 2.13s\n",
      "27:\tlearn: 0.1276067\ttotal: 344ms\tremaining: 2.11s\n",
      "28:\tlearn: 0.1217615\ttotal: 356ms\tremaining: 2.1s\n",
      "29:\tlearn: 0.1165328\ttotal: 368ms\tremaining: 2.09s\n",
      "30:\tlearn: 0.1105322\ttotal: 380ms\tremaining: 2.07s\n",
      "31:\tlearn: 0.1057104\ttotal: 392ms\tremaining: 2.06s\n",
      "32:\tlearn: 0.0996352\ttotal: 404ms\tremaining: 2.04s\n",
      "33:\tlearn: 0.0970926\ttotal: 416ms\tremaining: 2.03s\n",
      "34:\tlearn: 0.0938762\ttotal: 427ms\tremaining: 2.02s\n",
      "35:\tlearn: 0.0897370\ttotal: 440ms\tremaining: 2s\n",
      "36:\tlearn: 0.0869755\ttotal: 452ms\tremaining: 1.99s\n",
      "37:\tlearn: 0.0836491\ttotal: 464ms\tremaining: 1.98s\n",
      "38:\tlearn: 0.0810086\ttotal: 475ms\tremaining: 1.96s\n",
      "39:\tlearn: 0.0774649\ttotal: 487ms\tremaining: 1.95s\n",
      "40:\tlearn: 0.0755482\ttotal: 498ms\tremaining: 1.93s\n",
      "41:\tlearn: 0.0742091\ttotal: 510ms\tremaining: 1.92s\n",
      "42:\tlearn: 0.0713731\ttotal: 521ms\tremaining: 1.9s\n",
      "43:\tlearn: 0.0685228\ttotal: 533ms\tremaining: 1.89s\n",
      "44:\tlearn: 0.0657606\ttotal: 545ms\tremaining: 1.88s\n",
      "45:\tlearn: 0.0646703\ttotal: 556ms\tremaining: 1.86s\n",
      "46:\tlearn: 0.0627274\ttotal: 568ms\tremaining: 1.85s\n",
      "47:\tlearn: 0.0611634\ttotal: 579ms\tremaining: 1.83s\n",
      "48:\tlearn: 0.0579338\ttotal: 591ms\tremaining: 1.82s\n",
      "49:\tlearn: 0.0567877\ttotal: 602ms\tremaining: 1.81s\n",
      "50:\tlearn: 0.0557985\ttotal: 614ms\tremaining: 1.79s\n",
      "51:\tlearn: 0.0532817\ttotal: 626ms\tremaining: 1.78s\n",
      "52:\tlearn: 0.0524375\ttotal: 638ms\tremaining: 1.77s\n",
      "53:\tlearn: 0.0507417\ttotal: 649ms\tremaining: 1.75s\n",
      "54:\tlearn: 0.0492312\ttotal: 661ms\tremaining: 1.74s\n",
      "55:\tlearn: 0.0470865\ttotal: 673ms\tremaining: 1.73s\n",
      "56:\tlearn: 0.0464312\ttotal: 685ms\tremaining: 1.72s\n",
      "57:\tlearn: 0.0434469\ttotal: 696ms\tremaining: 1.7s\n",
      "58:\tlearn: 0.0420340\ttotal: 708ms\tremaining: 1.69s\n",
      "59:\tlearn: 0.0403699\ttotal: 720ms\tremaining: 1.68s\n",
      "60:\tlearn: 0.0394012\ttotal: 732ms\tremaining: 1.67s\n",
      "61:\tlearn: 0.0386496\ttotal: 744ms\tremaining: 1.66s\n",
      "62:\tlearn: 0.0375238\ttotal: 756ms\tremaining: 1.64s\n",
      "63:\tlearn: 0.0365149\ttotal: 768ms\tremaining: 1.63s\n",
      "64:\tlearn: 0.0346790\ttotal: 779ms\tremaining: 1.62s\n",
      "65:\tlearn: 0.0339492\ttotal: 791ms\tremaining: 1.61s\n",
      "66:\tlearn: 0.0330976\ttotal: 803ms\tremaining: 1.59s\n",
      "67:\tlearn: 0.0324445\ttotal: 815ms\tremaining: 1.58s\n",
      "68:\tlearn: 0.0318844\ttotal: 826ms\tremaining: 1.57s\n",
      "69:\tlearn: 0.0314524\ttotal: 838ms\tremaining: 1.55s\n",
      "70:\tlearn: 0.0310351\ttotal: 850ms\tremaining: 1.54s\n",
      "71:\tlearn: 0.0305436\ttotal: 861ms\tremaining: 1.53s\n",
      "72:\tlearn: 0.0297367\ttotal: 873ms\tremaining: 1.52s\n",
      "73:\tlearn: 0.0290076\ttotal: 885ms\tremaining: 1.51s\n",
      "74:\tlearn: 0.0284509\ttotal: 897ms\tremaining: 1.5s\n",
      "75:\tlearn: 0.0278318\ttotal: 909ms\tremaining: 1.48s\n",
      "76:\tlearn: 0.0273875\ttotal: 921ms\tremaining: 1.47s\n",
      "77:\tlearn: 0.0267460\ttotal: 932ms\tremaining: 1.46s\n",
      "78:\tlearn: 0.0262258\ttotal: 944ms\tremaining: 1.45s\n",
      "79:\tlearn: 0.0258247\ttotal: 956ms\tremaining: 1.43s\n",
      "80:\tlearn: 0.0253054\ttotal: 968ms\tremaining: 1.42s\n",
      "81:\tlearn: 0.0245267\ttotal: 979ms\tremaining: 1.41s\n",
      "82:\tlearn: 0.0236723\ttotal: 991ms\tremaining: 1.4s\n",
      "83:\tlearn: 0.0233404\ttotal: 1s\tremaining: 1.39s\n",
      "84:\tlearn: 0.0228185\ttotal: 1.01s\tremaining: 1.37s\n",
      "85:\tlearn: 0.0226152\ttotal: 1.03s\tremaining: 1.36s\n",
      "86:\tlearn: 0.0222039\ttotal: 1.04s\tremaining: 1.35s\n",
      "87:\tlearn: 0.0216522\ttotal: 1.05s\tremaining: 1.34s\n",
      "88:\tlearn: 0.0208556\ttotal: 1.06s\tremaining: 1.32s\n",
      "89:\tlearn: 0.0200130\ttotal: 1.07s\tremaining: 1.31s\n",
      "90:\tlearn: 0.0195307\ttotal: 1.09s\tremaining: 1.31s\n",
      "91:\tlearn: 0.0186304\ttotal: 1.11s\tremaining: 1.3s\n",
      "92:\tlearn: 0.0179603\ttotal: 1.13s\tremaining: 1.3s\n",
      "93:\tlearn: 0.0173985\ttotal: 1.15s\tremaining: 1.29s\n",
      "94:\tlearn: 0.0169615\ttotal: 1.16s\tremaining: 1.29s\n",
      "95:\tlearn: 0.0164184\ttotal: 1.18s\tremaining: 1.27s\n",
      "96:\tlearn: 0.0159918\ttotal: 1.19s\tremaining: 1.27s\n",
      "97:\tlearn: 0.0158071\ttotal: 1.21s\tremaining: 1.26s\n",
      "98:\tlearn: 0.0154502\ttotal: 1.22s\tremaining: 1.24s\n",
      "99:\tlearn: 0.0152872\ttotal: 1.23s\tremaining: 1.23s\n",
      "100:\tlearn: 0.0150107\ttotal: 1.24s\tremaining: 1.22s\n",
      "101:\tlearn: 0.0147143\ttotal: 1.26s\tremaining: 1.21s\n",
      "102:\tlearn: 0.0144514\ttotal: 1.27s\tremaining: 1.19s\n",
      "103:\tlearn: 0.0140280\ttotal: 1.28s\tremaining: 1.18s\n",
      "104:\tlearn: 0.0138139\ttotal: 1.29s\tremaining: 1.17s\n",
      "105:\tlearn: 0.0135865\ttotal: 1.3s\tremaining: 1.16s\n",
      "106:\tlearn: 0.0133413\ttotal: 1.31s\tremaining: 1.14s\n",
      "107:\tlearn: 0.0131148\ttotal: 1.33s\tremaining: 1.13s\n",
      "108:\tlearn: 0.0129944\ttotal: 1.34s\tremaining: 1.12s\n",
      "109:\tlearn: 0.0127214\ttotal: 1.35s\tremaining: 1.1s\n",
      "110:\tlearn: 0.0124064\ttotal: 1.36s\tremaining: 1.09s\n",
      "111:\tlearn: 0.0119928\ttotal: 1.37s\tremaining: 1.08s\n",
      "112:\tlearn: 0.0117322\ttotal: 1.39s\tremaining: 1.07s\n",
      "113:\tlearn: 0.0113448\ttotal: 1.4s\tremaining: 1.05s\n",
      "114:\tlearn: 0.0111000\ttotal: 1.41s\tremaining: 1.04s\n",
      "115:\tlearn: 0.0109947\ttotal: 1.42s\tremaining: 1.03s\n",
      "116:\tlearn: 0.0108232\ttotal: 1.43s\tremaining: 1.01s\n",
      "117:\tlearn: 0.0106867\ttotal: 1.44s\tremaining: 1s\n",
      "118:\tlearn: 0.0105649\ttotal: 1.46s\tremaining: 991ms\n",
      "119:\tlearn: 0.0104009\ttotal: 1.47s\tremaining: 978ms\n",
      "120:\tlearn: 0.0103236\ttotal: 1.48s\tremaining: 966ms\n",
      "121:\tlearn: 0.0101831\ttotal: 1.49s\tremaining: 953ms\n",
      "122:\tlearn: 0.0099095\ttotal: 1.5s\tremaining: 941ms\n",
      "123:\tlearn: 0.0096133\ttotal: 1.51s\tremaining: 928ms\n",
      "124:\tlearn: 0.0093661\ttotal: 1.52s\tremaining: 916ms\n",
      "125:\tlearn: 0.0091972\ttotal: 1.54s\tremaining: 903ms\n",
      "126:\tlearn: 0.0090532\ttotal: 1.55s\tremaining: 891ms\n",
      "127:\tlearn: 0.0087985\ttotal: 1.56s\tremaining: 878ms\n",
      "128:\tlearn: 0.0086431\ttotal: 1.57s\tremaining: 866ms\n",
      "129:\tlearn: 0.0085183\ttotal: 1.58s\tremaining: 853ms\n",
      "130:\tlearn: 0.0084343\ttotal: 1.6s\tremaining: 841ms\n",
      "131:\tlearn: 0.0083032\ttotal: 1.61s\tremaining: 829ms\n",
      "132:\tlearn: 0.0081738\ttotal: 1.62s\tremaining: 816ms\n",
      "133:\tlearn: 0.0080425\ttotal: 1.63s\tremaining: 804ms\n",
      "134:\tlearn: 0.0079426\ttotal: 1.64s\tremaining: 792ms\n",
      "135:\tlearn: 0.0078336\ttotal: 1.66s\tremaining: 779ms\n",
      "136:\tlearn: 0.0076611\ttotal: 1.67s\tremaining: 767ms\n",
      "137:\tlearn: 0.0075607\ttotal: 1.68s\tremaining: 755ms\n",
      "138:\tlearn: 0.0075022\ttotal: 1.69s\tremaining: 742ms\n",
      "139:\tlearn: 0.0073804\ttotal: 1.7s\tremaining: 730ms\n",
      "140:\tlearn: 0.0072195\ttotal: 1.71s\tremaining: 717ms\n",
      "141:\tlearn: 0.0070677\ttotal: 1.73s\tremaining: 705ms\n",
      "142:\tlearn: 0.0069386\ttotal: 1.74s\tremaining: 693ms\n",
      "143:\tlearn: 0.0068282\ttotal: 1.75s\tremaining: 680ms\n",
      "144:\tlearn: 0.0066423\ttotal: 1.76s\tremaining: 668ms\n",
      "145:\tlearn: 0.0064596\ttotal: 1.77s\tremaining: 656ms\n",
      "146:\tlearn: 0.0063369\ttotal: 1.78s\tremaining: 643ms\n",
      "147:\tlearn: 0.0062541\ttotal: 1.8s\tremaining: 631ms\n",
      "148:\tlearn: 0.0061560\ttotal: 1.81s\tremaining: 619ms\n",
      "149:\tlearn: 0.0060656\ttotal: 1.82s\tremaining: 607ms\n",
      "150:\tlearn: 0.0059484\ttotal: 1.83s\tremaining: 594ms\n",
      "151:\tlearn: 0.0058063\ttotal: 1.84s\tremaining: 582ms\n",
      "152:\tlearn: 0.0057351\ttotal: 1.85s\tremaining: 570ms\n",
      "153:\tlearn: 0.0056483\ttotal: 1.87s\tremaining: 558ms\n",
      "154:\tlearn: 0.0055407\ttotal: 1.88s\tremaining: 545ms\n",
      "155:\tlearn: 0.0054835\ttotal: 1.89s\tremaining: 533ms\n",
      "156:\tlearn: 0.0054030\ttotal: 1.9s\tremaining: 521ms\n",
      "157:\tlearn: 0.0053059\ttotal: 1.91s\tremaining: 509ms\n",
      "158:\tlearn: 0.0052654\ttotal: 1.93s\tremaining: 496ms\n",
      "159:\tlearn: 0.0050635\ttotal: 1.94s\tremaining: 484ms\n",
      "160:\tlearn: 0.0049383\ttotal: 1.95s\tremaining: 472ms\n",
      "161:\tlearn: 0.0048686\ttotal: 1.96s\tremaining: 460ms\n",
      "162:\tlearn: 0.0047778\ttotal: 1.97s\tremaining: 448ms\n",
      "163:\tlearn: 0.0047168\ttotal: 1.98s\tremaining: 436ms\n",
      "164:\tlearn: 0.0046598\ttotal: 2s\tremaining: 423ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165:\tlearn: 0.0045829\ttotal: 2.01s\tremaining: 411ms\n",
      "166:\tlearn: 0.0044599\ttotal: 2.02s\tremaining: 399ms\n",
      "167:\tlearn: 0.0043811\ttotal: 2.03s\tremaining: 387ms\n",
      "168:\tlearn: 0.0042962\ttotal: 2.04s\tremaining: 375ms\n",
      "169:\tlearn: 0.0042379\ttotal: 2.05s\tremaining: 363ms\n",
      "170:\tlearn: 0.0041425\ttotal: 2.07s\tremaining: 351ms\n",
      "171:\tlearn: 0.0040442\ttotal: 2.08s\tremaining: 339ms\n",
      "172:\tlearn: 0.0039969\ttotal: 2.1s\tremaining: 328ms\n",
      "173:\tlearn: 0.0039499\ttotal: 2.12s\tremaining: 316ms\n",
      "174:\tlearn: 0.0038478\ttotal: 2.13s\tremaining: 305ms\n",
      "175:\tlearn: 0.0037520\ttotal: 2.15s\tremaining: 293ms\n",
      "176:\tlearn: 0.0036604\ttotal: 2.16s\tremaining: 281ms\n",
      "177:\tlearn: 0.0035720\ttotal: 2.18s\tremaining: 269ms\n",
      "178:\tlearn: 0.0035122\ttotal: 2.19s\tremaining: 257ms\n",
      "179:\tlearn: 0.0034576\ttotal: 2.2s\tremaining: 245ms\n",
      "180:\tlearn: 0.0034219\ttotal: 2.22s\tremaining: 233ms\n",
      "181:\tlearn: 0.0033724\ttotal: 2.23s\tremaining: 220ms\n",
      "182:\tlearn: 0.0032668\ttotal: 2.24s\tremaining: 208ms\n",
      "183:\tlearn: 0.0032123\ttotal: 2.25s\tremaining: 196ms\n",
      "184:\tlearn: 0.0031230\ttotal: 2.26s\tremaining: 184ms\n",
      "185:\tlearn: 0.0030564\ttotal: 2.27s\tremaining: 171ms\n",
      "186:\tlearn: 0.0029982\ttotal: 2.29s\tremaining: 159ms\n",
      "187:\tlearn: 0.0029631\ttotal: 2.3s\tremaining: 147ms\n",
      "188:\tlearn: 0.0029379\ttotal: 2.31s\tremaining: 134ms\n",
      "189:\tlearn: 0.0029179\ttotal: 2.32s\tremaining: 122ms\n",
      "190:\tlearn: 0.0028387\ttotal: 2.33s\tremaining: 110ms\n",
      "191:\tlearn: 0.0028059\ttotal: 2.35s\tremaining: 97.8ms\n",
      "192:\tlearn: 0.0027546\ttotal: 2.36s\tremaining: 85.5ms\n",
      "193:\tlearn: 0.0026712\ttotal: 2.37s\tremaining: 73.3ms\n",
      "194:\tlearn: 0.0026199\ttotal: 2.38s\tremaining: 61.1ms\n",
      "195:\tlearn: 0.0025913\ttotal: 2.39s\tremaining: 48.8ms\n",
      "196:\tlearn: 0.0025269\ttotal: 2.4s\tremaining: 36.6ms\n",
      "197:\tlearn: 0.0024882\ttotal: 2.42s\tremaining: 24.4ms\n",
      "198:\tlearn: 0.0024685\ttotal: 2.43s\tremaining: 12.2ms\n",
      "199:\tlearn: 0.0024257\ttotal: 2.44s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343c73609b8c45d69114b3b7fcfd45d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5836091624107845, Recall = 0.9043951064793838, Aging Rate = 0.7002718622564568, Precision = 0.6457457133613718, f1 = 0.7534918837297093\n",
      "Epoch 2: Train Loss = 0.3829543048378348, Recall = 0.8912550974173086, Aging Rate = 0.5552786588128682, Precision = 0.8025295797633619, f1 = 0.8445684843280378\n",
      "Epoch 3: Train Loss = 0.2924078421377736, Recall = 0.9093792478477571, Aging Rate = 0.529451744449479, Precision = 0.858793324775353, f1 = 0.8833626760563379\n",
      "Epoch 4: Train Loss = 0.23674600465728732, Recall = 0.932487539646579, Aging Rate = 0.5260534662437698, Precision = 0.8863049095607235, f1 = 0.9088098918083461\n",
      "Epoch 5: Train Loss = 0.1907111623260734, Recall = 0.9501585863162664, Aging Rate = 0.5199365654734934, Precision = 0.9137254901960784, f1 = 0.9315859617947578\n",
      "Test Loss = 0.15205729828736445, Recall = 0.9777979157227005, Aging Rate = 0.5197100135931129, precision = 0.9407149084568439\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.13788596245299437, Recall = 0.9759855006796556, Aging Rate = 0.5133665609424558, Precision = 0.9505736981465137, f1 = 0.9631120053655264\n",
      "Epoch 7: Train Loss = 0.10590814491189547, Recall = 0.9864068871771635, Aging Rate = 0.5106479383778886, Precision = 0.9658385093167702, f1 = 0.9760143465590674\n",
      "Epoch 8: Train Loss = 0.0830486206026384, Recall = 0.9932034435885818, Aging Rate = 0.5090620752152243, Precision = 0.9755229194481531, f1 = 0.9842837898518186\n",
      "Epoch 9: Train Loss = 0.06512248190984933, Recall = 0.9972813774354327, Aging Rate = 0.5054372451291346, Precision = 0.9865531151949798, f1 = 0.9918882379450202\n",
      "Epoch 10: Train Loss = 0.05136981021104812, Recall = 0.9977344811961939, Aging Rate = 0.5029451744449479, Precision = 0.9918918918918919, f1 = 0.9948046080867404\n",
      "Test Loss = 0.04244913413148419, Recall = 0.9986406887177164, Aging Rate = 0.501132759401903, precision = 0.9963833634719711\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.041042656418717606, Recall = 0.9990937924784775, Aging Rate = 0.5018124150430449, Precision = 0.9954853273137697, f1 = 0.9972862957937585\n",
      "Epoch 12: Train Loss = 0.033943898761480386, Recall = 0.9981875849569551, Aging Rate = 0.5004531037607612, Precision = 0.9972838388411046, f1 = 0.9977355072463768\n",
      "Epoch 13: Train Loss = 0.02858399323558694, Recall = 0.9990937924784775, Aging Rate = 0.5004531037607612, Precision = 0.9981892258940697, f1 = 0.9986413043478259\n",
      "Epoch 14: Train Loss = 0.02402022979583036, Recall = 0.9990937924784775, Aging Rate = 0.5002265518803806, Precision = 0.998641304347826, f1 = 0.9988674971687429\n",
      "Epoch 15: Train Loss = 0.020530690445375217, Recall = 0.9990937924784775, Aging Rate = 0.5002265518803806, Precision = 0.998641304347826, f1 = 0.9988674971687429\n",
      "Test Loss = 0.017390090313043376, Recall = 0.9995468962392388, Aging Rate = 0.5002265518803806, precision = 0.9990942028985508\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.0177216790743108, Recall = 0.9995468962392388, Aging Rate = 0.5002265518803806, Precision = 0.9990942028985508, f1 = 0.9993204983012457\n",
      "Epoch 17: Train Loss = 0.015041100775285388, Recall = 0.9995468962392388, Aging Rate = 0.5002265518803806, Precision = 0.9990942028985508, f1 = 0.9993204983012457\n",
      "Epoch 18: Train Loss = 0.013199382925158815, Recall = 0.9995468962392388, Aging Rate = 0.5002265518803806, Precision = 0.9990942028985508, f1 = 0.9993204983012457\n",
      "Epoch 19: Train Loss = 0.011256470611233705, Recall = 0.9995468962392388, Aging Rate = 0.5002265518803806, Precision = 0.9990942028985508, f1 = 0.9993204983012457\n",
      "Epoch 20: Train Loss = 0.009849756649316815, Recall = 0.9995468962392388, Aging Rate = 0.5, Precision = 0.9995468962392388, f1 = 0.9995468962392388\n",
      "Test Loss = 0.008875655728984348, Recall = 1.0, Aging Rate = 0.5004531037607612, precision = 0.9990946129470348\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.008886628224264188, Recall = 0.9995468962392388, Aging Rate = 0.5002265518803806, Precision = 0.9990942028985508, f1 = 0.9993204983012457\n",
      "Epoch 22: Train Loss = 0.007731221553181286, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.006921525656802248, Recall = 0.9995468962392388, Aging Rate = 0.4997734481196194, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.006422363395925474, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.005645877655509456, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004959178729257941, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.0051250763994518805, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.004743721201723863, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.004286157331245014, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.004014378836282242, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.0037142246295700098, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00336417697617336, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.003518586493771157, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.003255023646869329, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.003036114823336468, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0029069648217536445, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.0027271797349883264, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0025374010905448736, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.002592902376637619, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.0025042267738852866, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.002385702910636837, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.002297742849517163, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.002242683312677583, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019457414124910452, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.002162025856043203, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.002190476870202254, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.002007603477110633, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0019450183207946473, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0019556667264209117, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017903858435386222, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.001881559999823388, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.001844900175355169, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0019033198578976935, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0017881315909613028, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0018408515414212103, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001537932545823557, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0017337711057112998, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0018173299438020913, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0018313750089525983, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.001637369475481879, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0016318888361382205, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015057102605315642, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0015823752312331474, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0016248852274484856, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Train Loss = 0.001723876645270352, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0017381214974958687, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0015247226363587255, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014417252578659934, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0015044200889670005, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0015696019158324317, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.00458535444128357, Recall = 0.9990937924784775, Aging Rate = 0.4997734481196194, Precision = 0.9995466908431551, f1 = 0.9993201903467029\n",
      "Epoch 64: Train Loss = 0.0025219699349102453, Recall = 1.0, Aging Rate = 0.5002265518803806, Precision = 0.9995471014492754, f1 = 0.9997734994337486\n",
      "Epoch 65: Train Loss = 0.0013889593739395073, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0011691484363677042, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0011909530354295757, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0012019081426278856, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0012364993031707637, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.001303391169296581, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0013499782092811891, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0012596748704086494, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0013718283204568569, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0014202478479431106, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0014665226372297285, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0014436276315504244, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0015284478039648275, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018966642043717445, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6257924\ttotal: 13.3ms\tremaining: 2.65s\n",
      "1:\tlearn: 0.5643598\ttotal: 26.6ms\tremaining: 2.63s\n",
      "2:\tlearn: 0.5150945\ttotal: 39.9ms\tremaining: 2.62s\n",
      "3:\tlearn: 0.4657639\ttotal: 53.3ms\tremaining: 2.61s\n",
      "4:\tlearn: 0.4280355\ttotal: 66.7ms\tremaining: 2.6s\n",
      "5:\tlearn: 0.3992325\ttotal: 79.6ms\tremaining: 2.57s\n",
      "6:\tlearn: 0.3681874\ttotal: 91.2ms\tremaining: 2.52s\n",
      "7:\tlearn: 0.3386103\ttotal: 102ms\tremaining: 2.46s\n",
      "8:\tlearn: 0.3193718\ttotal: 113ms\tremaining: 2.4s\n",
      "9:\tlearn: 0.3013096\ttotal: 125ms\tremaining: 2.37s\n",
      "10:\tlearn: 0.2830281\ttotal: 136ms\tremaining: 2.33s\n",
      "11:\tlearn: 0.2623335\ttotal: 147ms\tremaining: 2.3s\n",
      "12:\tlearn: 0.2525031\ttotal: 158ms\tremaining: 2.27s\n",
      "13:\tlearn: 0.2372285\ttotal: 168ms\tremaining: 2.23s\n",
      "14:\tlearn: 0.2223316\ttotal: 179ms\tremaining: 2.21s\n",
      "15:\tlearn: 0.2104284\ttotal: 192ms\tremaining: 2.2s\n",
      "16:\tlearn: 0.2007876\ttotal: 203ms\tremaining: 2.18s\n",
      "17:\tlearn: 0.1927244\ttotal: 215ms\tremaining: 2.17s\n",
      "18:\tlearn: 0.1788420\ttotal: 226ms\tremaining: 2.15s\n",
      "19:\tlearn: 0.1726553\ttotal: 238ms\tremaining: 2.14s\n",
      "20:\tlearn: 0.1646768\ttotal: 249ms\tremaining: 2.13s\n",
      "21:\tlearn: 0.1578802\ttotal: 261ms\tremaining: 2.11s\n",
      "22:\tlearn: 0.1512584\ttotal: 272ms\tremaining: 2.1s\n",
      "23:\tlearn: 0.1436147\ttotal: 284ms\tremaining: 2.08s\n",
      "24:\tlearn: 0.1362109\ttotal: 296ms\tremaining: 2.07s\n",
      "25:\tlearn: 0.1300888\ttotal: 308ms\tremaining: 2.06s\n",
      "26:\tlearn: 0.1273428\ttotal: 319ms\tremaining: 2.04s\n",
      "27:\tlearn: 0.1191560\ttotal: 332ms\tremaining: 2.04s\n",
      "28:\tlearn: 0.1153933\ttotal: 344ms\tremaining: 2.03s\n",
      "29:\tlearn: 0.1090029\ttotal: 357ms\tremaining: 2.02s\n",
      "30:\tlearn: 0.1062190\ttotal: 369ms\tremaining: 2.01s\n",
      "31:\tlearn: 0.1027600\ttotal: 382ms\tremaining: 2s\n",
      "32:\tlearn: 0.0993221\ttotal: 395ms\tremaining: 2s\n",
      "33:\tlearn: 0.0954277\ttotal: 408ms\tremaining: 1.99s\n",
      "34:\tlearn: 0.0900852\ttotal: 420ms\tremaining: 1.98s\n",
      "35:\tlearn: 0.0874944\ttotal: 433ms\tremaining: 1.97s\n",
      "36:\tlearn: 0.0845032\ttotal: 446ms\tremaining: 1.96s\n",
      "37:\tlearn: 0.0821604\ttotal: 459ms\tremaining: 1.96s\n",
      "38:\tlearn: 0.0801529\ttotal: 472ms\tremaining: 1.95s\n",
      "39:\tlearn: 0.0762350\ttotal: 485ms\tremaining: 1.94s\n",
      "40:\tlearn: 0.0736507\ttotal: 497ms\tremaining: 1.93s\n",
      "41:\tlearn: 0.0725312\ttotal: 510ms\tremaining: 1.92s\n",
      "42:\tlearn: 0.0692773\ttotal: 525ms\tremaining: 1.92s\n",
      "43:\tlearn: 0.0675675\ttotal: 538ms\tremaining: 1.91s\n",
      "44:\tlearn: 0.0662266\ttotal: 551ms\tremaining: 1.9s\n",
      "45:\tlearn: 0.0645476\ttotal: 564ms\tremaining: 1.89s\n",
      "46:\tlearn: 0.0628193\ttotal: 602ms\tremaining: 1.96s\n",
      "47:\tlearn: 0.0609846\ttotal: 625ms\tremaining: 1.98s\n",
      "48:\tlearn: 0.0586137\ttotal: 638ms\tremaining: 1.97s\n",
      "49:\tlearn: 0.0572136\ttotal: 652ms\tremaining: 1.96s\n",
      "50:\tlearn: 0.0556034\ttotal: 667ms\tremaining: 1.95s\n",
      "51:\tlearn: 0.0536821\ttotal: 681ms\tremaining: 1.94s\n",
      "52:\tlearn: 0.0528705\ttotal: 695ms\tremaining: 1.93s\n",
      "53:\tlearn: 0.0509585\ttotal: 710ms\tremaining: 1.92s\n",
      "54:\tlearn: 0.0496369\ttotal: 725ms\tremaining: 1.91s\n",
      "55:\tlearn: 0.0476043\ttotal: 742ms\tremaining: 1.91s\n",
      "56:\tlearn: 0.0468522\ttotal: 761ms\tremaining: 1.91s\n",
      "57:\tlearn: 0.0448408\ttotal: 780ms\tremaining: 1.91s\n",
      "58:\tlearn: 0.0439932\ttotal: 797ms\tremaining: 1.9s\n",
      "59:\tlearn: 0.0428794\ttotal: 813ms\tremaining: 1.9s\n",
      "60:\tlearn: 0.0421464\ttotal: 825ms\tremaining: 1.88s\n",
      "61:\tlearn: 0.0411346\ttotal: 838ms\tremaining: 1.86s\n",
      "62:\tlearn: 0.0398630\ttotal: 850ms\tremaining: 1.85s\n",
      "63:\tlearn: 0.0391725\ttotal: 862ms\tremaining: 1.83s\n",
      "64:\tlearn: 0.0382513\ttotal: 874ms\tremaining: 1.81s\n",
      "65:\tlearn: 0.0366138\ttotal: 886ms\tremaining: 1.8s\n",
      "66:\tlearn: 0.0359683\ttotal: 898ms\tremaining: 1.78s\n",
      "67:\tlearn: 0.0347538\ttotal: 910ms\tremaining: 1.76s\n",
      "68:\tlearn: 0.0340788\ttotal: 921ms\tremaining: 1.75s\n",
      "69:\tlearn: 0.0327922\ttotal: 933ms\tremaining: 1.73s\n",
      "70:\tlearn: 0.0318541\ttotal: 945ms\tremaining: 1.72s\n",
      "71:\tlearn: 0.0313293\ttotal: 956ms\tremaining: 1.7s\n",
      "72:\tlearn: 0.0308646\ttotal: 968ms\tremaining: 1.68s\n",
      "73:\tlearn: 0.0304220\ttotal: 980ms\tremaining: 1.67s\n",
      "74:\tlearn: 0.0295577\ttotal: 991ms\tremaining: 1.65s\n",
      "75:\tlearn: 0.0283947\ttotal: 1s\tremaining: 1.64s\n",
      "76:\tlearn: 0.0280292\ttotal: 1.01s\tremaining: 1.62s\n",
      "77:\tlearn: 0.0270364\ttotal: 1.03s\tremaining: 1.61s\n",
      "78:\tlearn: 0.0262315\ttotal: 1.04s\tremaining: 1.59s\n",
      "79:\tlearn: 0.0256911\ttotal: 1.05s\tremaining: 1.58s\n",
      "80:\tlearn: 0.0253991\ttotal: 1.06s\tremaining: 1.56s\n",
      "81:\tlearn: 0.0246683\ttotal: 1.07s\tremaining: 1.55s\n",
      "82:\tlearn: 0.0234555\ttotal: 1.09s\tremaining: 1.53s\n",
      "83:\tlearn: 0.0228653\ttotal: 1.1s\tremaining: 1.52s\n",
      "84:\tlearn: 0.0220861\ttotal: 1.11s\tremaining: 1.5s\n",
      "85:\tlearn: 0.0216000\ttotal: 1.12s\tremaining: 1.49s\n",
      "86:\tlearn: 0.0212337\ttotal: 1.13s\tremaining: 1.47s\n",
      "87:\tlearn: 0.0207564\ttotal: 1.15s\tremaining: 1.46s\n",
      "88:\tlearn: 0.0203445\ttotal: 1.16s\tremaining: 1.44s\n",
      "89:\tlearn: 0.0200613\ttotal: 1.17s\tremaining: 1.43s\n",
      "90:\tlearn: 0.0196350\ttotal: 1.18s\tremaining: 1.42s\n",
      "91:\tlearn: 0.0187106\ttotal: 1.19s\tremaining: 1.4s\n",
      "92:\tlearn: 0.0183014\ttotal: 1.21s\tremaining: 1.39s\n",
      "93:\tlearn: 0.0176643\ttotal: 1.22s\tremaining: 1.37s\n",
      "94:\tlearn: 0.0173296\ttotal: 1.23s\tremaining: 1.36s\n",
      "95:\tlearn: 0.0170484\ttotal: 1.24s\tremaining: 1.34s\n",
      "96:\tlearn: 0.0163985\ttotal: 1.25s\tremaining: 1.33s\n",
      "97:\tlearn: 0.0156142\ttotal: 1.26s\tremaining: 1.32s\n",
      "98:\tlearn: 0.0151994\ttotal: 1.28s\tremaining: 1.3s\n",
      "99:\tlearn: 0.0148376\ttotal: 1.29s\tremaining: 1.29s\n",
      "100:\tlearn: 0.0145620\ttotal: 1.3s\tremaining: 1.27s\n",
      "101:\tlearn: 0.0143560\ttotal: 1.31s\tremaining: 1.26s\n",
      "102:\tlearn: 0.0141823\ttotal: 1.32s\tremaining: 1.25s\n",
      "103:\tlearn: 0.0139903\ttotal: 1.34s\tremaining: 1.23s\n",
      "104:\tlearn: 0.0137943\ttotal: 1.35s\tremaining: 1.22s\n",
      "105:\tlearn: 0.0132447\ttotal: 1.36s\tremaining: 1.21s\n",
      "106:\tlearn: 0.0130189\ttotal: 1.37s\tremaining: 1.19s\n",
      "107:\tlearn: 0.0126214\ttotal: 1.38s\tremaining: 1.18s\n",
      "108:\tlearn: 0.0124513\ttotal: 1.39s\tremaining: 1.16s\n",
      "109:\tlearn: 0.0122411\ttotal: 1.41s\tremaining: 1.15s\n",
      "110:\tlearn: 0.0118333\ttotal: 1.42s\tremaining: 1.14s\n",
      "111:\tlearn: 0.0114552\ttotal: 1.43s\tremaining: 1.12s\n",
      "112:\tlearn: 0.0111849\ttotal: 1.44s\tremaining: 1.11s\n",
      "113:\tlearn: 0.0110027\ttotal: 1.45s\tremaining: 1.09s\n",
      "114:\tlearn: 0.0108722\ttotal: 1.46s\tremaining: 1.08s\n",
      "115:\tlearn: 0.0107274\ttotal: 1.48s\tremaining: 1.07s\n",
      "116:\tlearn: 0.0105069\ttotal: 1.49s\tremaining: 1.05s\n",
      "117:\tlearn: 0.0102829\ttotal: 1.5s\tremaining: 1.04s\n",
      "118:\tlearn: 0.0099390\ttotal: 1.51s\tremaining: 1.03s\n",
      "119:\tlearn: 0.0096688\ttotal: 1.52s\tremaining: 1.01s\n",
      "120:\tlearn: 0.0095439\ttotal: 1.53s\tremaining: 1s\n",
      "121:\tlearn: 0.0092475\ttotal: 1.55s\tremaining: 989ms\n",
      "122:\tlearn: 0.0090030\ttotal: 1.56s\tremaining: 976ms\n",
      "123:\tlearn: 0.0089466\ttotal: 1.57s\tremaining: 963ms\n",
      "124:\tlearn: 0.0087240\ttotal: 1.59s\tremaining: 952ms\n",
      "125:\tlearn: 0.0086173\ttotal: 1.6s\tremaining: 943ms\n",
      "126:\tlearn: 0.0084695\ttotal: 1.63s\tremaining: 934ms\n",
      "127:\tlearn: 0.0081784\ttotal: 1.64s\tremaining: 925ms\n",
      "128:\tlearn: 0.0079301\ttotal: 1.66s\tremaining: 914ms\n",
      "129:\tlearn: 0.0078263\ttotal: 1.67s\tremaining: 901ms\n",
      "130:\tlearn: 0.0076616\ttotal: 1.69s\tremaining: 888ms\n",
      "131:\tlearn: 0.0074501\ttotal: 1.7s\tremaining: 875ms\n",
      "132:\tlearn: 0.0072854\ttotal: 1.71s\tremaining: 861ms\n",
      "133:\tlearn: 0.0071721\ttotal: 1.72s\tremaining: 848ms\n",
      "134:\tlearn: 0.0070012\ttotal: 1.73s\tremaining: 835ms\n",
      "135:\tlearn: 0.0068623\ttotal: 1.75s\tremaining: 822ms\n",
      "136:\tlearn: 0.0066617\ttotal: 1.76s\tremaining: 808ms\n",
      "137:\tlearn: 0.0065072\ttotal: 1.77s\tremaining: 795ms\n",
      "138:\tlearn: 0.0064026\ttotal: 1.78s\tremaining: 781ms\n",
      "139:\tlearn: 0.0062728\ttotal: 1.79s\tremaining: 768ms\n",
      "140:\tlearn: 0.0061837\ttotal: 1.8s\tremaining: 755ms\n",
      "141:\tlearn: 0.0061055\ttotal: 1.82s\tremaining: 742ms\n",
      "142:\tlearn: 0.0060261\ttotal: 1.83s\tremaining: 729ms\n",
      "143:\tlearn: 0.0058537\ttotal: 1.84s\tremaining: 716ms\n",
      "144:\tlearn: 0.0057274\ttotal: 1.85s\tremaining: 702ms\n",
      "145:\tlearn: 0.0056430\ttotal: 1.86s\tremaining: 689ms\n",
      "146:\tlearn: 0.0054918\ttotal: 1.88s\tremaining: 676ms\n",
      "147:\tlearn: 0.0053037\ttotal: 1.89s\tremaining: 664ms\n",
      "148:\tlearn: 0.0052192\ttotal: 1.9s\tremaining: 651ms\n",
      "149:\tlearn: 0.0050785\ttotal: 1.91s\tremaining: 637ms\n",
      "150:\tlearn: 0.0050018\ttotal: 1.92s\tremaining: 624ms\n",
      "151:\tlearn: 0.0049559\ttotal: 1.94s\tremaining: 611ms\n",
      "152:\tlearn: 0.0048805\ttotal: 1.95s\tremaining: 598ms\n",
      "153:\tlearn: 0.0047459\ttotal: 1.96s\tremaining: 585ms\n",
      "154:\tlearn: 0.0046537\ttotal: 1.97s\tremaining: 572ms\n",
      "155:\tlearn: 0.0045770\ttotal: 1.98s\tremaining: 559ms\n",
      "156:\tlearn: 0.0044341\ttotal: 1.99s\tremaining: 546ms\n",
      "157:\tlearn: 0.0043445\ttotal: 2.01s\tremaining: 533ms\n",
      "158:\tlearn: 0.0042267\ttotal: 2.02s\tremaining: 521ms\n",
      "159:\tlearn: 0.0041687\ttotal: 2.03s\tremaining: 508ms\n",
      "160:\tlearn: 0.0040158\ttotal: 2.04s\tremaining: 495ms\n",
      "161:\tlearn: 0.0039648\ttotal: 2.05s\tremaining: 482ms\n",
      "162:\tlearn: 0.0039171\ttotal: 2.06s\tremaining: 469ms\n",
      "163:\tlearn: 0.0038803\ttotal: 2.08s\tremaining: 456ms\n",
      "164:\tlearn: 0.0038236\ttotal: 2.09s\tremaining: 443ms\n",
      "165:\tlearn: 0.0037548\ttotal: 2.1s\tremaining: 431ms\n",
      "166:\tlearn: 0.0037077\ttotal: 2.11s\tremaining: 418ms\n",
      "167:\tlearn: 0.0036424\ttotal: 2.13s\tremaining: 405ms\n",
      "168:\tlearn: 0.0035564\ttotal: 2.14s\tremaining: 392ms\n",
      "169:\tlearn: 0.0035081\ttotal: 2.15s\tremaining: 379ms\n",
      "170:\tlearn: 0.0034576\ttotal: 2.16s\tremaining: 367ms\n",
      "171:\tlearn: 0.0034038\ttotal: 2.17s\tremaining: 354ms\n",
      "172:\tlearn: 0.0033193\ttotal: 2.19s\tremaining: 341ms\n",
      "173:\tlearn: 0.0032765\ttotal: 2.2s\tremaining: 328ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174:\tlearn: 0.0032207\ttotal: 2.21s\tremaining: 316ms\n",
      "175:\tlearn: 0.0031791\ttotal: 2.22s\tremaining: 303ms\n",
      "176:\tlearn: 0.0031181\ttotal: 2.23s\tremaining: 290ms\n",
      "177:\tlearn: 0.0030903\ttotal: 2.25s\tremaining: 278ms\n",
      "178:\tlearn: 0.0030052\ttotal: 2.26s\tremaining: 265ms\n",
      "179:\tlearn: 0.0029721\ttotal: 2.27s\tremaining: 252ms\n",
      "180:\tlearn: 0.0028979\ttotal: 2.28s\tremaining: 240ms\n",
      "181:\tlearn: 0.0028548\ttotal: 2.29s\tremaining: 227ms\n",
      "182:\tlearn: 0.0028185\ttotal: 2.3s\tremaining: 214ms\n",
      "183:\tlearn: 0.0027730\ttotal: 2.32s\tremaining: 201ms\n",
      "184:\tlearn: 0.0027496\ttotal: 2.33s\tremaining: 189ms\n",
      "185:\tlearn: 0.0026961\ttotal: 2.34s\tremaining: 176ms\n",
      "186:\tlearn: 0.0026231\ttotal: 2.35s\tremaining: 164ms\n",
      "187:\tlearn: 0.0026044\ttotal: 2.36s\tremaining: 151ms\n",
      "188:\tlearn: 0.0025704\ttotal: 2.38s\tremaining: 138ms\n",
      "189:\tlearn: 0.0025084\ttotal: 2.39s\tremaining: 126ms\n",
      "190:\tlearn: 0.0024459\ttotal: 2.4s\tremaining: 113ms\n",
      "191:\tlearn: 0.0024191\ttotal: 2.41s\tremaining: 100ms\n",
      "192:\tlearn: 0.0023778\ttotal: 2.42s\tremaining: 87.9ms\n",
      "193:\tlearn: 0.0023369\ttotal: 2.43s\tremaining: 75.3ms\n",
      "194:\tlearn: 0.0023040\ttotal: 2.45s\tremaining: 62.7ms\n",
      "195:\tlearn: 0.0022874\ttotal: 2.46s\tremaining: 50.2ms\n",
      "196:\tlearn: 0.0022555\ttotal: 2.47s\tremaining: 37.6ms\n",
      "197:\tlearn: 0.0022175\ttotal: 2.48s\tremaining: 25.1ms\n",
      "198:\tlearn: 0.0021869\ttotal: 2.49s\tremaining: 12.5ms\n",
      "199:\tlearn: 0.0021431\ttotal: 2.51s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ed1ec1faff44c3a20e57c6ee2a17a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.588242049527406, Recall = 0.9165911151405258, Aging Rate = 0.7348141432456936, Precision = 0.6236890808143122, f1 = 0.7422907488986784\n",
      "Epoch 2: Train Loss = 0.38221265293205203, Recall = 0.8898458748866727, Aging Rate = 0.5498640072529465, Precision = 0.8091508656224238, f1 = 0.8475820379965457\n",
      "Epoch 3: Train Loss = 0.2813156940455017, Recall = 0.9238440616500453, Aging Rate = 0.5342248413417952, Precision = 0.8646584641493423, f1 = 0.893271970195047\n",
      "Epoch 4: Train Loss = 0.21919505102247513, Recall = 0.942429737080689, Aging Rate = 0.5240253853127833, Precision = 0.8992214532871973, f1 = 0.9203187250996016\n",
      "Epoch 5: Train Loss = 0.1717196343464951, Recall = 0.9614687216681777, Aging Rate = 0.5151858567543064, Precision = 0.9331280246370436, f1 = 0.9470864032150035\n",
      "Test Loss = 0.13480469311569782, Recall = 0.9764279238440616, Aging Rate = 0.5174524025385313, precision = 0.9434954007884363\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.12120241809599203, Recall = 0.9786944696282865, Aging Rate = 0.5108794197642792, Precision = 0.9578527062999113, f1 = 0.9681614349775786\n",
      "Epoch 7: Train Loss = 0.09335421809313628, Recall = 0.9873073436083409, Aging Rate = 0.507479601087942, Precision = 0.9727556945064761, f1 = 0.9799775028121486\n",
      "Epoch 8: Train Loss = 0.07255295141247976, Recall = 0.9913871260199456, Aging Rate = 0.5045330915684497, Precision = 0.9824797843665768, f1 = 0.9869133574007221\n",
      "Epoch 9: Train Loss = 0.05786805357429404, Recall = 0.9941069809610155, Aging Rate = 0.5022665457842248, Precision = 0.9896209386281588, f1 = 0.9918588873812756\n",
      "Epoch 10: Train Loss = 0.046048333616766836, Recall = 0.9959202175883953, Aging Rate = 0.5027198549410699, Precision = 0.9905320108205591, f1 = 0.9932188065099459\n",
      "Test Loss = 0.04099750800475357, Recall = 1.0, Aging Rate = 0.5040797824116047, precision = 0.9919064748201439\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.03811515242888078, Recall = 0.9972801450589301, Aging Rate = 0.5011332728921124, Precision = 0.9950248756218906, f1 = 0.996151233869142\n",
      "Epoch 12: Train Loss = 0.03108662838692138, Recall = 0.9986400725294651, Aging Rate = 0.5015865820489573, Precision = 0.9954812471757795, f1 = 0.9970581579542883\n",
      "Epoch 13: Train Loss = 0.025621241108866464, Recall = 0.9995466908431551, Aging Rate = 0.5013599274705349, Precision = 0.9968354430379747, f1 = 0.9981892258940697\n",
      "Epoch 14: Train Loss = 0.02203939527742639, Recall = 0.9995466908431551, Aging Rate = 0.50090661831369, Precision = 0.997737556561086, f1 = 0.998641304347826\n",
      "Epoch 15: Train Loss = 0.018556957424416504, Recall = 0.9995466908431551, Aging Rate = 0.5002266545784225, Precision = 0.9990937924784775, f1 = 0.9993201903467029\n",
      "Test Loss = 0.015800867260096836, Recall = 1.0, Aging Rate = 0.5004533091568449, precision = 0.9990942028985508\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.015979878319864745, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 17: Train Loss = 0.013857582969048945, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 18: Train Loss = 0.011999384721307082, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 19: Train Loss = 0.01072169553424322, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 20: Train Loss = 0.009519052092688444, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Test Loss = 0.008202957310262098, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.00848963795776934, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 22: Train Loss = 0.007577831137094542, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.006833080346514543, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.006124037956263353, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.005455047257560375, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0049533051215297965, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.004979351100174952, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.0045806780592171125, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.0043748717318970376, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.003993442239440768, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.003694949562704412, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00325819631649649, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.0034213239484967312, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.003228570115714522, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.0030277839413036, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.002828164301822781, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.002761368728635439, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0024105337684204023, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.0026233441710171515, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.002684334853131253, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.002503834265231046, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.002309921334800754, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.002190352943983412, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019763171798313174, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.002125356024669877, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0020712131950143026, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.002023591280848139, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0019831667376212702, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0019235890525974104, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001978521152329399, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.001976314969681683, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0018623072714380184, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0018807241665908657, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.001768113675857417, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0018319762761494379, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016839765691969305, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0017395430649373197, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0018033934672678047, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0016848829862807734, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.001881124804696492, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0023908631825766418, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029013535404786408, Recall = 1.0, Aging Rate = 0.5004533091568449, precision = 0.9990942028985508\n",
      "\n",
      "Epoch 56: Train Loss = 0.0017964314652061393, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.001487196027427905, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0014944180603673578, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0015050711432460894, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train Loss = 0.001661562477132932, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002260438977771946, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0016452140556454172, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0018707464428209884, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0016650616067101975, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0015783967108930971, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0016745237178901245, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015305693079578138, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0015461618938387887, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0014385573535563184, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0015567162224865863, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.001807188795578049, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0015640073487111254, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019626494516639236, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6260461\ttotal: 13.3ms\tremaining: 2.65s\n",
      "1:\tlearn: 0.5611758\ttotal: 26.7ms\tremaining: 2.65s\n",
      "2:\tlearn: 0.5100614\ttotal: 39.9ms\tremaining: 2.62s\n",
      "3:\tlearn: 0.4653619\ttotal: 53.2ms\tremaining: 2.61s\n",
      "4:\tlearn: 0.4249483\ttotal: 66.6ms\tremaining: 2.6s\n",
      "5:\tlearn: 0.4073546\ttotal: 80ms\tremaining: 2.58s\n",
      "6:\tlearn: 0.3812471\ttotal: 94.9ms\tremaining: 2.62s\n",
      "7:\tlearn: 0.3484237\ttotal: 107ms\tremaining: 2.57s\n",
      "8:\tlearn: 0.3192679\ttotal: 119ms\tremaining: 2.53s\n",
      "9:\tlearn: 0.2953686\ttotal: 131ms\tremaining: 2.49s\n",
      "10:\tlearn: 0.2774315\ttotal: 143ms\tremaining: 2.46s\n",
      "11:\tlearn: 0.2635832\ttotal: 155ms\tremaining: 2.44s\n",
      "12:\tlearn: 0.2496825\ttotal: 167ms\tremaining: 2.41s\n",
      "13:\tlearn: 0.2340708\ttotal: 179ms\tremaining: 2.37s\n",
      "14:\tlearn: 0.2225343\ttotal: 191ms\tremaining: 2.35s\n",
      "15:\tlearn: 0.2139059\ttotal: 203ms\tremaining: 2.33s\n",
      "16:\tlearn: 0.2059213\ttotal: 214ms\tremaining: 2.31s\n",
      "17:\tlearn: 0.1904471\ttotal: 226ms\tremaining: 2.29s\n",
      "18:\tlearn: 0.1820248\ttotal: 238ms\tremaining: 2.26s\n",
      "19:\tlearn: 0.1709247\ttotal: 250ms\tremaining: 2.25s\n",
      "20:\tlearn: 0.1626340\ttotal: 261ms\tremaining: 2.23s\n",
      "21:\tlearn: 0.1496957\ttotal: 273ms\tremaining: 2.21s\n",
      "22:\tlearn: 0.1439683\ttotal: 285ms\tremaining: 2.19s\n",
      "23:\tlearn: 0.1349181\ttotal: 296ms\tremaining: 2.17s\n",
      "24:\tlearn: 0.1284605\ttotal: 309ms\tremaining: 2.16s\n",
      "25:\tlearn: 0.1202721\ttotal: 321ms\tremaining: 2.15s\n",
      "26:\tlearn: 0.1149919\ttotal: 333ms\tremaining: 2.13s\n",
      "27:\tlearn: 0.1118775\ttotal: 345ms\tremaining: 2.12s\n",
      "28:\tlearn: 0.1079275\ttotal: 358ms\tremaining: 2.11s\n",
      "29:\tlearn: 0.1059039\ttotal: 369ms\tremaining: 2.09s\n",
      "30:\tlearn: 0.1018124\ttotal: 381ms\tremaining: 2.08s\n",
      "31:\tlearn: 0.0994585\ttotal: 393ms\tremaining: 2.06s\n",
      "32:\tlearn: 0.0944833\ttotal: 405ms\tremaining: 2.05s\n",
      "33:\tlearn: 0.0923955\ttotal: 417ms\tremaining: 2.03s\n",
      "34:\tlearn: 0.0896042\ttotal: 429ms\tremaining: 2.02s\n",
      "35:\tlearn: 0.0843707\ttotal: 441ms\tremaining: 2.01s\n",
      "36:\tlearn: 0.0824026\ttotal: 452ms\tremaining: 1.99s\n",
      "37:\tlearn: 0.0798555\ttotal: 464ms\tremaining: 1.98s\n",
      "38:\tlearn: 0.0780759\ttotal: 476ms\tremaining: 1.97s\n",
      "39:\tlearn: 0.0758213\ttotal: 488ms\tremaining: 1.95s\n",
      "40:\tlearn: 0.0725429\ttotal: 501ms\tremaining: 1.94s\n",
      "41:\tlearn: 0.0711047\ttotal: 512ms\tremaining: 1.93s\n",
      "42:\tlearn: 0.0665185\ttotal: 524ms\tremaining: 1.91s\n",
      "43:\tlearn: 0.0646533\ttotal: 536ms\tremaining: 1.9s\n",
      "44:\tlearn: 0.0618587\ttotal: 548ms\tremaining: 1.89s\n",
      "45:\tlearn: 0.0610921\ttotal: 559ms\tremaining: 1.87s\n",
      "46:\tlearn: 0.0590720\ttotal: 571ms\tremaining: 1.86s\n",
      "47:\tlearn: 0.0577020\ttotal: 583ms\tremaining: 1.84s\n",
      "48:\tlearn: 0.0546922\ttotal: 594ms\tremaining: 1.83s\n",
      "49:\tlearn: 0.0532895\ttotal: 607ms\tremaining: 1.82s\n",
      "50:\tlearn: 0.0522909\ttotal: 618ms\tremaining: 1.8s\n",
      "51:\tlearn: 0.0501123\ttotal: 630ms\tremaining: 1.79s\n",
      "52:\tlearn: 0.0490289\ttotal: 642ms\tremaining: 1.78s\n",
      "53:\tlearn: 0.0477580\ttotal: 655ms\tremaining: 1.77s\n",
      "54:\tlearn: 0.0464844\ttotal: 667ms\tremaining: 1.76s\n",
      "55:\tlearn: 0.0447001\ttotal: 678ms\tremaining: 1.74s\n",
      "56:\tlearn: 0.0441024\ttotal: 691ms\tremaining: 1.73s\n",
      "57:\tlearn: 0.0429000\ttotal: 702ms\tremaining: 1.72s\n",
      "58:\tlearn: 0.0421630\ttotal: 714ms\tremaining: 1.71s\n",
      "59:\tlearn: 0.0400732\ttotal: 726ms\tremaining: 1.69s\n",
      "60:\tlearn: 0.0392601\ttotal: 738ms\tremaining: 1.68s\n",
      "61:\tlearn: 0.0386495\ttotal: 750ms\tremaining: 1.67s\n",
      "62:\tlearn: 0.0370622\ttotal: 762ms\tremaining: 1.66s\n",
      "63:\tlearn: 0.0362368\ttotal: 773ms\tremaining: 1.64s\n",
      "64:\tlearn: 0.0349477\ttotal: 785ms\tremaining: 1.63s\n",
      "65:\tlearn: 0.0341481\ttotal: 797ms\tremaining: 1.62s\n",
      "66:\tlearn: 0.0334398\ttotal: 809ms\tremaining: 1.61s\n",
      "67:\tlearn: 0.0328049\ttotal: 823ms\tremaining: 1.6s\n",
      "68:\tlearn: 0.0319089\ttotal: 839ms\tremaining: 1.59s\n",
      "69:\tlearn: 0.0313654\ttotal: 853ms\tremaining: 1.58s\n",
      "70:\tlearn: 0.0311132\ttotal: 866ms\tremaining: 1.57s\n",
      "71:\tlearn: 0.0303551\ttotal: 879ms\tremaining: 1.56s\n",
      "72:\tlearn: 0.0295293\ttotal: 893ms\tremaining: 1.55s\n",
      "73:\tlearn: 0.0285758\ttotal: 906ms\tremaining: 1.54s\n",
      "74:\tlearn: 0.0275845\ttotal: 921ms\tremaining: 1.53s\n",
      "75:\tlearn: 0.0269831\ttotal: 935ms\tremaining: 1.52s\n",
      "76:\tlearn: 0.0264250\ttotal: 951ms\tremaining: 1.52s\n",
      "77:\tlearn: 0.0258235\ttotal: 963ms\tremaining: 1.51s\n",
      "78:\tlearn: 0.0252533\ttotal: 976ms\tremaining: 1.5s\n",
      "79:\tlearn: 0.0247840\ttotal: 988ms\tremaining: 1.48s\n",
      "80:\tlearn: 0.0242900\ttotal: 1s\tremaining: 1.47s\n",
      "81:\tlearn: 0.0235059\ttotal: 1.01s\tremaining: 1.46s\n",
      "82:\tlearn: 0.0228382\ttotal: 1.02s\tremaining: 1.44s\n",
      "83:\tlearn: 0.0224415\ttotal: 1.04s\tremaining: 1.43s\n",
      "84:\tlearn: 0.0219239\ttotal: 1.05s\tremaining: 1.42s\n",
      "85:\tlearn: 0.0216861\ttotal: 1.06s\tremaining: 1.41s\n",
      "86:\tlearn: 0.0211387\ttotal: 1.07s\tremaining: 1.39s\n",
      "87:\tlearn: 0.0205144\ttotal: 1.08s\tremaining: 1.38s\n",
      "88:\tlearn: 0.0199093\ttotal: 1.1s\tremaining: 1.37s\n",
      "89:\tlearn: 0.0191973\ttotal: 1.11s\tremaining: 1.35s\n",
      "90:\tlearn: 0.0188891\ttotal: 1.12s\tremaining: 1.34s\n",
      "91:\tlearn: 0.0183046\ttotal: 1.13s\tremaining: 1.33s\n",
      "92:\tlearn: 0.0177913\ttotal: 1.14s\tremaining: 1.32s\n",
      "93:\tlearn: 0.0175096\ttotal: 1.16s\tremaining: 1.3s\n",
      "94:\tlearn: 0.0171912\ttotal: 1.17s\tremaining: 1.29s\n",
      "95:\tlearn: 0.0165905\ttotal: 1.18s\tremaining: 1.28s\n",
      "96:\tlearn: 0.0161093\ttotal: 1.19s\tremaining: 1.26s\n",
      "97:\tlearn: 0.0157997\ttotal: 1.2s\tremaining: 1.25s\n",
      "98:\tlearn: 0.0154830\ttotal: 1.22s\tremaining: 1.24s\n",
      "99:\tlearn: 0.0151910\ttotal: 1.23s\tremaining: 1.23s\n",
      "100:\tlearn: 0.0148421\ttotal: 1.24s\tremaining: 1.21s\n",
      "101:\tlearn: 0.0146279\ttotal: 1.25s\tremaining: 1.2s\n",
      "102:\tlearn: 0.0143942\ttotal: 1.26s\tremaining: 1.19s\n",
      "103:\tlearn: 0.0142580\ttotal: 1.27s\tremaining: 1.18s\n",
      "104:\tlearn: 0.0139888\ttotal: 1.29s\tremaining: 1.16s\n",
      "105:\tlearn: 0.0135786\ttotal: 1.3s\tremaining: 1.15s\n",
      "106:\tlearn: 0.0131320\ttotal: 1.31s\tremaining: 1.14s\n",
      "107:\tlearn: 0.0129077\ttotal: 1.32s\tremaining: 1.13s\n",
      "108:\tlearn: 0.0127473\ttotal: 1.33s\tremaining: 1.11s\n",
      "109:\tlearn: 0.0123221\ttotal: 1.34s\tremaining: 1.1s\n",
      "110:\tlearn: 0.0120340\ttotal: 1.36s\tremaining: 1.09s\n",
      "111:\tlearn: 0.0116651\ttotal: 1.37s\tremaining: 1.07s\n",
      "112:\tlearn: 0.0112928\ttotal: 1.38s\tremaining: 1.06s\n",
      "113:\tlearn: 0.0108617\ttotal: 1.39s\tremaining: 1.05s\n",
      "114:\tlearn: 0.0104963\ttotal: 1.4s\tremaining: 1.04s\n",
      "115:\tlearn: 0.0103579\ttotal: 1.42s\tremaining: 1.02s\n",
      "116:\tlearn: 0.0100025\ttotal: 1.43s\tremaining: 1.01s\n",
      "117:\tlearn: 0.0097900\ttotal: 1.44s\tremaining: 1s\n",
      "118:\tlearn: 0.0097521\ttotal: 1.44s\tremaining: 981ms\n",
      "119:\tlearn: 0.0095701\ttotal: 1.45s\tremaining: 969ms\n",
      "120:\tlearn: 0.0092561\ttotal: 1.47s\tremaining: 957ms\n",
      "121:\tlearn: 0.0090865\ttotal: 1.48s\tremaining: 945ms\n",
      "122:\tlearn: 0.0089899\ttotal: 1.49s\tremaining: 933ms\n",
      "123:\tlearn: 0.0086650\ttotal: 1.5s\tremaining: 920ms\n",
      "124:\tlearn: 0.0084698\ttotal: 1.51s\tremaining: 908ms\n",
      "125:\tlearn: 0.0082389\ttotal: 1.52s\tremaining: 895ms\n",
      "126:\tlearn: 0.0080669\ttotal: 1.54s\tremaining: 883ms\n",
      "127:\tlearn: 0.0079280\ttotal: 1.55s\tremaining: 871ms\n",
      "128:\tlearn: 0.0077811\ttotal: 1.56s\tremaining: 858ms\n",
      "129:\tlearn: 0.0076768\ttotal: 1.57s\tremaining: 846ms\n",
      "130:\tlearn: 0.0074961\ttotal: 1.58s\tremaining: 834ms\n",
      "131:\tlearn: 0.0073454\ttotal: 1.59s\tremaining: 819ms\n",
      "132:\tlearn: 0.0072305\ttotal: 1.6s\tremaining: 807ms\n",
      "133:\tlearn: 0.0070309\ttotal: 1.61s\tremaining: 795ms\n",
      "134:\tlearn: 0.0069375\ttotal: 1.63s\tremaining: 783ms\n",
      "135:\tlearn: 0.0068333\ttotal: 1.64s\tremaining: 771ms\n",
      "136:\tlearn: 0.0067154\ttotal: 1.65s\tremaining: 759ms\n",
      "137:\tlearn: 0.0066016\ttotal: 1.66s\tremaining: 747ms\n",
      "138:\tlearn: 0.0064859\ttotal: 1.67s\tremaining: 735ms\n",
      "139:\tlearn: 0.0063577\ttotal: 1.69s\tremaining: 723ms\n",
      "140:\tlearn: 0.0062730\ttotal: 1.7s\tremaining: 710ms\n",
      "141:\tlearn: 0.0061245\ttotal: 1.71s\tremaining: 698ms\n",
      "142:\tlearn: 0.0060472\ttotal: 1.72s\tremaining: 686ms\n",
      "143:\tlearn: 0.0058856\ttotal: 1.73s\tremaining: 674ms\n",
      "144:\tlearn: 0.0057823\ttotal: 1.74s\tremaining: 662ms\n",
      "145:\tlearn: 0.0057038\ttotal: 1.76s\tremaining: 650ms\n",
      "146:\tlearn: 0.0055900\ttotal: 1.77s\tremaining: 638ms\n",
      "147:\tlearn: 0.0055117\ttotal: 1.78s\tremaining: 626ms\n",
      "148:\tlearn: 0.0053759\ttotal: 1.79s\tremaining: 614ms\n",
      "149:\tlearn: 0.0052738\ttotal: 1.81s\tremaining: 603ms\n",
      "150:\tlearn: 0.0051403\ttotal: 1.82s\tremaining: 592ms\n",
      "151:\tlearn: 0.0050952\ttotal: 1.84s\tremaining: 582ms\n",
      "152:\tlearn: 0.0049584\ttotal: 1.86s\tremaining: 572ms\n",
      "153:\tlearn: 0.0048450\ttotal: 1.88s\tremaining: 561ms\n",
      "154:\tlearn: 0.0047508\ttotal: 1.89s\tremaining: 550ms\n",
      "155:\tlearn: 0.0046676\ttotal: 1.91s\tremaining: 538ms\n",
      "156:\tlearn: 0.0045886\ttotal: 1.92s\tremaining: 526ms\n",
      "157:\tlearn: 0.0045354\ttotal: 1.93s\tremaining: 514ms\n",
      "158:\tlearn: 0.0044931\ttotal: 1.95s\tremaining: 502ms\n",
      "159:\tlearn: 0.0044456\ttotal: 1.96s\tremaining: 489ms\n",
      "160:\tlearn: 0.0043801\ttotal: 1.97s\tremaining: 477ms\n",
      "161:\tlearn: 0.0042925\ttotal: 1.98s\tremaining: 465ms\n",
      "162:\tlearn: 0.0042041\ttotal: 1.99s\tremaining: 452ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163:\tlearn: 0.0040807\ttotal: 2s\tremaining: 440ms\n",
      "164:\tlearn: 0.0040158\ttotal: 2.02s\tremaining: 428ms\n",
      "165:\tlearn: 0.0039343\ttotal: 2.03s\tremaining: 415ms\n",
      "166:\tlearn: 0.0039038\ttotal: 2.04s\tremaining: 403ms\n",
      "167:\tlearn: 0.0038342\ttotal: 2.05s\tremaining: 391ms\n",
      "168:\tlearn: 0.0037764\ttotal: 2.06s\tremaining: 379ms\n",
      "169:\tlearn: 0.0037104\ttotal: 2.08s\tremaining: 366ms\n",
      "170:\tlearn: 0.0036645\ttotal: 2.09s\tremaining: 354ms\n",
      "171:\tlearn: 0.0036149\ttotal: 2.1s\tremaining: 342ms\n",
      "172:\tlearn: 0.0035329\ttotal: 2.11s\tremaining: 329ms\n",
      "173:\tlearn: 0.0034318\ttotal: 2.12s\tremaining: 317ms\n",
      "174:\tlearn: 0.0033668\ttotal: 2.13s\tremaining: 305ms\n",
      "175:\tlearn: 0.0032517\ttotal: 2.15s\tremaining: 293ms\n",
      "176:\tlearn: 0.0032298\ttotal: 2.16s\tremaining: 280ms\n",
      "177:\tlearn: 0.0031854\ttotal: 2.17s\tremaining: 268ms\n",
      "178:\tlearn: 0.0031107\ttotal: 2.18s\tremaining: 256ms\n",
      "179:\tlearn: 0.0030579\ttotal: 2.19s\tremaining: 244ms\n",
      "180:\tlearn: 0.0029755\ttotal: 2.21s\tremaining: 232ms\n",
      "181:\tlearn: 0.0029000\ttotal: 2.22s\tremaining: 219ms\n",
      "182:\tlearn: 0.0028523\ttotal: 2.23s\tremaining: 207ms\n",
      "183:\tlearn: 0.0028093\ttotal: 2.24s\tremaining: 195ms\n",
      "184:\tlearn: 0.0027786\ttotal: 2.25s\tremaining: 183ms\n",
      "185:\tlearn: 0.0027102\ttotal: 2.27s\tremaining: 171ms\n",
      "186:\tlearn: 0.0026715\ttotal: 2.28s\tremaining: 158ms\n",
      "187:\tlearn: 0.0026143\ttotal: 2.29s\tremaining: 146ms\n",
      "188:\tlearn: 0.0025847\ttotal: 2.3s\tremaining: 134ms\n",
      "189:\tlearn: 0.0025636\ttotal: 2.31s\tremaining: 122ms\n",
      "190:\tlearn: 0.0025393\ttotal: 2.32s\tremaining: 110ms\n",
      "191:\tlearn: 0.0025039\ttotal: 2.34s\tremaining: 97.3ms\n",
      "192:\tlearn: 0.0024075\ttotal: 2.35s\tremaining: 85.2ms\n",
      "193:\tlearn: 0.0023822\ttotal: 2.36s\tremaining: 73ms\n",
      "194:\tlearn: 0.0023471\ttotal: 2.37s\tremaining: 60.8ms\n",
      "195:\tlearn: 0.0023050\ttotal: 2.38s\tremaining: 48.6ms\n",
      "196:\tlearn: 0.0022697\ttotal: 2.4s\tremaining: 36.5ms\n",
      "197:\tlearn: 0.0022270\ttotal: 2.41s\tremaining: 24.3ms\n",
      "198:\tlearn: 0.0021804\ttotal: 2.42s\tremaining: 12.2ms\n",
      "199:\tlearn: 0.0021604\ttotal: 2.43s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634ea920f42e4a1d9e75a28da302bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5809606198062707, Recall = 0.9138712601994561, Aging Rate = 0.7250679963735267, Precision = 0.6301969365426696, f1 = 0.7459759481961148\n",
      "Epoch 2: Train Loss = 0.37732642843089964, Recall = 0.8902991840435177, Aging Rate = 0.5455575702629193, Precision = 0.8159534690486082, f1 = 0.8515066117494039\n",
      "Epoch 3: Train Loss = 0.28880913021990307, Recall = 0.914777878513146, Aging Rate = 0.527651858567543, Precision = 0.8668384879725086, f1 = 0.8901632112924569\n",
      "Epoch 4: Train Loss = 0.2300376531922158, Recall = 0.9397098821396193, Aging Rate = 0.5228921124206709, Precision = 0.8985695708712613, f1 = 0.9186793707068469\n",
      "Epoch 5: Train Loss = 0.18784479848898658, Recall = 0.958295557570263, Aging Rate = 0.5240253853127833, Precision = 0.9143598615916955, f1 = 0.9358123063302347\n",
      "Test Loss = 0.14704347560595948, Recall = 0.9777878513145966, Aging Rate = 0.5179057116953762, precision = 0.9439824945295405\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.13481686647275526, Recall = 0.986400725294651, Aging Rate = 0.5203989120580236, Precision = 0.9477351916376306, f1 = 0.9666814749000444\n",
      "Epoch 7: Train Loss = 0.10630419511932303, Recall = 0.9913871260199456, Aging Rate = 0.5156391659111514, Precision = 0.9613186813186814, f1 = 0.9761214014728855\n",
      "Epoch 8: Train Loss = 0.08317330904500872, Recall = 0.9922937443336355, Aging Rate = 0.5099728014505893, Precision = 0.9728888888888889, f1 = 0.9824955116696589\n",
      "Epoch 9: Train Loss = 0.06688948743322279, Recall = 0.9945602901178604, Aging Rate = 0.5061196736174071, Precision = 0.9825347066726378, f1 = 0.9885109258842081\n",
      "Epoch 10: Train Loss = 0.05339201949097736, Recall = 0.9945602901178604, Aging Rate = 0.5033998186763372, Precision = 0.9878433138226025, f1 = 0.9911904224079513\n",
      "Test Loss = 0.044372238130971506, Recall = 0.9986400725294651, Aging Rate = 0.5045330915684497, precision = 0.9896675651392632\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04261459442276038, Recall = 0.9963735267452403, Aging Rate = 0.5024932003626473, Precision = 0.9914298601714028, f1 = 0.9938955460094957\n",
      "Epoch 12: Train Loss = 0.03612701980023901, Recall = 0.9963735267452403, Aging Rate = 0.5011332728921124, Precision = 0.9941203075531434, f1 = 0.9952456418383518\n",
      "Epoch 13: Train Loss = 0.029045225287418635, Recall = 0.9986400725294651, Aging Rate = 0.5013599274705349, Precision = 0.9959312839059674, f1 = 0.9972838388411046\n",
      "Epoch 14: Train Loss = 0.02491913425144237, Recall = 0.9981867633726201, Aging Rate = 0.5004533091568449, Precision = 0.9972826086956522, f1 = 0.9977344811961939\n",
      "Epoch 15: Train Loss = 0.021041588997932747, Recall = 0.9986400725294651, Aging Rate = 0.5004533091568449, Precision = 0.9977355072463768, f1 = 0.9981875849569551\n",
      "Test Loss = 0.017846646104835426, Recall = 1.0, Aging Rate = 0.5011332728921124, precision = 0.9977385798281321\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.018062370320138667, Recall = 0.9995466908431551, Aging Rate = 0.5006799637352675, Precision = 0.9981892258940697, f1 = 0.9988674971687429\n",
      "Epoch 17: Train Loss = 0.01543541489918788, Recall = 1.0, Aging Rate = 0.5006799637352675, Precision = 0.9986419194205522, f1 = 0.9993204983012457\n",
      "Epoch 18: Train Loss = 0.013568234488077577, Recall = 0.9995466908431551, Aging Rate = 0.5004533091568449, Precision = 0.998641304347826, f1 = 0.9990937924784776\n",
      "Epoch 19: Train Loss = 0.011893010881905895, Recall = 1.0, Aging Rate = 0.5004533091568449, Precision = 0.9990942028985508, f1 = 0.9995468962392388\n",
      "Epoch 20: Train Loss = 0.01021767812227984, Recall = 1.0, Aging Rate = 0.5004533091568449, Precision = 0.9990942028985508, f1 = 0.9995468962392388\n",
      "Test Loss = 0.008952250802958703, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.008932830698295063, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 22: Train Loss = 0.007971418086787245, Recall = 1.0, Aging Rate = 0.5002266545784225, Precision = 0.9995468962392388, f1 = 0.9997733967822342\n",
      "Epoch 23: Train Loss = 0.0071766540312722445, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.0065761558548568435, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.005813941030221858, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00508446257024436, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.00523449308285363, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.004817523216429325, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.004349937618575848, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.004117672839932353, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.00380760330769317, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003373462844645083, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.0035040040521781995, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.0033386573897326807, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.003123512215057856, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.002828358345794224, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.002777547074620319, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00261711462714109, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.002582687446349389, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.002542146383628506, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.0023445243630997314, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0022752229506377863, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.0021980132963356383, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002014835106744241, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.0021900503418272395, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0020303882796533492, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0019559954913250313, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.001987959605921144, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.00208990163159669, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017900489729171382, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.001912745567288162, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0018290975017106652, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0018120949661717197, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0017353742233969791, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0017374720037488414, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019971510137457497, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0019887761039898335, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.001716147681211855, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0017208896563848581, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.001692134458892527, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.001720940337684394, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014669449492471659, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0016088460873709427, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0017333921115357405, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0016468201364126269, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0015634199097146325, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train Loss = 0.0016321950922031985, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015901295396018899, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.00164017429451204, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.001632020833453709, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.001560664212023831, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0017703431280598329, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.001982618831864331, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018332761159109278, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0015552429827365008, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0014585647914610999, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.001830400088472682, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0014548306354973382, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0014084343654285113, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00133282438081842, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6225142\ttotal: 13.2ms\tremaining: 2.63s\n",
      "1:\tlearn: 0.5628587\ttotal: 26.4ms\tremaining: 2.62s\n",
      "2:\tlearn: 0.5190954\ttotal: 39.8ms\tremaining: 2.61s\n",
      "3:\tlearn: 0.4782933\ttotal: 53.2ms\tremaining: 2.61s\n",
      "4:\tlearn: 0.4399292\ttotal: 66.7ms\tremaining: 2.6s\n",
      "5:\tlearn: 0.4138888\ttotal: 79.9ms\tremaining: 2.58s\n",
      "6:\tlearn: 0.3855621\ttotal: 92.4ms\tremaining: 2.55s\n",
      "7:\tlearn: 0.3676570\ttotal: 105ms\tremaining: 2.52s\n",
      "8:\tlearn: 0.3475479\ttotal: 117ms\tremaining: 2.48s\n",
      "9:\tlearn: 0.3265099\ttotal: 129ms\tremaining: 2.45s\n",
      "10:\tlearn: 0.3062138\ttotal: 141ms\tremaining: 2.42s\n",
      "11:\tlearn: 0.2918342\ttotal: 153ms\tremaining: 2.39s\n",
      "12:\tlearn: 0.2730070\ttotal: 165ms\tremaining: 2.37s\n",
      "13:\tlearn: 0.2504460\ttotal: 176ms\tremaining: 2.34s\n",
      "14:\tlearn: 0.2377268\ttotal: 188ms\tremaining: 2.32s\n",
      "15:\tlearn: 0.2289086\ttotal: 200ms\tremaining: 2.3s\n",
      "16:\tlearn: 0.2199423\ttotal: 211ms\tremaining: 2.28s\n",
      "17:\tlearn: 0.2122089\ttotal: 223ms\tremaining: 2.26s\n",
      "18:\tlearn: 0.2016170\ttotal: 235ms\tremaining: 2.24s\n",
      "19:\tlearn: 0.1899033\ttotal: 247ms\tremaining: 2.23s\n",
      "20:\tlearn: 0.1777889\ttotal: 260ms\tremaining: 2.21s\n",
      "21:\tlearn: 0.1693288\ttotal: 271ms\tremaining: 2.2s\n",
      "22:\tlearn: 0.1580289\ttotal: 283ms\tremaining: 2.18s\n",
      "23:\tlearn: 0.1515001\ttotal: 295ms\tremaining: 2.17s\n",
      "24:\tlearn: 0.1451716\ttotal: 307ms\tremaining: 2.15s\n",
      "25:\tlearn: 0.1379273\ttotal: 319ms\tremaining: 2.14s\n",
      "26:\tlearn: 0.1319046\ttotal: 331ms\tremaining: 2.12s\n",
      "27:\tlearn: 0.1261903\ttotal: 342ms\tremaining: 2.1s\n",
      "28:\tlearn: 0.1216354\ttotal: 359ms\tremaining: 2.11s\n",
      "29:\tlearn: 0.1151804\ttotal: 374ms\tremaining: 2.12s\n",
      "30:\tlearn: 0.1093344\ttotal: 386ms\tremaining: 2.1s\n",
      "31:\tlearn: 0.1057406\ttotal: 398ms\tremaining: 2.09s\n",
      "32:\tlearn: 0.1023992\ttotal: 410ms\tremaining: 2.07s\n",
      "33:\tlearn: 0.0997136\ttotal: 422ms\tremaining: 2.06s\n",
      "34:\tlearn: 0.0962285\ttotal: 434ms\tremaining: 2.05s\n",
      "35:\tlearn: 0.0916868\ttotal: 446ms\tremaining: 2.03s\n",
      "36:\tlearn: 0.0880461\ttotal: 458ms\tremaining: 2.02s\n",
      "37:\tlearn: 0.0857660\ttotal: 470ms\tremaining: 2s\n",
      "38:\tlearn: 0.0840777\ttotal: 481ms\tremaining: 1.99s\n",
      "39:\tlearn: 0.0803384\ttotal: 493ms\tremaining: 1.97s\n",
      "40:\tlearn: 0.0780313\ttotal: 505ms\tremaining: 1.96s\n",
      "41:\tlearn: 0.0739095\ttotal: 516ms\tremaining: 1.94s\n",
      "42:\tlearn: 0.0716119\ttotal: 528ms\tremaining: 1.93s\n",
      "43:\tlearn: 0.0691919\ttotal: 540ms\tremaining: 1.91s\n",
      "44:\tlearn: 0.0665623\ttotal: 552ms\tremaining: 1.9s\n",
      "45:\tlearn: 0.0648841\ttotal: 564ms\tremaining: 1.89s\n",
      "46:\tlearn: 0.0618011\ttotal: 575ms\tremaining: 1.87s\n",
      "47:\tlearn: 0.0602055\ttotal: 588ms\tremaining: 1.86s\n",
      "48:\tlearn: 0.0588249\ttotal: 600ms\tremaining: 1.85s\n",
      "49:\tlearn: 0.0573663\ttotal: 612ms\tremaining: 1.83s\n",
      "50:\tlearn: 0.0551478\ttotal: 624ms\tremaining: 1.82s\n",
      "51:\tlearn: 0.0533184\ttotal: 636ms\tremaining: 1.81s\n",
      "52:\tlearn: 0.0506177\ttotal: 648ms\tremaining: 1.8s\n",
      "53:\tlearn: 0.0494270\ttotal: 659ms\tremaining: 1.78s\n",
      "54:\tlearn: 0.0483541\ttotal: 671ms\tremaining: 1.77s\n",
      "55:\tlearn: 0.0462501\ttotal: 683ms\tremaining: 1.76s\n",
      "56:\tlearn: 0.0440795\ttotal: 695ms\tremaining: 1.74s\n",
      "57:\tlearn: 0.0418008\ttotal: 707ms\tremaining: 1.73s\n",
      "58:\tlearn: 0.0409133\ttotal: 718ms\tremaining: 1.72s\n",
      "59:\tlearn: 0.0396918\ttotal: 730ms\tremaining: 1.7s\n",
      "60:\tlearn: 0.0395718\ttotal: 732ms\tremaining: 1.67s\n",
      "61:\tlearn: 0.0381411\ttotal: 744ms\tremaining: 1.66s\n",
      "62:\tlearn: 0.0367638\ttotal: 756ms\tremaining: 1.64s\n",
      "63:\tlearn: 0.0357557\ttotal: 768ms\tremaining: 1.63s\n",
      "64:\tlearn: 0.0349077\ttotal: 780ms\tremaining: 1.62s\n",
      "65:\tlearn: 0.0341222\ttotal: 792ms\tremaining: 1.61s\n",
      "66:\tlearn: 0.0333464\ttotal: 804ms\tremaining: 1.59s\n",
      "67:\tlearn: 0.0328676\ttotal: 816ms\tremaining: 1.58s\n",
      "68:\tlearn: 0.0326022\ttotal: 818ms\tremaining: 1.55s\n",
      "69:\tlearn: 0.0315664\ttotal: 830ms\tremaining: 1.54s\n",
      "70:\tlearn: 0.0309170\ttotal: 842ms\tremaining: 1.53s\n",
      "71:\tlearn: 0.0304428\ttotal: 853ms\tremaining: 1.52s\n",
      "72:\tlearn: 0.0294232\ttotal: 865ms\tremaining: 1.5s\n",
      "73:\tlearn: 0.0289744\ttotal: 876ms\tremaining: 1.49s\n",
      "74:\tlearn: 0.0284785\ttotal: 888ms\tremaining: 1.48s\n",
      "75:\tlearn: 0.0277568\ttotal: 900ms\tremaining: 1.47s\n",
      "76:\tlearn: 0.0272491\ttotal: 912ms\tremaining: 1.46s\n",
      "77:\tlearn: 0.0265624\ttotal: 924ms\tremaining: 1.44s\n",
      "78:\tlearn: 0.0257640\ttotal: 935ms\tremaining: 1.43s\n",
      "79:\tlearn: 0.0249049\ttotal: 947ms\tremaining: 1.42s\n",
      "80:\tlearn: 0.0238448\ttotal: 959ms\tremaining: 1.41s\n",
      "81:\tlearn: 0.0234537\ttotal: 970ms\tremaining: 1.4s\n",
      "82:\tlearn: 0.0231237\ttotal: 983ms\tremaining: 1.39s\n",
      "83:\tlearn: 0.0227081\ttotal: 995ms\tremaining: 1.37s\n",
      "84:\tlearn: 0.0214016\ttotal: 1.01s\tremaining: 1.36s\n",
      "85:\tlearn: 0.0205886\ttotal: 1.02s\tremaining: 1.35s\n",
      "86:\tlearn: 0.0196185\ttotal: 1.03s\tremaining: 1.34s\n",
      "87:\tlearn: 0.0192886\ttotal: 1.04s\tremaining: 1.33s\n",
      "88:\tlearn: 0.0187656\ttotal: 1.05s\tremaining: 1.31s\n",
      "89:\tlearn: 0.0183012\ttotal: 1.07s\tremaining: 1.3s\n",
      "90:\tlearn: 0.0177663\ttotal: 1.08s\tremaining: 1.29s\n",
      "91:\tlearn: 0.0173245\ttotal: 1.09s\tremaining: 1.28s\n",
      "92:\tlearn: 0.0170769\ttotal: 1.1s\tremaining: 1.27s\n",
      "93:\tlearn: 0.0169102\ttotal: 1.11s\tremaining: 1.25s\n",
      "94:\tlearn: 0.0165259\ttotal: 1.12s\tremaining: 1.24s\n",
      "95:\tlearn: 0.0160691\ttotal: 1.14s\tremaining: 1.23s\n",
      "96:\tlearn: 0.0157762\ttotal: 1.15s\tremaining: 1.22s\n",
      "97:\tlearn: 0.0155893\ttotal: 1.16s\tremaining: 1.21s\n",
      "98:\tlearn: 0.0149160\ttotal: 1.17s\tremaining: 1.2s\n",
      "99:\tlearn: 0.0146776\ttotal: 1.18s\tremaining: 1.18s\n",
      "100:\tlearn: 0.0144163\ttotal: 1.2s\tremaining: 1.17s\n",
      "101:\tlearn: 0.0140902\ttotal: 1.21s\tremaining: 1.16s\n",
      "102:\tlearn: 0.0137441\ttotal: 1.22s\tremaining: 1.15s\n",
      "103:\tlearn: 0.0134664\ttotal: 1.23s\tremaining: 1.14s\n",
      "104:\tlearn: 0.0131363\ttotal: 1.24s\tremaining: 1.13s\n",
      "105:\tlearn: 0.0129137\ttotal: 1.26s\tremaining: 1.11s\n",
      "106:\tlearn: 0.0126925\ttotal: 1.27s\tremaining: 1.1s\n",
      "107:\tlearn: 0.0123371\ttotal: 1.28s\tremaining: 1.09s\n",
      "108:\tlearn: 0.0120387\ttotal: 1.29s\tremaining: 1.08s\n",
      "109:\tlearn: 0.0117418\ttotal: 1.3s\tremaining: 1.07s\n",
      "110:\tlearn: 0.0113335\ttotal: 1.32s\tremaining: 1.05s\n",
      "111:\tlearn: 0.0112063\ttotal: 1.33s\tremaining: 1.04s\n",
      "112:\tlearn: 0.0110184\ttotal: 1.35s\tremaining: 1.04s\n",
      "113:\tlearn: 0.0108064\ttotal: 1.36s\tremaining: 1.03s\n",
      "114:\tlearn: 0.0106282\ttotal: 1.38s\tremaining: 1.02s\n",
      "115:\tlearn: 0.0104240\ttotal: 1.4s\tremaining: 1.01s\n",
      "116:\tlearn: 0.0102304\ttotal: 1.41s\tremaining: 1s\n",
      "117:\tlearn: 0.0098075\ttotal: 1.43s\tremaining: 994ms\n",
      "118:\tlearn: 0.0095364\ttotal: 1.45s\tremaining: 985ms\n",
      "119:\tlearn: 0.0091189\ttotal: 1.46s\tremaining: 973ms\n",
      "120:\tlearn: 0.0089632\ttotal: 1.47s\tremaining: 961ms\n",
      "121:\tlearn: 0.0087685\ttotal: 1.48s\tremaining: 949ms\n",
      "122:\tlearn: 0.0085729\ttotal: 1.5s\tremaining: 937ms\n",
      "123:\tlearn: 0.0082226\ttotal: 1.51s\tremaining: 925ms\n",
      "124:\tlearn: 0.0080650\ttotal: 1.52s\tremaining: 912ms\n",
      "125:\tlearn: 0.0079430\ttotal: 1.53s\tremaining: 900ms\n",
      "126:\tlearn: 0.0078141\ttotal: 1.54s\tremaining: 888ms\n",
      "127:\tlearn: 0.0076824\ttotal: 1.56s\tremaining: 876ms\n",
      "128:\tlearn: 0.0075464\ttotal: 1.57s\tremaining: 863ms\n",
      "129:\tlearn: 0.0071689\ttotal: 1.58s\tremaining: 851ms\n",
      "130:\tlearn: 0.0070555\ttotal: 1.59s\tremaining: 838ms\n",
      "131:\tlearn: 0.0068815\ttotal: 1.6s\tremaining: 826ms\n",
      "132:\tlearn: 0.0067807\ttotal: 1.61s\tremaining: 814ms\n",
      "133:\tlearn: 0.0066227\ttotal: 1.63s\tremaining: 801ms\n",
      "134:\tlearn: 0.0065123\ttotal: 1.64s\tremaining: 789ms\n",
      "135:\tlearn: 0.0064047\ttotal: 1.65s\tremaining: 777ms\n",
      "136:\tlearn: 0.0061736\ttotal: 1.66s\tremaining: 764ms\n",
      "137:\tlearn: 0.0060072\ttotal: 1.67s\tremaining: 752ms\n",
      "138:\tlearn: 0.0058741\ttotal: 1.69s\tremaining: 740ms\n",
      "139:\tlearn: 0.0056670\ttotal: 1.7s\tremaining: 728ms\n",
      "140:\tlearn: 0.0055141\ttotal: 1.71s\tremaining: 715ms\n",
      "141:\tlearn: 0.0054104\ttotal: 1.72s\tremaining: 703ms\n",
      "142:\tlearn: 0.0052799\ttotal: 1.73s\tremaining: 691ms\n",
      "143:\tlearn: 0.0051627\ttotal: 1.74s\tremaining: 679ms\n",
      "144:\tlearn: 0.0050730\ttotal: 1.76s\tremaining: 666ms\n",
      "145:\tlearn: 0.0049084\ttotal: 1.77s\tremaining: 654ms\n",
      "146:\tlearn: 0.0047900\ttotal: 1.78s\tremaining: 642ms\n",
      "147:\tlearn: 0.0046707\ttotal: 1.79s\tremaining: 630ms\n",
      "148:\tlearn: 0.0046000\ttotal: 1.8s\tremaining: 617ms\n",
      "149:\tlearn: 0.0044964\ttotal: 1.81s\tremaining: 605ms\n",
      "150:\tlearn: 0.0043607\ttotal: 1.83s\tremaining: 593ms\n",
      "151:\tlearn: 0.0042999\ttotal: 1.84s\tremaining: 581ms\n",
      "152:\tlearn: 0.0042331\ttotal: 1.85s\tremaining: 569ms\n",
      "153:\tlearn: 0.0041357\ttotal: 1.86s\tremaining: 557ms\n",
      "154:\tlearn: 0.0040525\ttotal: 1.88s\tremaining: 544ms\n",
      "155:\tlearn: 0.0039847\ttotal: 1.89s\tremaining: 532ms\n",
      "156:\tlearn: 0.0038945\ttotal: 1.9s\tremaining: 520ms\n",
      "157:\tlearn: 0.0038380\ttotal: 1.91s\tremaining: 508ms\n",
      "158:\tlearn: 0.0038037\ttotal: 1.92s\tremaining: 496ms\n",
      "159:\tlearn: 0.0037407\ttotal: 1.93s\tremaining: 484ms\n",
      "160:\tlearn: 0.0036382\ttotal: 1.95s\tremaining: 472ms\n",
      "161:\tlearn: 0.0035728\ttotal: 1.96s\tremaining: 459ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162:\tlearn: 0.0035350\ttotal: 1.97s\tremaining: 447ms\n",
      "163:\tlearn: 0.0034901\ttotal: 1.98s\tremaining: 435ms\n",
      "164:\tlearn: 0.0034318\ttotal: 1.99s\tremaining: 423ms\n",
      "165:\tlearn: 0.0033842\ttotal: 2s\tremaining: 411ms\n",
      "166:\tlearn: 0.0033150\ttotal: 2.02s\tremaining: 399ms\n",
      "167:\tlearn: 0.0032847\ttotal: 2.03s\tremaining: 387ms\n",
      "168:\tlearn: 0.0032236\ttotal: 2.04s\tremaining: 374ms\n",
      "169:\tlearn: 0.0031636\ttotal: 2.05s\tremaining: 362ms\n",
      "170:\tlearn: 0.0030623\ttotal: 2.06s\tremaining: 350ms\n",
      "171:\tlearn: 0.0030196\ttotal: 2.08s\tremaining: 338ms\n",
      "172:\tlearn: 0.0029713\ttotal: 2.09s\tremaining: 326ms\n",
      "173:\tlearn: 0.0029056\ttotal: 2.1s\tremaining: 314ms\n",
      "174:\tlearn: 0.0027928\ttotal: 2.11s\tremaining: 302ms\n",
      "175:\tlearn: 0.0027421\ttotal: 2.12s\tremaining: 290ms\n",
      "176:\tlearn: 0.0026919\ttotal: 2.13s\tremaining: 278ms\n",
      "177:\tlearn: 0.0026352\ttotal: 2.15s\tremaining: 265ms\n",
      "178:\tlearn: 0.0025879\ttotal: 2.16s\tremaining: 253ms\n",
      "179:\tlearn: 0.0025228\ttotal: 2.17s\tremaining: 241ms\n",
      "180:\tlearn: 0.0024548\ttotal: 2.18s\tremaining: 229ms\n",
      "181:\tlearn: 0.0023813\ttotal: 2.19s\tremaining: 217ms\n",
      "182:\tlearn: 0.0023389\ttotal: 2.21s\tremaining: 205ms\n",
      "183:\tlearn: 0.0022942\ttotal: 2.22s\tremaining: 193ms\n",
      "184:\tlearn: 0.0022658\ttotal: 2.23s\tremaining: 181ms\n",
      "185:\tlearn: 0.0022141\ttotal: 2.24s\tremaining: 169ms\n",
      "186:\tlearn: 0.0022002\ttotal: 2.25s\tremaining: 157ms\n",
      "187:\tlearn: 0.0021572\ttotal: 2.27s\tremaining: 145ms\n",
      "188:\tlearn: 0.0021257\ttotal: 2.28s\tremaining: 133ms\n",
      "189:\tlearn: 0.0020719\ttotal: 2.29s\tremaining: 121ms\n",
      "190:\tlearn: 0.0020287\ttotal: 2.3s\tremaining: 108ms\n",
      "191:\tlearn: 0.0019885\ttotal: 2.32s\tremaining: 96.6ms\n",
      "192:\tlearn: 0.0019620\ttotal: 2.33s\tremaining: 84.7ms\n",
      "193:\tlearn: 0.0019382\ttotal: 2.35s\tremaining: 72.8ms\n",
      "194:\tlearn: 0.0019198\ttotal: 2.37s\tremaining: 60.8ms\n",
      "195:\tlearn: 0.0018544\ttotal: 2.39s\tremaining: 48.8ms\n",
      "196:\tlearn: 0.0018152\ttotal: 2.4s\tremaining: 36.6ms\n",
      "197:\tlearn: 0.0017846\ttotal: 2.42s\tremaining: 24.4ms\n",
      "198:\tlearn: 0.0017640\ttotal: 2.43s\tremaining: 12.2ms\n",
      "199:\tlearn: 0.0017512\ttotal: 2.44s\tremaining: 0us\n",
      "Dataset 2:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736bd888551f45ff8f38fc3ea765c527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0358267eadd34e9f8085f593f373819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5422780687051103, Recall = 0.6920529801324503, Aging Rate = 0.4458609271523179, Precision = 0.7760861492759005, f1 = 0.7316646245405216\n",
      "Epoch 2: Train Loss = 0.36002741558662316, Recall = 0.8586092715231788, Aging Rate = 0.5067880794701987, Precision = 0.847108787977785, f1 = 0.8528202598256865\n",
      "Epoch 3: Train Loss = 0.27555385038552693, Recall = 0.9019867549668874, Aging Rate = 0.5054635761589404, Precision = 0.8922371437929905, f1 = 0.897085460233822\n",
      "Epoch 4: Train Loss = 0.2133238449100627, Recall = 0.926158940397351, Aging Rate = 0.49933774834437084, Precision = 0.9273872679045093, f1 = 0.9267726971504308\n",
      "Epoch 5: Train Loss = 0.17529147492339278, Recall = 0.9413907284768211, Aging Rate = 0.498841059602649, Precision = 0.9435778294059077, f1 = 0.9424830101110558\n",
      "Test Loss = 0.142156861031687, Recall = 0.9380794701986755, Aging Rate = 0.4794701986754967, precision = 0.9782458563535912\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.12781292066866198, Recall = 0.9602649006622517, Aging Rate = 0.4966887417218543, Precision = 0.9666666666666667, f1 = 0.9634551495016612\n",
      "Epoch 7: Train Loss = 0.10509679813850795, Recall = 0.9692052980132451, Aging Rate = 0.4986754966887417, Precision = 0.9717795484727756, f1 = 0.9704907161803714\n",
      "Epoch 8: Train Loss = 0.08754581290957154, Recall = 0.9728476821192052, Aging Rate = 0.49701986754966887, Precision = 0.9786808794137242, f1 = 0.9757555629359016\n",
      "Epoch 9: Train Loss = 0.07546038783543947, Recall = 0.9771523178807947, Aging Rate = 0.49718543046357616, Precision = 0.9826839826839827, f1 = 0.9799103436825503\n",
      "Epoch 10: Train Loss = 0.06605030579973531, Recall = 0.9811258278145696, Aging Rate = 0.498841059602649, Precision = 0.9834052439429141, f1 = 0.9822642134924583\n",
      "Test Loss = 0.0546829521705378, Recall = 0.9870860927152317, Aging Rate = 0.4996688741721854, precision = 0.987740225314778\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05645115284236851, Recall = 0.9854304635761589, Aging Rate = 0.4995033112582781, Precision = 0.9864103413987405, f1 = 0.9859201590193805\n",
      "Epoch 12: Train Loss = 0.050256207238710084, Recall = 0.9877483443708609, Aging Rate = 0.4995033112582781, Precision = 0.9887305270135897, f1 = 0.9882391916514824\n",
      "Epoch 13: Train Loss = 0.044919282893668734, Recall = 0.9890728476821192, Aging Rate = 0.4995033112582781, Precision = 0.9900563473649321, f1 = 0.9895643531555409\n",
      "Epoch 14: Train Loss = 0.040006583756366315, Recall = 0.9897350993377484, Aging Rate = 0.49933774834437084, Precision = 0.9910477453580901, f1 = 0.99039098740888\n",
      "Epoch 15: Train Loss = 0.03570824492451371, Recall = 0.9920529801324504, Aging Rate = 0.49933774834437084, Precision = 0.993368700265252, f1 = 0.9927104042412194\n",
      "Test Loss = 0.030237846826480713, Recall = 0.9947019867549669, Aging Rate = 0.5, precision = 0.9947019867549669\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03318433373771756, Recall = 0.993046357615894, Aging Rate = 0.5004966887417218, Precision = 0.9920608666887198, f1 = 0.9925533675326825\n",
      "Epoch 17: Train Loss = 0.028857414352874092, Recall = 0.9940397350993377, Aging Rate = 0.5003311258278146, Precision = 0.9933818663136995, f1 = 0.9937106918238994\n",
      "Epoch 18: Train Loss = 0.027601687740904605, Recall = 0.993046357615894, Aging Rate = 0.4991721854304636, Precision = 0.9946932006633499, f1 = 0.9938690969345485\n",
      "Epoch 19: Train Loss = 0.024621554787289227, Recall = 0.9947019867549669, Aging Rate = 0.4990066225165563, Precision = 0.9966821499668215, f1 = 0.9956910838581372\n",
      "Epoch 20: Train Loss = 0.023149800009482743, Recall = 0.9960264900662251, Aging Rate = 0.5004966887417218, Precision = 0.9950380416804498, f1 = 0.9955320205196094\n",
      "Test Loss = 0.01895784418659889, Recall = 0.9980132450331126, Aging Rate = 0.5006622516556292, precision = 0.9966931216931217\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.021185962315525442, Recall = 0.9966887417218543, Aging Rate = 0.5006622516556292, Precision = 0.9953703703703703, f1 = 0.9960291197882197\n",
      "Epoch 22: Train Loss = 0.01869294922305456, Recall = 0.997682119205298, Aging Rate = 0.5001655629139072, Precision = 0.9973518702416418, f1 = 0.9975169673895051\n",
      "Epoch 23: Train Loss = 0.016993174991810953, Recall = 0.9970198675496689, Aging Rate = 0.4996688741721854, Precision = 0.9976805831676607, f1 = 0.997350115932428\n",
      "Epoch 24: Train Loss = 0.016180200090274117, Recall = 0.9983443708609272, Aging Rate = 0.5008278145695364, Precision = 0.996694214876033, f1 = 0.9975186104218362\n",
      "Epoch 25: Train Loss = 0.014866824344491327, Recall = 0.9980132450331126, Aging Rate = 0.5001655629139072, Precision = 0.9976828864614367, f1 = 0.9978480384042379\n",
      "Test Loss = 0.012019518095481869, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, precision = 0.9983454665784249\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.01398863863409651, Recall = 0.9980132450331126, Aging Rate = 0.5, Precision = 0.9980132450331126, f1 = 0.9980132450331126\n",
      "Epoch 27: Train Loss = 0.012951997835312458, Recall = 0.9980132450331126, Aging Rate = 0.4998344370860927, Precision = 0.9983438224577674, f1 = 0.9981785063752276\n",
      "Epoch 28: Train Loss = 0.011952138934839542, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Epoch 29: Train Loss = 0.011233362530216279, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Epoch 30: Train Loss = 0.010858583567812052, Recall = 0.9983443708609272, Aging Rate = 0.5001655629139072, Precision = 0.9980139026812314, f1 = 0.9981791094189705\n",
      "Test Loss = 0.008562042990958453, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, precision = 0.9990069513406157\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.009523578739373494, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 32: Train Loss = 0.009165924401731779, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 33: Train Loss = 0.009375008428777686, Recall = 0.9986754966887417, Aging Rate = 0.5001655629139072, Precision = 0.9983449189010262, f1 = 0.998510180433703\n",
      "Epoch 34: Train Loss = 0.008800263158261578, Recall = 0.9986754966887417, Aging Rate = 0.5003311258278146, Precision = 0.9980145598941098, f1 = 0.9983449189010262\n",
      "Epoch 35: Train Loss = 0.008277131949508229, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Test Loss = 0.0064368631397774875, Recall = 1.0, Aging Rate = 0.5004966887417218, precision = 0.9990076083360899\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.00769429797652048, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 37: Train Loss = 0.006920856251905592, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 38: Train Loss = 0.007855580853828729, Recall = 0.9990066225165563, Aging Rate = 0.5004966887417218, Precision = 0.99801521667218, f1 = 0.9985106735065364\n",
      "Epoch 39: Train Loss = 0.007160417585393155, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 40: Train Loss = 0.007026003813062677, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Test Loss = 0.005476599456804083, Recall = 1.0, Aging Rate = 0.5008278145695364, precision = 0.9983471074380166\n",
      "\n",
      "Epoch 41: Train Loss = 0.00620783254232825, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 42: Train Loss = 0.006253832143373689, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 43: Train Loss = 0.006336089255695312, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Epoch 44: Train Loss = 0.0061944938307961096, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Epoch 45: Train Loss = 0.006864960484564403, Recall = 0.9986754966887417, Aging Rate = 0.4998344370860927, Precision = 0.9990062934746605, f1 = 0.9988408676933267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.004857931727703833, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, precision = 0.9993379675604105\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.0057132837794326395, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 47: Train Loss = 0.00521724426144401, Recall = 0.9990066225165563, Aging Rate = 0.4996688741721854, Precision = 0.9996686547382373, f1 = 0.999337528983107\n",
      "Epoch 48: Train Loss = 0.005080345467917177, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 49: Train Loss = 0.004670725804476055, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 50: Train Loss = 0.004618790923519097, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Test Loss = 0.005295334137908325, Recall = 1.0, Aging Rate = 0.5009933774834437, precision = 0.9980171844018506\n",
      "\n",
      "Epoch 51: Train Loss = 0.005610982315392783, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 52: Train Loss = 0.005201214834097896, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, Precision = 0.9986768111147867, f1 = 0.9991725963925203\n",
      "Epoch 53: Train Loss = 0.004712174911459057, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 54: Train Loss = 0.00462568393378019, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 55: Train Loss = 0.004802323365584016, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Test Loss = 0.0051993641478089705, Recall = 0.9983443708609272, Aging Rate = 0.4991721854304636, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.004590166663590646, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 57: Train Loss = 0.005190472582483121, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 58: Train Loss = 0.00413076302425208, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 59: Train Loss = 0.005943333993153955, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 60: Train Loss = 0.004502643579894689, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.003484701620883598, Recall = 1.0, Aging Rate = 0.5001655629139072, precision = 0.9996689837802052\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.004527311280850839, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 62: Train Loss = 0.003800658533497658, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 63: Train Loss = 0.004460132505362298, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 64: Train Loss = 0.0038456140875520297, Recall = 1.0, Aging Rate = 0.5004966887417218, Precision = 0.9990076083360899, f1 = 0.9995035578355121\n",
      "Epoch 65: Train Loss = 0.003983818019573756, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.003687575629824427, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.004092685692320695, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 67: Train Loss = 0.004372741531161283, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 68: Train Loss = 0.004336176228105084, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 69: Train Loss = 0.003692131522512495, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 70: Train Loss = 0.0034022558563765112, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003388824917542569, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 71: Train Loss = 0.004343437986904817, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 72: Train Loss = 0.005230986725212081, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 73: Train Loss = 0.003332324055584752, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 74: Train Loss = 0.004133657038840928, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 75: Train Loss = 0.004488334828518972, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.0038193597786732956, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 76: Train Loss = 0.005705208113626731, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 77: Train Loss = 0.004682653483669489, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 78: Train Loss = 0.003269319787709388, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 79: Train Loss = 0.0030101987573361356, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 80: Train Loss = 0.00302238078149566, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Test Loss = 0.002632043137992208, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.003745898600075616, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.004581446242241176, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 83: Train Loss = 0.0033933573359290494, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 84: Train Loss = 0.004130765193431879, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 85: Train Loss = 0.003943235404320703, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Test Loss = 0.0037578491780857573, Recall = 0.9983443708609272, Aging Rate = 0.4991721854304636, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.004078687559998292, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 87: Train Loss = 0.00384694093758154, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 88: Train Loss = 0.0036917367236170644, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.004413506344565197, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 90: Train Loss = 0.003104410943766402, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029739444252775845, Recall = 1.0, Aging Rate = 0.5001655629139072, precision = 0.9996689837802052\n",
      "\n",
      "Epoch 91: Train Loss = 0.0040618040248912, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss = 0.003344039243356084, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 93: Train Loss = 0.0031982771188810172, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 94: Train Loss = 0.0035217341936088554, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 95: Train Loss = 0.002887204218199415, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0031741768526474617, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 96: Train Loss = 0.004154894970178555, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 97: Train Loss = 0.0038325396776569404, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 98: Train Loss = 0.0035833403523641313, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 99: Train Loss = 0.003776249091221106, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 100: Train Loss = 0.003625157804518247, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.004270449373562703, Recall = 0.9990066225165563, Aging Rate = 0.4995033112582781, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.0036302639704987112, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 102: Train Loss = 0.003974565941476516, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 103: Train Loss = 0.0038130943148415412, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 104: Train Loss = 0.003909103811058579, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 105: Train Loss = 0.0036962694484610975, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Test Loss = 0.0030911168620138376, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.004209437005727595, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 107: Train Loss = 0.0036013095150855892, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 108: Train Loss = 0.0036040651431578576, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 109: Train Loss = 0.0037555217856251395, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 110: Train Loss = 0.004309995969239351, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Test Loss = 0.0024264164270512414, Recall = 1.0, Aging Rate = 0.5001655629139072, precision = 0.9996689837802052\n",
      "\n",
      "Epoch 111: Train Loss = 0.0034789092686113637, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 112: Train Loss = 0.0035400034746445024, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 113: Train Loss = 0.0044796896787724205, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 114: Train Loss = 0.0031553603138231878, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.004070898642553794, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.0027535976424184166, Recall = 1.0, Aging Rate = 0.5001655629139072, precision = 0.9996689837802052\n",
      "\n",
      "Training Finished at epoch 115.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5491000\ttotal: 15.6ms\tremaining: 4.66s\n",
      "1:\tlearn: 0.4684943\ttotal: 31.3ms\tremaining: 4.67s\n",
      "2:\tlearn: 0.3896602\ttotal: 46.8ms\tremaining: 4.64s\n",
      "3:\tlearn: 0.3554129\ttotal: 63ms\tremaining: 4.67s\n",
      "4:\tlearn: 0.3280997\ttotal: 78.9ms\tremaining: 4.65s\n",
      "5:\tlearn: 0.2996572\ttotal: 94.1ms\tremaining: 4.61s\n",
      "6:\tlearn: 0.2673687\ttotal: 110ms\tremaining: 4.58s\n",
      "7:\tlearn: 0.2464214\ttotal: 124ms\tremaining: 4.53s\n",
      "8:\tlearn: 0.2284420\ttotal: 139ms\tremaining: 4.48s\n",
      "9:\tlearn: 0.2178473\ttotal: 153ms\tremaining: 4.44s\n",
      "10:\tlearn: 0.2015909\ttotal: 168ms\tremaining: 4.42s\n",
      "11:\tlearn: 0.1890149\ttotal: 183ms\tremaining: 4.39s\n",
      "12:\tlearn: 0.1816003\ttotal: 197ms\tremaining: 4.36s\n",
      "13:\tlearn: 0.1631778\ttotal: 212ms\tremaining: 4.33s\n",
      "14:\tlearn: 0.1528707\ttotal: 226ms\tremaining: 4.3s\n",
      "15:\tlearn: 0.1439202\ttotal: 240ms\tremaining: 4.26s\n",
      "16:\tlearn: 0.1359400\ttotal: 255ms\tremaining: 4.24s\n",
      "17:\tlearn: 0.1343765\ttotal: 258ms\tremaining: 4.03s\n",
      "18:\tlearn: 0.1255536\ttotal: 272ms\tremaining: 4.02s\n",
      "19:\tlearn: 0.1169776\ttotal: 286ms\tremaining: 4.01s\n",
      "20:\tlearn: 0.1129624\ttotal: 301ms\tremaining: 4s\n",
      "21:\tlearn: 0.1080070\ttotal: 315ms\tremaining: 3.98s\n",
      "22:\tlearn: 0.1028999\ttotal: 330ms\tremaining: 3.97s\n",
      "23:\tlearn: 0.0948560\ttotal: 344ms\tremaining: 3.96s\n",
      "24:\tlearn: 0.0911747\ttotal: 358ms\tremaining: 3.94s\n",
      "25:\tlearn: 0.0858595\ttotal: 373ms\tremaining: 3.93s\n",
      "26:\tlearn: 0.0832415\ttotal: 388ms\tremaining: 3.92s\n",
      "27:\tlearn: 0.0801307\ttotal: 402ms\tremaining: 3.9s\n",
      "28:\tlearn: 0.0786074\ttotal: 420ms\tremaining: 3.92s\n",
      "29:\tlearn: 0.0744500\ttotal: 438ms\tremaining: 3.94s\n",
      "30:\tlearn: 0.0706105\ttotal: 457ms\tremaining: 3.96s\n",
      "31:\tlearn: 0.0686691\ttotal: 474ms\tremaining: 3.97s\n",
      "32:\tlearn: 0.0663451\ttotal: 491ms\tremaining: 3.97s\n",
      "33:\tlearn: 0.0639996\ttotal: 506ms\tremaining: 3.96s\n",
      "34:\tlearn: 0.0639884\ttotal: 508ms\tremaining: 3.85s\n",
      "35:\tlearn: 0.0631402\ttotal: 523ms\tremaining: 3.83s\n",
      "36:\tlearn: 0.0617438\ttotal: 537ms\tremaining: 3.82s\n",
      "37:\tlearn: 0.0596944\ttotal: 552ms\tremaining: 3.8s\n",
      "38:\tlearn: 0.0574641\ttotal: 566ms\tremaining: 3.79s\n",
      "39:\tlearn: 0.0565466\ttotal: 580ms\tremaining: 3.77s\n",
      "40:\tlearn: 0.0545771\ttotal: 594ms\tremaining: 3.75s\n",
      "41:\tlearn: 0.0527096\ttotal: 609ms\tremaining: 3.74s\n",
      "42:\tlearn: 0.0512993\ttotal: 624ms\tremaining: 3.73s\n",
      "43:\tlearn: 0.0495650\ttotal: 638ms\tremaining: 3.71s\n",
      "44:\tlearn: 0.0483919\ttotal: 652ms\tremaining: 3.7s\n",
      "45:\tlearn: 0.0464589\ttotal: 667ms\tremaining: 3.68s\n",
      "46:\tlearn: 0.0455903\ttotal: 682ms\tremaining: 3.67s\n",
      "47:\tlearn: 0.0440468\ttotal: 696ms\tremaining: 3.66s\n",
      "48:\tlearn: 0.0424441\ttotal: 711ms\tremaining: 3.64s\n",
      "49:\tlearn: 0.0415927\ttotal: 719ms\tremaining: 3.59s\n",
      "50:\tlearn: 0.0409620\ttotal: 733ms\tremaining: 3.58s\n",
      "51:\tlearn: 0.0402953\ttotal: 748ms\tremaining: 3.57s\n",
      "52:\tlearn: 0.0395274\ttotal: 762ms\tremaining: 3.55s\n",
      "53:\tlearn: 0.0389118\ttotal: 776ms\tremaining: 3.54s\n",
      "54:\tlearn: 0.0377868\ttotal: 791ms\tremaining: 3.52s\n",
      "55:\tlearn: 0.0366859\ttotal: 805ms\tremaining: 3.51s\n",
      "56:\tlearn: 0.0354536\ttotal: 820ms\tremaining: 3.5s\n",
      "57:\tlearn: 0.0339860\ttotal: 835ms\tremaining: 3.48s\n",
      "58:\tlearn: 0.0336244\ttotal: 849ms\tremaining: 3.47s\n",
      "59:\tlearn: 0.0332869\ttotal: 864ms\tremaining: 3.45s\n",
      "60:\tlearn: 0.0332869\ttotal: 865ms\tremaining: 3.39s\n",
      "61:\tlearn: 0.0326863\ttotal: 880ms\tremaining: 3.38s\n",
      "62:\tlearn: 0.0317769\ttotal: 895ms\tremaining: 3.37s\n",
      "63:\tlearn: 0.0316415\ttotal: 909ms\tremaining: 3.35s\n",
      "64:\tlearn: 0.0308543\ttotal: 923ms\tremaining: 3.34s\n",
      "65:\tlearn: 0.0302047\ttotal: 938ms\tremaining: 3.33s\n",
      "66:\tlearn: 0.0294407\ttotal: 953ms\tremaining: 3.31s\n",
      "67:\tlearn: 0.0287378\ttotal: 967ms\tremaining: 3.3s\n",
      "68:\tlearn: 0.0279994\ttotal: 982ms\tremaining: 3.29s\n",
      "69:\tlearn: 0.0272058\ttotal: 997ms\tremaining: 3.27s\n",
      "70:\tlearn: 0.0265735\ttotal: 1.01s\tremaining: 3.26s\n",
      "71:\tlearn: 0.0258800\ttotal: 1.03s\tremaining: 3.25s\n",
      "72:\tlearn: 0.0254172\ttotal: 1.04s\tremaining: 3.24s\n",
      "73:\tlearn: 0.0250285\ttotal: 1.05s\tremaining: 3.22s\n",
      "74:\tlearn: 0.0248472\ttotal: 1.07s\tremaining: 3.21s\n",
      "75:\tlearn: 0.0245866\ttotal: 1.08s\tremaining: 3.2s\n",
      "76:\tlearn: 0.0241054\ttotal: 1.1s\tremaining: 3.18s\n",
      "77:\tlearn: 0.0238343\ttotal: 1.11s\tremaining: 3.17s\n",
      "78:\tlearn: 0.0231971\ttotal: 1.13s\tremaining: 3.15s\n",
      "79:\tlearn: 0.0228782\ttotal: 1.14s\tremaining: 3.14s\n",
      "80:\tlearn: 0.0226808\ttotal: 1.16s\tremaining: 3.13s\n",
      "81:\tlearn: 0.0220804\ttotal: 1.17s\tremaining: 3.11s\n",
      "82:\tlearn: 0.0216825\ttotal: 1.19s\tremaining: 3.1s\n",
      "83:\tlearn: 0.0213190\ttotal: 1.2s\tremaining: 3.08s\n",
      "84:\tlearn: 0.0210138\ttotal: 1.21s\tremaining: 3.07s\n",
      "85:\tlearn: 0.0207459\ttotal: 1.23s\tremaining: 3.06s\n",
      "86:\tlearn: 0.0203789\ttotal: 1.24s\tremaining: 3.04s\n",
      "87:\tlearn: 0.0198513\ttotal: 1.26s\tremaining: 3.03s\n",
      "88:\tlearn: 0.0194941\ttotal: 1.27s\tremaining: 3.02s\n",
      "89:\tlearn: 0.0189814\ttotal: 1.29s\tremaining: 3s\n",
      "90:\tlearn: 0.0186940\ttotal: 1.3s\tremaining: 2.99s\n",
      "91:\tlearn: 0.0184642\ttotal: 1.31s\tremaining: 2.97s\n",
      "92:\tlearn: 0.0180118\ttotal: 1.33s\tremaining: 2.96s\n",
      "93:\tlearn: 0.0176788\ttotal: 1.34s\tremaining: 2.94s\n",
      "94:\tlearn: 0.0175345\ttotal: 1.36s\tremaining: 2.93s\n",
      "95:\tlearn: 0.0173103\ttotal: 1.37s\tremaining: 2.91s\n",
      "96:\tlearn: 0.0171229\ttotal: 1.39s\tremaining: 2.9s\n",
      "97:\tlearn: 0.0169890\ttotal: 1.4s\tremaining: 2.89s\n",
      "98:\tlearn: 0.0167706\ttotal: 1.42s\tremaining: 2.89s\n",
      "99:\tlearn: 0.0165596\ttotal: 1.44s\tremaining: 2.88s\n",
      "100:\tlearn: 0.0163481\ttotal: 1.46s\tremaining: 2.88s\n",
      "101:\tlearn: 0.0162379\ttotal: 1.49s\tremaining: 2.89s\n",
      "102:\tlearn: 0.0159860\ttotal: 1.51s\tremaining: 2.89s\n",
      "103:\tlearn: 0.0156436\ttotal: 1.53s\tremaining: 2.88s\n",
      "104:\tlearn: 0.0154705\ttotal: 1.54s\tremaining: 2.87s\n",
      "105:\tlearn: 0.0152114\ttotal: 1.56s\tremaining: 2.85s\n",
      "106:\tlearn: 0.0151689\ttotal: 1.57s\tremaining: 2.84s\n",
      "107:\tlearn: 0.0150424\ttotal: 1.59s\tremaining: 2.82s\n",
      "108:\tlearn: 0.0149158\ttotal: 1.6s\tremaining: 2.8s\n",
      "109:\tlearn: 0.0147585\ttotal: 1.61s\tremaining: 2.79s\n",
      "110:\tlearn: 0.0145331\ttotal: 1.63s\tremaining: 2.77s\n",
      "111:\tlearn: 0.0144200\ttotal: 1.64s\tremaining: 2.75s\n",
      "112:\tlearn: 0.0141785\ttotal: 1.65s\tremaining: 2.74s\n",
      "113:\tlearn: 0.0140597\ttotal: 1.67s\tremaining: 2.72s\n",
      "114:\tlearn: 0.0140597\ttotal: 1.68s\tremaining: 2.71s\n",
      "115:\tlearn: 0.0139902\ttotal: 1.7s\tremaining: 2.69s\n",
      "116:\tlearn: 0.0139902\ttotal: 1.7s\tremaining: 2.65s\n",
      "117:\tlearn: 0.0139901\ttotal: 1.71s\tremaining: 2.64s\n",
      "118:\tlearn: 0.0138006\ttotal: 1.72s\tremaining: 2.62s\n",
      "119:\tlearn: 0.0138006\ttotal: 1.74s\tremaining: 2.6s\n",
      "120:\tlearn: 0.0138002\ttotal: 1.75s\tremaining: 2.59s\n",
      "121:\tlearn: 0.0138002\ttotal: 1.76s\tremaining: 2.57s\n",
      "122:\tlearn: 0.0136549\ttotal: 1.77s\tremaining: 2.56s\n",
      "123:\tlearn: 0.0136189\ttotal: 1.79s\tremaining: 2.54s\n",
      "124:\tlearn: 0.0136189\ttotal: 1.79s\tremaining: 2.51s\n",
      "125:\tlearn: 0.0136188\ttotal: 1.8s\tremaining: 2.49s\n",
      "126:\tlearn: 0.0136187\ttotal: 1.8s\tremaining: 2.46s\n",
      "127:\tlearn: 0.0136165\ttotal: 1.82s\tremaining: 2.44s\n",
      "128:\tlearn: 0.0135785\ttotal: 1.83s\tremaining: 2.43s\n",
      "129:\tlearn: 0.0135080\ttotal: 1.84s\tremaining: 2.41s\n",
      "130:\tlearn: 0.0134049\ttotal: 1.86s\tremaining: 2.4s\n",
      "131:\tlearn: 0.0132775\ttotal: 1.87s\tremaining: 2.38s\n",
      "132:\tlearn: 0.0131970\ttotal: 1.89s\tremaining: 2.37s\n",
      "133:\tlearn: 0.0131540\ttotal: 1.9s\tremaining: 2.35s\n",
      "134:\tlearn: 0.0130999\ttotal: 1.91s\tremaining: 2.34s\n",
      "135:\tlearn: 0.0128992\ttotal: 1.93s\tremaining: 2.32s\n",
      "136:\tlearn: 0.0127683\ttotal: 1.94s\tremaining: 2.31s\n",
      "137:\tlearn: 0.0127682\ttotal: 1.96s\tremaining: 2.29s\n",
      "138:\tlearn: 0.0126287\ttotal: 1.97s\tremaining: 2.28s\n",
      "139:\tlearn: 0.0125263\ttotal: 1.99s\tremaining: 2.27s\n",
      "140:\tlearn: 0.0122723\ttotal: 2s\tremaining: 2.26s\n",
      "141:\tlearn: 0.0122713\ttotal: 2.02s\tremaining: 2.24s\n",
      "142:\tlearn: 0.0121826\ttotal: 2.03s\tremaining: 2.23s\n",
      "143:\tlearn: 0.0121824\ttotal: 2.05s\tremaining: 2.22s\n",
      "144:\tlearn: 0.0121823\ttotal: 2.06s\tremaining: 2.2s\n",
      "145:\tlearn: 0.0121058\ttotal: 2.08s\tremaining: 2.19s\n",
      "146:\tlearn: 0.0119304\ttotal: 2.09s\tremaining: 2.18s\n",
      "147:\tlearn: 0.0117901\ttotal: 2.11s\tremaining: 2.16s\n",
      "148:\tlearn: 0.0117900\ttotal: 2.12s\tremaining: 2.15s\n",
      "149:\tlearn: 0.0116884\ttotal: 2.14s\tremaining: 2.14s\n",
      "150:\tlearn: 0.0115689\ttotal: 2.15s\tremaining: 2.13s\n",
      "151:\tlearn: 0.0114259\ttotal: 2.17s\tremaining: 2.11s\n",
      "152:\tlearn: 0.0113340\ttotal: 2.23s\tremaining: 2.15s\n",
      "153:\tlearn: 0.0111699\ttotal: 2.25s\tremaining: 2.13s\n",
      "154:\tlearn: 0.0111296\ttotal: 2.27s\tremaining: 2.12s\n",
      "155:\tlearn: 0.0109819\ttotal: 2.28s\tremaining: 2.11s\n",
      "156:\tlearn: 0.0107911\ttotal: 2.3s\tremaining: 2.09s\n",
      "157:\tlearn: 0.0106657\ttotal: 2.31s\tremaining: 2.08s\n",
      "158:\tlearn: 0.0106366\ttotal: 2.33s\tremaining: 2.07s\n",
      "159:\tlearn: 0.0105197\ttotal: 2.35s\tremaining: 2.05s\n",
      "160:\tlearn: 0.0104161\ttotal: 2.36s\tremaining: 2.04s\n",
      "161:\tlearn: 0.0104161\ttotal: 2.38s\tremaining: 2.02s\n",
      "162:\tlearn: 0.0102966\ttotal: 2.39s\tremaining: 2.01s\n",
      "163:\tlearn: 0.0102966\ttotal: 2.41s\tremaining: 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164:\tlearn: 0.0102425\ttotal: 2.43s\tremaining: 1.99s\n",
      "165:\tlearn: 0.0101961\ttotal: 2.45s\tremaining: 1.98s\n",
      "166:\tlearn: 0.0100909\ttotal: 2.47s\tremaining: 1.97s\n",
      "167:\tlearn: 0.0100909\ttotal: 2.48s\tremaining: 1.95s\n",
      "168:\tlearn: 0.0100908\ttotal: 2.5s\tremaining: 1.93s\n",
      "169:\tlearn: 0.0100907\ttotal: 2.51s\tremaining: 1.92s\n",
      "170:\tlearn: 0.0100433\ttotal: 2.53s\tremaining: 1.91s\n",
      "171:\tlearn: 0.0100433\ttotal: 2.53s\tremaining: 1.88s\n",
      "172:\tlearn: 0.0100427\ttotal: 2.55s\tremaining: 1.87s\n",
      "173:\tlearn: 0.0100426\ttotal: 2.56s\tremaining: 1.86s\n",
      "174:\tlearn: 0.0100037\ttotal: 2.58s\tremaining: 1.84s\n",
      "175:\tlearn: 0.0100036\ttotal: 2.59s\tremaining: 1.83s\n",
      "176:\tlearn: 0.0100035\ttotal: 2.61s\tremaining: 1.81s\n",
      "177:\tlearn: 0.0099519\ttotal: 2.62s\tremaining: 1.8s\n",
      "178:\tlearn: 0.0098555\ttotal: 2.64s\tremaining: 1.78s\n",
      "179:\tlearn: 0.0097781\ttotal: 2.65s\tremaining: 1.77s\n",
      "180:\tlearn: 0.0097146\ttotal: 2.67s\tremaining: 1.75s\n",
      "181:\tlearn: 0.0096376\ttotal: 2.68s\tremaining: 1.74s\n",
      "182:\tlearn: 0.0095588\ttotal: 2.7s\tremaining: 1.72s\n",
      "183:\tlearn: 0.0094731\ttotal: 2.71s\tremaining: 1.71s\n",
      "184:\tlearn: 0.0094210\ttotal: 2.73s\tremaining: 1.69s\n",
      "185:\tlearn: 0.0093474\ttotal: 2.74s\tremaining: 1.68s\n",
      "186:\tlearn: 0.0092846\ttotal: 2.75s\tremaining: 1.66s\n",
      "187:\tlearn: 0.0091665\ttotal: 2.77s\tremaining: 1.65s\n",
      "188:\tlearn: 0.0090635\ttotal: 2.78s\tremaining: 1.63s\n",
      "189:\tlearn: 0.0089968\ttotal: 2.8s\tremaining: 1.62s\n",
      "190:\tlearn: 0.0088931\ttotal: 2.81s\tremaining: 1.6s\n",
      "191:\tlearn: 0.0088572\ttotal: 2.83s\tremaining: 1.59s\n",
      "192:\tlearn: 0.0087600\ttotal: 2.84s\tremaining: 1.57s\n",
      "193:\tlearn: 0.0086997\ttotal: 2.85s\tremaining: 1.56s\n",
      "194:\tlearn: 0.0086347\ttotal: 2.87s\tremaining: 1.54s\n",
      "195:\tlearn: 0.0085781\ttotal: 2.88s\tremaining: 1.53s\n",
      "196:\tlearn: 0.0085143\ttotal: 2.9s\tremaining: 1.51s\n",
      "197:\tlearn: 0.0084278\ttotal: 2.91s\tremaining: 1.5s\n",
      "198:\tlearn: 0.0083503\ttotal: 2.93s\tremaining: 1.49s\n",
      "199:\tlearn: 0.0082809\ttotal: 2.94s\tremaining: 1.47s\n",
      "200:\tlearn: 0.0082303\ttotal: 2.96s\tremaining: 1.46s\n",
      "201:\tlearn: 0.0081971\ttotal: 2.97s\tremaining: 1.44s\n",
      "202:\tlearn: 0.0081466\ttotal: 2.98s\tremaining: 1.43s\n",
      "203:\tlearn: 0.0080882\ttotal: 3s\tremaining: 1.41s\n",
      "204:\tlearn: 0.0080061\ttotal: 3.01s\tremaining: 1.4s\n",
      "205:\tlearn: 0.0079182\ttotal: 3.03s\tremaining: 1.38s\n",
      "206:\tlearn: 0.0078778\ttotal: 3.04s\tremaining: 1.37s\n",
      "207:\tlearn: 0.0078214\ttotal: 3.06s\tremaining: 1.35s\n",
      "208:\tlearn: 0.0078009\ttotal: 3.07s\tremaining: 1.34s\n",
      "209:\tlearn: 0.0077814\ttotal: 3.09s\tremaining: 1.32s\n",
      "210:\tlearn: 0.0077814\ttotal: 3.1s\tremaining: 1.31s\n",
      "211:\tlearn: 0.0077436\ttotal: 3.12s\tremaining: 1.29s\n",
      "212:\tlearn: 0.0076844\ttotal: 3.13s\tremaining: 1.28s\n",
      "213:\tlearn: 0.0076403\ttotal: 3.14s\tremaining: 1.26s\n",
      "214:\tlearn: 0.0076173\ttotal: 3.16s\tremaining: 1.25s\n",
      "215:\tlearn: 0.0075692\ttotal: 3.17s\tremaining: 1.23s\n",
      "216:\tlearn: 0.0075167\ttotal: 3.19s\tremaining: 1.22s\n",
      "217:\tlearn: 0.0074836\ttotal: 3.2s\tremaining: 1.2s\n",
      "218:\tlearn: 0.0074548\ttotal: 3.22s\tremaining: 1.19s\n",
      "219:\tlearn: 0.0073646\ttotal: 3.23s\tremaining: 1.18s\n",
      "220:\tlearn: 0.0073079\ttotal: 3.25s\tremaining: 1.16s\n",
      "221:\tlearn: 0.0072443\ttotal: 3.26s\tremaining: 1.15s\n",
      "222:\tlearn: 0.0071823\ttotal: 3.27s\tremaining: 1.13s\n",
      "223:\tlearn: 0.0071822\ttotal: 3.29s\tremaining: 1.12s\n",
      "224:\tlearn: 0.0071313\ttotal: 3.3s\tremaining: 1.1s\n",
      "225:\tlearn: 0.0070755\ttotal: 3.32s\tremaining: 1.09s\n",
      "226:\tlearn: 0.0070754\ttotal: 3.33s\tremaining: 1.07s\n",
      "227:\tlearn: 0.0069913\ttotal: 3.35s\tremaining: 1.06s\n",
      "228:\tlearn: 0.0069524\ttotal: 3.36s\tremaining: 1.04s\n",
      "229:\tlearn: 0.0068889\ttotal: 3.38s\tremaining: 1.03s\n",
      "230:\tlearn: 0.0068618\ttotal: 3.4s\tremaining: 1.01s\n",
      "231:\tlearn: 0.0068214\ttotal: 3.41s\tremaining: 1s\n",
      "232:\tlearn: 0.0067545\ttotal: 3.43s\tremaining: 988ms\n",
      "233:\tlearn: 0.0067147\ttotal: 3.45s\tremaining: 974ms\n",
      "234:\tlearn: 0.0066388\ttotal: 3.47s\tremaining: 959ms\n",
      "235:\tlearn: 0.0066254\ttotal: 3.48s\tremaining: 945ms\n",
      "236:\tlearn: 0.0066254\ttotal: 3.5s\tremaining: 931ms\n",
      "237:\tlearn: 0.0065971\ttotal: 3.52s\tremaining: 917ms\n",
      "238:\tlearn: 0.0065966\ttotal: 3.54s\tremaining: 903ms\n",
      "239:\tlearn: 0.0065484\ttotal: 3.55s\tremaining: 888ms\n",
      "240:\tlearn: 0.0065061\ttotal: 3.57s\tremaining: 873ms\n",
      "241:\tlearn: 0.0064685\ttotal: 3.58s\tremaining: 858ms\n",
      "242:\tlearn: 0.0064233\ttotal: 3.6s\tremaining: 844ms\n",
      "243:\tlearn: 0.0064233\ttotal: 3.61s\tremaining: 829ms\n",
      "244:\tlearn: 0.0063722\ttotal: 3.63s\tremaining: 814ms\n",
      "245:\tlearn: 0.0063433\ttotal: 3.64s\tremaining: 799ms\n",
      "246:\tlearn: 0.0063058\ttotal: 3.65s\tremaining: 784ms\n",
      "247:\tlearn: 0.0062503\ttotal: 3.67s\tremaining: 769ms\n",
      "248:\tlearn: 0.0062057\ttotal: 3.68s\tremaining: 754ms\n",
      "249:\tlearn: 0.0061734\ttotal: 3.7s\tremaining: 740ms\n",
      "250:\tlearn: 0.0061413\ttotal: 3.71s\tremaining: 725ms\n",
      "251:\tlearn: 0.0060948\ttotal: 3.73s\tremaining: 710ms\n",
      "252:\tlearn: 0.0060915\ttotal: 3.74s\tremaining: 695ms\n",
      "253:\tlearn: 0.0060914\ttotal: 3.75s\tremaining: 680ms\n",
      "254:\tlearn: 0.0060914\ttotal: 3.77s\tremaining: 665ms\n",
      "255:\tlearn: 0.0060411\ttotal: 3.78s\tremaining: 650ms\n",
      "256:\tlearn: 0.0060009\ttotal: 3.8s\tremaining: 635ms\n",
      "257:\tlearn: 0.0059641\ttotal: 3.81s\tremaining: 620ms\n",
      "258:\tlearn: 0.0059434\ttotal: 3.83s\tremaining: 606ms\n",
      "259:\tlearn: 0.0059305\ttotal: 3.84s\tremaining: 591ms\n",
      "260:\tlearn: 0.0059237\ttotal: 3.85s\tremaining: 576ms\n",
      "261:\tlearn: 0.0058930\ttotal: 3.87s\tremaining: 561ms\n",
      "262:\tlearn: 0.0058716\ttotal: 3.88s\tremaining: 546ms\n",
      "263:\tlearn: 0.0058716\ttotal: 3.9s\tremaining: 531ms\n",
      "264:\tlearn: 0.0058213\ttotal: 3.91s\tremaining: 517ms\n",
      "265:\tlearn: 0.0058212\ttotal: 3.92s\tremaining: 502ms\n",
      "266:\tlearn: 0.0057915\ttotal: 3.94s\tremaining: 487ms\n",
      "267:\tlearn: 0.0057915\ttotal: 3.95s\tremaining: 472ms\n",
      "268:\tlearn: 0.0057915\ttotal: 3.97s\tremaining: 457ms\n",
      "269:\tlearn: 0.0057623\ttotal: 3.98s\tremaining: 443ms\n",
      "270:\tlearn: 0.0057308\ttotal: 4s\tremaining: 428ms\n",
      "271:\tlearn: 0.0057076\ttotal: 4.01s\tremaining: 413ms\n",
      "272:\tlearn: 0.0056711\ttotal: 4.03s\tremaining: 398ms\n",
      "273:\tlearn: 0.0056707\ttotal: 4.04s\tremaining: 383ms\n",
      "274:\tlearn: 0.0056474\ttotal: 4.05s\tremaining: 369ms\n",
      "275:\tlearn: 0.0056160\ttotal: 4.07s\tremaining: 354ms\n",
      "276:\tlearn: 0.0055896\ttotal: 4.08s\tremaining: 339ms\n",
      "277:\tlearn: 0.0055894\ttotal: 4.1s\tremaining: 324ms\n",
      "278:\tlearn: 0.0055894\ttotal: 4.11s\tremaining: 309ms\n",
      "279:\tlearn: 0.0055512\ttotal: 4.13s\tremaining: 295ms\n",
      "280:\tlearn: 0.0055511\ttotal: 4.14s\tremaining: 280ms\n",
      "281:\tlearn: 0.0055260\ttotal: 4.15s\tremaining: 265ms\n",
      "282:\tlearn: 0.0054965\ttotal: 4.17s\tremaining: 250ms\n",
      "283:\tlearn: 0.0054533\ttotal: 4.18s\tremaining: 236ms\n",
      "284:\tlearn: 0.0054408\ttotal: 4.2s\tremaining: 221ms\n",
      "285:\tlearn: 0.0054127\ttotal: 4.21s\tremaining: 206ms\n",
      "286:\tlearn: 0.0053916\ttotal: 4.22s\tremaining: 191ms\n",
      "287:\tlearn: 0.0053583\ttotal: 4.24s\tremaining: 177ms\n",
      "288:\tlearn: 0.0053396\ttotal: 4.25s\tremaining: 162ms\n",
      "289:\tlearn: 0.0052782\ttotal: 4.27s\tremaining: 147ms\n",
      "290:\tlearn: 0.0052556\ttotal: 4.28s\tremaining: 132ms\n",
      "291:\tlearn: 0.0052555\ttotal: 4.3s\tremaining: 118ms\n",
      "292:\tlearn: 0.0052555\ttotal: 4.31s\tremaining: 103ms\n",
      "293:\tlearn: 0.0052555\ttotal: 4.32s\tremaining: 88.2ms\n",
      "294:\tlearn: 0.0052554\ttotal: 4.34s\tremaining: 73.5ms\n",
      "295:\tlearn: 0.0052554\ttotal: 4.35s\tremaining: 58.8ms\n",
      "296:\tlearn: 0.0052377\ttotal: 4.37s\tremaining: 44.2ms\n",
      "297:\tlearn: 0.0052376\ttotal: 4.39s\tremaining: 29.5ms\n",
      "298:\tlearn: 0.0052375\ttotal: 4.41s\tremaining: 14.7ms\n",
      "299:\tlearn: 0.0052374\ttotal: 4.42s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e092c04c0e9044d9bcab5e43f2a43443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5506771031594434, Recall = 0.7672185430463576, Aging Rate = 0.5253311258278146, Precision = 0.7302237630003151, f1 = 0.7482641692233166\n",
      "Epoch 2: Train Loss = 0.3665400597433381, Recall = 0.8516556291390729, Aging Rate = 0.5044701986754967, Precision = 0.8441089596324254, f1 = 0.8478655018955003\n",
      "Epoch 3: Train Loss = 0.27964030821986546, Recall = 0.9013245033112582, Aging Rate = 0.5057947019867549, Precision = 0.8909983633387889, f1 = 0.8961316872427983\n",
      "Epoch 4: Train Loss = 0.22545488230045269, Recall = 0.9195364238410596, Aging Rate = 0.4996688741721854, Precision = 0.9201457919151756, f1 = 0.9198410069559456\n",
      "Epoch 5: Train Loss = 0.18695584251391179, Recall = 0.9354304635761589, Aging Rate = 0.49850993377483444, Precision = 0.9382265028229824, f1 = 0.9368263969490963\n",
      "Test Loss = 0.14708364309854066, Recall = 0.9622516556291391, Aging Rate = 0.5029801324503311, precision = 0.956550362080316\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1358195798039831, Recall = 0.9552980132450332, Aging Rate = 0.4960264900662252, Precision = 0.9629506008010681, f1 = 0.9591090425531915\n",
      "Epoch 7: Train Loss = 0.10981011853510181, Recall = 0.9668874172185431, Aging Rate = 0.4958609271523179, Precision = 0.9749582637729549, f1 = 0.970906068162926\n",
      "Epoch 8: Train Loss = 0.09222713888480963, Recall = 0.9718543046357616, Aging Rate = 0.49619205298013247, Precision = 0.9793126459793127, f1 = 0.9755692205417982\n",
      "Epoch 9: Train Loss = 0.07914705726978005, Recall = 0.9781456953642385, Aging Rate = 0.4966887417218543, Precision = 0.9846666666666667, f1 = 0.9813953488372092\n",
      "Epoch 10: Train Loss = 0.06729589415504443, Recall = 0.9834437086092715, Aging Rate = 0.4990066225165563, Precision = 0.9854014598540146, f1 = 0.9844216108717269\n",
      "Test Loss = 0.057833787626185955, Recall = 0.9847682119205298, Aging Rate = 0.4966887417218543, precision = 0.9913333333333333\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05845369019897174, Recall = 0.9844370860927152, Aging Rate = 0.49850993377483444, Precision = 0.98737960810362, f1 = 0.9859061515503234\n",
      "Epoch 12: Train Loss = 0.05276770768378744, Recall = 0.9867549668874173, Aging Rate = 0.4996688741721854, Precision = 0.9874088800530152, f1 = 0.9870818151705864\n",
      "Epoch 13: Train Loss = 0.04681187566147735, Recall = 0.9877483443708609, Aging Rate = 0.49933774834437084, Precision = 0.9890583554376657, f1 = 0.9884029158383035\n",
      "Epoch 14: Train Loss = 0.04229752336264841, Recall = 0.990728476821192, Aging Rate = 0.5003311258278146, Precision = 0.9900727994705493, f1 = 0.9904005296259516\n",
      "Epoch 15: Train Loss = 0.037368723915408776, Recall = 0.9917218543046358, Aging Rate = 0.4995033112582781, Precision = 0.9927079880676168, f1 = 0.9922146761636574\n",
      "Test Loss = 0.03296599598564454, Recall = 0.9890728476821192, Aging Rate = 0.4955298013245033, precision = 0.9979953224189776\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03429071006445301, Recall = 0.9923841059602649, Aging Rate = 0.5003311258278146, Precision = 0.9917273328921244, f1 = 0.9920556107249254\n",
      "Epoch 17: Train Loss = 0.03230323908690191, Recall = 0.9927152317880795, Aging Rate = 0.4996688741721854, Precision = 0.9933730947647449, f1 = 0.9930440543226234\n",
      "Epoch 18: Train Loss = 0.027989823554969387, Recall = 0.9947019867549669, Aging Rate = 0.5, Precision = 0.9947019867549669, f1 = 0.9947019867549669\n",
      "Epoch 19: Train Loss = 0.02671804955068803, Recall = 0.9937086092715232, Aging Rate = 0.4991721854304636, Precision = 0.9953565505804312, f1 = 0.9945318972659485\n",
      "Epoch 20: Train Loss = 0.023159053295061287, Recall = 0.9950331125827815, Aging Rate = 0.4996688741721854, Precision = 0.9956925115970842, f1 = 0.9953627028817489\n",
      "Test Loss = 0.021155825885173105, Recall = 0.9950331125827815, Aging Rate = 0.49933774834437084, precision = 0.9963527851458885\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.022491286526008552, Recall = 0.9953642384105961, Aging Rate = 0.5, Precision = 0.9953642384105961, f1 = 0.9953642384105961\n",
      "Epoch 22: Train Loss = 0.020749668695575353, Recall = 0.9953642384105961, Aging Rate = 0.4996688741721854, Precision = 0.9960238568588469, f1 = 0.9956939383901954\n",
      "Epoch 23: Train Loss = 0.01872015159009703, Recall = 0.9960264900662251, Aging Rate = 0.4998344370860927, Precision = 0.9963564094070885, f1 = 0.9961914224209306\n",
      "Epoch 24: Train Loss = 0.017381440459547058, Recall = 0.9973509933774835, Aging Rate = 0.5004966887417218, Precision = 0.9963612305656633, f1 = 0.9968558662915771\n",
      "Epoch 25: Train Loss = 0.016574897062008744, Recall = 0.9970198675496689, Aging Rate = 0.5008278145695364, Precision = 0.9953719008264463, f1 = 0.9961952026468156\n",
      "Test Loss = 0.015309120174990781, Recall = 0.9963576158940397, Aging Rate = 0.4990066225165563, precision = 0.9983410749834107\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.015527963232471059, Recall = 0.9970198675496689, Aging Rate = 0.4996688741721854, Precision = 0.9976805831676607, f1 = 0.997350115932428\n",
      "Epoch 27: Train Loss = 0.014160590096444681, Recall = 0.9966887417218543, Aging Rate = 0.49933774834437084, Precision = 0.9980106100795756, f1 = 0.9973492379058979\n",
      "Epoch 28: Train Loss = 0.013954926977267131, Recall = 0.9973509933774835, Aging Rate = 0.5004966887417218, Precision = 0.9963612305656633, f1 = 0.9968558662915771\n",
      "Epoch 29: Train Loss = 0.01210180830259789, Recall = 0.997682119205298, Aging Rate = 0.4998344370860927, Precision = 0.9980125869493209, f1 = 0.9978473257161782\n",
      "Epoch 30: Train Loss = 0.01232650250933497, Recall = 0.997682119205298, Aging Rate = 0.5, Precision = 0.997682119205298, f1 = 0.997682119205298\n",
      "Test Loss = 0.009570949686851526, Recall = 0.9993377483443708, Aging Rate = 0.5006622516556292, precision = 0.998015873015873\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.011505516233317406, Recall = 0.9970198675496689, Aging Rate = 0.4996688741721854, Precision = 0.9976805831676607, f1 = 0.997350115932428\n",
      "Epoch 32: Train Loss = 0.011300103263066009, Recall = 0.9980132450331126, Aging Rate = 0.5001655629139072, Precision = 0.9976828864614367, f1 = 0.9978480384042379\n",
      "Epoch 33: Train Loss = 0.009769284521655137, Recall = 0.9980132450331126, Aging Rate = 0.5, Precision = 0.9980132450331126, f1 = 0.9980132450331126\n",
      "Epoch 34: Train Loss = 0.010210852012877037, Recall = 0.9990066225165563, Aging Rate = 0.5006622516556292, Precision = 0.9976851851851852, f1 = 0.9983454665784248\n",
      "Epoch 35: Train Loss = 0.009081195220227866, Recall = 0.9990066225165563, Aging Rate = 0.5006622516556292, Precision = 0.9976851851851852, f1 = 0.9983454665784248\n",
      "Test Loss = 0.007317909005017865, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, precision = 0.9993379675604105\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.008354311340352805, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 37: Train Loss = 0.007951348084981078, Recall = 0.9986754966887417, Aging Rate = 0.4998344370860927, Precision = 0.9990062934746605, f1 = 0.9988408676933267\n",
      "Epoch 38: Train Loss = 0.007628757757634319, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Epoch 39: Train Loss = 0.008405021653627817, Recall = 0.9986754966887417, Aging Rate = 0.4998344370860927, Precision = 0.9990062934746605, f1 = 0.9988408676933267\n",
      "Epoch 40: Train Loss = 0.007166796006009427, Recall = 0.9993377483443708, Aging Rate = 0.5006622516556292, Precision = 0.998015873015873, f1 = 0.99867637326274\n",
      "Test Loss = 0.010757339433982771, Recall = 0.997682119205298, Aging Rate = 0.498841059602649, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.007163395147326569, Recall = 0.9986754966887417, Aging Rate = 0.4996688741721854, Precision = 0.9993373094764745, f1 = 0.9990062934746605\n",
      "Epoch 42: Train Loss = 0.008054145738568823, Recall = 0.9983443708609272, Aging Rate = 0.5, Precision = 0.9983443708609272, f1 = 0.9983443708609272\n",
      "Epoch 43: Train Loss = 0.007021337567477037, Recall = 0.9983443708609272, Aging Rate = 0.4998344370860927, Precision = 0.998675057966214, f1 = 0.9985096870342773\n",
      "Epoch 44: Train Loss = 0.006481973027859796, Recall = 0.9986754966887417, Aging Rate = 0.5001655629139072, Precision = 0.9983449189010262, f1 = 0.998510180433703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.006680327689965928, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Test Loss = 0.007257293134816731, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, precision = 0.9983460138934833\n",
      "\n",
      "Epoch 46: Train Loss = 0.006946253729521587, Recall = 0.9983443708609272, Aging Rate = 0.4998344370860927, Precision = 0.998675057966214, f1 = 0.9985096870342773\n",
      "Epoch 47: Train Loss = 0.007144708295864676, Recall = 0.9986754966887417, Aging Rate = 0.4998344370860927, Precision = 0.9990062934746605, f1 = 0.9988408676933267\n",
      "Epoch 48: Train Loss = 0.0064965197733227184, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 49: Train Loss = 0.005891553860231741, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 50: Train Loss = 0.0054989924637403416, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Test Loss = 0.0044207318929572964, Recall = 0.9996688741721854, Aging Rate = 0.5, precision = 0.9996688741721854\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.005189026316738878, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 52: Train Loss = 0.006164208751800932, Recall = 0.9986754966887417, Aging Rate = 0.4996688741721854, Precision = 0.9993373094764745, f1 = 0.9990062934746605\n",
      "Epoch 53: Train Loss = 0.005382151726559298, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 54: Train Loss = 0.005802126086181313, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Epoch 55: Train Loss = 0.0049912001576817395, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.0048473645461143446, Recall = 0.9983443708609272, Aging Rate = 0.4991721854304636, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0051498098231062585, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 57: Train Loss = 0.004426345884775287, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 58: Train Loss = 0.004817218619663984, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 59: Train Loss = 0.005304243375504056, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 60: Train Loss = 0.005951749260031575, Recall = 0.9986754966887417, Aging Rate = 0.4998344370860927, Precision = 0.9990062934746605, f1 = 0.9988408676933267\n",
      "Test Loss = 0.004606508014921025, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, precision = 0.9986768111147867\n",
      "\n",
      "Epoch 61: Train Loss = 0.005757774246720881, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Epoch 62: Train Loss = 0.005209925468850708, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 63: Train Loss = 0.005063530038703011, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 64: Train Loss = 0.004764890714412839, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 65: Train Loss = 0.0057723085638073995, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Test Loss = 0.0033419864793425265, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.004877703918902734, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 67: Train Loss = 0.003708885478114845, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 68: Train Loss = 0.004889072248610415, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 69: Train Loss = 0.004008938285170604, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 70: Train Loss = 0.004522863687756193, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Test Loss = 0.004109771302145665, Recall = 1.0, Aging Rate = 0.5004966887417218, precision = 0.9990076083360899\n",
      "\n",
      "Epoch 71: Train Loss = 0.00475998552306401, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 72: Train Loss = 0.004608508457417372, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 73: Train Loss = 0.0043092325194091195, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, Precision = 0.9986768111147867, f1 = 0.9991725963925203\n",
      "Epoch 74: Train Loss = 0.006211613949437607, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 75: Train Loss = 0.004614930180215974, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.004172524173484979, Recall = 0.9990066225165563, Aging Rate = 0.4995033112582781, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0046821327127565615, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 77: Train Loss = 0.0037437950299485335, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 78: Train Loss = 0.003818347914736496, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 79: Train Loss = 0.004282068949044738, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 80: Train Loss = 0.004305280098346191, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.003370829799755273, Recall = 0.9996688741721854, Aging Rate = 0.5, precision = 0.9996688741721854\n",
      "\n",
      "Epoch 81: Train Loss = 0.004370873471697336, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 82: Train Loss = 0.004413054947932528, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 83: Train Loss = 0.004768267811880453, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 84: Train Loss = 0.004576369499732179, Recall = 0.9986754966887417, Aging Rate = 0.4998344370860927, Precision = 0.9990062934746605, f1 = 0.9988408676933267\n",
      "Epoch 85: Train Loss = 0.004764991397321817, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, Precision = 0.9986768111147867, f1 = 0.9991725963925203\n",
      "Test Loss = 0.0036550388532684535, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, precision = 0.9996687644915535\n",
      "\n",
      "Epoch 86: Train Loss = 0.0036850723666169787, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 87: Train Loss = 0.004470342251361156, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 88: Train Loss = 0.003762374252130259, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 89: Train Loss = 0.003956258636652585, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Train Loss = 0.003908423035535098, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0036765941477300513, Recall = 1.0, Aging Rate = 0.5006622516556292, precision = 0.9986772486772487\n",
      "\n",
      "Epoch 91: Train Loss = 0.004122794126561343, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 92: Train Loss = 0.004387472693604843, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 93: Train Loss = 0.004132641307294171, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 94: Train Loss = 0.004402573193139736, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 95: Train Loss = 0.003971891286659567, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.004119403891586113, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.004801887002409689, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 97: Train Loss = 0.00419009066439721, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 98: Train Loss = 0.004428167379498235, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 99: Train Loss = 0.003426310189826944, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0036236112559559627, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.0029742734802775824, Recall = 0.9996688741721854, Aging Rate = 0.5, precision = 0.9996688741721854\n",
      "\n",
      "Epoch 101: Train Loss = 0.004077987962507254, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 102: Train Loss = 0.004532915826529166, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 103: Train Loss = 0.0033144459399758585, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0034547005911875787, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 105: Train Loss = 0.0036477837480283926, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033985182816403245, Recall = 1.0, Aging Rate = 0.5004966887417218, precision = 0.9990076083360899\n",
      "\n",
      "Epoch 106: Train Loss = 0.004028309390275733, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 107: Train Loss = 0.003773841966523339, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 108: Train Loss = 0.005563122558866979, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 109: Train Loss = 0.004646259372736272, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 110: Train Loss = 0.0044166839562837555, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Test Loss = 0.00294750460241582, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.003076492019756296, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.004238522569309698, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 113: Train Loss = 0.004094180395820529, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 114: Train Loss = 0.0031445137524224866, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.004353657787795246, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Test Loss = 0.004993110852908121, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, precision = 0.999337528983107\n",
      "\n",
      "Training Finished at epoch 115.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5429536\ttotal: 15.8ms\tremaining: 4.74s\n",
      "1:\tlearn: 0.4388356\ttotal: 31.8ms\tremaining: 4.74s\n",
      "2:\tlearn: 0.3839549\ttotal: 47.5ms\tremaining: 4.7s\n",
      "3:\tlearn: 0.3474593\ttotal: 63.3ms\tremaining: 4.68s\n",
      "4:\tlearn: 0.3212769\ttotal: 78.9ms\tremaining: 4.65s\n",
      "5:\tlearn: 0.3022896\ttotal: 93.2ms\tremaining: 4.57s\n",
      "6:\tlearn: 0.2770021\ttotal: 108ms\tremaining: 4.51s\n",
      "7:\tlearn: 0.2595566\ttotal: 123ms\tremaining: 4.47s\n",
      "8:\tlearn: 0.2322189\ttotal: 138ms\tremaining: 4.47s\n",
      "9:\tlearn: 0.2127780\ttotal: 154ms\tremaining: 4.47s\n",
      "10:\tlearn: 0.1997181\ttotal: 168ms\tremaining: 4.42s\n",
      "11:\tlearn: 0.1827681\ttotal: 183ms\tremaining: 4.39s\n",
      "12:\tlearn: 0.1707311\ttotal: 198ms\tremaining: 4.36s\n",
      "13:\tlearn: 0.1587449\ttotal: 212ms\tremaining: 4.33s\n",
      "14:\tlearn: 0.1483934\ttotal: 226ms\tremaining: 4.3s\n",
      "15:\tlearn: 0.1417471\ttotal: 241ms\tremaining: 4.28s\n",
      "16:\tlearn: 0.1345529\ttotal: 256ms\tremaining: 4.26s\n",
      "17:\tlearn: 0.1296731\ttotal: 270ms\tremaining: 4.23s\n",
      "18:\tlearn: 0.1257695\ttotal: 285ms\tremaining: 4.21s\n",
      "19:\tlearn: 0.1201537\ttotal: 299ms\tremaining: 4.19s\n",
      "20:\tlearn: 0.1143387\ttotal: 314ms\tremaining: 4.17s\n",
      "21:\tlearn: 0.1097654\ttotal: 328ms\tremaining: 4.14s\n",
      "22:\tlearn: 0.1048874\ttotal: 342ms\tremaining: 4.12s\n",
      "23:\tlearn: 0.1009458\ttotal: 357ms\tremaining: 4.11s\n",
      "24:\tlearn: 0.0962372\ttotal: 372ms\tremaining: 4.09s\n",
      "25:\tlearn: 0.0928837\ttotal: 386ms\tremaining: 4.07s\n",
      "26:\tlearn: 0.0879541\ttotal: 402ms\tremaining: 4.06s\n",
      "27:\tlearn: 0.0849453\ttotal: 416ms\tremaining: 4.04s\n",
      "28:\tlearn: 0.0809656\ttotal: 430ms\tremaining: 4.02s\n",
      "29:\tlearn: 0.0786231\ttotal: 445ms\tremaining: 4.01s\n",
      "30:\tlearn: 0.0747060\ttotal: 460ms\tremaining: 3.99s\n",
      "31:\tlearn: 0.0718710\ttotal: 474ms\tremaining: 3.97s\n",
      "32:\tlearn: 0.0697313\ttotal: 489ms\tremaining: 3.95s\n",
      "33:\tlearn: 0.0672125\ttotal: 504ms\tremaining: 3.94s\n",
      "34:\tlearn: 0.0671663\ttotal: 506ms\tremaining: 3.83s\n",
      "35:\tlearn: 0.0648850\ttotal: 521ms\tremaining: 3.82s\n",
      "36:\tlearn: 0.0625590\ttotal: 536ms\tremaining: 3.81s\n",
      "37:\tlearn: 0.0625190\ttotal: 539ms\tremaining: 3.72s\n",
      "38:\tlearn: 0.0611896\ttotal: 553ms\tremaining: 3.7s\n",
      "39:\tlearn: 0.0611606\ttotal: 556ms\tremaining: 3.61s\n",
      "40:\tlearn: 0.0599971\ttotal: 570ms\tremaining: 3.6s\n",
      "41:\tlearn: 0.0570610\ttotal: 584ms\tremaining: 3.59s\n",
      "42:\tlearn: 0.0554877\ttotal: 599ms\tremaining: 3.58s\n",
      "43:\tlearn: 0.0533014\ttotal: 614ms\tremaining: 3.57s\n",
      "44:\tlearn: 0.0518730\ttotal: 628ms\tremaining: 3.56s\n",
      "45:\tlearn: 0.0518541\ttotal: 631ms\tremaining: 3.48s\n",
      "46:\tlearn: 0.0507639\ttotal: 646ms\tremaining: 3.48s\n",
      "47:\tlearn: 0.0507490\ttotal: 649ms\tremaining: 3.41s\n",
      "48:\tlearn: 0.0498318\ttotal: 669ms\tremaining: 3.43s\n",
      "49:\tlearn: 0.0486019\ttotal: 687ms\tremaining: 3.44s\n",
      "50:\tlearn: 0.0474869\ttotal: 704ms\tremaining: 3.44s\n",
      "51:\tlearn: 0.0457815\ttotal: 723ms\tremaining: 3.45s\n",
      "52:\tlearn: 0.0457739\ttotal: 726ms\tremaining: 3.38s\n",
      "53:\tlearn: 0.0448515\ttotal: 745ms\tremaining: 3.39s\n",
      "54:\tlearn: 0.0435274\ttotal: 761ms\tremaining: 3.39s\n",
      "55:\tlearn: 0.0418848\ttotal: 776ms\tremaining: 3.38s\n",
      "56:\tlearn: 0.0408582\ttotal: 790ms\tremaining: 3.37s\n",
      "57:\tlearn: 0.0397414\ttotal: 805ms\tremaining: 3.36s\n",
      "58:\tlearn: 0.0385946\ttotal: 819ms\tremaining: 3.35s\n",
      "59:\tlearn: 0.0379091\ttotal: 834ms\tremaining: 3.33s\n",
      "60:\tlearn: 0.0367084\ttotal: 849ms\tremaining: 3.32s\n",
      "61:\tlearn: 0.0363330\ttotal: 862ms\tremaining: 3.31s\n",
      "62:\tlearn: 0.0356647\ttotal: 877ms\tremaining: 3.3s\n",
      "63:\tlearn: 0.0348755\ttotal: 891ms\tremaining: 3.29s\n",
      "64:\tlearn: 0.0337212\ttotal: 906ms\tremaining: 3.27s\n",
      "65:\tlearn: 0.0337171\ttotal: 909ms\tremaining: 3.22s\n",
      "66:\tlearn: 0.0331439\ttotal: 923ms\tremaining: 3.21s\n",
      "67:\tlearn: 0.0323021\ttotal: 938ms\tremaining: 3.2s\n",
      "68:\tlearn: 0.0317017\ttotal: 952ms\tremaining: 3.19s\n",
      "69:\tlearn: 0.0311495\ttotal: 967ms\tremaining: 3.18s\n",
      "70:\tlearn: 0.0310196\ttotal: 981ms\tremaining: 3.17s\n",
      "71:\tlearn: 0.0306510\ttotal: 996ms\tremaining: 3.15s\n",
      "72:\tlearn: 0.0301592\ttotal: 1.01s\tremaining: 3.14s\n",
      "73:\tlearn: 0.0293523\ttotal: 1.02s\tremaining: 3.13s\n",
      "74:\tlearn: 0.0288412\ttotal: 1.04s\tremaining: 3.12s\n",
      "75:\tlearn: 0.0285514\ttotal: 1.05s\tremaining: 3.11s\n",
      "76:\tlearn: 0.0279903\ttotal: 1.07s\tremaining: 3.1s\n",
      "77:\tlearn: 0.0274781\ttotal: 1.08s\tremaining: 3.08s\n",
      "78:\tlearn: 0.0270258\ttotal: 1.1s\tremaining: 3.07s\n",
      "79:\tlearn: 0.0266007\ttotal: 1.11s\tremaining: 3.06s\n",
      "80:\tlearn: 0.0261351\ttotal: 1.13s\tremaining: 3.04s\n",
      "81:\tlearn: 0.0257041\ttotal: 1.14s\tremaining: 3.03s\n",
      "82:\tlearn: 0.0251901\ttotal: 1.15s\tremaining: 3.02s\n",
      "83:\tlearn: 0.0244348\ttotal: 1.17s\tremaining: 3.01s\n",
      "84:\tlearn: 0.0239892\ttotal: 1.18s\tremaining: 2.99s\n",
      "85:\tlearn: 0.0235322\ttotal: 1.2s\tremaining: 2.98s\n",
      "86:\tlearn: 0.0235176\ttotal: 1.21s\tremaining: 2.97s\n",
      "87:\tlearn: 0.0232942\ttotal: 1.23s\tremaining: 2.95s\n",
      "88:\tlearn: 0.0231658\ttotal: 1.24s\tremaining: 2.94s\n",
      "89:\tlearn: 0.0227898\ttotal: 1.25s\tremaining: 2.93s\n",
      "90:\tlearn: 0.0223289\ttotal: 1.27s\tremaining: 2.91s\n",
      "91:\tlearn: 0.0220199\ttotal: 1.28s\tremaining: 2.9s\n",
      "92:\tlearn: 0.0219138\ttotal: 1.29s\tremaining: 2.88s\n",
      "93:\tlearn: 0.0216105\ttotal: 1.31s\tremaining: 2.86s\n",
      "94:\tlearn: 0.0213497\ttotal: 1.32s\tremaining: 2.85s\n",
      "95:\tlearn: 0.0210519\ttotal: 1.33s\tremaining: 2.83s\n",
      "96:\tlearn: 0.0207072\ttotal: 1.35s\tremaining: 2.82s\n",
      "97:\tlearn: 0.0205881\ttotal: 1.36s\tremaining: 2.8s\n",
      "98:\tlearn: 0.0201532\ttotal: 1.37s\tremaining: 2.79s\n",
      "99:\tlearn: 0.0198261\ttotal: 1.39s\tremaining: 2.77s\n",
      "100:\tlearn: 0.0196490\ttotal: 1.4s\tremaining: 2.76s\n",
      "101:\tlearn: 0.0193773\ttotal: 1.41s\tremaining: 2.74s\n",
      "102:\tlearn: 0.0192299\ttotal: 1.43s\tremaining: 2.73s\n",
      "103:\tlearn: 0.0189641\ttotal: 1.44s\tremaining: 2.71s\n",
      "104:\tlearn: 0.0187147\ttotal: 1.45s\tremaining: 2.7s\n",
      "105:\tlearn: 0.0186461\ttotal: 1.47s\tremaining: 2.68s\n",
      "106:\tlearn: 0.0186260\ttotal: 1.47s\tremaining: 2.66s\n",
      "107:\tlearn: 0.0185466\ttotal: 1.49s\tremaining: 2.64s\n",
      "108:\tlearn: 0.0184939\ttotal: 1.5s\tremaining: 2.63s\n",
      "109:\tlearn: 0.0184200\ttotal: 1.51s\tremaining: 2.61s\n",
      "110:\tlearn: 0.0182152\ttotal: 1.53s\tremaining: 2.6s\n",
      "111:\tlearn: 0.0179888\ttotal: 1.54s\tremaining: 2.59s\n",
      "112:\tlearn: 0.0178799\ttotal: 1.56s\tremaining: 2.58s\n",
      "113:\tlearn: 0.0177024\ttotal: 1.57s\tremaining: 2.57s\n",
      "114:\tlearn: 0.0176693\ttotal: 1.59s\tremaining: 2.56s\n",
      "115:\tlearn: 0.0174617\ttotal: 1.6s\tremaining: 2.55s\n",
      "116:\tlearn: 0.0173054\ttotal: 1.62s\tremaining: 2.54s\n",
      "117:\tlearn: 0.0170541\ttotal: 1.64s\tremaining: 2.52s\n",
      "118:\tlearn: 0.0168921\ttotal: 1.65s\tremaining: 2.51s\n",
      "119:\tlearn: 0.0166184\ttotal: 1.67s\tremaining: 2.5s\n",
      "120:\tlearn: 0.0164984\ttotal: 1.68s\tremaining: 2.49s\n",
      "121:\tlearn: 0.0164205\ttotal: 1.7s\tremaining: 2.48s\n",
      "122:\tlearn: 0.0162377\ttotal: 1.71s\tremaining: 2.46s\n",
      "123:\tlearn: 0.0161826\ttotal: 1.73s\tremaining: 2.45s\n",
      "124:\tlearn: 0.0161514\ttotal: 1.75s\tremaining: 2.44s\n",
      "125:\tlearn: 0.0161015\ttotal: 1.76s\tremaining: 2.43s\n",
      "126:\tlearn: 0.0158854\ttotal: 1.78s\tremaining: 2.42s\n",
      "127:\tlearn: 0.0156585\ttotal: 1.79s\tremaining: 2.41s\n",
      "128:\tlearn: 0.0154122\ttotal: 1.85s\tremaining: 2.45s\n",
      "129:\tlearn: 0.0152041\ttotal: 1.86s\tremaining: 2.44s\n",
      "130:\tlearn: 0.0150933\ttotal: 1.88s\tremaining: 2.43s\n",
      "131:\tlearn: 0.0148224\ttotal: 1.9s\tremaining: 2.41s\n",
      "132:\tlearn: 0.0148224\ttotal: 1.91s\tremaining: 2.4s\n",
      "133:\tlearn: 0.0146089\ttotal: 1.93s\tremaining: 2.39s\n",
      "134:\tlearn: 0.0145612\ttotal: 1.95s\tremaining: 2.38s\n",
      "135:\tlearn: 0.0145562\ttotal: 1.97s\tremaining: 2.38s\n",
      "136:\tlearn: 0.0145086\ttotal: 2s\tremaining: 2.38s\n",
      "137:\tlearn: 0.0144999\ttotal: 2.02s\tremaining: 2.37s\n",
      "138:\tlearn: 0.0143457\ttotal: 2.04s\tremaining: 2.36s\n",
      "139:\tlearn: 0.0141687\ttotal: 2.06s\tremaining: 2.35s\n",
      "140:\tlearn: 0.0140374\ttotal: 2.08s\tremaining: 2.34s\n",
      "141:\tlearn: 0.0139183\ttotal: 2.1s\tremaining: 2.33s\n",
      "142:\tlearn: 0.0137714\ttotal: 2.11s\tremaining: 2.32s\n",
      "143:\tlearn: 0.0135503\ttotal: 2.13s\tremaining: 2.31s\n",
      "144:\tlearn: 0.0133743\ttotal: 2.15s\tremaining: 2.29s\n",
      "145:\tlearn: 0.0132408\ttotal: 2.16s\tremaining: 2.28s\n",
      "146:\tlearn: 0.0131593\ttotal: 2.18s\tremaining: 2.26s\n",
      "147:\tlearn: 0.0130635\ttotal: 2.19s\tremaining: 2.25s\n",
      "148:\tlearn: 0.0129136\ttotal: 2.21s\tremaining: 2.23s\n",
      "149:\tlearn: 0.0128377\ttotal: 2.22s\tremaining: 2.22s\n",
      "150:\tlearn: 0.0127184\ttotal: 2.23s\tremaining: 2.21s\n",
      "151:\tlearn: 0.0126049\ttotal: 2.25s\tremaining: 2.19s\n",
      "152:\tlearn: 0.0126048\ttotal: 2.27s\tremaining: 2.18s\n",
      "153:\tlearn: 0.0126048\ttotal: 2.28s\tremaining: 2.16s\n",
      "154:\tlearn: 0.0126047\ttotal: 2.29s\tremaining: 2.15s\n",
      "155:\tlearn: 0.0126046\ttotal: 2.31s\tremaining: 2.13s\n",
      "156:\tlearn: 0.0126045\ttotal: 2.32s\tremaining: 2.11s\n",
      "157:\tlearn: 0.0124967\ttotal: 2.33s\tremaining: 2.1s\n",
      "158:\tlearn: 0.0123857\ttotal: 2.35s\tremaining: 2.08s\n",
      "159:\tlearn: 0.0122135\ttotal: 2.36s\tremaining: 2.07s\n",
      "160:\tlearn: 0.0122068\ttotal: 2.38s\tremaining: 2.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.0120904\ttotal: 2.39s\tremaining: 2.04s\n",
      "162:\tlearn: 0.0119668\ttotal: 2.41s\tremaining: 2.02s\n",
      "163:\tlearn: 0.0117953\ttotal: 2.42s\tremaining: 2.01s\n",
      "164:\tlearn: 0.0116662\ttotal: 2.44s\tremaining: 1.99s\n",
      "165:\tlearn: 0.0116662\ttotal: 2.45s\tremaining: 1.98s\n",
      "166:\tlearn: 0.0115733\ttotal: 2.46s\tremaining: 1.96s\n",
      "167:\tlearn: 0.0114990\ttotal: 2.48s\tremaining: 1.95s\n",
      "168:\tlearn: 0.0114546\ttotal: 2.49s\tremaining: 1.93s\n",
      "169:\tlearn: 0.0114545\ttotal: 2.5s\tremaining: 1.91s\n",
      "170:\tlearn: 0.0113645\ttotal: 2.51s\tremaining: 1.89s\n",
      "171:\tlearn: 0.0112717\ttotal: 2.53s\tremaining: 1.88s\n",
      "172:\tlearn: 0.0111258\ttotal: 2.54s\tremaining: 1.86s\n",
      "173:\tlearn: 0.0110722\ttotal: 2.56s\tremaining: 1.85s\n",
      "174:\tlearn: 0.0110329\ttotal: 2.57s\tremaining: 1.83s\n",
      "175:\tlearn: 0.0109535\ttotal: 2.58s\tremaining: 1.82s\n",
      "176:\tlearn: 0.0108404\ttotal: 2.6s\tremaining: 1.8s\n",
      "177:\tlearn: 0.0107245\ttotal: 2.61s\tremaining: 1.79s\n",
      "178:\tlearn: 0.0106256\ttotal: 2.63s\tremaining: 1.78s\n",
      "179:\tlearn: 0.0105311\ttotal: 2.65s\tremaining: 1.77s\n",
      "180:\tlearn: 0.0105311\ttotal: 2.67s\tremaining: 1.75s\n",
      "181:\tlearn: 0.0104748\ttotal: 2.69s\tremaining: 1.74s\n",
      "182:\tlearn: 0.0104748\ttotal: 2.71s\tremaining: 1.73s\n",
      "183:\tlearn: 0.0104748\ttotal: 2.72s\tremaining: 1.71s\n",
      "184:\tlearn: 0.0104222\ttotal: 2.73s\tremaining: 1.7s\n",
      "185:\tlearn: 0.0103496\ttotal: 2.75s\tremaining: 1.69s\n",
      "186:\tlearn: 0.0103496\ttotal: 2.76s\tremaining: 1.67s\n",
      "187:\tlearn: 0.0103496\ttotal: 2.78s\tremaining: 1.65s\n",
      "188:\tlearn: 0.0102497\ttotal: 2.79s\tremaining: 1.64s\n",
      "189:\tlearn: 0.0102497\ttotal: 2.8s\tremaining: 1.62s\n",
      "190:\tlearn: 0.0101264\ttotal: 2.82s\tremaining: 1.61s\n",
      "191:\tlearn: 0.0101263\ttotal: 2.83s\tremaining: 1.59s\n",
      "192:\tlearn: 0.0100838\ttotal: 2.85s\tremaining: 1.58s\n",
      "193:\tlearn: 0.0100838\ttotal: 2.86s\tremaining: 1.56s\n",
      "194:\tlearn: 0.0100164\ttotal: 2.88s\tremaining: 1.55s\n",
      "195:\tlearn: 0.0099827\ttotal: 2.89s\tremaining: 1.53s\n",
      "196:\tlearn: 0.0099308\ttotal: 2.9s\tremaining: 1.52s\n",
      "197:\tlearn: 0.0098530\ttotal: 2.92s\tremaining: 1.5s\n",
      "198:\tlearn: 0.0098530\ttotal: 2.93s\tremaining: 1.49s\n",
      "199:\tlearn: 0.0098529\ttotal: 2.95s\tremaining: 1.47s\n",
      "200:\tlearn: 0.0098529\ttotal: 2.96s\tremaining: 1.46s\n",
      "201:\tlearn: 0.0098528\ttotal: 2.97s\tremaining: 1.44s\n",
      "202:\tlearn: 0.0098525\ttotal: 2.99s\tremaining: 1.43s\n",
      "203:\tlearn: 0.0098524\ttotal: 3s\tremaining: 1.41s\n",
      "204:\tlearn: 0.0097789\ttotal: 3.02s\tremaining: 1.4s\n",
      "205:\tlearn: 0.0096818\ttotal: 3.03s\tremaining: 1.38s\n",
      "206:\tlearn: 0.0096214\ttotal: 3.04s\tremaining: 1.37s\n",
      "207:\tlearn: 0.0095909\ttotal: 3.06s\tremaining: 1.35s\n",
      "208:\tlearn: 0.0095237\ttotal: 3.07s\tremaining: 1.34s\n",
      "209:\tlearn: 0.0094601\ttotal: 3.09s\tremaining: 1.32s\n",
      "210:\tlearn: 0.0094114\ttotal: 3.1s\tremaining: 1.31s\n",
      "211:\tlearn: 0.0094113\ttotal: 3.12s\tremaining: 1.29s\n",
      "212:\tlearn: 0.0094113\ttotal: 3.13s\tremaining: 1.28s\n",
      "213:\tlearn: 0.0093254\ttotal: 3.15s\tremaining: 1.26s\n",
      "214:\tlearn: 0.0092639\ttotal: 3.16s\tremaining: 1.25s\n",
      "215:\tlearn: 0.0091942\ttotal: 3.17s\tremaining: 1.23s\n",
      "216:\tlearn: 0.0090601\ttotal: 3.19s\tremaining: 1.22s\n",
      "217:\tlearn: 0.0089866\ttotal: 3.2s\tremaining: 1.2s\n",
      "218:\tlearn: 0.0089098\ttotal: 3.22s\tremaining: 1.19s\n",
      "219:\tlearn: 0.0088057\ttotal: 3.23s\tremaining: 1.18s\n",
      "220:\tlearn: 0.0087847\ttotal: 3.25s\tremaining: 1.16s\n",
      "221:\tlearn: 0.0087360\ttotal: 3.26s\tremaining: 1.15s\n",
      "222:\tlearn: 0.0086960\ttotal: 3.27s\tremaining: 1.13s\n",
      "223:\tlearn: 0.0086456\ttotal: 3.29s\tremaining: 1.12s\n",
      "224:\tlearn: 0.0085955\ttotal: 3.3s\tremaining: 1.1s\n",
      "225:\tlearn: 0.0085635\ttotal: 3.32s\tremaining: 1.09s\n",
      "226:\tlearn: 0.0085258\ttotal: 3.33s\tremaining: 1.07s\n",
      "227:\tlearn: 0.0084872\ttotal: 3.35s\tremaining: 1.06s\n",
      "228:\tlearn: 0.0084872\ttotal: 3.36s\tremaining: 1.04s\n",
      "229:\tlearn: 0.0084688\ttotal: 3.38s\tremaining: 1.03s\n",
      "230:\tlearn: 0.0084344\ttotal: 3.39s\tremaining: 1.01s\n",
      "231:\tlearn: 0.0084344\ttotal: 3.4s\tremaining: 998ms\n",
      "232:\tlearn: 0.0084344\ttotal: 3.42s\tremaining: 983ms\n",
      "233:\tlearn: 0.0084001\ttotal: 3.43s\tremaining: 968ms\n",
      "234:\tlearn: 0.0083577\ttotal: 3.44s\tremaining: 951ms\n",
      "235:\tlearn: 0.0082877\ttotal: 3.45s\tremaining: 936ms\n",
      "236:\tlearn: 0.0082432\ttotal: 3.47s\tremaining: 921ms\n",
      "237:\tlearn: 0.0082432\ttotal: 3.48s\tremaining: 907ms\n",
      "238:\tlearn: 0.0082023\ttotal: 3.49s\tremaining: 892ms\n",
      "239:\tlearn: 0.0081682\ttotal: 3.51s\tremaining: 877ms\n",
      "240:\tlearn: 0.0081681\ttotal: 3.52s\tremaining: 862ms\n",
      "241:\tlearn: 0.0081681\ttotal: 3.54s\tremaining: 848ms\n",
      "242:\tlearn: 0.0081681\ttotal: 3.55s\tremaining: 833ms\n",
      "243:\tlearn: 0.0081681\ttotal: 3.56s\tremaining: 818ms\n",
      "244:\tlearn: 0.0081376\ttotal: 3.58s\tremaining: 803ms\n",
      "245:\tlearn: 0.0080819\ttotal: 3.59s\tremaining: 789ms\n",
      "246:\tlearn: 0.0080570\ttotal: 3.61s\tremaining: 774ms\n",
      "247:\tlearn: 0.0080114\ttotal: 3.63s\tremaining: 760ms\n",
      "248:\tlearn: 0.0079306\ttotal: 3.64s\tremaining: 746ms\n",
      "249:\tlearn: 0.0079297\ttotal: 3.66s\tremaining: 732ms\n",
      "250:\tlearn: 0.0078840\ttotal: 3.68s\tremaining: 719ms\n",
      "251:\tlearn: 0.0078259\ttotal: 3.7s\tremaining: 705ms\n",
      "252:\tlearn: 0.0077701\ttotal: 3.72s\tremaining: 691ms\n",
      "253:\tlearn: 0.0077393\ttotal: 3.73s\tremaining: 677ms\n",
      "254:\tlearn: 0.0077393\ttotal: 3.75s\tremaining: 662ms\n",
      "255:\tlearn: 0.0077113\ttotal: 3.77s\tremaining: 648ms\n",
      "256:\tlearn: 0.0076876\ttotal: 3.79s\tremaining: 634ms\n",
      "257:\tlearn: 0.0076505\ttotal: 3.8s\tremaining: 619ms\n",
      "258:\tlearn: 0.0076179\ttotal: 3.82s\tremaining: 605ms\n",
      "259:\tlearn: 0.0075963\ttotal: 3.83s\tremaining: 590ms\n",
      "260:\tlearn: 0.0075556\ttotal: 3.85s\tremaining: 575ms\n",
      "261:\tlearn: 0.0075421\ttotal: 3.86s\tremaining: 560ms\n",
      "262:\tlearn: 0.0075417\ttotal: 3.88s\tremaining: 545ms\n",
      "263:\tlearn: 0.0074616\ttotal: 3.89s\tremaining: 531ms\n",
      "264:\tlearn: 0.0074020\ttotal: 3.91s\tremaining: 516ms\n",
      "265:\tlearn: 0.0073325\ttotal: 3.92s\tremaining: 501ms\n",
      "266:\tlearn: 0.0072688\ttotal: 3.94s\tremaining: 487ms\n",
      "267:\tlearn: 0.0072281\ttotal: 3.95s\tremaining: 472ms\n",
      "268:\tlearn: 0.0072230\ttotal: 3.96s\tremaining: 457ms\n",
      "269:\tlearn: 0.0071777\ttotal: 3.98s\tremaining: 442ms\n",
      "270:\tlearn: 0.0071380\ttotal: 3.99s\tremaining: 427ms\n",
      "271:\tlearn: 0.0070928\ttotal: 4.01s\tremaining: 413ms\n",
      "272:\tlearn: 0.0070500\ttotal: 4.02s\tremaining: 398ms\n",
      "273:\tlearn: 0.0070071\ttotal: 4.04s\tremaining: 383ms\n",
      "274:\tlearn: 0.0069992\ttotal: 4.05s\tremaining: 368ms\n",
      "275:\tlearn: 0.0069311\ttotal: 4.07s\tremaining: 354ms\n",
      "276:\tlearn: 0.0068699\ttotal: 4.08s\tremaining: 339ms\n",
      "277:\tlearn: 0.0068402\ttotal: 4.09s\tremaining: 324ms\n",
      "278:\tlearn: 0.0067929\ttotal: 4.11s\tremaining: 309ms\n",
      "279:\tlearn: 0.0067643\ttotal: 4.12s\tremaining: 295ms\n",
      "280:\tlearn: 0.0067128\ttotal: 4.14s\tremaining: 280ms\n",
      "281:\tlearn: 0.0066782\ttotal: 4.15s\tremaining: 265ms\n",
      "282:\tlearn: 0.0065807\ttotal: 4.17s\tremaining: 250ms\n",
      "283:\tlearn: 0.0065807\ttotal: 4.18s\tremaining: 236ms\n",
      "284:\tlearn: 0.0065519\ttotal: 4.2s\tremaining: 221ms\n",
      "285:\tlearn: 0.0065207\ttotal: 4.21s\tremaining: 206ms\n",
      "286:\tlearn: 0.0064655\ttotal: 4.22s\tremaining: 191ms\n",
      "287:\tlearn: 0.0064354\ttotal: 4.24s\tremaining: 177ms\n",
      "288:\tlearn: 0.0063908\ttotal: 4.25s\tremaining: 162ms\n",
      "289:\tlearn: 0.0063356\ttotal: 4.27s\tremaining: 147ms\n",
      "290:\tlearn: 0.0062924\ttotal: 4.28s\tremaining: 132ms\n",
      "291:\tlearn: 0.0062517\ttotal: 4.29s\tremaining: 118ms\n",
      "292:\tlearn: 0.0062516\ttotal: 4.31s\tremaining: 103ms\n",
      "293:\tlearn: 0.0062105\ttotal: 4.32s\tremaining: 88.2ms\n",
      "294:\tlearn: 0.0061608\ttotal: 4.33s\tremaining: 73.5ms\n",
      "295:\tlearn: 0.0061608\ttotal: 4.35s\tremaining: 58.8ms\n",
      "296:\tlearn: 0.0061608\ttotal: 4.36s\tremaining: 44ms\n",
      "297:\tlearn: 0.0061608\ttotal: 4.37s\tremaining: 29.4ms\n",
      "298:\tlearn: 0.0061607\ttotal: 4.39s\tremaining: 14.7ms\n",
      "299:\tlearn: 0.0061607\ttotal: 4.4s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb644cb6ec0436d8938a9ba96265a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5450847185210677, Recall = 0.8301324503311258, Aging Rate = 0.5778145695364238, Precision = 0.7183381088825215, f1 = 0.770199692780338\n",
      "Epoch 2: Train Loss = 0.36527955466943074, Recall = 0.8559602649006622, Aging Rate = 0.503476821192053, Precision = 0.8500493258796449, f1 = 0.852994555353902\n",
      "Epoch 3: Train Loss = 0.29014903731298763, Recall = 0.8927152317880794, Aging Rate = 0.506953642384106, Precision = 0.8804702808621816, f1 = 0.8865504768168365\n",
      "Epoch 4: Train Loss = 0.23595049872698373, Recall = 0.9182119205298013, Aging Rate = 0.5031456953642384, Precision = 0.9124712076340902, f1 = 0.9153325631292292\n",
      "Epoch 5: Train Loss = 0.19977077863469028, Recall = 0.9317880794701987, Aging Rate = 0.5008278145695364, Precision = 0.9302479338842975, f1 = 0.9310173697270472\n",
      "Test Loss = 0.15669992335190047, Recall = 0.9529801324503311, Aging Rate = 0.5018211920529801, precision = 0.9495216100296932\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14725575456950837, Recall = 0.952317880794702, Aging Rate = 0.5001655629139072, Precision = 0.9520026481297583, f1 = 0.9521602383711306\n",
      "Epoch 7: Train Loss = 0.12192778744247576, Recall = 0.9605960264900663, Aging Rate = 0.495364238410596, Precision = 0.9695855614973262, f1 = 0.9650698602794412\n",
      "Epoch 8: Train Loss = 0.10096550743706179, Recall = 0.9708609271523179, Aging Rate = 0.49751655629139074, Precision = 0.9757071547420965, f1 = 0.9732780082987551\n",
      "Epoch 9: Train Loss = 0.08628236371160343, Recall = 0.9745033112582782, Aging Rate = 0.49801324503311256, Precision = 0.9783909574468085, f1 = 0.9764432647644326\n",
      "Epoch 10: Train Loss = 0.07487091442804462, Recall = 0.9801324503311258, Aging Rate = 0.4996688741721854, Precision = 0.9807819748177601, f1 = 0.9804571050016562\n",
      "Test Loss = 0.06398162200356161, Recall = 0.9827814569536424, Aging Rate = 0.49801324503311256, precision = 0.9867021276595744\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06451708367526136, Recall = 0.983112582781457, Aging Rate = 0.49834437086092714, Precision = 0.9863787375415283, f1 = 0.9847429519071311\n",
      "Epoch 12: Train Loss = 0.05809497833251953, Recall = 0.9827814569536424, Aging Rate = 0.4990066225165563, Precision = 0.9847378898473789, f1 = 0.9837587006960556\n",
      "Epoch 13: Train Loss = 0.050370538010206446, Recall = 0.9884105960264901, Aging Rate = 0.5001655629139072, Precision = 0.9880834160873883, f1 = 0.9882469789769905\n",
      "Epoch 14: Train Loss = 0.04425588504886193, Recall = 0.9910596026490066, Aging Rate = 0.5013245033112583, Precision = 0.9884412153236459, f1 = 0.9897486772486772\n",
      "Epoch 15: Train Loss = 0.04083166214705303, Recall = 0.9903973509933774, Aging Rate = 0.4996688741721854, Precision = 0.9910536779324056, f1 = 0.9907254057634979\n",
      "Test Loss = 0.036499122942224245, Recall = 0.990728476821192, Aging Rate = 0.49850993377483444, precision = 0.9936898040518101\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03632020998918852, Recall = 0.9923841059602649, Aging Rate = 0.5, Precision = 0.9923841059602649, f1 = 0.9923841059602649\n",
      "Epoch 17: Train Loss = 0.03367665865218008, Recall = 0.9920529801324504, Aging Rate = 0.5003311258278146, Precision = 0.9913964262078094, f1 = 0.9917245945051307\n",
      "Epoch 18: Train Loss = 0.030432048627477608, Recall = 0.9927152317880795, Aging Rate = 0.5, Precision = 0.9927152317880795, f1 = 0.9927152317880795\n",
      "Epoch 19: Train Loss = 0.02668161503970623, Recall = 0.9947019867549669, Aging Rate = 0.5004966887417218, Precision = 0.9937148527952365, f1 = 0.9942081747476419\n",
      "Epoch 20: Train Loss = 0.02486920989926485, Recall = 0.9947019867549669, Aging Rate = 0.4991721854304636, Precision = 0.996351575456053, f1 = 0.9955260977630489\n",
      "Test Loss = 0.021258849925700797, Recall = 0.9980132450331126, Aging Rate = 0.501158940397351, precision = 0.9957053187974892\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.022255166411547865, Recall = 0.9963576158940397, Aging Rate = 0.4998344370860927, Precision = 0.996687644915535, f1 = 0.9965226030799801\n",
      "Epoch 22: Train Loss = 0.020832271645746088, Recall = 0.9963576158940397, Aging Rate = 0.49933774834437084, Precision = 0.9976790450928382, f1 = 0.9970178926441351\n",
      "Epoch 23: Train Loss = 0.019924082975859278, Recall = 0.9973509933774835, Aging Rate = 0.5004966887417218, Precision = 0.9963612305656633, f1 = 0.9968558662915771\n",
      "Epoch 24: Train Loss = 0.01883696720485162, Recall = 0.9963576158940397, Aging Rate = 0.49933774834437084, Precision = 0.9976790450928382, f1 = 0.9970178926441351\n",
      "Epoch 25: Train Loss = 0.016629095684539602, Recall = 0.9973509933774835, Aging Rate = 0.4996688741721854, Precision = 0.9980119284294234, f1 = 0.9976813514408744\n",
      "Test Loss = 0.01731013437017601, Recall = 0.9990066225165563, Aging Rate = 0.5018211920529801, precision = 0.9953810623556582\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.01565714366117258, Recall = 0.9966887417218543, Aging Rate = 0.4991721854304636, Precision = 0.9983416252072969, f1 = 0.9975144987572493\n",
      "Epoch 27: Train Loss = 0.014724400922631388, Recall = 0.9983443708609272, Aging Rate = 0.5003311258278146, Precision = 0.9976836532097948, f1 = 0.9980139026812314\n",
      "Epoch 28: Train Loss = 0.013469203827372253, Recall = 0.9983443708609272, Aging Rate = 0.5003311258278146, Precision = 0.9976836532097948, f1 = 0.9980139026812314\n",
      "Epoch 29: Train Loss = 0.012622882102597629, Recall = 0.9980132450331126, Aging Rate = 0.5, Precision = 0.9980132450331126, f1 = 0.9980132450331126\n",
      "Epoch 30: Train Loss = 0.012075005537392387, Recall = 0.9980132450331126, Aging Rate = 0.4996688741721854, Precision = 0.998674618952949, f1 = 0.9983438224577675\n",
      "Test Loss = 0.01061584966331227, Recall = 0.9996688741721854, Aging Rate = 0.5008278145695364, precision = 0.9980165289256199\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.01231737599603289, Recall = 0.9980132450331126, Aging Rate = 0.5003311258278146, Precision = 0.9973527465254798, f1 = 0.9976828864614365\n",
      "Epoch 32: Train Loss = 0.010873979138864192, Recall = 0.9983443708609272, Aging Rate = 0.4998344370860927, Precision = 0.998675057966214, f1 = 0.9985096870342773\n",
      "Epoch 33: Train Loss = 0.0097550293430686, Recall = 0.9990066225165563, Aging Rate = 0.5004966887417218, Precision = 0.99801521667218, f1 = 0.9985106735065364\n",
      "Epoch 34: Train Loss = 0.010078139827950605, Recall = 0.9980132450331126, Aging Rate = 0.5001655629139072, Precision = 0.9976828864614367, f1 = 0.9978480384042379\n",
      "Epoch 35: Train Loss = 0.00914562685285627, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Test Loss = 0.008381266188221855, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, precision = 0.999007279947055\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.00897661424426991, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Epoch 37: Train Loss = 0.008711244350311575, Recall = 0.9983443708609272, Aging Rate = 0.5, Precision = 0.9983443708609272, f1 = 0.9983443708609272\n",
      "Epoch 38: Train Loss = 0.007993465303042471, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 39: Train Loss = 0.00821785748479826, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 40: Train Loss = 0.007707881455661248, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Test Loss = 0.005989200618296911, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, precision = 0.999337528983107\n",
      "\n",
      "Epoch 41: Train Loss = 0.006578409127175611, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 42: Train Loss = 0.008296245713995782, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 43: Train Loss = 0.006694819796707062, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 44: Train Loss = 0.006454711218537233, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.006351468309263594, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Test Loss = 0.0048660648801666226, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, precision = 0.9993379675604105\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.005676268065496292, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 47: Train Loss = 0.006180750949937382, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 48: Train Loss = 0.0069713656596272, Recall = 0.9983443708609272, Aging Rate = 0.4998344370860927, Precision = 0.998675057966214, f1 = 0.9985096870342773\n",
      "Epoch 49: Train Loss = 0.006145142897229568, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 50: Train Loss = 0.005797252637204646, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Test Loss = 0.004195149291294399, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.005393376564977086, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 52: Train Loss = 0.005883367250161554, Recall = 0.9986754966887417, Aging Rate = 0.4995033112582781, Precision = 0.9996685449121644, f1 = 0.9991717740599634\n",
      "Epoch 53: Train Loss = 0.005254291317237553, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 54: Train Loss = 0.006011032580169818, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Epoch 55: Train Loss = 0.00575096524992822, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Test Loss = 0.0038123030607150683, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.004574413481939411, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 57: Train Loss = 0.004731497230526331, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 58: Train Loss = 0.004779009301437448, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 59: Train Loss = 0.005162140313036345, Recall = 0.9990066225165563, Aging Rate = 0.4996688741721854, Precision = 0.9996686547382373, f1 = 0.999337528983107\n",
      "Epoch 60: Train Loss = 0.005280918417350445, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Test Loss = 0.004175553228151029, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 61: Train Loss = 0.0051951481743302465, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 62: Train Loss = 0.0040572882105676545, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 63: Train Loss = 0.004934866035770353, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 64: Train Loss = 0.004721428911102972, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 65: Train Loss = 0.004747536922311595, Recall = 0.9990066225165563, Aging Rate = 0.4996688741721854, Precision = 0.9996686547382373, f1 = 0.999337528983107\n",
      "Test Loss = 0.0062829884151551895, Recall = 1.0, Aging Rate = 0.5008278145695364, precision = 0.9983471074380166\n",
      "\n",
      "Epoch 66: Train Loss = 0.005866621562514133, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Epoch 67: Train Loss = 0.004178555458722427, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 68: Train Loss = 0.0038579670737022595, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0037837086165237507, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 70: Train Loss = 0.004141358233562714, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Test Loss = 0.0033520748148770964, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, precision = 0.9993379675604105\n",
      "\n",
      "Epoch 71: Train Loss = 0.005235731673877168, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 72: Train Loss = 0.005583196955837852, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Epoch 73: Train Loss = 0.0037921169371843734, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 74: Train Loss = 0.004171199685221673, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 75: Train Loss = 0.0037682316087136996, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.003951832173478525, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, precision = 0.999007279947055\n",
      "\n",
      "Epoch 76: Train Loss = 0.004587374870520431, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 77: Train Loss = 0.004370812323740036, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 78: Train Loss = 0.005790592375711001, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 79: Train Loss = 0.004017132119775213, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 80: Train Loss = 0.003926194838821789, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.003412366174284788, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 81: Train Loss = 0.003956668522511146, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 82: Train Loss = 0.004219492499651819, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 83: Train Loss = 0.004172871372236913, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 84: Train Loss = 0.005073338169882086, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 85: Train Loss = 0.004772882763024987, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Test Loss = 0.002770250014319325, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.004442055573353902, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.004081880293219097, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 88: Train Loss = 0.004890867764136403, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 89: Train Loss = 0.003933067427676739, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 90: Train Loss = 0.004298548969680702, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.0030878057596983007, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, precision = 0.999007279947055\n",
      "\n",
      "Epoch 91: Train Loss = 0.004556134247702133, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 92: Train Loss = 0.0033869222483431166, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 93: Train Loss = 0.003471653236674503, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 94: Train Loss = 0.003711195110454415, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 95: Train Loss = 0.0037718317560876246, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.005212264722336503, Recall = 0.9980132450331126, Aging Rate = 0.49933774834437084, precision = 0.9993368700265252\n",
      "\n",
      "Epoch 96: Train Loss = 0.004587672080147691, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 97: Train Loss = 0.003678602389898768, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 98: Train Loss = 0.0047052545061285625, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 99: Train Loss = 0.0041057935692308265, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 100: Train Loss = 0.004265019667391906, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Test Loss = 0.0038636483790180243, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, precision = 0.9993379675604105\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5590353\ttotal: 17.7ms\tremaining: 5.29s\n",
      "1:\tlearn: 0.4671359\ttotal: 36.5ms\tremaining: 5.44s\n",
      "2:\tlearn: 0.3844253\ttotal: 58.1ms\tremaining: 5.75s\n",
      "3:\tlearn: 0.3509524\ttotal: 73ms\tremaining: 5.4s\n",
      "4:\tlearn: 0.3211328\ttotal: 86.6ms\tremaining: 5.11s\n",
      "5:\tlearn: 0.2922830\ttotal: 99.5ms\tremaining: 4.88s\n",
      "6:\tlearn: 0.2654794\ttotal: 113ms\tremaining: 4.71s\n",
      "7:\tlearn: 0.2486761\ttotal: 125ms\tremaining: 4.58s\n",
      "8:\tlearn: 0.2303233\ttotal: 138ms\tremaining: 4.47s\n",
      "9:\tlearn: 0.2130363\ttotal: 151ms\tremaining: 4.38s\n",
      "10:\tlearn: 0.1954764\ttotal: 164ms\tremaining: 4.29s\n",
      "11:\tlearn: 0.1844542\ttotal: 176ms\tremaining: 4.23s\n",
      "12:\tlearn: 0.1699604\ttotal: 189ms\tremaining: 4.16s\n",
      "13:\tlearn: 0.1618440\ttotal: 209ms\tremaining: 4.26s\n",
      "14:\tlearn: 0.1532512\ttotal: 234ms\tremaining: 4.46s\n",
      "15:\tlearn: 0.1445971\ttotal: 254ms\tremaining: 4.51s\n",
      "16:\tlearn: 0.1355753\ttotal: 293ms\tremaining: 4.87s\n",
      "17:\tlearn: 0.1314045\ttotal: 315ms\tremaining: 4.93s\n",
      "18:\tlearn: 0.1255901\ttotal: 341ms\tremaining: 5.04s\n",
      "19:\tlearn: 0.1192053\ttotal: 370ms\tremaining: 5.19s\n",
      "20:\tlearn: 0.1105288\ttotal: 395ms\tremaining: 5.25s\n",
      "21:\tlearn: 0.1073752\ttotal: 429ms\tremaining: 5.42s\n",
      "22:\tlearn: 0.1041462\ttotal: 453ms\tremaining: 5.45s\n",
      "23:\tlearn: 0.0992265\ttotal: 476ms\tremaining: 5.47s\n",
      "24:\tlearn: 0.0940431\ttotal: 512ms\tremaining: 5.63s\n",
      "25:\tlearn: 0.0905252\ttotal: 533ms\tremaining: 5.62s\n",
      "26:\tlearn: 0.0865222\ttotal: 564ms\tremaining: 5.7s\n",
      "27:\tlearn: 0.0864853\ttotal: 570ms\tremaining: 5.54s\n",
      "28:\tlearn: 0.0845263\ttotal: 592ms\tremaining: 5.53s\n",
      "29:\tlearn: 0.0845007\ttotal: 595ms\tremaining: 5.35s\n",
      "30:\tlearn: 0.0812335\ttotal: 623ms\tremaining: 5.4s\n",
      "31:\tlearn: 0.0812087\ttotal: 633ms\tremaining: 5.3s\n",
      "32:\tlearn: 0.0770831\ttotal: 658ms\tremaining: 5.33s\n",
      "33:\tlearn: 0.0741770\ttotal: 682ms\tremaining: 5.33s\n",
      "34:\tlearn: 0.0704959\ttotal: 717ms\tremaining: 5.43s\n",
      "35:\tlearn: 0.0704781\ttotal: 721ms\tremaining: 5.29s\n",
      "36:\tlearn: 0.0681620\ttotal: 739ms\tremaining: 5.25s\n",
      "37:\tlearn: 0.0681505\ttotal: 742ms\tremaining: 5.12s\n",
      "38:\tlearn: 0.0651300\ttotal: 767ms\tremaining: 5.13s\n",
      "39:\tlearn: 0.0626506\ttotal: 799ms\tremaining: 5.19s\n",
      "40:\tlearn: 0.0609059\ttotal: 824ms\tremaining: 5.21s\n",
      "41:\tlearn: 0.0609057\ttotal: 828ms\tremaining: 5.08s\n",
      "42:\tlearn: 0.0608962\ttotal: 833ms\tremaining: 4.98s\n",
      "43:\tlearn: 0.0600812\ttotal: 865ms\tremaining: 5.03s\n",
      "44:\tlearn: 0.0592517\ttotal: 890ms\tremaining: 5.04s\n",
      "45:\tlearn: 0.0575806\ttotal: 915ms\tremaining: 5.05s\n",
      "46:\tlearn: 0.0553783\ttotal: 945ms\tremaining: 5.09s\n",
      "47:\tlearn: 0.0548662\ttotal: 969ms\tremaining: 5.09s\n",
      "48:\tlearn: 0.0532667\ttotal: 1s\tremaining: 5.12s\n",
      "49:\tlearn: 0.0532622\ttotal: 1s\tremaining: 5.03s\n",
      "50:\tlearn: 0.0532581\ttotal: 1.01s\tremaining: 4.95s\n",
      "51:\tlearn: 0.0532539\ttotal: 1.02s\tremaining: 4.85s\n",
      "52:\tlearn: 0.0509576\ttotal: 1.04s\tremaining: 4.85s\n",
      "53:\tlearn: 0.0493607\ttotal: 1.07s\tremaining: 4.86s\n",
      "54:\tlearn: 0.0476783\ttotal: 1.1s\tremaining: 4.89s\n",
      "55:\tlearn: 0.0462192\ttotal: 1.13s\tremaining: 4.91s\n",
      "56:\tlearn: 0.0445903\ttotal: 1.16s\tremaining: 4.94s\n",
      "57:\tlearn: 0.0434123\ttotal: 1.19s\tremaining: 4.96s\n",
      "58:\tlearn: 0.0421299\ttotal: 1.22s\tremaining: 4.97s\n",
      "59:\tlearn: 0.0410999\ttotal: 1.24s\tremaining: 4.96s\n",
      "60:\tlearn: 0.0402497\ttotal: 1.26s\tremaining: 4.94s\n",
      "61:\tlearn: 0.0395113\ttotal: 1.27s\tremaining: 4.89s\n",
      "62:\tlearn: 0.0385146\ttotal: 1.29s\tremaining: 4.84s\n",
      "63:\tlearn: 0.0374540\ttotal: 1.3s\tremaining: 4.8s\n",
      "64:\tlearn: 0.0361631\ttotal: 1.31s\tremaining: 4.75s\n",
      "65:\tlearn: 0.0355664\ttotal: 1.32s\tremaining: 4.7s\n",
      "66:\tlearn: 0.0348966\ttotal: 1.34s\tremaining: 4.66s\n",
      "67:\tlearn: 0.0345459\ttotal: 1.35s\tremaining: 4.61s\n",
      "68:\tlearn: 0.0341482\ttotal: 1.36s\tremaining: 4.56s\n",
      "69:\tlearn: 0.0336508\ttotal: 1.38s\tremaining: 4.52s\n",
      "70:\tlearn: 0.0330443\ttotal: 1.39s\tremaining: 4.48s\n",
      "71:\tlearn: 0.0323569\ttotal: 1.4s\tremaining: 4.43s\n",
      "72:\tlearn: 0.0322979\ttotal: 1.41s\tremaining: 4.39s\n",
      "73:\tlearn: 0.0322973\ttotal: 1.41s\tremaining: 4.32s\n",
      "74:\tlearn: 0.0322952\ttotal: 1.42s\tremaining: 4.25s\n",
      "75:\tlearn: 0.0320573\ttotal: 1.43s\tremaining: 4.22s\n",
      "76:\tlearn: 0.0320571\ttotal: 1.43s\tremaining: 4.16s\n",
      "77:\tlearn: 0.0312951\ttotal: 1.45s\tremaining: 4.13s\n",
      "78:\tlearn: 0.0312951\ttotal: 1.45s\tremaining: 4.06s\n",
      "79:\tlearn: 0.0312948\ttotal: 1.45s\tremaining: 4s\n",
      "80:\tlearn: 0.0308426\ttotal: 1.47s\tremaining: 3.97s\n",
      "81:\tlearn: 0.0308426\ttotal: 1.47s\tremaining: 3.91s\n",
      "82:\tlearn: 0.0304252\ttotal: 1.49s\tremaining: 3.89s\n",
      "83:\tlearn: 0.0298831\ttotal: 1.5s\tremaining: 3.86s\n",
      "84:\tlearn: 0.0291334\ttotal: 1.51s\tremaining: 3.83s\n",
      "85:\tlearn: 0.0286352\ttotal: 1.53s\tremaining: 3.8s\n",
      "86:\tlearn: 0.0280798\ttotal: 1.54s\tremaining: 3.77s\n",
      "87:\tlearn: 0.0279010\ttotal: 1.55s\tremaining: 3.74s\n",
      "88:\tlearn: 0.0273425\ttotal: 1.56s\tremaining: 3.71s\n",
      "89:\tlearn: 0.0264931\ttotal: 1.58s\tremaining: 3.68s\n",
      "90:\tlearn: 0.0258924\ttotal: 1.59s\tremaining: 3.65s\n",
      "91:\tlearn: 0.0258009\ttotal: 1.6s\tremaining: 3.62s\n",
      "92:\tlearn: 0.0252895\ttotal: 1.61s\tremaining: 3.6s\n",
      "93:\tlearn: 0.0250114\ttotal: 1.63s\tremaining: 3.57s\n",
      "94:\tlearn: 0.0245973\ttotal: 1.64s\tremaining: 3.54s\n",
      "95:\tlearn: 0.0241983\ttotal: 1.65s\tremaining: 3.51s\n",
      "96:\tlearn: 0.0235843\ttotal: 1.67s\tremaining: 3.49s\n",
      "97:\tlearn: 0.0230508\ttotal: 1.68s\tremaining: 3.46s\n",
      "98:\tlearn: 0.0225746\ttotal: 1.69s\tremaining: 3.44s\n",
      "99:\tlearn: 0.0224373\ttotal: 1.71s\tremaining: 3.41s\n",
      "100:\tlearn: 0.0220756\ttotal: 1.72s\tremaining: 3.39s\n",
      "101:\tlearn: 0.0216813\ttotal: 1.73s\tremaining: 3.37s\n",
      "102:\tlearn: 0.0210678\ttotal: 1.75s\tremaining: 3.35s\n",
      "103:\tlearn: 0.0210444\ttotal: 1.76s\tremaining: 3.33s\n",
      "104:\tlearn: 0.0207612\ttotal: 1.78s\tremaining: 3.31s\n",
      "105:\tlearn: 0.0203803\ttotal: 1.8s\tremaining: 3.29s\n",
      "106:\tlearn: 0.0201199\ttotal: 1.82s\tremaining: 3.28s\n",
      "107:\tlearn: 0.0199027\ttotal: 1.83s\tremaining: 3.26s\n",
      "108:\tlearn: 0.0197957\ttotal: 1.85s\tremaining: 3.25s\n",
      "109:\tlearn: 0.0195120\ttotal: 1.87s\tremaining: 3.23s\n",
      "110:\tlearn: 0.0193024\ttotal: 1.89s\tremaining: 3.21s\n",
      "111:\tlearn: 0.0188985\ttotal: 1.91s\tremaining: 3.2s\n",
      "112:\tlearn: 0.0187074\ttotal: 1.93s\tremaining: 3.19s\n",
      "113:\tlearn: 0.0185020\ttotal: 1.94s\tremaining: 3.17s\n",
      "114:\tlearn: 0.0182620\ttotal: 1.96s\tremaining: 3.16s\n",
      "115:\tlearn: 0.0179947\ttotal: 1.99s\tremaining: 3.15s\n",
      "116:\tlearn: 0.0178299\ttotal: 2s\tremaining: 3.13s\n",
      "117:\tlearn: 0.0175214\ttotal: 2.02s\tremaining: 3.12s\n",
      "118:\tlearn: 0.0174147\ttotal: 2.04s\tremaining: 3.11s\n",
      "119:\tlearn: 0.0172535\ttotal: 2.07s\tremaining: 3.1s\n",
      "120:\tlearn: 0.0171570\ttotal: 2.09s\tremaining: 3.09s\n",
      "121:\tlearn: 0.0170833\ttotal: 2.1s\tremaining: 3.07s\n",
      "122:\tlearn: 0.0169701\ttotal: 2.12s\tremaining: 3.06s\n",
      "123:\tlearn: 0.0169336\ttotal: 2.14s\tremaining: 3.04s\n",
      "124:\tlearn: 0.0169336\ttotal: 2.15s\tremaining: 3s\n",
      "125:\tlearn: 0.0169022\ttotal: 2.16s\tremaining: 2.99s\n",
      "126:\tlearn: 0.0167073\ttotal: 2.19s\tremaining: 2.98s\n",
      "127:\tlearn: 0.0164925\ttotal: 2.21s\tremaining: 2.96s\n",
      "128:\tlearn: 0.0162203\ttotal: 2.22s\tremaining: 2.95s\n",
      "129:\tlearn: 0.0160605\ttotal: 2.25s\tremaining: 2.94s\n",
      "130:\tlearn: 0.0158795\ttotal: 2.27s\tremaining: 2.93s\n",
      "131:\tlearn: 0.0156511\ttotal: 2.3s\tremaining: 2.92s\n",
      "132:\tlearn: 0.0154684\ttotal: 2.32s\tremaining: 2.92s\n",
      "133:\tlearn: 0.0152488\ttotal: 2.35s\tremaining: 2.91s\n",
      "134:\tlearn: 0.0150793\ttotal: 2.36s\tremaining: 2.88s\n",
      "135:\tlearn: 0.0148734\ttotal: 2.37s\tremaining: 2.86s\n",
      "136:\tlearn: 0.0146259\ttotal: 2.38s\tremaining: 2.84s\n",
      "137:\tlearn: 0.0143853\ttotal: 2.4s\tremaining: 2.81s\n",
      "138:\tlearn: 0.0140921\ttotal: 2.41s\tremaining: 2.79s\n",
      "139:\tlearn: 0.0140708\ttotal: 2.42s\tremaining: 2.77s\n",
      "140:\tlearn: 0.0139389\ttotal: 2.44s\tremaining: 2.75s\n",
      "141:\tlearn: 0.0138155\ttotal: 2.45s\tremaining: 2.72s\n",
      "142:\tlearn: 0.0136223\ttotal: 2.46s\tremaining: 2.7s\n",
      "143:\tlearn: 0.0134022\ttotal: 2.47s\tremaining: 2.68s\n",
      "144:\tlearn: 0.0132341\ttotal: 2.49s\tremaining: 2.66s\n",
      "145:\tlearn: 0.0131865\ttotal: 2.5s\tremaining: 2.63s\n",
      "146:\tlearn: 0.0131865\ttotal: 2.5s\tremaining: 2.6s\n",
      "147:\tlearn: 0.0130601\ttotal: 2.51s\tremaining: 2.58s\n",
      "148:\tlearn: 0.0128853\ttotal: 2.52s\tremaining: 2.56s\n",
      "149:\tlearn: 0.0127704\ttotal: 2.54s\tremaining: 2.54s\n",
      "150:\tlearn: 0.0125672\ttotal: 2.55s\tremaining: 2.52s\n",
      "151:\tlearn: 0.0123981\ttotal: 2.56s\tremaining: 2.5s\n",
      "152:\tlearn: 0.0122960\ttotal: 2.58s\tremaining: 2.48s\n",
      "153:\tlearn: 0.0121914\ttotal: 2.59s\tremaining: 2.45s\n",
      "154:\tlearn: 0.0120474\ttotal: 2.6s\tremaining: 2.43s\n",
      "155:\tlearn: 0.0119321\ttotal: 2.61s\tremaining: 2.41s\n",
      "156:\tlearn: 0.0118022\ttotal: 2.63s\tremaining: 2.39s\n",
      "157:\tlearn: 0.0117373\ttotal: 2.64s\tremaining: 2.37s\n",
      "158:\tlearn: 0.0115977\ttotal: 2.65s\tremaining: 2.35s\n",
      "159:\tlearn: 0.0115143\ttotal: 2.66s\tremaining: 2.33s\n",
      "160:\tlearn: 0.0114609\ttotal: 2.68s\tremaining: 2.31s\n",
      "161:\tlearn: 0.0113562\ttotal: 2.69s\tremaining: 2.29s\n",
      "162:\tlearn: 0.0111649\ttotal: 2.7s\tremaining: 2.27s\n",
      "163:\tlearn: 0.0110396\ttotal: 2.71s\tremaining: 2.25s\n",
      "164:\tlearn: 0.0109716\ttotal: 2.73s\tremaining: 2.23s\n",
      "165:\tlearn: 0.0108993\ttotal: 2.74s\tremaining: 2.21s\n",
      "166:\tlearn: 0.0108330\ttotal: 2.75s\tremaining: 2.19s\n",
      "167:\tlearn: 0.0107617\ttotal: 2.77s\tremaining: 2.17s\n",
      "168:\tlearn: 0.0107086\ttotal: 2.78s\tremaining: 2.16s\n",
      "169:\tlearn: 0.0105905\ttotal: 2.8s\tremaining: 2.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170:\tlearn: 0.0105316\ttotal: 2.81s\tremaining: 2.12s\n",
      "171:\tlearn: 0.0105316\ttotal: 2.82s\tremaining: 2.1s\n",
      "172:\tlearn: 0.0104478\ttotal: 2.84s\tremaining: 2.08s\n",
      "173:\tlearn: 0.0103717\ttotal: 2.85s\tremaining: 2.06s\n",
      "174:\tlearn: 0.0102775\ttotal: 2.86s\tremaining: 2.04s\n",
      "175:\tlearn: 0.0101788\ttotal: 2.88s\tremaining: 2.03s\n",
      "176:\tlearn: 0.0101126\ttotal: 2.89s\tremaining: 2.01s\n",
      "177:\tlearn: 0.0100753\ttotal: 2.9s\tremaining: 1.99s\n",
      "178:\tlearn: 0.0100753\ttotal: 2.92s\tremaining: 1.97s\n",
      "179:\tlearn: 0.0099975\ttotal: 2.93s\tremaining: 1.95s\n",
      "180:\tlearn: 0.0099463\ttotal: 2.94s\tremaining: 1.94s\n",
      "181:\tlearn: 0.0099174\ttotal: 2.96s\tremaining: 1.92s\n",
      "182:\tlearn: 0.0099174\ttotal: 2.97s\tremaining: 1.9s\n",
      "183:\tlearn: 0.0099174\ttotal: 2.97s\tremaining: 1.87s\n",
      "184:\tlearn: 0.0098148\ttotal: 2.99s\tremaining: 1.86s\n",
      "185:\tlearn: 0.0097405\ttotal: 3s\tremaining: 1.84s\n",
      "186:\tlearn: 0.0096771\ttotal: 3.01s\tremaining: 1.82s\n",
      "187:\tlearn: 0.0095766\ttotal: 3.03s\tremaining: 1.8s\n",
      "188:\tlearn: 0.0095325\ttotal: 3.04s\tremaining: 1.79s\n",
      "189:\tlearn: 0.0094722\ttotal: 3.06s\tremaining: 1.77s\n",
      "190:\tlearn: 0.0094072\ttotal: 3.07s\tremaining: 1.75s\n",
      "191:\tlearn: 0.0092538\ttotal: 3.08s\tremaining: 1.73s\n",
      "192:\tlearn: 0.0092004\ttotal: 3.09s\tremaining: 1.72s\n",
      "193:\tlearn: 0.0091058\ttotal: 3.11s\tremaining: 1.7s\n",
      "194:\tlearn: 0.0091058\ttotal: 3.11s\tremaining: 1.67s\n",
      "195:\tlearn: 0.0090739\ttotal: 3.12s\tremaining: 1.66s\n",
      "196:\tlearn: 0.0089832\ttotal: 3.14s\tremaining: 1.64s\n",
      "197:\tlearn: 0.0089251\ttotal: 3.16s\tremaining: 1.63s\n",
      "198:\tlearn: 0.0088509\ttotal: 3.18s\tremaining: 1.61s\n",
      "199:\tlearn: 0.0087402\ttotal: 3.2s\tremaining: 1.6s\n",
      "200:\tlearn: 0.0087087\ttotal: 3.23s\tremaining: 1.59s\n",
      "201:\tlearn: 0.0086960\ttotal: 3.25s\tremaining: 1.58s\n",
      "202:\tlearn: 0.0086960\ttotal: 3.27s\tremaining: 1.56s\n",
      "203:\tlearn: 0.0086147\ttotal: 3.28s\tremaining: 1.54s\n",
      "204:\tlearn: 0.0085052\ttotal: 3.3s\tremaining: 1.53s\n",
      "205:\tlearn: 0.0085052\ttotal: 3.31s\tremaining: 1.51s\n",
      "206:\tlearn: 0.0084931\ttotal: 3.33s\tremaining: 1.49s\n",
      "207:\tlearn: 0.0084929\ttotal: 3.34s\tremaining: 1.48s\n",
      "208:\tlearn: 0.0084202\ttotal: 3.35s\tremaining: 1.46s\n",
      "209:\tlearn: 0.0084202\ttotal: 3.35s\tremaining: 1.44s\n",
      "210:\tlearn: 0.0084201\ttotal: 3.37s\tremaining: 1.42s\n",
      "211:\tlearn: 0.0083659\ttotal: 3.38s\tremaining: 1.4s\n",
      "212:\tlearn: 0.0083128\ttotal: 3.39s\tremaining: 1.39s\n",
      "213:\tlearn: 0.0083124\ttotal: 3.41s\tremaining: 1.37s\n",
      "214:\tlearn: 0.0082662\ttotal: 3.42s\tremaining: 1.35s\n",
      "215:\tlearn: 0.0082572\ttotal: 3.43s\tremaining: 1.33s\n",
      "216:\tlearn: 0.0082125\ttotal: 3.45s\tremaining: 1.32s\n",
      "217:\tlearn: 0.0081674\ttotal: 3.46s\tremaining: 1.3s\n",
      "218:\tlearn: 0.0081045\ttotal: 3.47s\tremaining: 1.28s\n",
      "219:\tlearn: 0.0080964\ttotal: 3.48s\tremaining: 1.27s\n",
      "220:\tlearn: 0.0080191\ttotal: 3.5s\tremaining: 1.25s\n",
      "221:\tlearn: 0.0079343\ttotal: 3.51s\tremaining: 1.23s\n",
      "222:\tlearn: 0.0079343\ttotal: 3.52s\tremaining: 1.22s\n",
      "223:\tlearn: 0.0078917\ttotal: 3.54s\tremaining: 1.2s\n",
      "224:\tlearn: 0.0078916\ttotal: 3.55s\tremaining: 1.18s\n",
      "225:\tlearn: 0.0078899\ttotal: 3.56s\tremaining: 1.17s\n",
      "226:\tlearn: 0.0078891\ttotal: 3.57s\tremaining: 1.15s\n",
      "227:\tlearn: 0.0078488\ttotal: 3.58s\tremaining: 1.13s\n",
      "228:\tlearn: 0.0077856\ttotal: 3.6s\tremaining: 1.11s\n",
      "229:\tlearn: 0.0077052\ttotal: 3.61s\tremaining: 1.1s\n",
      "230:\tlearn: 0.0076750\ttotal: 3.62s\tremaining: 1.08s\n",
      "231:\tlearn: 0.0076270\ttotal: 3.64s\tremaining: 1.07s\n",
      "232:\tlearn: 0.0075843\ttotal: 3.65s\tremaining: 1.05s\n",
      "233:\tlearn: 0.0075842\ttotal: 3.66s\tremaining: 1.03s\n",
      "234:\tlearn: 0.0075788\ttotal: 3.67s\tremaining: 1.02s\n",
      "235:\tlearn: 0.0075262\ttotal: 3.69s\tremaining: 1s\n",
      "236:\tlearn: 0.0074611\ttotal: 3.7s\tremaining: 984ms\n",
      "237:\tlearn: 0.0074611\ttotal: 3.71s\tremaining: 968ms\n",
      "238:\tlearn: 0.0074611\ttotal: 3.73s\tremaining: 951ms\n",
      "239:\tlearn: 0.0074611\ttotal: 3.74s\tremaining: 935ms\n",
      "240:\tlearn: 0.0073983\ttotal: 3.75s\tremaining: 919ms\n",
      "241:\tlearn: 0.0073213\ttotal: 3.77s\tremaining: 903ms\n",
      "242:\tlearn: 0.0073213\ttotal: 3.78s\tremaining: 887ms\n",
      "243:\tlearn: 0.0072622\ttotal: 3.79s\tremaining: 871ms\n",
      "244:\tlearn: 0.0072355\ttotal: 3.81s\tremaining: 855ms\n",
      "245:\tlearn: 0.0071752\ttotal: 3.82s\tremaining: 839ms\n",
      "246:\tlearn: 0.0071751\ttotal: 3.84s\tremaining: 823ms\n",
      "247:\tlearn: 0.0071333\ttotal: 3.85s\tremaining: 807ms\n",
      "248:\tlearn: 0.0071333\ttotal: 3.86s\tremaining: 791ms\n",
      "249:\tlearn: 0.0070997\ttotal: 3.88s\tremaining: 775ms\n",
      "250:\tlearn: 0.0070531\ttotal: 3.89s\tremaining: 760ms\n",
      "251:\tlearn: 0.0070141\ttotal: 3.9s\tremaining: 744ms\n",
      "252:\tlearn: 0.0069890\ttotal: 3.92s\tremaining: 728ms\n",
      "253:\tlearn: 0.0068990\ttotal: 3.93s\tremaining: 712ms\n",
      "254:\tlearn: 0.0068469\ttotal: 3.94s\tremaining: 696ms\n",
      "255:\tlearn: 0.0068034\ttotal: 3.96s\tremaining: 680ms\n",
      "256:\tlearn: 0.0067767\ttotal: 3.97s\tremaining: 664ms\n",
      "257:\tlearn: 0.0066980\ttotal: 3.98s\tremaining: 648ms\n",
      "258:\tlearn: 0.0066316\ttotal: 4s\tremaining: 633ms\n",
      "259:\tlearn: 0.0066310\ttotal: 4.01s\tremaining: 617ms\n",
      "260:\tlearn: 0.0065865\ttotal: 4.02s\tremaining: 601ms\n",
      "261:\tlearn: 0.0065860\ttotal: 4.03s\tremaining: 585ms\n",
      "262:\tlearn: 0.0065488\ttotal: 4.05s\tremaining: 569ms\n",
      "263:\tlearn: 0.0065487\ttotal: 4.06s\tremaining: 554ms\n",
      "264:\tlearn: 0.0065487\ttotal: 4.07s\tremaining: 538ms\n",
      "265:\tlearn: 0.0065487\ttotal: 4.07s\tremaining: 521ms\n",
      "266:\tlearn: 0.0065486\ttotal: 4.09s\tremaining: 505ms\n",
      "267:\tlearn: 0.0065486\ttotal: 4.09s\tremaining: 488ms\n",
      "268:\tlearn: 0.0064988\ttotal: 4.1s\tremaining: 473ms\n",
      "269:\tlearn: 0.0064609\ttotal: 4.11s\tremaining: 457ms\n",
      "270:\tlearn: 0.0064301\ttotal: 4.13s\tremaining: 442ms\n",
      "271:\tlearn: 0.0064301\ttotal: 4.15s\tremaining: 427ms\n",
      "272:\tlearn: 0.0063720\ttotal: 4.17s\tremaining: 413ms\n",
      "273:\tlearn: 0.0063316\ttotal: 4.2s\tremaining: 398ms\n",
      "274:\tlearn: 0.0063316\ttotal: 4.22s\tremaining: 384ms\n",
      "275:\tlearn: 0.0062731\ttotal: 4.24s\tremaining: 369ms\n",
      "276:\tlearn: 0.0062374\ttotal: 4.25s\tremaining: 353ms\n",
      "277:\tlearn: 0.0062308\ttotal: 4.27s\tremaining: 338ms\n",
      "278:\tlearn: 0.0062030\ttotal: 4.28s\tremaining: 322ms\n",
      "279:\tlearn: 0.0061484\ttotal: 4.3s\tremaining: 307ms\n",
      "280:\tlearn: 0.0061131\ttotal: 4.31s\tremaining: 291ms\n",
      "281:\tlearn: 0.0060661\ttotal: 4.32s\tremaining: 276ms\n",
      "282:\tlearn: 0.0060661\ttotal: 4.33s\tremaining: 260ms\n",
      "283:\tlearn: 0.0060365\ttotal: 4.35s\tremaining: 245ms\n",
      "284:\tlearn: 0.0060177\ttotal: 4.36s\tremaining: 230ms\n",
      "285:\tlearn: 0.0059698\ttotal: 4.37s\tremaining: 214ms\n",
      "286:\tlearn: 0.0059693\ttotal: 4.39s\tremaining: 199ms\n",
      "287:\tlearn: 0.0059175\ttotal: 4.4s\tremaining: 183ms\n",
      "288:\tlearn: 0.0059174\ttotal: 4.41s\tremaining: 168ms\n",
      "289:\tlearn: 0.0058747\ttotal: 4.43s\tremaining: 153ms\n",
      "290:\tlearn: 0.0058747\ttotal: 4.44s\tremaining: 137ms\n",
      "291:\tlearn: 0.0058746\ttotal: 4.45s\tremaining: 122ms\n",
      "292:\tlearn: 0.0058740\ttotal: 4.47s\tremaining: 107ms\n",
      "293:\tlearn: 0.0058740\ttotal: 4.47s\tremaining: 91.3ms\n",
      "294:\tlearn: 0.0058739\ttotal: 4.49s\tremaining: 76.1ms\n",
      "295:\tlearn: 0.0058525\ttotal: 4.5s\tremaining: 60.8ms\n",
      "296:\tlearn: 0.0058421\ttotal: 4.51s\tremaining: 45.6ms\n",
      "297:\tlearn: 0.0058421\ttotal: 4.53s\tremaining: 30.4ms\n",
      "298:\tlearn: 0.0058421\ttotal: 4.54s\tremaining: 15.2ms\n",
      "299:\tlearn: 0.0058074\ttotal: 4.55s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dcdbc9a66e4965914aa19aed80228b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5419852213749033, Recall = 0.7599337748344371, Aging Rate = 0.5072847682119205, Precision = 0.7490208877284595, f1 = 0.7544378698224852\n",
      "Epoch 2: Train Loss = 0.36441636314455245, Recall = 0.8639072847682119, Aging Rate = 0.5134105960264901, Precision = 0.8413415027410512, f1 = 0.852475085770299\n",
      "Epoch 3: Train Loss = 0.2846506411468746, Recall = 0.9, Aging Rate = 0.5089403973509934, Precision = 0.8841899804814574, f1 = 0.8920249425664588\n",
      "Epoch 4: Train Loss = 0.2276448534419205, Recall = 0.9248344370860927, Aging Rate = 0.5057947019867549, Precision = 0.9142389525368249, f1 = 0.9195061728395062\n",
      "Epoch 5: Train Loss = 0.191498310439634, Recall = 0.936092715231788, Aging Rate = 0.5021523178807947, Precision = 0.9320804484009232, f1 = 0.9340822732529325\n",
      "Test Loss = 0.15100311220481696, Recall = 0.9493377483443709, Aging Rate = 0.49387417218543045, precision = 0.9611129735165941\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14432433667463182, Recall = 0.9509933774834437, Aging Rate = 0.496523178807947, Precision = 0.9576525508502834, f1 = 0.9543113473999003\n",
      "Epoch 7: Train Loss = 0.1172389433774727, Recall = 0.963907284768212, Aging Rate = 0.49784768211920527, Precision = 0.9680744928500167, f1 = 0.9659863945578232\n",
      "Epoch 8: Train Loss = 0.09846643186760265, Recall = 0.9708609271523179, Aging Rate = 0.49817880794701985, Precision = 0.9744101030242606, f1 = 0.9726322773262563\n",
      "Epoch 9: Train Loss = 0.08394974700070375, Recall = 0.9771523178807947, Aging Rate = 0.4995033112582781, Precision = 0.9781239642028505, f1 = 0.977637899619016\n",
      "Epoch 10: Train Loss = 0.07326911604957076, Recall = 0.980794701986755, Aging Rate = 0.4991721854304636, Precision = 0.9824212271973466, f1 = 0.9816072908036454\n",
      "Test Loss = 0.06353993872735673, Recall = 0.9890728476821192, Aging Rate = 0.5031456953642384, precision = 0.9828891082592959\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06438541723698969, Recall = 0.983112582781457, Aging Rate = 0.4990066225165563, Precision = 0.9850696748506967, f1 = 0.9840901557838914\n",
      "Epoch 12: Train Loss = 0.05787196381399963, Recall = 0.9860927152317881, Aging Rate = 0.4998344370860927, Precision = 0.9864193441536933, f1 = 0.9862560026494452\n",
      "Epoch 13: Train Loss = 0.05026933155115077, Recall = 0.9870860927152317, Aging Rate = 0.498841059602649, Precision = 0.989379356123465, f1 = 0.9882313939996685\n",
      "Epoch 14: Train Loss = 0.04457498203395613, Recall = 0.9887417218543046, Aging Rate = 0.4996688741721854, Precision = 0.9893969516235918, f1 = 0.9890692282212653\n",
      "Epoch 15: Train Loss = 0.04037743462928083, Recall = 0.9923841059602649, Aging Rate = 0.4998344370860927, Precision = 0.9927128188141768, f1 = 0.9925484351713859\n",
      "Test Loss = 0.03453782535102588, Recall = 0.9956953642384105, Aging Rate = 0.5009933774834437, precision = 0.9937210839391937\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03643843488434687, Recall = 0.9900662251655629, Aging Rate = 0.49784768211920527, Precision = 0.9943465247755238, f1 = 0.9922017587522814\n",
      "Epoch 17: Train Loss = 0.032171408433639845, Recall = 0.9923841059602649, Aging Rate = 0.4986754966887417, Precision = 0.9950199203187251, f1 = 0.9937002652519894\n",
      "Epoch 18: Train Loss = 0.02913828668807516, Recall = 0.9943708609271523, Aging Rate = 0.49933774834437084, Precision = 0.9956896551724138, f1 = 0.9950298210735588\n",
      "Epoch 19: Train Loss = 0.027512547899259637, Recall = 0.9937086092715232, Aging Rate = 0.4990066225165563, Precision = 0.995686794956868, f1 = 0.9946967185946304\n",
      "Epoch 20: Train Loss = 0.025656382443494354, Recall = 0.9947019867549669, Aging Rate = 0.49933774834437084, Precision = 0.9960212201591512, f1 = 0.9953611663353215\n",
      "Test Loss = 0.021387783748819337, Recall = 0.9960264900662251, Aging Rate = 0.4995033112582781, precision = 0.9970169042094796\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.023058602925612043, Recall = 0.9950331125827815, Aging Rate = 0.4995033112582781, Precision = 0.9960225389459728, f1 = 0.9955275799238033\n",
      "Epoch 22: Train Loss = 0.020566587897622033, Recall = 0.9966887417218543, Aging Rate = 0.5, Precision = 0.9966887417218543, f1 = 0.9966887417218543\n",
      "Epoch 23: Train Loss = 0.019995207193967524, Recall = 0.9966887417218543, Aging Rate = 0.5, Precision = 0.9966887417218543, f1 = 0.9966887417218543\n",
      "Epoch 24: Train Loss = 0.01767178176723371, Recall = 0.9960264900662251, Aging Rate = 0.4991721854304636, Precision = 0.9976782752902156, f1 = 0.9968516984258493\n",
      "Epoch 25: Train Loss = 0.016752568072337187, Recall = 0.9966887417218543, Aging Rate = 0.4996688741721854, Precision = 0.9973492379058979, f1 = 0.9970188804239813\n",
      "Test Loss = 0.013860215791902004, Recall = 0.9986754966887417, Aging Rate = 0.5, precision = 0.9986754966887417\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.015023995298176806, Recall = 0.997682119205298, Aging Rate = 0.4998344370860927, Precision = 0.9980125869493209, f1 = 0.9978473257161782\n",
      "Epoch 27: Train Loss = 0.014076711160951104, Recall = 0.9980132450331126, Aging Rate = 0.5, Precision = 0.9980132450331126, f1 = 0.9980132450331126\n",
      "Epoch 28: Train Loss = 0.013315232306262416, Recall = 0.9983443708609272, Aging Rate = 0.5, Precision = 0.9983443708609272, f1 = 0.9983443708609272\n",
      "Epoch 29: Train Loss = 0.012450053043885538, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Epoch 30: Train Loss = 0.01220490706973518, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Test Loss = 0.00919626819475597, Recall = 1.0, Aging Rate = 0.5008278145695364, precision = 0.9983471074380166\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.010861067682665013, Recall = 0.9983443708609272, Aging Rate = 0.5, Precision = 0.9983443708609272, f1 = 0.9983443708609272\n",
      "Epoch 32: Train Loss = 0.01008617551380534, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 33: Train Loss = 0.009776620313969274, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Epoch 34: Train Loss = 0.009573185141044145, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Epoch 35: Train Loss = 0.008902520471443699, Recall = 0.9996688741721854, Aging Rate = 0.5006622516556292, Precision = 0.9983465608465608, f1 = 0.999007279947055\n",
      "Test Loss = 0.010078381670219516, Recall = 1.0, Aging Rate = 0.5008278145695364, precision = 0.9983471074380166\n",
      "\n",
      "Epoch 36: Train Loss = 0.009001969213949036, Recall = 0.9990066225165563, Aging Rate = 0.5003311258278146, Precision = 0.9983454665784249, f1 = 0.9986759351208209\n",
      "Epoch 37: Train Loss = 0.00796587120121752, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 38: Train Loss = 0.007182654084033326, Recall = 0.9996688741721854, Aging Rate = 0.5006622516556292, Precision = 0.9983465608465608, f1 = 0.999007279947055\n",
      "Epoch 39: Train Loss = 0.008290130121707423, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Epoch 40: Train Loss = 0.006481681014133605, Recall = 1.0, Aging Rate = 0.5004966887417218, Precision = 0.9990076083360899, f1 = 0.9995035578355121\n",
      "Test Loss = 0.005455336773718726, Recall = 1.0, Aging Rate = 0.5004966887417218, precision = 0.9990076083360899\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.006795005660676799, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 42: Train Loss = 0.006029446920016102, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 43: Train Loss = 0.005976392341691335, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 44: Train Loss = 0.005620081356991798, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 45: Train Loss = 0.005365308042344273, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Test Loss = 0.0053711289623197146, Recall = 1.0, Aging Rate = 0.5004966887417218, precision = 0.9990076083360899\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss = 0.0060602516940461825, Recall = 0.9986754966887417, Aging Rate = 0.4995033112582781, Precision = 0.9996685449121644, f1 = 0.9991717740599634\n",
      "Epoch 47: Train Loss = 0.005484988524683362, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 48: Train Loss = 0.006226124257058991, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 49: Train Loss = 0.005891565143574794, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 50: Train Loss = 0.004790135100483895, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, Precision = 0.9986768111147867, f1 = 0.9991725963925203\n",
      "Test Loss = 0.006606825172210371, Recall = 0.9986754966887417, Aging Rate = 0.49933774834437084, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.005192608437161671, Recall = 1.0, Aging Rate = 0.5004966887417218, Precision = 0.9990076083360899, f1 = 0.9995035578355121\n",
      "Epoch 52: Train Loss = 0.00513753188891176, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 53: Train Loss = 0.004540003204274157, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004472730401440813, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 55: Train Loss = 0.0044314886195067045, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.003133944859604863, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.0037259033807115443, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 57: Train Loss = 0.004241243501452865, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 58: Train Loss = 0.0042055790391226375, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.004563956999139774, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 60: Train Loss = 0.004056996905047521, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Test Loss = 0.003441437577908945, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 61: Train Loss = 0.00391359535186237, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 62: Train Loss = 0.004048579454187624, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 63: Train Loss = 0.003990037119268927, Recall = 1.0, Aging Rate = 0.5004966887417218, Precision = 0.9990076083360899, f1 = 0.9995035578355121\n",
      "Epoch 64: Train Loss = 0.0035252409041951725, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 65: Train Loss = 0.0038187296283955607, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.004193847360157266, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, precision = 0.9996687644915535\n",
      "\n",
      "Epoch 66: Train Loss = 0.004270828405660361, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.004191379552546716, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 68: Train Loss = 0.0039725363552138605, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.003657186596302797, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 70: Train Loss = 0.003684444435323212, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Test Loss = 0.0026493219001101492, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.004085620526002338, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0034520358155598705, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 73: Train Loss = 0.003385800328367815, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.003918194982673455, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 75: Train Loss = 0.003702623761006144, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003370668112904771, Recall = 1.0, Aging Rate = 0.5003311258278146, precision = 0.99933818663137\n",
      "\n",
      "Epoch 76: Train Loss = 0.004083506246282417, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 77: Train Loss = 0.003422775650813263, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 78: Train Loss = 0.003392130189223696, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.00316361564239302, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.003022558907577336, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002754952600040775, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0029484586302955815, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0033618409300704075, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0034809240570988827, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.003496244304431047, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.003287265832159693, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00315609649687216, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0031434076677835147, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0031191038929191204, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.003284193860946704, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 89: Train Loss = 0.003538492861641756, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.002889258448506354, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029264520749292455, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.003285634817882435, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 92: Train Loss = 0.004542556975425424, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 93: Train Loss = 0.004240669408467727, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 94: Train Loss = 0.003666004365069533, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 95: Train Loss = 0.003155186540441008, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Test Loss = 0.0022372356038509763, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.002708076868462222, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0026256084382774566, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 98: Train Loss = 0.0030503588091791763, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss = 0.0028208490077171796, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.003142581586972718, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Test Loss = 0.0025518193585021016, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.0032084554406067987, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 102: Train Loss = 0.0035741560864231445, Recall = 1.0, Aging Rate = 0.5001655629139072, Precision = 0.9996689837802052, f1 = 0.9998344644926337\n",
      "Epoch 103: Train Loss = 0.003991587940042234, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 104: Train Loss = 0.0031100381571269094, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.002985173890317809, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0023592239085412185, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 105.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5418287\ttotal: 16.3ms\tremaining: 4.86s\n",
      "1:\tlearn: 0.4656313\ttotal: 31.7ms\tremaining: 4.73s\n",
      "2:\tlearn: 0.3956210\ttotal: 47.3ms\tremaining: 4.68s\n",
      "3:\tlearn: 0.3493083\ttotal: 62.9ms\tremaining: 4.66s\n",
      "4:\tlearn: 0.3193404\ttotal: 79ms\tremaining: 4.66s\n",
      "5:\tlearn: 0.2940443\ttotal: 96.1ms\tremaining: 4.71s\n",
      "6:\tlearn: 0.2658447\ttotal: 110ms\tremaining: 4.62s\n",
      "7:\tlearn: 0.2404087\ttotal: 124ms\tremaining: 4.51s\n",
      "8:\tlearn: 0.2149132\ttotal: 137ms\tremaining: 4.42s\n",
      "9:\tlearn: 0.1954562\ttotal: 150ms\tremaining: 4.34s\n",
      "10:\tlearn: 0.1819533\ttotal: 162ms\tremaining: 4.26s\n",
      "11:\tlearn: 0.1706550\ttotal: 175ms\tremaining: 4.2s\n",
      "12:\tlearn: 0.1558406\ttotal: 188ms\tremaining: 4.14s\n",
      "13:\tlearn: 0.1487150\ttotal: 200ms\tremaining: 4.09s\n",
      "14:\tlearn: 0.1431091\ttotal: 213ms\tremaining: 4.05s\n",
      "15:\tlearn: 0.1367034\ttotal: 226ms\tremaining: 4s\n",
      "16:\tlearn: 0.1288619\ttotal: 238ms\tremaining: 3.97s\n",
      "17:\tlearn: 0.1208548\ttotal: 251ms\tremaining: 3.93s\n",
      "18:\tlearn: 0.1163118\ttotal: 264ms\tremaining: 3.9s\n",
      "19:\tlearn: 0.1091106\ttotal: 277ms\tremaining: 3.88s\n",
      "20:\tlearn: 0.1036541\ttotal: 292ms\tremaining: 3.88s\n",
      "21:\tlearn: 0.0997771\ttotal: 315ms\tremaining: 3.98s\n",
      "22:\tlearn: 0.0953043\ttotal: 328ms\tremaining: 3.95s\n",
      "23:\tlearn: 0.0919637\ttotal: 341ms\tremaining: 3.92s\n",
      "24:\tlearn: 0.0876201\ttotal: 355ms\tremaining: 3.91s\n",
      "25:\tlearn: 0.0843824\ttotal: 369ms\tremaining: 3.89s\n",
      "26:\tlearn: 0.0816402\ttotal: 383ms\tremaining: 3.88s\n",
      "27:\tlearn: 0.0816158\ttotal: 385ms\tremaining: 3.74s\n",
      "28:\tlearn: 0.0783381\ttotal: 399ms\tremaining: 3.73s\n",
      "29:\tlearn: 0.0762545\ttotal: 414ms\tremaining: 3.73s\n",
      "30:\tlearn: 0.0747513\ttotal: 428ms\tremaining: 3.71s\n",
      "31:\tlearn: 0.0747267\ttotal: 430ms\tremaining: 3.6s\n",
      "32:\tlearn: 0.0726472\ttotal: 444ms\tremaining: 3.6s\n",
      "33:\tlearn: 0.0699174\ttotal: 458ms\tremaining: 3.59s\n",
      "34:\tlearn: 0.0682439\ttotal: 473ms\tremaining: 3.58s\n",
      "35:\tlearn: 0.0649092\ttotal: 487ms\tremaining: 3.57s\n",
      "36:\tlearn: 0.0625785\ttotal: 501ms\tremaining: 3.56s\n",
      "37:\tlearn: 0.0609674\ttotal: 515ms\tremaining: 3.55s\n",
      "38:\tlearn: 0.0594288\ttotal: 530ms\tremaining: 3.54s\n",
      "39:\tlearn: 0.0579014\ttotal: 543ms\tremaining: 3.53s\n",
      "40:\tlearn: 0.0559929\ttotal: 557ms\tremaining: 3.52s\n",
      "41:\tlearn: 0.0559814\ttotal: 559ms\tremaining: 3.44s\n",
      "42:\tlearn: 0.0559734\ttotal: 561ms\tremaining: 3.35s\n",
      "43:\tlearn: 0.0545221\ttotal: 574ms\tremaining: 3.34s\n",
      "44:\tlearn: 0.0521536\ttotal: 588ms\tremaining: 3.33s\n",
      "45:\tlearn: 0.0508096\ttotal: 601ms\tremaining: 3.32s\n",
      "46:\tlearn: 0.0494973\ttotal: 614ms\tremaining: 3.31s\n",
      "47:\tlearn: 0.0481878\ttotal: 628ms\tremaining: 3.29s\n",
      "48:\tlearn: 0.0468950\ttotal: 640ms\tremaining: 3.28s\n",
      "49:\tlearn: 0.0452725\ttotal: 654ms\tremaining: 3.27s\n",
      "50:\tlearn: 0.0440031\ttotal: 667ms\tremaining: 3.26s\n",
      "51:\tlearn: 0.0423514\ttotal: 680ms\tremaining: 3.25s\n",
      "52:\tlearn: 0.0408538\ttotal: 694ms\tremaining: 3.23s\n",
      "53:\tlearn: 0.0394719\ttotal: 707ms\tremaining: 3.22s\n",
      "54:\tlearn: 0.0386271\ttotal: 720ms\tremaining: 3.21s\n",
      "55:\tlearn: 0.0378774\ttotal: 733ms\tremaining: 3.19s\n",
      "56:\tlearn: 0.0369557\ttotal: 747ms\tremaining: 3.18s\n",
      "57:\tlearn: 0.0360655\ttotal: 760ms\tremaining: 3.17s\n",
      "58:\tlearn: 0.0353622\ttotal: 773ms\tremaining: 3.16s\n",
      "59:\tlearn: 0.0346598\ttotal: 786ms\tremaining: 3.14s\n",
      "60:\tlearn: 0.0337511\ttotal: 799ms\tremaining: 3.13s\n",
      "61:\tlearn: 0.0333263\ttotal: 812ms\tremaining: 3.12s\n",
      "62:\tlearn: 0.0325579\ttotal: 825ms\tremaining: 3.1s\n",
      "63:\tlearn: 0.0317928\ttotal: 838ms\tremaining: 3.09s\n",
      "64:\tlearn: 0.0313243\ttotal: 852ms\tremaining: 3.08s\n",
      "65:\tlearn: 0.0307040\ttotal: 865ms\tremaining: 3.07s\n",
      "66:\tlearn: 0.0303106\ttotal: 878ms\tremaining: 3.05s\n",
      "67:\tlearn: 0.0301067\ttotal: 891ms\tremaining: 3.04s\n",
      "68:\tlearn: 0.0293274\ttotal: 904ms\tremaining: 3.03s\n",
      "69:\tlearn: 0.0288474\ttotal: 918ms\tremaining: 3.02s\n",
      "70:\tlearn: 0.0282480\ttotal: 931ms\tremaining: 3s\n",
      "71:\tlearn: 0.0277223\ttotal: 944ms\tremaining: 2.99s\n",
      "72:\tlearn: 0.0274653\ttotal: 958ms\tremaining: 2.98s\n",
      "73:\tlearn: 0.0273274\ttotal: 971ms\tremaining: 2.96s\n",
      "74:\tlearn: 0.0270172\ttotal: 984ms\tremaining: 2.95s\n",
      "75:\tlearn: 0.0264124\ttotal: 998ms\tremaining: 2.94s\n",
      "76:\tlearn: 0.0259587\ttotal: 1.01s\tremaining: 2.93s\n",
      "77:\tlearn: 0.0255699\ttotal: 1.02s\tremaining: 2.92s\n",
      "78:\tlearn: 0.0254644\ttotal: 1.04s\tremaining: 2.91s\n",
      "79:\tlearn: 0.0251066\ttotal: 1.05s\tremaining: 2.9s\n",
      "80:\tlearn: 0.0244797\ttotal: 1.07s\tremaining: 2.88s\n",
      "81:\tlearn: 0.0241580\ttotal: 1.08s\tremaining: 2.87s\n",
      "82:\tlearn: 0.0237616\ttotal: 1.09s\tremaining: 2.86s\n",
      "83:\tlearn: 0.0236895\ttotal: 1.11s\tremaining: 2.85s\n",
      "84:\tlearn: 0.0231945\ttotal: 1.12s\tremaining: 2.84s\n",
      "85:\tlearn: 0.0231317\ttotal: 1.14s\tremaining: 2.83s\n",
      "86:\tlearn: 0.0231317\ttotal: 1.14s\tremaining: 2.78s\n",
      "87:\tlearn: 0.0229151\ttotal: 1.15s\tremaining: 2.77s\n",
      "88:\tlearn: 0.0228742\ttotal: 1.16s\tremaining: 2.76s\n",
      "89:\tlearn: 0.0225458\ttotal: 1.18s\tremaining: 2.75s\n",
      "90:\tlearn: 0.0221272\ttotal: 1.19s\tremaining: 2.73s\n",
      "91:\tlearn: 0.0217041\ttotal: 1.2s\tremaining: 2.72s\n",
      "92:\tlearn: 0.0212364\ttotal: 1.22s\tremaining: 2.71s\n",
      "93:\tlearn: 0.0209232\ttotal: 1.23s\tremaining: 2.7s\n",
      "94:\tlearn: 0.0208629\ttotal: 1.25s\tremaining: 2.69s\n",
      "95:\tlearn: 0.0205906\ttotal: 1.26s\tremaining: 2.68s\n",
      "96:\tlearn: 0.0202270\ttotal: 1.27s\tremaining: 2.67s\n",
      "97:\tlearn: 0.0199189\ttotal: 1.3s\tremaining: 2.67s\n",
      "98:\tlearn: 0.0197846\ttotal: 1.32s\tremaining: 2.68s\n",
      "99:\tlearn: 0.0195206\ttotal: 1.34s\tremaining: 2.69s\n",
      "100:\tlearn: 0.0191401\ttotal: 1.37s\tremaining: 2.69s\n",
      "101:\tlearn: 0.0191072\ttotal: 1.39s\tremaining: 2.7s\n",
      "102:\tlearn: 0.0190280\ttotal: 1.42s\tremaining: 2.71s\n",
      "103:\tlearn: 0.0187612\ttotal: 1.44s\tremaining: 2.71s\n",
      "104:\tlearn: 0.0186070\ttotal: 1.45s\tremaining: 2.69s\n",
      "105:\tlearn: 0.0183023\ttotal: 1.47s\tremaining: 2.68s\n",
      "106:\tlearn: 0.0180028\ttotal: 1.48s\tremaining: 2.67s\n",
      "107:\tlearn: 0.0178267\ttotal: 1.49s\tremaining: 2.66s\n",
      "108:\tlearn: 0.0175909\ttotal: 1.51s\tremaining: 2.65s\n",
      "109:\tlearn: 0.0174404\ttotal: 1.52s\tremaining: 2.63s\n",
      "110:\tlearn: 0.0171605\ttotal: 1.54s\tremaining: 2.62s\n",
      "111:\tlearn: 0.0168897\ttotal: 1.55s\tremaining: 2.6s\n",
      "112:\tlearn: 0.0166870\ttotal: 1.56s\tremaining: 2.59s\n",
      "113:\tlearn: 0.0164056\ttotal: 1.58s\tremaining: 2.58s\n",
      "114:\tlearn: 0.0162976\ttotal: 1.59s\tremaining: 2.56s\n",
      "115:\tlearn: 0.0161433\ttotal: 1.61s\tremaining: 2.55s\n",
      "116:\tlearn: 0.0159277\ttotal: 1.62s\tremaining: 2.54s\n",
      "117:\tlearn: 0.0156513\ttotal: 1.64s\tremaining: 2.52s\n",
      "118:\tlearn: 0.0154214\ttotal: 1.65s\tremaining: 2.51s\n",
      "119:\tlearn: 0.0151944\ttotal: 1.66s\tremaining: 2.49s\n",
      "120:\tlearn: 0.0150708\ttotal: 1.68s\tremaining: 2.48s\n",
      "121:\tlearn: 0.0148360\ttotal: 1.69s\tremaining: 2.47s\n",
      "122:\tlearn: 0.0146230\ttotal: 1.7s\tremaining: 2.45s\n",
      "123:\tlearn: 0.0144185\ttotal: 1.72s\tremaining: 2.44s\n",
      "124:\tlearn: 0.0142973\ttotal: 1.73s\tremaining: 2.42s\n",
      "125:\tlearn: 0.0141304\ttotal: 1.75s\tremaining: 2.41s\n",
      "126:\tlearn: 0.0139049\ttotal: 1.76s\tremaining: 2.4s\n",
      "127:\tlearn: 0.0137167\ttotal: 1.77s\tremaining: 2.38s\n",
      "128:\tlearn: 0.0136032\ttotal: 1.79s\tremaining: 2.37s\n",
      "129:\tlearn: 0.0134323\ttotal: 1.8s\tremaining: 2.35s\n",
      "130:\tlearn: 0.0132631\ttotal: 1.81s\tremaining: 2.34s\n",
      "131:\tlearn: 0.0130589\ttotal: 1.83s\tremaining: 2.33s\n",
      "132:\tlearn: 0.0129163\ttotal: 1.84s\tremaining: 2.31s\n",
      "133:\tlearn: 0.0129159\ttotal: 1.85s\tremaining: 2.29s\n",
      "134:\tlearn: 0.0129157\ttotal: 1.86s\tremaining: 2.28s\n",
      "135:\tlearn: 0.0127694\ttotal: 1.88s\tremaining: 2.26s\n",
      "136:\tlearn: 0.0127320\ttotal: 1.89s\tremaining: 2.25s\n",
      "137:\tlearn: 0.0126492\ttotal: 1.9s\tremaining: 2.23s\n",
      "138:\tlearn: 0.0125796\ttotal: 1.92s\tremaining: 2.22s\n",
      "139:\tlearn: 0.0125793\ttotal: 1.92s\tremaining: 2.2s\n",
      "140:\tlearn: 0.0124441\ttotal: 1.94s\tremaining: 2.19s\n",
      "141:\tlearn: 0.0124441\ttotal: 1.95s\tremaining: 2.17s\n",
      "142:\tlearn: 0.0124159\ttotal: 1.97s\tremaining: 2.16s\n",
      "143:\tlearn: 0.0123005\ttotal: 1.98s\tremaining: 2.15s\n",
      "144:\tlearn: 0.0121514\ttotal: 2s\tremaining: 2.13s\n",
      "145:\tlearn: 0.0120868\ttotal: 2.01s\tremaining: 2.12s\n",
      "146:\tlearn: 0.0119570\ttotal: 2.02s\tremaining: 2.11s\n",
      "147:\tlearn: 0.0117924\ttotal: 2.04s\tremaining: 2.09s\n",
      "148:\tlearn: 0.0117779\ttotal: 2.05s\tremaining: 2.08s\n",
      "149:\tlearn: 0.0116864\ttotal: 2.07s\tremaining: 2.07s\n",
      "150:\tlearn: 0.0116081\ttotal: 2.08s\tremaining: 2.05s\n",
      "151:\tlearn: 0.0115728\ttotal: 2.1s\tremaining: 2.04s\n",
      "152:\tlearn: 0.0114502\ttotal: 2.11s\tremaining: 2.03s\n",
      "153:\tlearn: 0.0113680\ttotal: 2.12s\tremaining: 2.01s\n",
      "154:\tlearn: 0.0113024\ttotal: 2.14s\tremaining: 2s\n",
      "155:\tlearn: 0.0112508\ttotal: 2.15s\tremaining: 1.99s\n",
      "156:\tlearn: 0.0112159\ttotal: 2.16s\tremaining: 1.97s\n",
      "157:\tlearn: 0.0111747\ttotal: 2.18s\tremaining: 1.96s\n",
      "158:\tlearn: 0.0110732\ttotal: 2.19s\tremaining: 1.94s\n",
      "159:\tlearn: 0.0108929\ttotal: 2.2s\tremaining: 1.93s\n",
      "160:\tlearn: 0.0108033\ttotal: 2.22s\tremaining: 1.91s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.0106524\ttotal: 2.23s\tremaining: 1.9s\n",
      "162:\tlearn: 0.0106524\ttotal: 2.24s\tremaining: 1.88s\n",
      "163:\tlearn: 0.0105518\ttotal: 2.25s\tremaining: 1.86s\n",
      "164:\tlearn: 0.0104693\ttotal: 2.27s\tremaining: 1.85s\n",
      "165:\tlearn: 0.0103839\ttotal: 2.29s\tremaining: 1.85s\n",
      "166:\tlearn: 0.0103409\ttotal: 2.31s\tremaining: 1.84s\n",
      "167:\tlearn: 0.0103408\ttotal: 2.33s\tremaining: 1.83s\n",
      "168:\tlearn: 0.0103399\ttotal: 2.36s\tremaining: 1.83s\n",
      "169:\tlearn: 0.0102971\ttotal: 2.38s\tremaining: 1.82s\n",
      "170:\tlearn: 0.0102971\ttotal: 2.39s\tremaining: 1.8s\n",
      "171:\tlearn: 0.0102307\ttotal: 2.4s\tremaining: 1.79s\n",
      "172:\tlearn: 0.0102307\ttotal: 2.42s\tremaining: 1.77s\n",
      "173:\tlearn: 0.0101831\ttotal: 2.43s\tremaining: 1.76s\n",
      "174:\tlearn: 0.0101831\ttotal: 2.43s\tremaining: 1.74s\n",
      "175:\tlearn: 0.0101234\ttotal: 2.44s\tremaining: 1.72s\n",
      "176:\tlearn: 0.0101234\ttotal: 2.44s\tremaining: 1.7s\n",
      "177:\tlearn: 0.0101051\ttotal: 2.46s\tremaining: 1.69s\n",
      "178:\tlearn: 0.0100786\ttotal: 2.47s\tremaining: 1.67s\n",
      "179:\tlearn: 0.0100786\ttotal: 2.47s\tremaining: 1.65s\n",
      "180:\tlearn: 0.0100785\ttotal: 2.49s\tremaining: 1.63s\n",
      "181:\tlearn: 0.0100652\ttotal: 2.5s\tremaining: 1.62s\n",
      "182:\tlearn: 0.0100652\ttotal: 2.5s\tremaining: 1.6s\n",
      "183:\tlearn: 0.0100652\ttotal: 2.5s\tremaining: 1.58s\n",
      "184:\tlearn: 0.0100652\ttotal: 2.5s\tremaining: 1.56s\n",
      "185:\tlearn: 0.0100563\ttotal: 2.52s\tremaining: 1.54s\n",
      "186:\tlearn: 0.0100563\ttotal: 2.52s\tremaining: 1.52s\n",
      "187:\tlearn: 0.0100561\ttotal: 2.53s\tremaining: 1.51s\n",
      "188:\tlearn: 0.0100561\ttotal: 2.53s\tremaining: 1.49s\n",
      "189:\tlearn: 0.0100559\ttotal: 2.55s\tremaining: 1.47s\n",
      "190:\tlearn: 0.0100558\ttotal: 2.55s\tremaining: 1.45s\n",
      "191:\tlearn: 0.0100558\ttotal: 2.56s\tremaining: 1.44s\n",
      "192:\tlearn: 0.0100558\ttotal: 2.56s\tremaining: 1.42s\n",
      "193:\tlearn: 0.0100558\ttotal: 2.56s\tremaining: 1.4s\n",
      "194:\tlearn: 0.0100557\ttotal: 2.58s\tremaining: 1.39s\n",
      "195:\tlearn: 0.0100557\ttotal: 2.58s\tremaining: 1.37s\n",
      "196:\tlearn: 0.0100214\ttotal: 2.59s\tremaining: 1.35s\n",
      "197:\tlearn: 0.0100106\ttotal: 2.6s\tremaining: 1.34s\n",
      "198:\tlearn: 0.0099331\ttotal: 2.62s\tremaining: 1.33s\n",
      "199:\tlearn: 0.0099330\ttotal: 2.62s\tremaining: 1.31s\n",
      "200:\tlearn: 0.0099011\ttotal: 2.63s\tremaining: 1.3s\n",
      "201:\tlearn: 0.0099011\ttotal: 2.64s\tremaining: 1.28s\n",
      "202:\tlearn: 0.0099011\ttotal: 2.64s\tremaining: 1.26s\n",
      "203:\tlearn: 0.0098560\ttotal: 2.65s\tremaining: 1.25s\n",
      "204:\tlearn: 0.0098093\ttotal: 2.67s\tremaining: 1.24s\n",
      "205:\tlearn: 0.0098093\ttotal: 2.67s\tremaining: 1.22s\n",
      "206:\tlearn: 0.0098093\ttotal: 2.68s\tremaining: 1.2s\n",
      "207:\tlearn: 0.0098093\ttotal: 2.68s\tremaining: 1.19s\n",
      "208:\tlearn: 0.0098093\ttotal: 2.69s\tremaining: 1.17s\n",
      "209:\tlearn: 0.0098093\ttotal: 2.69s\tremaining: 1.15s\n",
      "210:\tlearn: 0.0097353\ttotal: 2.7s\tremaining: 1.14s\n",
      "211:\tlearn: 0.0096264\ttotal: 2.71s\tremaining: 1.13s\n",
      "212:\tlearn: 0.0095518\ttotal: 2.73s\tremaining: 1.11s\n",
      "213:\tlearn: 0.0094672\ttotal: 2.74s\tremaining: 1.1s\n",
      "214:\tlearn: 0.0094311\ttotal: 2.75s\tremaining: 1.09s\n",
      "215:\tlearn: 0.0093516\ttotal: 2.77s\tremaining: 1.08s\n",
      "216:\tlearn: 0.0092478\ttotal: 2.78s\tremaining: 1.06s\n",
      "217:\tlearn: 0.0091494\ttotal: 2.79s\tremaining: 1.05s\n",
      "218:\tlearn: 0.0090803\ttotal: 2.81s\tremaining: 1.04s\n",
      "219:\tlearn: 0.0090525\ttotal: 2.82s\tremaining: 1.03s\n",
      "220:\tlearn: 0.0089902\ttotal: 2.84s\tremaining: 1.01s\n",
      "221:\tlearn: 0.0088891\ttotal: 2.85s\tremaining: 1s\n",
      "222:\tlearn: 0.0088376\ttotal: 2.87s\tremaining: 989ms\n",
      "223:\tlearn: 0.0088376\ttotal: 2.88s\tremaining: 977ms\n",
      "224:\tlearn: 0.0088092\ttotal: 2.89s\tremaining: 964ms\n",
      "225:\tlearn: 0.0087754\ttotal: 2.91s\tremaining: 952ms\n",
      "226:\tlearn: 0.0087754\ttotal: 2.92s\tremaining: 939ms\n",
      "227:\tlearn: 0.0087179\ttotal: 2.93s\tremaining: 927ms\n",
      "228:\tlearn: 0.0086404\ttotal: 2.95s\tremaining: 914ms\n",
      "229:\tlearn: 0.0086404\ttotal: 2.96s\tremaining: 901ms\n",
      "230:\tlearn: 0.0085773\ttotal: 2.98s\tremaining: 889ms\n",
      "231:\tlearn: 0.0084927\ttotal: 2.99s\tremaining: 876ms\n",
      "232:\tlearn: 0.0084927\ttotal: 3s\tremaining: 864ms\n",
      "233:\tlearn: 0.0083973\ttotal: 3.02s\tremaining: 851ms\n",
      "234:\tlearn: 0.0083084\ttotal: 3.03s\tremaining: 838ms\n",
      "235:\tlearn: 0.0082418\ttotal: 3.04s\tremaining: 826ms\n",
      "236:\tlearn: 0.0082018\ttotal: 3.06s\tremaining: 813ms\n",
      "237:\tlearn: 0.0082018\ttotal: 3.07s\tremaining: 800ms\n",
      "238:\tlearn: 0.0080859\ttotal: 3.08s\tremaining: 787ms\n",
      "239:\tlearn: 0.0080848\ttotal: 3.1s\tremaining: 774ms\n",
      "240:\tlearn: 0.0080383\ttotal: 3.11s\tremaining: 762ms\n",
      "241:\tlearn: 0.0080383\ttotal: 3.12s\tremaining: 749ms\n",
      "242:\tlearn: 0.0080382\ttotal: 3.14s\tremaining: 736ms\n",
      "243:\tlearn: 0.0080381\ttotal: 3.15s\tremaining: 723ms\n",
      "244:\tlearn: 0.0080381\ttotal: 3.16s\tremaining: 710ms\n",
      "245:\tlearn: 0.0080381\ttotal: 3.17s\tremaining: 697ms\n",
      "246:\tlearn: 0.0080381\ttotal: 3.19s\tremaining: 684ms\n",
      "247:\tlearn: 0.0080381\ttotal: 3.2s\tremaining: 671ms\n",
      "248:\tlearn: 0.0080381\ttotal: 3.21s\tremaining: 658ms\n",
      "249:\tlearn: 0.0079970\ttotal: 3.23s\tremaining: 645ms\n",
      "250:\tlearn: 0.0079970\ttotal: 3.24s\tremaining: 632ms\n",
      "251:\tlearn: 0.0079969\ttotal: 3.26s\tremaining: 621ms\n",
      "252:\tlearn: 0.0079213\ttotal: 3.29s\tremaining: 610ms\n",
      "253:\tlearn: 0.0078707\ttotal: 3.31s\tremaining: 599ms\n",
      "254:\tlearn: 0.0078151\ttotal: 3.33s\tremaining: 588ms\n",
      "255:\tlearn: 0.0077117\ttotal: 3.35s\tremaining: 576ms\n",
      "256:\tlearn: 0.0076573\ttotal: 3.37s\tremaining: 564ms\n",
      "257:\tlearn: 0.0076233\ttotal: 3.39s\tremaining: 552ms\n",
      "258:\tlearn: 0.0076232\ttotal: 3.4s\tremaining: 539ms\n",
      "259:\tlearn: 0.0075733\ttotal: 3.42s\tremaining: 526ms\n",
      "260:\tlearn: 0.0074787\ttotal: 3.43s\tremaining: 513ms\n",
      "261:\tlearn: 0.0074079\ttotal: 3.45s\tremaining: 500ms\n",
      "262:\tlearn: 0.0073688\ttotal: 3.46s\tremaining: 487ms\n",
      "263:\tlearn: 0.0072982\ttotal: 3.48s\tremaining: 474ms\n",
      "264:\tlearn: 0.0072419\ttotal: 3.49s\tremaining: 461ms\n",
      "265:\tlearn: 0.0071884\ttotal: 3.51s\tremaining: 448ms\n",
      "266:\tlearn: 0.0071167\ttotal: 3.52s\tremaining: 435ms\n",
      "267:\tlearn: 0.0070848\ttotal: 3.54s\tremaining: 422ms\n",
      "268:\tlearn: 0.0070454\ttotal: 3.55s\tremaining: 409ms\n",
      "269:\tlearn: 0.0070075\ttotal: 3.56s\tremaining: 396ms\n",
      "270:\tlearn: 0.0069558\ttotal: 3.58s\tremaining: 383ms\n",
      "271:\tlearn: 0.0069558\ttotal: 3.59s\tremaining: 369ms\n",
      "272:\tlearn: 0.0069254\ttotal: 3.6s\tremaining: 356ms\n",
      "273:\tlearn: 0.0068854\ttotal: 3.61s\tremaining: 343ms\n",
      "274:\tlearn: 0.0068854\ttotal: 3.63s\tremaining: 330ms\n",
      "275:\tlearn: 0.0068853\ttotal: 3.64s\tremaining: 317ms\n",
      "276:\tlearn: 0.0068853\ttotal: 3.65s\tremaining: 303ms\n",
      "277:\tlearn: 0.0068298\ttotal: 3.67s\tremaining: 290ms\n",
      "278:\tlearn: 0.0068076\ttotal: 3.69s\tremaining: 278ms\n",
      "279:\tlearn: 0.0068076\ttotal: 3.75s\tremaining: 268ms\n",
      "280:\tlearn: 0.0068076\ttotal: 3.77s\tremaining: 255ms\n",
      "281:\tlearn: 0.0067854\ttotal: 3.83s\tremaining: 244ms\n",
      "282:\tlearn: 0.0067854\ttotal: 3.85s\tremaining: 231ms\n",
      "283:\tlearn: 0.0067854\ttotal: 3.91s\tremaining: 220ms\n",
      "284:\tlearn: 0.0067305\ttotal: 3.93s\tremaining: 207ms\n",
      "285:\tlearn: 0.0067305\ttotal: 3.95s\tremaining: 193ms\n",
      "286:\tlearn: 0.0066803\ttotal: 4s\tremaining: 181ms\n",
      "287:\tlearn: 0.0066373\ttotal: 4.03s\tremaining: 168ms\n",
      "288:\tlearn: 0.0066373\ttotal: 4.08s\tremaining: 155ms\n",
      "289:\tlearn: 0.0065985\ttotal: 4.11s\tremaining: 142ms\n",
      "290:\tlearn: 0.0065985\ttotal: 4.16s\tremaining: 129ms\n",
      "291:\tlearn: 0.0065981\ttotal: 4.18s\tremaining: 115ms\n",
      "292:\tlearn: 0.0065726\ttotal: 4.23s\tremaining: 101ms\n",
      "293:\tlearn: 0.0065373\ttotal: 4.27s\tremaining: 87.1ms\n",
      "294:\tlearn: 0.0065373\ttotal: 4.3s\tremaining: 72.9ms\n",
      "295:\tlearn: 0.0065373\ttotal: 4.35s\tremaining: 58.8ms\n",
      "296:\tlearn: 0.0065372\ttotal: 4.38s\tremaining: 44.3ms\n",
      "297:\tlearn: 0.0065370\ttotal: 4.42s\tremaining: 29.7ms\n",
      "298:\tlearn: 0.0065137\ttotal: 4.49s\tremaining: 15ms\n",
      "299:\tlearn: 0.0065137\ttotal: 4.52s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29160f8cfd94ba1a2b05b02af784de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5540653332574478, Recall = 0.6907284768211921, Aging Rate = 0.45778145695364236, Precision = 0.7544303797468355, f1 = 0.7211754537597235\n",
      "Epoch 2: Train Loss = 0.35855193469698066, Recall = 0.854635761589404, Aging Rate = 0.5039735099337749, Precision = 0.8478975032851511, f1 = 0.8512532981530344\n",
      "Epoch 3: Train Loss = 0.2793860390683673, Recall = 0.8960264900662251, Aging Rate = 0.5029801324503311, Precision = 0.8907175773535221, f1 = 0.8933641465830306\n",
      "Epoch 4: Train Loss = 0.22873626037543973, Recall = 0.916887417218543, Aging Rate = 0.49850993377483444, Precision = 0.919628030554633, f1 = 0.9182556789918753\n",
      "Epoch 5: Train Loss = 0.18930852425019473, Recall = 0.9374172185430464, Aging Rate = 0.4996688741721854, Precision = 0.9380384360503645, f1 = 0.9377277244120571\n",
      "Test Loss = 0.15364158451557158, Recall = 0.964569536423841, Aging Rate = 0.5120860927152318, precision = 0.94180407371484\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14186548663685652, Recall = 0.9516556291390729, Aging Rate = 0.4960264900662252, Precision = 0.959279038718291, f1 = 0.9554521276595745\n",
      "Epoch 7: Train Loss = 0.11408173216889236, Recall = 0.9602649006622517, Aging Rate = 0.4947019867549669, Precision = 0.9705488621151271, f1 = 0.9653794940079894\n",
      "Epoch 8: Train Loss = 0.09604950526494853, Recall = 0.9708609271523179, Aging Rate = 0.49768211920529803, Precision = 0.9753825681969395, f1 = 0.9731164951875207\n",
      "Epoch 9: Train Loss = 0.081289694729625, Recall = 0.9741721854304636, Aging Rate = 0.4968543046357616, Precision = 0.9803398867044318, f1 = 0.9772463046005647\n",
      "Epoch 10: Train Loss = 0.06896327616560538, Recall = 0.9824503311258278, Aging Rate = 0.49850993377483444, Precision = 0.9853869146462969, f1 = 0.9839164317691924\n",
      "Test Loss = 0.05995802769795159, Recall = 0.9788079470198675, Aging Rate = 0.49337748344370863, precision = 0.9919463087248322\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06005226863634507, Recall = 0.9834437086092715, Aging Rate = 0.49784768211920527, Precision = 0.9876953774526106, f1 = 0.9855649576903933\n",
      "Epoch 12: Train Loss = 0.052726297369165134, Recall = 0.9887417218543046, Aging Rate = 0.5006622516556292, Precision = 0.9874338624338624, f1 = 0.9880873593646592\n",
      "Epoch 13: Train Loss = 0.0457694073830614, Recall = 0.9910596026490066, Aging Rate = 0.5008278145695364, Precision = 0.9894214876033058, f1 = 0.9902398676592225\n",
      "Epoch 14: Train Loss = 0.04051818734540647, Recall = 0.9923841059602649, Aging Rate = 0.5001655629139072, Precision = 0.9920556107249255, f1 = 0.9922198311537825\n",
      "Epoch 15: Train Loss = 0.036422298439093774, Recall = 0.993046357615894, Aging Rate = 0.5006622516556292, Precision = 0.9917328042328042, f1 = 0.9923891462607545\n",
      "Test Loss = 0.03062849851534856, Recall = 0.9956953642384105, Aging Rate = 0.501158940397351, precision = 0.9933927981499835\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03283833023407404, Recall = 0.9947019867549669, Aging Rate = 0.5001655629139072, Precision = 0.9943727242634889, f1 = 0.9945373282569112\n",
      "Epoch 17: Train Loss = 0.029461615145305136, Recall = 0.9953642384105961, Aging Rate = 0.5009933774834437, Precision = 0.9933906146728354, f1 = 0.9943764472378431\n",
      "Epoch 18: Train Loss = 0.027404822423955463, Recall = 0.9943708609271523, Aging Rate = 0.5003311258278146, Precision = 0.9937127729980145, f1 = 0.9940417080436942\n",
      "Epoch 19: Train Loss = 0.024820178137808447, Recall = 0.9943708609271523, Aging Rate = 0.4995033112582781, Precision = 0.9953596287703016, f1 = 0.994864999171774\n",
      "Epoch 20: Train Loss = 0.021820466019928653, Recall = 0.9960264900662251, Aging Rate = 0.5001655629139072, Precision = 0.995696789142668, f1 = 0.9958616123158417\n",
      "Test Loss = 0.018015551044056746, Recall = 0.9986754966887417, Aging Rate = 0.5009933774834437, precision = 0.9966953073364178\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.019989606513586264, Recall = 0.9966887417218543, Aging Rate = 0.5001655629139072, Precision = 0.9963588215822575, f1 = 0.996523754345307\n",
      "Epoch 22: Train Loss = 0.018744961380810533, Recall = 0.9973509933774835, Aging Rate = 0.5, Precision = 0.9973509933774835, f1 = 0.9973509933774835\n",
      "Epoch 23: Train Loss = 0.017762330537723588, Recall = 0.9973509933774835, Aging Rate = 0.5, Precision = 0.9973509933774835, f1 = 0.9973509933774835\n",
      "Epoch 24: Train Loss = 0.01567334703644676, Recall = 0.9980132450331126, Aging Rate = 0.5004966887417218, Precision = 0.9970228250082699, f1 = 0.9975177891775608\n",
      "Epoch 25: Train Loss = 0.01506372397728511, Recall = 0.9966887417218543, Aging Rate = 0.49933774834437084, Precision = 0.9980106100795756, f1 = 0.9973492379058979\n",
      "Test Loss = 0.01240138075505661, Recall = 1.0, Aging Rate = 0.501158940397351, precision = 0.9976874793524942\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.015080159710140418, Recall = 0.9983443708609272, Aging Rate = 0.5006622516556292, Precision = 0.9970238095238095, f1 = 0.9976836532097948\n",
      "Epoch 27: Train Loss = 0.013174128096353337, Recall = 0.9983443708609272, Aging Rate = 0.5004966887417218, Precision = 0.9973536222295732, f1 = 0.9978487506205528\n",
      "Epoch 28: Train Loss = 0.011480175249204532, Recall = 0.9986754966887417, Aging Rate = 0.5003311258278146, Precision = 0.9980145598941098, f1 = 0.9983449189010262\n",
      "Epoch 29: Train Loss = 0.011069794941619531, Recall = 0.9986754966887417, Aging Rate = 0.5001655629139072, Precision = 0.9983449189010262, f1 = 0.998510180433703\n",
      "Epoch 30: Train Loss = 0.010711706077544302, Recall = 0.9990066225165563, Aging Rate = 0.5008278145695364, Precision = 0.9973553719008265, f1 = 0.9981803143093465\n",
      "Test Loss = 0.008452466849410376, Recall = 0.9990066225165563, Aging Rate = 0.4996688741721854, precision = 0.9996686547382373\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.009706970830355457, Recall = 0.9983443708609272, Aging Rate = 0.5, Precision = 0.9983443708609272, f1 = 0.9983443708609272\n",
      "Epoch 32: Train Loss = 0.010048462935285458, Recall = 0.9986754966887417, Aging Rate = 0.5004966887417218, Precision = 0.9976844194508766, f1 = 0.9981797120635445\n",
      "Epoch 33: Train Loss = 0.009640632274973866, Recall = 0.9980132450331126, Aging Rate = 0.4995033112582781, Precision = 0.9990056347364932, f1 = 0.9985091933079344\n",
      "Epoch 34: Train Loss = 0.008831337942057196, Recall = 0.9986754966887417, Aging Rate = 0.5001655629139072, Precision = 0.9983449189010262, f1 = 0.998510180433703\n",
      "Epoch 35: Train Loss = 0.008215292313734426, Recall = 0.9983443708609272, Aging Rate = 0.5, Precision = 0.9983443708609272, f1 = 0.9983443708609272\n",
      "Test Loss = 0.007672708857779866, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, precision = 0.9986759351208209\n",
      "\n",
      "Epoch 36: Train Loss = 0.00866904000979879, Recall = 0.9986754966887417, Aging Rate = 0.5001655629139072, Precision = 0.9983449189010262, f1 = 0.998510180433703\n",
      "Epoch 37: Train Loss = 0.0076987502393343595, Recall = 0.9983443708609272, Aging Rate = 0.4996688741721854, Precision = 0.9990059642147118, f1 = 0.998675057966214\n",
      "Epoch 38: Train Loss = 0.007208486970218009, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 39: Train Loss = 0.006888195799527974, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Epoch 40: Train Loss = 0.007061861967359553, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Test Loss = 0.005622321896760768, Recall = 0.9990066225165563, Aging Rate = 0.5, precision = 0.9990066225165563\n",
      "\n",
      "Epoch 41: Train Loss = 0.0072496803680940575, Recall = 0.9983443708609272, Aging Rate = 0.5001655629139072, Precision = 0.9980139026812314, f1 = 0.9981791094189705\n",
      "Epoch 42: Train Loss = 0.007201649624367078, Recall = 0.9983443708609272, Aging Rate = 0.4998344370860927, Precision = 0.998675057966214, f1 = 0.9985096870342773\n",
      "Epoch 43: Train Loss = 0.006678493698713499, Recall = 0.9990066225165563, Aging Rate = 0.5004966887417218, Precision = 0.99801521667218, f1 = 0.9985106735065364\n",
      "Epoch 44: Train Loss = 0.00628831725080331, Recall = 0.9983443708609272, Aging Rate = 0.4996688741721854, Precision = 0.9990059642147118, f1 = 0.998675057966214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.006351865075115386, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Test Loss = 0.0049732628424917135, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, precision = 0.9986768111147867\n",
      "\n",
      "Epoch 46: Train Loss = 0.0060540122035520755, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 47: Train Loss = 0.006717956369274006, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 48: Train Loss = 0.005453770995658163, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 49: Train Loss = 0.004895411506649847, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 50: Train Loss = 0.005582886290989372, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Test Loss = 0.004895973863351523, Recall = 0.9990066225165563, Aging Rate = 0.4995033112582781, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.004572514755441653, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 52: Train Loss = 0.006346792336522497, Recall = 0.9990066225165563, Aging Rate = 0.5001655629139072, Precision = 0.9986759351208209, f1 = 0.9988412514484356\n",
      "Epoch 53: Train Loss = 0.0054573330306602235, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 54: Train Loss = 0.004815639198344472, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 55: Train Loss = 0.0048697341298252735, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Test Loss = 0.004337088667343537, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, precision = 0.999007279947055\n",
      "\n",
      "Epoch 56: Train Loss = 0.00455387336177295, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 57: Train Loss = 0.0050573281475052925, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 58: Train Loss = 0.003836419760474676, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 59: Train Loss = 0.005041567613725359, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 60: Train Loss = 0.005173625301150297, Recall = 0.9986754966887417, Aging Rate = 0.4996688741721854, Precision = 0.9993373094764745, f1 = 0.9990062934746605\n",
      "Test Loss = 0.0037758113486408597, Recall = 0.9996688741721854, Aging Rate = 0.5004966887417218, precision = 0.9986768111147867\n",
      "\n",
      "Epoch 61: Train Loss = 0.005241384462219052, Recall = 0.9996688741721854, Aging Rate = 0.5006622516556292, Precision = 0.9983465608465608, f1 = 0.999007279947055\n",
      "Epoch 62: Train Loss = 0.004947124534619614, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 63: Train Loss = 0.004756142018942643, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 64: Train Loss = 0.004820873594491292, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 65: Train Loss = 0.004916434324753551, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Test Loss = 0.004191064399309358, Recall = 0.9986754966887417, Aging Rate = 0.4995033112582781, precision = 0.9996685449121644\n",
      "\n",
      "Epoch 66: Train Loss = 0.004561034099789435, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 67: Train Loss = 0.00530145154269073, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 68: Train Loss = 0.00400652418943085, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 69: Train Loss = 0.005189702516908924, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Epoch 70: Train Loss = 0.003791560987257306, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Test Loss = 0.003439015654125405, Recall = 0.9990066225165563, Aging Rate = 0.4995033112582781, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0035430204249696424, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 72: Train Loss = 0.0038445153628284766, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 73: Train Loss = 0.0044295673813695546, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 74: Train Loss = 0.004793052445320006, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 75: Train Loss = 0.004240500613960703, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Test Loss = 0.004611723757979602, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, precision = 0.999007279947055\n",
      "\n",
      "Epoch 76: Train Loss = 0.0040593386388735365, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 77: Train Loss = 0.004646233505945628, Recall = 0.9993377483443708, Aging Rate = 0.5001655629139072, Precision = 0.9990069513406157, f1 = 0.9991723224631683\n",
      "Epoch 78: Train Loss = 0.0052389474822337806, Recall = 0.9993377483443708, Aging Rate = 0.5004966887417218, Precision = 0.9983460138934833, f1 = 0.9988416349495285\n",
      "Epoch 79: Train Loss = 0.0035455312194389026, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.004006951666338406, Recall = 0.9990066225165563, Aging Rate = 0.5, Precision = 0.9990066225165563, f1 = 0.9990066225165563\n",
      "Test Loss = 0.0032965821133840163, Recall = 1.0, Aging Rate = 0.5004966887417218, precision = 0.9990076083360899\n",
      "\n",
      "Epoch 81: Train Loss = 0.003459821165915554, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 82: Train Loss = 0.004550334657943308, Recall = 0.9990066225165563, Aging Rate = 0.4996688741721854, Precision = 0.9996686547382373, f1 = 0.999337528983107\n",
      "Epoch 83: Train Loss = 0.004500975636595749, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 84: Train Loss = 0.003736374770223305, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 85: Train Loss = 0.004628026795061613, Recall = 0.9993377483443708, Aging Rate = 0.5003311258278146, Precision = 0.99867637326274, f1 = 0.9990069513406157\n",
      "Test Loss = 0.005078084174148886, Recall = 0.9983443708609272, Aging Rate = 0.4991721854304636, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.004051952494542725, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.005011052961942326, Recall = 0.9990066225165563, Aging Rate = 0.4998344370860927, Precision = 0.999337528983107, f1 = 0.9991720483523762\n",
      "Epoch 88: Train Loss = 0.00452991236025924, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 89: Train Loss = 0.0031467531014400356, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0038436597659536297, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Test Loss = 0.0037470922981148328, Recall = 1.0, Aging Rate = 0.5006622516556292, precision = 0.9986772486772487\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss = 0.004510003812179364, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Epoch 92: Train Loss = 0.0038796419604675263, Recall = 0.9993377483443708, Aging Rate = 0.4996688741721854, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.0030216642578335985, Recall = 1.0, Aging Rate = 0.5003311258278146, Precision = 0.99933818663137, f1 = 0.9996689837802052\n",
      "Epoch 94: Train Loss = 0.0033509928130217833, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.003995280471377519, Recall = 0.9996688741721854, Aging Rate = 0.5003311258278146, Precision = 0.999007279947055, f1 = 0.9993379675604105\n",
      "Test Loss = 0.0037785936581797353, Recall = 0.9990066225165563, Aging Rate = 0.4996688741721854, precision = 0.9996686547382373\n",
      "\n",
      "Epoch 96: Train Loss = 0.004261502646378532, Recall = 0.9993377483443708, Aging Rate = 0.4998344370860927, Precision = 0.9996687644915535, f1 = 0.9995032290114256\n",
      "Epoch 97: Train Loss = 0.0035770828317579447, Recall = 0.9996688741721854, Aging Rate = 0.5, Precision = 0.9996688741721854, f1 = 0.9996688741721854\n",
      "Epoch 98: Train Loss = 0.0040668606973987146, Recall = 0.9996688741721854, Aging Rate = 0.5001655629139072, Precision = 0.9993379675604105, f1 = 0.9995033934779011\n",
      "Epoch 99: Train Loss = 0.0038771712292133774, Recall = 0.9993377483443708, Aging Rate = 0.5, Precision = 0.9993377483443708, f1 = 0.9993377483443708\n",
      "Epoch 100: Train Loss = 0.003350158387622395, Recall = 0.9996688741721854, Aging Rate = 0.4998344370860927, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00528897275247716, Recall = 1.0, Aging Rate = 0.5008278145695364, precision = 0.9983471074380166\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5337040\ttotal: 15.1ms\tremaining: 4.51s\n",
      "1:\tlearn: 0.4519037\ttotal: 30.3ms\tremaining: 4.51s\n",
      "2:\tlearn: 0.3909872\ttotal: 45.5ms\tremaining: 4.5s\n",
      "3:\tlearn: 0.3536295\ttotal: 60.5ms\tremaining: 4.47s\n",
      "4:\tlearn: 0.3119841\ttotal: 74.1ms\tremaining: 4.37s\n",
      "5:\tlearn: 0.2844935\ttotal: 87.5ms\tremaining: 4.29s\n",
      "6:\tlearn: 0.2614069\ttotal: 101ms\tremaining: 4.23s\n",
      "7:\tlearn: 0.2435450\ttotal: 114ms\tremaining: 4.17s\n",
      "8:\tlearn: 0.2209102\ttotal: 127ms\tremaining: 4.1s\n",
      "9:\tlearn: 0.2052129\ttotal: 140ms\tremaining: 4.07s\n",
      "10:\tlearn: 0.1924028\ttotal: 153ms\tremaining: 4.03s\n",
      "11:\tlearn: 0.1784397\ttotal: 167ms\tremaining: 4s\n",
      "12:\tlearn: 0.1687510\ttotal: 180ms\tremaining: 3.96s\n",
      "13:\tlearn: 0.1603517\ttotal: 193ms\tremaining: 3.94s\n",
      "14:\tlearn: 0.1538450\ttotal: 206ms\tremaining: 3.91s\n",
      "15:\tlearn: 0.1441335\ttotal: 219ms\tremaining: 3.88s\n",
      "16:\tlearn: 0.1312893\ttotal: 232ms\tremaining: 3.86s\n",
      "17:\tlearn: 0.1243180\ttotal: 245ms\tremaining: 3.83s\n",
      "18:\tlearn: 0.1186825\ttotal: 257ms\tremaining: 3.8s\n",
      "19:\tlearn: 0.1152540\ttotal: 270ms\tremaining: 3.78s\n",
      "20:\tlearn: 0.1102248\ttotal: 287ms\tremaining: 3.82s\n",
      "21:\tlearn: 0.1056552\ttotal: 307ms\tremaining: 3.87s\n",
      "22:\tlearn: 0.0996130\ttotal: 355ms\tremaining: 4.28s\n",
      "23:\tlearn: 0.0963590\ttotal: 378ms\tremaining: 4.34s\n",
      "24:\tlearn: 0.0913567\ttotal: 428ms\tremaining: 4.71s\n",
      "25:\tlearn: 0.0891058\ttotal: 463ms\tremaining: 4.88s\n",
      "26:\tlearn: 0.0867430\ttotal: 508ms\tremaining: 5.13s\n",
      "27:\tlearn: 0.0833725\ttotal: 546ms\tremaining: 5.3s\n",
      "28:\tlearn: 0.0798197\ttotal: 603ms\tremaining: 5.64s\n",
      "29:\tlearn: 0.0771549\ttotal: 637ms\tremaining: 5.73s\n",
      "30:\tlearn: 0.0747438\ttotal: 670ms\tremaining: 5.81s\n",
      "31:\tlearn: 0.0727653\ttotal: 689ms\tremaining: 5.77s\n",
      "32:\tlearn: 0.0693334\ttotal: 702ms\tremaining: 5.68s\n",
      "33:\tlearn: 0.0662217\ttotal: 715ms\tremaining: 5.59s\n",
      "34:\tlearn: 0.0632291\ttotal: 728ms\tremaining: 5.51s\n",
      "35:\tlearn: 0.0598040\ttotal: 739ms\tremaining: 5.42s\n",
      "36:\tlearn: 0.0582761\ttotal: 752ms\tremaining: 5.34s\n",
      "37:\tlearn: 0.0561394\ttotal: 764ms\tremaining: 5.27s\n",
      "38:\tlearn: 0.0538701\ttotal: 776ms\tremaining: 5.19s\n",
      "39:\tlearn: 0.0516962\ttotal: 788ms\tremaining: 5.12s\n",
      "40:\tlearn: 0.0502682\ttotal: 800ms\tremaining: 5.05s\n",
      "41:\tlearn: 0.0485844\ttotal: 812ms\tremaining: 4.99s\n",
      "42:\tlearn: 0.0476391\ttotal: 826ms\tremaining: 4.93s\n",
      "43:\tlearn: 0.0469115\ttotal: 841ms\tremaining: 4.89s\n",
      "44:\tlearn: 0.0455762\ttotal: 856ms\tremaining: 4.85s\n",
      "45:\tlearn: 0.0442570\ttotal: 868ms\tremaining: 4.79s\n",
      "46:\tlearn: 0.0432890\ttotal: 880ms\tremaining: 4.74s\n",
      "47:\tlearn: 0.0416779\ttotal: 892ms\tremaining: 4.68s\n",
      "48:\tlearn: 0.0404158\ttotal: 904ms\tremaining: 4.63s\n",
      "49:\tlearn: 0.0391075\ttotal: 915ms\tremaining: 4.58s\n",
      "50:\tlearn: 0.0378770\ttotal: 928ms\tremaining: 4.53s\n",
      "51:\tlearn: 0.0370955\ttotal: 941ms\tremaining: 4.49s\n",
      "52:\tlearn: 0.0364001\ttotal: 954ms\tremaining: 4.45s\n",
      "53:\tlearn: 0.0354264\ttotal: 967ms\tremaining: 4.4s\n",
      "54:\tlearn: 0.0344798\ttotal: 979ms\tremaining: 4.36s\n",
      "55:\tlearn: 0.0335581\ttotal: 992ms\tremaining: 4.32s\n",
      "56:\tlearn: 0.0326372\ttotal: 1s\tremaining: 4.28s\n",
      "57:\tlearn: 0.0319892\ttotal: 1.03s\tremaining: 4.28s\n",
      "58:\tlearn: 0.0307638\ttotal: 1.05s\tremaining: 4.3s\n",
      "59:\tlearn: 0.0297861\ttotal: 1.09s\tremaining: 4.36s\n",
      "60:\tlearn: 0.0289411\ttotal: 1.11s\tremaining: 4.37s\n",
      "61:\tlearn: 0.0283364\ttotal: 1.16s\tremaining: 4.46s\n",
      "62:\tlearn: 0.0278787\ttotal: 1.19s\tremaining: 4.47s\n",
      "63:\tlearn: 0.0273220\ttotal: 1.24s\tremaining: 4.56s\n",
      "64:\tlearn: 0.0267536\ttotal: 1.26s\tremaining: 4.57s\n",
      "65:\tlearn: 0.0260083\ttotal: 1.31s\tremaining: 4.66s\n",
      "66:\tlearn: 0.0254368\ttotal: 1.35s\tremaining: 4.69s\n",
      "67:\tlearn: 0.0250927\ttotal: 1.38s\tremaining: 4.69s\n",
      "68:\tlearn: 0.0248706\ttotal: 1.39s\tremaining: 4.65s\n",
      "69:\tlearn: 0.0245095\ttotal: 1.4s\tremaining: 4.6s\n",
      "70:\tlearn: 0.0239498\ttotal: 1.41s\tremaining: 4.56s\n",
      "71:\tlearn: 0.0234974\ttotal: 1.43s\tremaining: 4.51s\n",
      "72:\tlearn: 0.0228705\ttotal: 1.44s\tremaining: 4.48s\n",
      "73:\tlearn: 0.0226191\ttotal: 1.46s\tremaining: 4.46s\n",
      "74:\tlearn: 0.0223788\ttotal: 1.48s\tremaining: 4.45s\n",
      "75:\tlearn: 0.0219490\ttotal: 1.5s\tremaining: 4.44s\n",
      "76:\tlearn: 0.0216572\ttotal: 1.52s\tremaining: 4.4s\n",
      "77:\tlearn: 0.0213607\ttotal: 1.53s\tremaining: 4.36s\n",
      "78:\tlearn: 0.0209640\ttotal: 1.54s\tremaining: 4.32s\n",
      "79:\tlearn: 0.0207384\ttotal: 1.56s\tremaining: 4.28s\n",
      "80:\tlearn: 0.0203367\ttotal: 1.57s\tremaining: 4.25s\n",
      "81:\tlearn: 0.0201118\ttotal: 1.59s\tremaining: 4.22s\n",
      "82:\tlearn: 0.0196967\ttotal: 1.6s\tremaining: 4.18s\n",
      "83:\tlearn: 0.0193820\ttotal: 1.61s\tremaining: 4.15s\n",
      "84:\tlearn: 0.0190388\ttotal: 1.63s\tremaining: 4.11s\n",
      "85:\tlearn: 0.0187224\ttotal: 1.64s\tremaining: 4.08s\n",
      "86:\tlearn: 0.0184584\ttotal: 1.65s\tremaining: 4.04s\n",
      "87:\tlearn: 0.0182282\ttotal: 1.66s\tremaining: 4s\n",
      "88:\tlearn: 0.0180234\ttotal: 1.68s\tremaining: 3.97s\n",
      "89:\tlearn: 0.0178212\ttotal: 1.69s\tremaining: 3.94s\n",
      "90:\tlearn: 0.0176006\ttotal: 1.7s\tremaining: 3.9s\n",
      "91:\tlearn: 0.0173608\ttotal: 1.71s\tremaining: 3.87s\n",
      "92:\tlearn: 0.0171091\ttotal: 1.73s\tremaining: 3.84s\n",
      "93:\tlearn: 0.0168568\ttotal: 1.75s\tremaining: 3.83s\n",
      "94:\tlearn: 0.0167333\ttotal: 1.77s\tremaining: 3.83s\n",
      "95:\tlearn: 0.0165006\ttotal: 1.81s\tremaining: 3.85s\n",
      "96:\tlearn: 0.0163234\ttotal: 1.83s\tremaining: 3.83s\n",
      "97:\tlearn: 0.0160852\ttotal: 1.89s\tremaining: 3.89s\n",
      "98:\tlearn: 0.0159745\ttotal: 1.91s\tremaining: 3.88s\n",
      "99:\tlearn: 0.0158391\ttotal: 1.96s\tremaining: 3.92s\n",
      "100:\tlearn: 0.0157401\ttotal: 1.98s\tremaining: 3.9s\n",
      "101:\tlearn: 0.0155678\ttotal: 2.02s\tremaining: 3.93s\n",
      "102:\tlearn: 0.0153773\ttotal: 2.05s\tremaining: 3.92s\n",
      "103:\tlearn: 0.0151945\ttotal: 2.09s\tremaining: 3.93s\n",
      "104:\tlearn: 0.0150605\ttotal: 2.1s\tremaining: 3.9s\n",
      "105:\tlearn: 0.0149483\ttotal: 2.11s\tremaining: 3.87s\n",
      "106:\tlearn: 0.0146097\ttotal: 2.13s\tremaining: 3.83s\n",
      "107:\tlearn: 0.0144042\ttotal: 2.14s\tremaining: 3.8s\n",
      "108:\tlearn: 0.0142783\ttotal: 2.15s\tremaining: 3.77s\n",
      "109:\tlearn: 0.0141618\ttotal: 2.16s\tremaining: 3.74s\n",
      "110:\tlearn: 0.0140622\ttotal: 2.17s\tremaining: 3.7s\n",
      "111:\tlearn: 0.0139391\ttotal: 2.19s\tremaining: 3.67s\n",
      "112:\tlearn: 0.0137910\ttotal: 2.2s\tremaining: 3.64s\n",
      "113:\tlearn: 0.0137233\ttotal: 2.21s\tremaining: 3.61s\n",
      "114:\tlearn: 0.0134889\ttotal: 2.22s\tremaining: 3.58s\n",
      "115:\tlearn: 0.0133147\ttotal: 2.24s\tremaining: 3.55s\n",
      "116:\tlearn: 0.0131175\ttotal: 2.25s\tremaining: 3.52s\n",
      "117:\tlearn: 0.0129697\ttotal: 2.26s\tremaining: 3.49s\n",
      "118:\tlearn: 0.0129173\ttotal: 2.27s\tremaining: 3.46s\n",
      "119:\tlearn: 0.0128219\ttotal: 2.29s\tremaining: 3.43s\n",
      "120:\tlearn: 0.0127149\ttotal: 2.3s\tremaining: 3.41s\n",
      "121:\tlearn: 0.0125756\ttotal: 2.32s\tremaining: 3.38s\n",
      "122:\tlearn: 0.0124392\ttotal: 2.33s\tremaining: 3.35s\n",
      "123:\tlearn: 0.0123422\ttotal: 2.34s\tremaining: 3.32s\n",
      "124:\tlearn: 0.0122088\ttotal: 2.35s\tremaining: 3.29s\n",
      "125:\tlearn: 0.0120957\ttotal: 2.37s\tremaining: 3.27s\n",
      "126:\tlearn: 0.0119831\ttotal: 2.38s\tremaining: 3.24s\n",
      "127:\tlearn: 0.0118529\ttotal: 2.39s\tremaining: 3.21s\n",
      "128:\tlearn: 0.0117777\ttotal: 2.4s\tremaining: 3.18s\n",
      "129:\tlearn: 0.0117150\ttotal: 2.42s\tremaining: 3.16s\n",
      "130:\tlearn: 0.0116370\ttotal: 2.44s\tremaining: 3.14s\n",
      "131:\tlearn: 0.0114983\ttotal: 2.46s\tremaining: 3.13s\n",
      "132:\tlearn: 0.0114172\ttotal: 2.48s\tremaining: 3.11s\n",
      "133:\tlearn: 0.0112852\ttotal: 2.5s\tremaining: 3.1s\n",
      "134:\tlearn: 0.0112463\ttotal: 2.52s\tremaining: 3.08s\n",
      "135:\tlearn: 0.0111789\ttotal: 2.53s\tremaining: 3.06s\n",
      "136:\tlearn: 0.0110699\ttotal: 2.55s\tremaining: 3.03s\n",
      "137:\tlearn: 0.0108926\ttotal: 2.56s\tremaining: 3.01s\n",
      "138:\tlearn: 0.0108158\ttotal: 2.57s\tremaining: 2.98s\n",
      "139:\tlearn: 0.0107518\ttotal: 2.59s\tremaining: 2.96s\n",
      "140:\tlearn: 0.0106226\ttotal: 2.6s\tremaining: 2.93s\n",
      "141:\tlearn: 0.0105716\ttotal: 2.61s\tremaining: 2.91s\n",
      "142:\tlearn: 0.0104404\ttotal: 2.63s\tremaining: 2.88s\n",
      "143:\tlearn: 0.0103792\ttotal: 2.64s\tremaining: 2.86s\n",
      "144:\tlearn: 0.0103125\ttotal: 2.65s\tremaining: 2.84s\n",
      "145:\tlearn: 0.0101515\ttotal: 2.67s\tremaining: 2.81s\n",
      "146:\tlearn: 0.0100278\ttotal: 2.68s\tremaining: 2.79s\n",
      "147:\tlearn: 0.0099873\ttotal: 2.7s\tremaining: 2.77s\n",
      "148:\tlearn: 0.0098960\ttotal: 2.74s\tremaining: 2.77s\n",
      "149:\tlearn: 0.0098061\ttotal: 2.76s\tremaining: 2.76s\n",
      "150:\tlearn: 0.0096821\ttotal: 2.79s\tremaining: 2.75s\n",
      "151:\tlearn: 0.0096162\ttotal: 2.84s\tremaining: 2.76s\n",
      "152:\tlearn: 0.0095633\ttotal: 2.86s\tremaining: 2.75s\n",
      "153:\tlearn: 0.0094720\ttotal: 2.9s\tremaining: 2.75s\n",
      "154:\tlearn: 0.0094720\ttotal: 2.93s\tremaining: 2.74s\n",
      "155:\tlearn: 0.0094720\ttotal: 2.96s\tremaining: 2.73s\n",
      "156:\tlearn: 0.0094100\ttotal: 3.01s\tremaining: 2.74s\n",
      "157:\tlearn: 0.0092916\ttotal: 3.04s\tremaining: 2.73s\n",
      "158:\tlearn: 0.0091844\ttotal: 3.06s\tremaining: 2.71s\n",
      "159:\tlearn: 0.0091523\ttotal: 3.07s\tremaining: 2.68s\n",
      "160:\tlearn: 0.0090588\ttotal: 3.08s\tremaining: 2.66s\n",
      "161:\tlearn: 0.0089934\ttotal: 3.09s\tremaining: 2.63s\n",
      "162:\tlearn: 0.0089934\ttotal: 3.1s\tremaining: 2.61s\n",
      "163:\tlearn: 0.0089196\ttotal: 3.12s\tremaining: 2.58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164:\tlearn: 0.0088912\ttotal: 3.13s\tremaining: 2.56s\n",
      "165:\tlearn: 0.0088448\ttotal: 3.14s\tremaining: 2.54s\n",
      "166:\tlearn: 0.0087920\ttotal: 3.15s\tremaining: 2.51s\n",
      "167:\tlearn: 0.0087708\ttotal: 3.17s\tremaining: 2.49s\n",
      "168:\tlearn: 0.0087708\ttotal: 3.18s\tremaining: 2.46s\n",
      "169:\tlearn: 0.0086954\ttotal: 3.19s\tremaining: 2.44s\n",
      "170:\tlearn: 0.0086513\ttotal: 3.2s\tremaining: 2.42s\n",
      "171:\tlearn: 0.0085691\ttotal: 3.21s\tremaining: 2.39s\n",
      "172:\tlearn: 0.0085329\ttotal: 3.23s\tremaining: 2.37s\n",
      "173:\tlearn: 0.0084550\ttotal: 3.24s\tremaining: 2.35s\n",
      "174:\tlearn: 0.0084082\ttotal: 3.26s\tremaining: 2.33s\n",
      "175:\tlearn: 0.0083261\ttotal: 3.27s\tremaining: 2.31s\n",
      "176:\tlearn: 0.0082893\ttotal: 3.28s\tremaining: 2.28s\n",
      "177:\tlearn: 0.0082349\ttotal: 3.3s\tremaining: 2.26s\n",
      "178:\tlearn: 0.0082295\ttotal: 3.31s\tremaining: 2.24s\n",
      "179:\tlearn: 0.0081946\ttotal: 3.32s\tremaining: 2.21s\n",
      "180:\tlearn: 0.0081479\ttotal: 3.33s\tremaining: 2.19s\n",
      "181:\tlearn: 0.0080774\ttotal: 3.35s\tremaining: 2.17s\n",
      "182:\tlearn: 0.0080285\ttotal: 3.36s\tremaining: 2.15s\n",
      "183:\tlearn: 0.0079960\ttotal: 3.37s\tremaining: 2.12s\n",
      "184:\tlearn: 0.0079593\ttotal: 3.38s\tremaining: 2.1s\n",
      "185:\tlearn: 0.0078874\ttotal: 3.4s\tremaining: 2.08s\n",
      "186:\tlearn: 0.0078181\ttotal: 3.41s\tremaining: 2.06s\n",
      "187:\tlearn: 0.0077667\ttotal: 3.44s\tremaining: 2.05s\n",
      "188:\tlearn: 0.0077121\ttotal: 3.47s\tremaining: 2.04s\n",
      "189:\tlearn: 0.0076648\ttotal: 3.5s\tremaining: 2.03s\n",
      "190:\tlearn: 0.0076092\ttotal: 3.55s\tremaining: 2.03s\n",
      "191:\tlearn: 0.0075400\ttotal: 3.58s\tremaining: 2.02s\n",
      "192:\tlearn: 0.0074429\ttotal: 3.63s\tremaining: 2.01s\n",
      "193:\tlearn: 0.0074035\ttotal: 3.65s\tremaining: 2s\n",
      "194:\tlearn: 0.0073714\ttotal: 3.69s\tremaining: 1.99s\n",
      "195:\tlearn: 0.0073258\ttotal: 3.72s\tremaining: 1.98s\n",
      "196:\tlearn: 0.0072508\ttotal: 3.75s\tremaining: 1.96s\n",
      "197:\tlearn: 0.0072238\ttotal: 3.79s\tremaining: 1.95s\n",
      "198:\tlearn: 0.0071763\ttotal: 3.82s\tremaining: 1.94s\n",
      "199:\tlearn: 0.0071227\ttotal: 3.87s\tremaining: 1.93s\n",
      "200:\tlearn: 0.0070880\ttotal: 3.88s\tremaining: 1.91s\n",
      "201:\tlearn: 0.0070556\ttotal: 3.89s\tremaining: 1.89s\n",
      "202:\tlearn: 0.0070158\ttotal: 3.9s\tremaining: 1.86s\n",
      "203:\tlearn: 0.0069758\ttotal: 3.92s\tremaining: 1.84s\n",
      "204:\tlearn: 0.0069380\ttotal: 3.93s\tremaining: 1.82s\n",
      "205:\tlearn: 0.0068919\ttotal: 3.94s\tremaining: 1.8s\n",
      "206:\tlearn: 0.0068513\ttotal: 3.95s\tremaining: 1.77s\n",
      "207:\tlearn: 0.0067974\ttotal: 3.96s\tremaining: 1.75s\n",
      "208:\tlearn: 0.0067643\ttotal: 3.98s\tremaining: 1.73s\n",
      "209:\tlearn: 0.0067642\ttotal: 3.99s\tremaining: 1.71s\n",
      "210:\tlearn: 0.0067642\ttotal: 4s\tremaining: 1.69s\n",
      "211:\tlearn: 0.0067176\ttotal: 4.01s\tremaining: 1.67s\n",
      "212:\tlearn: 0.0066591\ttotal: 4.03s\tremaining: 1.64s\n",
      "213:\tlearn: 0.0066058\ttotal: 4.04s\tremaining: 1.62s\n",
      "214:\tlearn: 0.0065675\ttotal: 4.05s\tremaining: 1.6s\n",
      "215:\tlearn: 0.0065672\ttotal: 4.06s\tremaining: 1.58s\n",
      "216:\tlearn: 0.0065625\ttotal: 4.07s\tremaining: 1.56s\n",
      "217:\tlearn: 0.0065414\ttotal: 4.09s\tremaining: 1.54s\n",
      "218:\tlearn: 0.0065413\ttotal: 4.1s\tremaining: 1.52s\n",
      "219:\tlearn: 0.0065413\ttotal: 4.12s\tremaining: 1.5s\n",
      "220:\tlearn: 0.0065054\ttotal: 4.13s\tremaining: 1.48s\n",
      "221:\tlearn: 0.0064603\ttotal: 4.14s\tremaining: 1.45s\n",
      "222:\tlearn: 0.0064170\ttotal: 4.15s\tremaining: 1.43s\n",
      "223:\tlearn: 0.0064105\ttotal: 4.16s\tremaining: 1.41s\n",
      "224:\tlearn: 0.0064105\ttotal: 4.18s\tremaining: 1.39s\n",
      "225:\tlearn: 0.0064104\ttotal: 4.19s\tremaining: 1.37s\n",
      "226:\tlearn: 0.0064104\ttotal: 4.2s\tremaining: 1.35s\n",
      "227:\tlearn: 0.0063745\ttotal: 4.21s\tremaining: 1.33s\n",
      "228:\tlearn: 0.0063451\ttotal: 4.23s\tremaining: 1.31s\n",
      "229:\tlearn: 0.0062972\ttotal: 4.24s\tremaining: 1.29s\n",
      "230:\tlearn: 0.0062724\ttotal: 4.25s\tremaining: 1.27s\n",
      "231:\tlearn: 0.0062723\ttotal: 4.27s\tremaining: 1.25s\n",
      "232:\tlearn: 0.0062365\ttotal: 4.29s\tremaining: 1.23s\n",
      "233:\tlearn: 0.0061974\ttotal: 4.34s\tremaining: 1.22s\n",
      "234:\tlearn: 0.0061974\ttotal: 4.36s\tremaining: 1.21s\n",
      "235:\tlearn: 0.0061580\ttotal: 4.42s\tremaining: 1.2s\n",
      "236:\tlearn: 0.0061394\ttotal: 4.45s\tremaining: 1.18s\n",
      "237:\tlearn: 0.0060978\ttotal: 4.48s\tremaining: 1.17s\n",
      "238:\tlearn: 0.0060549\ttotal: 4.54s\tremaining: 1.16s\n",
      "239:\tlearn: 0.0060142\ttotal: 4.57s\tremaining: 1.14s\n",
      "240:\tlearn: 0.0059700\ttotal: 4.59s\tremaining: 1.12s\n",
      "241:\tlearn: 0.0059700\ttotal: 4.63s\tremaining: 1.11s\n",
      "242:\tlearn: 0.0059700\ttotal: 4.66s\tremaining: 1.09s\n",
      "243:\tlearn: 0.0059309\ttotal: 4.69s\tremaining: 1.08s\n",
      "244:\tlearn: 0.0059015\ttotal: 4.71s\tremaining: 1.06s\n",
      "245:\tlearn: 0.0058669\ttotal: 4.72s\tremaining: 1.04s\n",
      "246:\tlearn: 0.0058669\ttotal: 4.73s\tremaining: 1.01s\n",
      "247:\tlearn: 0.0058390\ttotal: 4.74s\tremaining: 995ms\n",
      "248:\tlearn: 0.0058092\ttotal: 4.76s\tremaining: 974ms\n",
      "249:\tlearn: 0.0057738\ttotal: 4.77s\tremaining: 954ms\n",
      "250:\tlearn: 0.0057574\ttotal: 4.78s\tremaining: 933ms\n",
      "251:\tlearn: 0.0057113\ttotal: 4.79s\tremaining: 913ms\n",
      "252:\tlearn: 0.0057113\ttotal: 4.8s\tremaining: 893ms\n",
      "253:\tlearn: 0.0057112\ttotal: 4.82s\tremaining: 872ms\n",
      "254:\tlearn: 0.0057112\ttotal: 4.83s\tremaining: 852ms\n",
      "255:\tlearn: 0.0056794\ttotal: 4.84s\tremaining: 832ms\n",
      "256:\tlearn: 0.0056794\ttotal: 4.85s\tremaining: 812ms\n",
      "257:\tlearn: 0.0056794\ttotal: 4.86s\tremaining: 792ms\n",
      "258:\tlearn: 0.0056382\ttotal: 4.88s\tremaining: 772ms\n",
      "259:\tlearn: 0.0056033\ttotal: 4.89s\tremaining: 752ms\n",
      "260:\tlearn: 0.0056032\ttotal: 4.9s\tremaining: 732ms\n",
      "261:\tlearn: 0.0055733\ttotal: 4.91s\tremaining: 713ms\n",
      "262:\tlearn: 0.0055395\ttotal: 4.93s\tremaining: 694ms\n",
      "263:\tlearn: 0.0055394\ttotal: 4.94s\tremaining: 674ms\n",
      "264:\tlearn: 0.0055393\ttotal: 4.95s\tremaining: 654ms\n",
      "265:\tlearn: 0.0055137\ttotal: 4.97s\tremaining: 635ms\n",
      "266:\tlearn: 0.0055136\ttotal: 4.98s\tremaining: 615ms\n",
      "267:\tlearn: 0.0055136\ttotal: 4.99s\tremaining: 596ms\n",
      "268:\tlearn: 0.0055135\ttotal: 5s\tremaining: 576ms\n",
      "269:\tlearn: 0.0055135\ttotal: 5.01s\tremaining: 557ms\n",
      "270:\tlearn: 0.0055135\ttotal: 5.03s\tremaining: 538ms\n",
      "271:\tlearn: 0.0055134\ttotal: 5.04s\tremaining: 519ms\n",
      "272:\tlearn: 0.0055133\ttotal: 5.05s\tremaining: 500ms\n",
      "273:\tlearn: 0.0055133\ttotal: 5.06s\tremaining: 480ms\n",
      "274:\tlearn: 0.0054793\ttotal: 5.08s\tremaining: 461ms\n",
      "275:\tlearn: 0.0054792\ttotal: 5.09s\tremaining: 442ms\n",
      "276:\tlearn: 0.0054789\ttotal: 5.1s\tremaining: 424ms\n",
      "277:\tlearn: 0.0054789\ttotal: 5.11s\tremaining: 405ms\n",
      "278:\tlearn: 0.0054445\ttotal: 5.13s\tremaining: 386ms\n",
      "279:\tlearn: 0.0054000\ttotal: 5.14s\tremaining: 367ms\n",
      "280:\tlearn: 0.0053999\ttotal: 5.15s\tremaining: 348ms\n",
      "281:\tlearn: 0.0053821\ttotal: 5.16s\tremaining: 330ms\n",
      "282:\tlearn: 0.0053527\ttotal: 5.18s\tremaining: 311ms\n",
      "283:\tlearn: 0.0053147\ttotal: 5.19s\tremaining: 292ms\n",
      "284:\tlearn: 0.0052922\ttotal: 5.2s\tremaining: 274ms\n",
      "285:\tlearn: 0.0052677\ttotal: 5.21s\tremaining: 255ms\n",
      "286:\tlearn: 0.0052422\ttotal: 5.23s\tremaining: 237ms\n",
      "287:\tlearn: 0.0052421\ttotal: 5.24s\tremaining: 218ms\n",
      "288:\tlearn: 0.0052421\ttotal: 5.25s\tremaining: 200ms\n",
      "289:\tlearn: 0.0052420\ttotal: 5.27s\tremaining: 182ms\n",
      "290:\tlearn: 0.0052315\ttotal: 5.29s\tremaining: 164ms\n",
      "291:\tlearn: 0.0052315\ttotal: 5.34s\tremaining: 146ms\n",
      "292:\tlearn: 0.0052315\ttotal: 5.36s\tremaining: 128ms\n",
      "293:\tlearn: 0.0052314\ttotal: 5.39s\tremaining: 110ms\n",
      "294:\tlearn: 0.0052314\ttotal: 5.45s\tremaining: 92.4ms\n",
      "295:\tlearn: 0.0052313\ttotal: 5.49s\tremaining: 74.1ms\n",
      "296:\tlearn: 0.0052313\ttotal: 5.53s\tremaining: 55.9ms\n",
      "297:\tlearn: 0.0052312\ttotal: 5.58s\tremaining: 37.4ms\n",
      "298:\tlearn: 0.0052312\ttotal: 5.61s\tremaining: 18.8ms\n",
      "299:\tlearn: 0.0052311\ttotal: 5.66s\tremaining: 0us\n",
      "Dataset 3:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93accba113f48a58484d2f168bd0c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51bef1316b4ae9bf0a7b9448ef5873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.590937832476061, Recall = 0.9436790923824959, Aging Rate = 0.945097244732577, Precision = 0.4992497320471597, f1 = 0.6530211692135147\n",
      "Epoch 2: Train Loss = 0.47890244286682465, Recall = 0.9825769854132901, Aging Rate = 0.8964748784440842, Precision = 0.5480225988700564, f1 = 0.7036123603655882\n",
      "Epoch 3: Train Loss = 0.4078043966857502, Recall = 0.9444894651539708, Aging Rate = 0.713128038897893, Precision = 0.6622159090909091, f1 = 0.778557114228457\n",
      "Epoch 4: Train Loss = 0.3600413371813355, Recall = 0.9384116693679092, Aging Rate = 0.6482982171799028, Precision = 0.72375, f1 = 0.8172194777699364\n",
      "Epoch 5: Train Loss = 0.31353549478119735, Recall = 0.9473257698541329, Aging Rate = 0.6138573743922204, Precision = 0.7716171617161716, f1 = 0.850491087668243\n",
      "Test Loss = 0.2782100286195885, Recall = 0.9590761750405187, Aging Rate = 0.5883306320907618, precision = 0.8150826446280992\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2618569576179962, Recall = 0.9546191247974068, Aging Rate = 0.5698946515397083, Precision = 0.8375399928901529, f1 = 0.8922552546866125\n",
      "Epoch 7: Train Loss = 0.22004674534453758, Recall = 0.9667747163695299, Aging Rate = 0.5567260940032415, Precision = 0.8682678311499272, f1 = 0.9148773006134969\n",
      "Epoch 8: Train Loss = 0.1844152336520546, Recall = 0.9740680713128039, Aging Rate = 0.5370745542949756, Precision = 0.9068276122218031, f1 = 0.9392459464739207\n",
      "Epoch 9: Train Loss = 0.15549364287520923, Recall = 0.9837925445705025, Aging Rate = 0.5283630470016207, Precision = 0.9309815950920245, f1 = 0.9566587864460205\n",
      "Epoch 10: Train Loss = 0.13397705890660927, Recall = 0.9882495948136143, Aging Rate = 0.5247163695299838, Precision = 0.9416988416988417, f1 = 0.9644128113879004\n",
      "Test Loss = 0.1179195915860527, Recall = 0.9890599675850892, Aging Rate = 0.5121555915721232, precision = 0.9655854430379747\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.11240523396666664, Recall = 0.9902755267423015, Aging Rate = 0.5153970826580226, Precision = 0.960691823899371, f1 = 0.9752593774940144\n",
      "Epoch 12: Train Loss = 0.09805614065312489, Recall = 0.9927066450567261, Aging Rate = 0.5125607779578606, Precision = 0.9683794466403162, f1 = 0.9803921568627451\n",
      "Epoch 13: Train Loss = 0.08400458104703183, Recall = 0.9955429497568882, Aging Rate = 0.5109400324149108, Precision = 0.9742268041237113, f1 = 0.9847695390781562\n",
      "Epoch 14: Train Loss = 0.07374326614255457, Recall = 0.9959481361426256, Aging Rate = 0.5076985413290114, Precision = 0.9808459696727854, f1 = 0.9883393646964214\n",
      "Epoch 15: Train Loss = 0.06509594033612617, Recall = 0.997163695299838, Aging Rate = 0.50790113452188, Precision = 0.981651376146789, f1 = 0.9893467336683417\n",
      "Test Loss = 0.05815896271113638, Recall = 0.9979740680713128, Aging Rate = 0.5062803889789304, precision = 0.985594237695078\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05716324737069093, Recall = 0.9983792544570502, Aging Rate = 0.5058752025931929, Precision = 0.986784140969163, f1 = 0.9925478348439073\n",
      "Epoch 17: Train Loss = 0.05078873693991326, Recall = 0.9991896272285251, Aging Rate = 0.5044570502431118, Precision = 0.9903614457831326, f1 = 0.9947559499798305\n",
      "Epoch 18: Train Loss = 0.0453463337977373, Recall = 0.9987844408427877, Aging Rate = 0.5022285251215559, Precision = 0.9943525615167407, f1 = 0.9965635738831615\n",
      "Epoch 19: Train Loss = 0.04059551378509794, Recall = 0.9991896272285251, Aging Rate = 0.5020259319286872, Precision = 0.9951573849878934, f1 = 0.9971694298422968\n",
      "Epoch 20: Train Loss = 0.03685216227398409, Recall = 0.9995948136142626, Aging Rate = 0.5026337115072933, Precision = 0.9943571140669085, f1 = 0.9969690846635685\n",
      "Test Loss = 0.03342444135700284, Recall = 0.9995948136142626, Aging Rate = 0.5016207455429498, precision = 0.9963651050080775\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03321649089479756, Recall = 0.9995948136142626, Aging Rate = 0.5016207455429498, Precision = 0.9963651050080775, f1 = 0.9979773462783172\n",
      "Epoch 22: Train Loss = 0.03042287048634679, Recall = 0.9995948136142626, Aging Rate = 0.501418152350081, Precision = 0.9967676767676767, f1 = 0.998179243374469\n",
      "Epoch 23: Train Loss = 0.02743162513926408, Recall = 0.9995948136142626, Aging Rate = 0.5016207455429498, Precision = 0.9963651050080775, f1 = 0.9979773462783172\n",
      "Epoch 24: Train Loss = 0.02549008205623594, Recall = 0.9995948136142626, Aging Rate = 0.5012155591572123, Precision = 0.9971705739692805, f1 = 0.9983812221772562\n",
      "Epoch 25: Train Loss = 0.02307626955223238, Recall = 1.0, Aging Rate = 0.5010129659643436, Precision = 0.9979781641730692, f1 = 0.9989880590973488\n",
      "Test Loss = 0.021757017854281146, Recall = 1.0, Aging Rate = 0.5016207455429498, precision = 0.9967689822294022\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.021564808147363182, Recall = 1.0, Aging Rate = 0.5008103727714749, Precision = 0.9983818770226537, f1 = 0.9991902834008096\n",
      "Epoch 27: Train Loss = 0.020156288148180206, Recall = 1.0, Aging Rate = 0.5008103727714749, Precision = 0.9983818770226537, f1 = 0.9991902834008096\n",
      "Epoch 28: Train Loss = 0.018235098763793176, Recall = 1.0, Aging Rate = 0.5008103727714749, Precision = 0.9983818770226537, f1 = 0.9991902834008096\n",
      "Epoch 29: Train Loss = 0.0167112819989313, Recall = 1.0, Aging Rate = 0.5006077795786061, Precision = 0.9987859166329421, f1 = 0.999392589593035\n",
      "Epoch 30: Train Loss = 0.01548514749426123, Recall = 1.0, Aging Rate = 0.5006077795786061, Precision = 0.9987859166329421, f1 = 0.999392589593035\n",
      "Test Loss = 0.014369962996587375, Recall = 1.0, Aging Rate = 0.5004051863857374, precision = 0.9991902834008097\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.014584824334184007, Recall = 1.0, Aging Rate = 0.5004051863857374, Precision = 0.9991902834008097, f1 = 0.9995949777237748\n",
      "Epoch 32: Train Loss = 0.013579077663192921, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.012733840418306316, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.011967650075809882, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.011428007080160818, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.010551813609499599, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.010654154673635332, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.00998556422374575, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.009538914649060518, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.008991675614417083, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.00845420070979714, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.007935963512958303, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.008060493962341132, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.00769720605017566, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0073583450264975165, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.007025404784955217, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.006716811060422437, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006393457448179419, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.006528431398347262, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.006201349970140469, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0058933802566383895, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.005717346925481647, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.005462656189299624, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005187835030007136, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.005268359583209313, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.005051888675674965, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Train Loss = 0.004858876745302586, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004760290613032672, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0045559779473699664, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004297108460003374, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.004390840597505073, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.004303061513900395, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.00416729064519416, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.003981487171549344, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0039917744101422145, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003733720207764998, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0038645923672760182, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0037227355742244415, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0036028076998732466, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.003582229487963226, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0034556217435777574, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0032888548667550184, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0034030025562262417, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.003327748174454588, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.003230643546211758, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0031921429381474875, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.003154269935460565, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029232746123742295, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.003048699943883106, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.003033675196032663, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0029391159561600057, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.002898454958213738, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.002811200230728044, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026645369730011305, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.002764504066271469, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.002743698465237862, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0027009502517035293, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0026753048602690863, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.002631683011253997, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0024718568400695705, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.002573607073994157, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.002546717757676129, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.002494592978083189, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.002480288627300852, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0024819933787453164, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022952935108960255, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3034295\ttotal: 18.5ms\tremaining: 5.53s\n",
      "1:\tlearn: 0.2183356\ttotal: 24.3ms\tremaining: 3.62s\n",
      "2:\tlearn: 0.1752688\ttotal: 30.2ms\tremaining: 2.99s\n",
      "3:\tlearn: 0.1521585\ttotal: 36.2ms\tremaining: 2.68s\n",
      "4:\tlearn: 0.1367428\ttotal: 42.7ms\tremaining: 2.52s\n",
      "5:\tlearn: 0.1295684\ttotal: 48.8ms\tremaining: 2.39s\n",
      "6:\tlearn: 0.1208967\ttotal: 54.7ms\tremaining: 2.29s\n",
      "7:\tlearn: 0.1153189\ttotal: 60.7ms\tremaining: 2.22s\n",
      "8:\tlearn: 0.1061114\ttotal: 66.8ms\tremaining: 2.16s\n",
      "9:\tlearn: 0.1020035\ttotal: 72.9ms\tremaining: 2.11s\n",
      "10:\tlearn: 0.1004657\ttotal: 78.8ms\tremaining: 2.07s\n",
      "11:\tlearn: 0.0973551\ttotal: 84.2ms\tremaining: 2.02s\n",
      "12:\tlearn: 0.0942926\ttotal: 89.7ms\tremaining: 1.98s\n",
      "13:\tlearn: 0.0908788\ttotal: 95.4ms\tremaining: 1.95s\n",
      "14:\tlearn: 0.0877356\ttotal: 101ms\tremaining: 1.92s\n",
      "15:\tlearn: 0.0838495\ttotal: 106ms\tremaining: 1.89s\n",
      "16:\tlearn: 0.0805760\ttotal: 112ms\tremaining: 1.86s\n",
      "17:\tlearn: 0.0780555\ttotal: 117ms\tremaining: 1.84s\n",
      "18:\tlearn: 0.0740809\ttotal: 123ms\tremaining: 1.81s\n",
      "19:\tlearn: 0.0731983\ttotal: 128ms\tremaining: 1.79s\n",
      "20:\tlearn: 0.0700400\ttotal: 134ms\tremaining: 1.78s\n",
      "21:\tlearn: 0.0670603\ttotal: 139ms\tremaining: 1.76s\n",
      "22:\tlearn: 0.0645331\ttotal: 145ms\tremaining: 1.74s\n",
      "23:\tlearn: 0.0627383\ttotal: 150ms\tremaining: 1.72s\n",
      "24:\tlearn: 0.0595135\ttotal: 155ms\tremaining: 1.71s\n",
      "25:\tlearn: 0.0590046\ttotal: 161ms\tremaining: 1.7s\n",
      "26:\tlearn: 0.0563671\ttotal: 167ms\tremaining: 1.68s\n",
      "27:\tlearn: 0.0552648\ttotal: 172ms\tremaining: 1.67s\n",
      "28:\tlearn: 0.0533304\ttotal: 178ms\tremaining: 1.66s\n",
      "29:\tlearn: 0.0488975\ttotal: 183ms\tremaining: 1.65s\n",
      "30:\tlearn: 0.0461657\ttotal: 189ms\tremaining: 1.64s\n",
      "31:\tlearn: 0.0438854\ttotal: 194ms\tremaining: 1.62s\n",
      "32:\tlearn: 0.0422536\ttotal: 200ms\tremaining: 1.61s\n",
      "33:\tlearn: 0.0398236\ttotal: 205ms\tremaining: 1.61s\n",
      "34:\tlearn: 0.0386542\ttotal: 211ms\tremaining: 1.59s\n",
      "35:\tlearn: 0.0373066\ttotal: 216ms\tremaining: 1.58s\n",
      "36:\tlearn: 0.0353910\ttotal: 222ms\tremaining: 1.57s\n",
      "37:\tlearn: 0.0339869\ttotal: 227ms\tremaining: 1.56s\n",
      "38:\tlearn: 0.0329751\ttotal: 232ms\tremaining: 1.55s\n",
      "39:\tlearn: 0.0310142\ttotal: 238ms\tremaining: 1.55s\n",
      "40:\tlearn: 0.0302026\ttotal: 243ms\tremaining: 1.54s\n",
      "41:\tlearn: 0.0288475\ttotal: 249ms\tremaining: 1.53s\n",
      "42:\tlearn: 0.0280600\ttotal: 254ms\tremaining: 1.52s\n",
      "43:\tlearn: 0.0268570\ttotal: 259ms\tremaining: 1.51s\n",
      "44:\tlearn: 0.0253718\ttotal: 264ms\tremaining: 1.5s\n",
      "45:\tlearn: 0.0243202\ttotal: 270ms\tremaining: 1.49s\n",
      "46:\tlearn: 0.0232369\ttotal: 275ms\tremaining: 1.48s\n",
      "47:\tlearn: 0.0220125\ttotal: 280ms\tremaining: 1.47s\n",
      "48:\tlearn: 0.0197559\ttotal: 285ms\tremaining: 1.46s\n",
      "49:\tlearn: 0.0189502\ttotal: 290ms\tremaining: 1.45s\n",
      "50:\tlearn: 0.0175499\ttotal: 295ms\tremaining: 1.44s\n",
      "51:\tlearn: 0.0166958\ttotal: 300ms\tremaining: 1.43s\n",
      "52:\tlearn: 0.0158745\ttotal: 305ms\tremaining: 1.42s\n",
      "53:\tlearn: 0.0150598\ttotal: 310ms\tremaining: 1.41s\n",
      "54:\tlearn: 0.0144961\ttotal: 315ms\tremaining: 1.4s\n",
      "55:\tlearn: 0.0137875\ttotal: 320ms\tremaining: 1.39s\n",
      "56:\tlearn: 0.0129396\ttotal: 325ms\tremaining: 1.39s\n",
      "57:\tlearn: 0.0125755\ttotal: 330ms\tremaining: 1.38s\n",
      "58:\tlearn: 0.0119970\ttotal: 335ms\tremaining: 1.37s\n",
      "59:\tlearn: 0.0115912\ttotal: 340ms\tremaining: 1.36s\n",
      "60:\tlearn: 0.0111977\ttotal: 345ms\tremaining: 1.35s\n",
      "61:\tlearn: 0.0106686\ttotal: 350ms\tremaining: 1.34s\n",
      "62:\tlearn: 0.0103233\ttotal: 355ms\tremaining: 1.33s\n",
      "63:\tlearn: 0.0098496\ttotal: 360ms\tremaining: 1.33s\n",
      "64:\tlearn: 0.0092262\ttotal: 366ms\tremaining: 1.32s\n",
      "65:\tlearn: 0.0087111\ttotal: 371ms\tremaining: 1.31s\n",
      "66:\tlearn: 0.0082677\ttotal: 376ms\tremaining: 1.31s\n",
      "67:\tlearn: 0.0077334\ttotal: 381ms\tremaining: 1.3s\n",
      "68:\tlearn: 0.0074044\ttotal: 387ms\tremaining: 1.29s\n",
      "69:\tlearn: 0.0071179\ttotal: 392ms\tremaining: 1.29s\n",
      "70:\tlearn: 0.0068403\ttotal: 397ms\tremaining: 1.28s\n",
      "71:\tlearn: 0.0065549\ttotal: 402ms\tremaining: 1.27s\n",
      "72:\tlearn: 0.0063849\ttotal: 407ms\tremaining: 1.26s\n",
      "73:\tlearn: 0.0061232\ttotal: 412ms\tremaining: 1.26s\n",
      "74:\tlearn: 0.0058143\ttotal: 417ms\tremaining: 1.25s\n",
      "75:\tlearn: 0.0056945\ttotal: 421ms\tremaining: 1.24s\n",
      "76:\tlearn: 0.0055162\ttotal: 427ms\tremaining: 1.24s\n",
      "77:\tlearn: 0.0053331\ttotal: 432ms\tremaining: 1.23s\n",
      "78:\tlearn: 0.0051459\ttotal: 436ms\tremaining: 1.22s\n",
      "79:\tlearn: 0.0049291\ttotal: 441ms\tremaining: 1.21s\n",
      "80:\tlearn: 0.0047293\ttotal: 446ms\tremaining: 1.21s\n",
      "81:\tlearn: 0.0045902\ttotal: 451ms\tremaining: 1.2s\n",
      "82:\tlearn: 0.0044369\ttotal: 457ms\tremaining: 1.19s\n",
      "83:\tlearn: 0.0042807\ttotal: 462ms\tremaining: 1.19s\n",
      "84:\tlearn: 0.0041109\ttotal: 467ms\tremaining: 1.18s\n",
      "85:\tlearn: 0.0040040\ttotal: 472ms\tremaining: 1.18s\n",
      "86:\tlearn: 0.0039516\ttotal: 477ms\tremaining: 1.17s\n",
      "87:\tlearn: 0.0037246\ttotal: 483ms\tremaining: 1.16s\n",
      "88:\tlearn: 0.0035676\ttotal: 488ms\tremaining: 1.16s\n",
      "89:\tlearn: 0.0034211\ttotal: 493ms\tremaining: 1.15s\n",
      "90:\tlearn: 0.0032984\ttotal: 499ms\tremaining: 1.14s\n",
      "91:\tlearn: 0.0032351\ttotal: 504ms\tremaining: 1.14s\n",
      "92:\tlearn: 0.0031368\ttotal: 509ms\tremaining: 1.13s\n",
      "93:\tlearn: 0.0030537\ttotal: 514ms\tremaining: 1.13s\n",
      "94:\tlearn: 0.0029781\ttotal: 520ms\tremaining: 1.12s\n",
      "95:\tlearn: 0.0028595\ttotal: 525ms\tremaining: 1.11s\n",
      "96:\tlearn: 0.0027843\ttotal: 530ms\tremaining: 1.11s\n",
      "97:\tlearn: 0.0027222\ttotal: 535ms\tremaining: 1.1s\n",
      "98:\tlearn: 0.0025862\ttotal: 540ms\tremaining: 1.1s\n",
      "99:\tlearn: 0.0025117\ttotal: 545ms\tremaining: 1.09s\n",
      "100:\tlearn: 0.0024253\ttotal: 550ms\tremaining: 1.08s\n",
      "101:\tlearn: 0.0023718\ttotal: 555ms\tremaining: 1.08s\n",
      "102:\tlearn: 0.0023197\ttotal: 560ms\tremaining: 1.07s\n",
      "103:\tlearn: 0.0022454\ttotal: 565ms\tremaining: 1.06s\n",
      "104:\tlearn: 0.0021816\ttotal: 571ms\tremaining: 1.06s\n",
      "105:\tlearn: 0.0021206\ttotal: 576ms\tremaining: 1.05s\n",
      "106:\tlearn: 0.0020677\ttotal: 581ms\tremaining: 1.05s\n",
      "107:\tlearn: 0.0020321\ttotal: 587ms\tremaining: 1.04s\n",
      "108:\tlearn: 0.0019675\ttotal: 592ms\tremaining: 1.04s\n",
      "109:\tlearn: 0.0019349\ttotal: 597ms\tremaining: 1.03s\n",
      "110:\tlearn: 0.0018863\ttotal: 602ms\tremaining: 1.02s\n",
      "111:\tlearn: 0.0017530\ttotal: 608ms\tremaining: 1.02s\n",
      "112:\tlearn: 0.0017044\ttotal: 613ms\tremaining: 1.01s\n",
      "113:\tlearn: 0.0016327\ttotal: 618ms\tremaining: 1.01s\n",
      "114:\tlearn: 0.0015903\ttotal: 623ms\tremaining: 1s\n",
      "115:\tlearn: 0.0015167\ttotal: 628ms\tremaining: 996ms\n",
      "116:\tlearn: 0.0014645\ttotal: 633ms\tremaining: 991ms\n",
      "117:\tlearn: 0.0014098\ttotal: 639ms\tremaining: 985ms\n",
      "118:\tlearn: 0.0013819\ttotal: 644ms\tremaining: 979ms\n",
      "119:\tlearn: 0.0013440\ttotal: 649ms\tremaining: 973ms\n",
      "120:\tlearn: 0.0012918\ttotal: 654ms\tremaining: 967ms\n",
      "121:\tlearn: 0.0012593\ttotal: 659ms\tremaining: 962ms\n",
      "122:\tlearn: 0.0012309\ttotal: 664ms\tremaining: 956ms\n",
      "123:\tlearn: 0.0012307\ttotal: 670ms\tremaining: 950ms\n",
      "124:\tlearn: 0.0011847\ttotal: 675ms\tremaining: 945ms\n",
      "125:\tlearn: 0.0011476\ttotal: 680ms\tremaining: 939ms\n",
      "126:\tlearn: 0.0011475\ttotal: 685ms\tremaining: 933ms\n",
      "127:\tlearn: 0.0011211\ttotal: 690ms\tremaining: 927ms\n",
      "128:\tlearn: 0.0011045\ttotal: 695ms\tremaining: 921ms\n",
      "129:\tlearn: 0.0010781\ttotal: 700ms\tremaining: 915ms\n",
      "130:\tlearn: 0.0010510\ttotal: 705ms\tremaining: 910ms\n",
      "131:\tlearn: 0.0010139\ttotal: 711ms\tremaining: 904ms\n",
      "132:\tlearn: 0.0009848\ttotal: 716ms\tremaining: 899ms\n",
      "133:\tlearn: 0.0009847\ttotal: 721ms\tremaining: 893ms\n",
      "134:\tlearn: 0.0009847\ttotal: 726ms\tremaining: 887ms\n",
      "135:\tlearn: 0.0009847\ttotal: 730ms\tremaining: 881ms\n",
      "136:\tlearn: 0.0009847\ttotal: 735ms\tremaining: 875ms\n",
      "137:\tlearn: 0.0009847\ttotal: 740ms\tremaining: 869ms\n",
      "138:\tlearn: 0.0009847\ttotal: 745ms\tremaining: 863ms\n",
      "139:\tlearn: 0.0009674\ttotal: 751ms\tremaining: 858ms\n",
      "140:\tlearn: 0.0009674\ttotal: 755ms\tremaining: 852ms\n",
      "141:\tlearn: 0.0009674\ttotal: 761ms\tremaining: 846ms\n",
      "142:\tlearn: 0.0009673\ttotal: 766ms\tremaining: 841ms\n",
      "143:\tlearn: 0.0009673\ttotal: 771ms\tremaining: 835ms\n",
      "144:\tlearn: 0.0009673\ttotal: 776ms\tremaining: 830ms\n",
      "145:\tlearn: 0.0009672\ttotal: 781ms\tremaining: 824ms\n",
      "146:\tlearn: 0.0009672\ttotal: 787ms\tremaining: 819ms\n",
      "147:\tlearn: 0.0009672\ttotal: 792ms\tremaining: 813ms\n",
      "148:\tlearn: 0.0009672\ttotal: 797ms\tremaining: 807ms\n",
      "149:\tlearn: 0.0009672\ttotal: 802ms\tremaining: 802ms\n",
      "150:\tlearn: 0.0009671\ttotal: 807ms\tremaining: 796ms\n",
      "151:\tlearn: 0.0009671\ttotal: 812ms\tremaining: 791ms\n",
      "152:\tlearn: 0.0009671\ttotal: 817ms\tremaining: 785ms\n",
      "153:\tlearn: 0.0009671\ttotal: 822ms\tremaining: 780ms\n",
      "154:\tlearn: 0.0009671\ttotal: 827ms\tremaining: 774ms\n",
      "155:\tlearn: 0.0009670\ttotal: 833ms\tremaining: 769ms\n",
      "156:\tlearn: 0.0009670\ttotal: 838ms\tremaining: 763ms\n",
      "157:\tlearn: 0.0009670\ttotal: 843ms\tremaining: 757ms\n",
      "158:\tlearn: 0.0009670\ttotal: 847ms\tremaining: 751ms\n",
      "159:\tlearn: 0.0009670\ttotal: 852ms\tremaining: 746ms\n",
      "160:\tlearn: 0.0009403\ttotal: 857ms\tremaining: 740ms\n",
      "161:\tlearn: 0.0009083\ttotal: 862ms\tremaining: 735ms\n",
      "162:\tlearn: 0.0009083\ttotal: 871ms\tremaining: 732ms\n",
      "163:\tlearn: 0.0009083\ttotal: 876ms\tremaining: 726ms\n",
      "164:\tlearn: 0.0009083\ttotal: 880ms\tremaining: 720ms\n",
      "165:\tlearn: 0.0009083\ttotal: 885ms\tremaining: 714ms\n",
      "166:\tlearn: 0.0009082\ttotal: 890ms\tremaining: 708ms\n",
      "167:\tlearn: 0.0009082\ttotal: 894ms\tremaining: 703ms\n",
      "168:\tlearn: 0.0009082\ttotal: 899ms\tremaining: 697ms\n",
      "169:\tlearn: 0.0009082\ttotal: 904ms\tremaining: 691ms\n",
      "170:\tlearn: 0.0009081\ttotal: 908ms\tremaining: 685ms\n",
      "171:\tlearn: 0.0009081\ttotal: 913ms\tremaining: 679ms\n",
      "172:\tlearn: 0.0009081\ttotal: 917ms\tremaining: 673ms\n",
      "173:\tlearn: 0.0009080\ttotal: 922ms\tremaining: 668ms\n",
      "174:\tlearn: 0.0008902\ttotal: 927ms\tremaining: 662ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175:\tlearn: 0.0008902\ttotal: 932ms\tremaining: 657ms\n",
      "176:\tlearn: 0.0008902\ttotal: 937ms\tremaining: 651ms\n",
      "177:\tlearn: 0.0008752\ttotal: 941ms\tremaining: 645ms\n",
      "178:\tlearn: 0.0008752\ttotal: 946ms\tremaining: 640ms\n",
      "179:\tlearn: 0.0008752\ttotal: 951ms\tremaining: 634ms\n",
      "180:\tlearn: 0.0008653\ttotal: 956ms\tremaining: 628ms\n",
      "181:\tlearn: 0.0008653\ttotal: 960ms\tremaining: 623ms\n",
      "182:\tlearn: 0.0008653\ttotal: 965ms\tremaining: 617ms\n",
      "183:\tlearn: 0.0008652\ttotal: 970ms\tremaining: 612ms\n",
      "184:\tlearn: 0.0008652\ttotal: 975ms\tremaining: 606ms\n",
      "185:\tlearn: 0.0008652\ttotal: 981ms\tremaining: 601ms\n",
      "186:\tlearn: 0.0008652\ttotal: 986ms\tremaining: 596ms\n",
      "187:\tlearn: 0.0008652\ttotal: 991ms\tremaining: 590ms\n",
      "188:\tlearn: 0.0008448\ttotal: 996ms\tremaining: 585ms\n",
      "189:\tlearn: 0.0008447\ttotal: 1s\tremaining: 580ms\n",
      "190:\tlearn: 0.0008447\ttotal: 1s\tremaining: 574ms\n",
      "191:\tlearn: 0.0008447\ttotal: 1.01s\tremaining: 568ms\n",
      "192:\tlearn: 0.0008447\ttotal: 1.01s\tremaining: 563ms\n",
      "193:\tlearn: 0.0008447\ttotal: 1.02s\tremaining: 557ms\n",
      "194:\tlearn: 0.0008446\ttotal: 1.02s\tremaining: 552ms\n",
      "195:\tlearn: 0.0008446\ttotal: 1.03s\tremaining: 546ms\n",
      "196:\tlearn: 0.0008446\ttotal: 1.03s\tremaining: 541ms\n",
      "197:\tlearn: 0.0008446\ttotal: 1.04s\tremaining: 535ms\n",
      "198:\tlearn: 0.0008446\ttotal: 1.04s\tremaining: 530ms\n",
      "199:\tlearn: 0.0008446\ttotal: 1.05s\tremaining: 524ms\n",
      "200:\tlearn: 0.0008446\ttotal: 1.05s\tremaining: 518ms\n",
      "201:\tlearn: 0.0008445\ttotal: 1.06s\tremaining: 513ms\n",
      "202:\tlearn: 0.0008445\ttotal: 1.06s\tremaining: 507ms\n",
      "203:\tlearn: 0.0008318\ttotal: 1.07s\tremaining: 502ms\n",
      "204:\tlearn: 0.0008318\ttotal: 1.07s\tremaining: 496ms\n",
      "205:\tlearn: 0.0008318\ttotal: 1.07s\tremaining: 491ms\n",
      "206:\tlearn: 0.0008317\ttotal: 1.08s\tremaining: 485ms\n",
      "207:\tlearn: 0.0008317\ttotal: 1.08s\tremaining: 480ms\n",
      "208:\tlearn: 0.0008317\ttotal: 1.09s\tremaining: 475ms\n",
      "209:\tlearn: 0.0008317\ttotal: 1.09s\tremaining: 469ms\n",
      "210:\tlearn: 0.0008317\ttotal: 1.1s\tremaining: 464ms\n",
      "211:\tlearn: 0.0008028\ttotal: 1.1s\tremaining: 459ms\n",
      "212:\tlearn: 0.0008028\ttotal: 1.11s\tremaining: 454ms\n",
      "213:\tlearn: 0.0008028\ttotal: 1.11s\tremaining: 448ms\n",
      "214:\tlearn: 0.0008028\ttotal: 1.12s\tremaining: 443ms\n",
      "215:\tlearn: 0.0008028\ttotal: 1.13s\tremaining: 438ms\n",
      "216:\tlearn: 0.0008028\ttotal: 1.13s\tremaining: 432ms\n",
      "217:\tlearn: 0.0008028\ttotal: 1.13s\tremaining: 427ms\n",
      "218:\tlearn: 0.0008028\ttotal: 1.14s\tremaining: 421ms\n",
      "219:\tlearn: 0.0008028\ttotal: 1.14s\tremaining: 416ms\n",
      "220:\tlearn: 0.0008027\ttotal: 1.15s\tremaining: 411ms\n",
      "221:\tlearn: 0.0008027\ttotal: 1.15s\tremaining: 406ms\n",
      "222:\tlearn: 0.0008027\ttotal: 1.16s\tremaining: 401ms\n",
      "223:\tlearn: 0.0008027\ttotal: 1.17s\tremaining: 395ms\n",
      "224:\tlearn: 0.0008027\ttotal: 1.17s\tremaining: 390ms\n",
      "225:\tlearn: 0.0008027\ttotal: 1.18s\tremaining: 385ms\n",
      "226:\tlearn: 0.0008027\ttotal: 1.18s\tremaining: 380ms\n",
      "227:\tlearn: 0.0008027\ttotal: 1.19s\tremaining: 374ms\n",
      "228:\tlearn: 0.0008027\ttotal: 1.19s\tremaining: 369ms\n",
      "229:\tlearn: 0.0008026\ttotal: 1.2s\tremaining: 364ms\n",
      "230:\tlearn: 0.0008026\ttotal: 1.2s\tremaining: 359ms\n",
      "231:\tlearn: 0.0008026\ttotal: 1.21s\tremaining: 353ms\n",
      "232:\tlearn: 0.0008026\ttotal: 1.21s\tremaining: 348ms\n",
      "233:\tlearn: 0.0008026\ttotal: 1.22s\tremaining: 343ms\n",
      "234:\tlearn: 0.0008026\ttotal: 1.22s\tremaining: 338ms\n",
      "235:\tlearn: 0.0008026\ttotal: 1.23s\tremaining: 333ms\n",
      "236:\tlearn: 0.0008025\ttotal: 1.23s\tremaining: 327ms\n",
      "237:\tlearn: 0.0008025\ttotal: 1.24s\tremaining: 322ms\n",
      "238:\tlearn: 0.0008025\ttotal: 1.24s\tremaining: 317ms\n",
      "239:\tlearn: 0.0008025\ttotal: 1.25s\tremaining: 311ms\n",
      "240:\tlearn: 0.0008025\ttotal: 1.25s\tremaining: 306ms\n",
      "241:\tlearn: 0.0008024\ttotal: 1.25s\tremaining: 301ms\n",
      "242:\tlearn: 0.0008024\ttotal: 1.26s\tremaining: 296ms\n",
      "243:\tlearn: 0.0008024\ttotal: 1.26s\tremaining: 290ms\n",
      "244:\tlearn: 0.0008024\ttotal: 1.27s\tremaining: 285ms\n",
      "245:\tlearn: 0.0008024\ttotal: 1.27s\tremaining: 280ms\n",
      "246:\tlearn: 0.0008024\ttotal: 1.28s\tremaining: 274ms\n",
      "247:\tlearn: 0.0008024\ttotal: 1.28s\tremaining: 269ms\n",
      "248:\tlearn: 0.0008023\ttotal: 1.29s\tremaining: 264ms\n",
      "249:\tlearn: 0.0008023\ttotal: 1.29s\tremaining: 259ms\n",
      "250:\tlearn: 0.0008023\ttotal: 1.3s\tremaining: 253ms\n",
      "251:\tlearn: 0.0008023\ttotal: 1.3s\tremaining: 248ms\n",
      "252:\tlearn: 0.0008023\ttotal: 1.31s\tremaining: 243ms\n",
      "253:\tlearn: 0.0008023\ttotal: 1.31s\tremaining: 238ms\n",
      "254:\tlearn: 0.0008023\ttotal: 1.32s\tremaining: 232ms\n",
      "255:\tlearn: 0.0008023\ttotal: 1.32s\tremaining: 227ms\n",
      "256:\tlearn: 0.0008023\ttotal: 1.32s\tremaining: 222ms\n",
      "257:\tlearn: 0.0007906\ttotal: 1.33s\tremaining: 217ms\n",
      "258:\tlearn: 0.0007906\ttotal: 1.33s\tremaining: 211ms\n",
      "259:\tlearn: 0.0007905\ttotal: 1.34s\tremaining: 206ms\n",
      "260:\tlearn: 0.0007904\ttotal: 1.34s\tremaining: 201ms\n",
      "261:\tlearn: 0.0007904\ttotal: 1.35s\tremaining: 196ms\n",
      "262:\tlearn: 0.0007904\ttotal: 1.35s\tremaining: 191ms\n",
      "263:\tlearn: 0.0007904\ttotal: 1.36s\tremaining: 185ms\n",
      "264:\tlearn: 0.0007904\ttotal: 1.36s\tremaining: 180ms\n",
      "265:\tlearn: 0.0007904\ttotal: 1.37s\tremaining: 175ms\n",
      "266:\tlearn: 0.0007904\ttotal: 1.37s\tremaining: 170ms\n",
      "267:\tlearn: 0.0007904\ttotal: 1.38s\tremaining: 165ms\n",
      "268:\tlearn: 0.0007904\ttotal: 1.38s\tremaining: 159ms\n",
      "269:\tlearn: 0.0007904\ttotal: 1.39s\tremaining: 154ms\n",
      "270:\tlearn: 0.0007904\ttotal: 1.39s\tremaining: 149ms\n",
      "271:\tlearn: 0.0007904\ttotal: 1.4s\tremaining: 144ms\n",
      "272:\tlearn: 0.0007904\ttotal: 1.4s\tremaining: 139ms\n",
      "273:\tlearn: 0.0007904\ttotal: 1.41s\tremaining: 134ms\n",
      "274:\tlearn: 0.0007904\ttotal: 1.41s\tremaining: 128ms\n",
      "275:\tlearn: 0.0007904\ttotal: 1.42s\tremaining: 123ms\n",
      "276:\tlearn: 0.0007904\ttotal: 1.42s\tremaining: 118ms\n",
      "277:\tlearn: 0.0007904\ttotal: 1.43s\tremaining: 113ms\n",
      "278:\tlearn: 0.0007904\ttotal: 1.43s\tremaining: 108ms\n",
      "279:\tlearn: 0.0007904\ttotal: 1.44s\tremaining: 103ms\n",
      "280:\tlearn: 0.0007903\ttotal: 1.44s\tremaining: 97.4ms\n",
      "281:\tlearn: 0.0007654\ttotal: 1.45s\tremaining: 92.2ms\n",
      "282:\tlearn: 0.0007654\ttotal: 1.45s\tremaining: 87.1ms\n",
      "283:\tlearn: 0.0007654\ttotal: 1.45s\tremaining: 81.9ms\n",
      "284:\tlearn: 0.0007417\ttotal: 1.46s\tremaining: 76.8ms\n",
      "285:\tlearn: 0.0007416\ttotal: 1.46s\tremaining: 71.7ms\n",
      "286:\tlearn: 0.0007416\ttotal: 1.47s\tremaining: 66.5ms\n",
      "287:\tlearn: 0.0007416\ttotal: 1.47s\tremaining: 61.4ms\n",
      "288:\tlearn: 0.0007416\ttotal: 1.48s\tremaining: 56.3ms\n",
      "289:\tlearn: 0.0007416\ttotal: 1.48s\tremaining: 51.2ms\n",
      "290:\tlearn: 0.0007416\ttotal: 1.49s\tremaining: 46ms\n",
      "291:\tlearn: 0.0007416\ttotal: 1.49s\tremaining: 40.9ms\n",
      "292:\tlearn: 0.0007416\ttotal: 1.5s\tremaining: 35.8ms\n",
      "293:\tlearn: 0.0007416\ttotal: 1.5s\tremaining: 30.7ms\n",
      "294:\tlearn: 0.0007416\ttotal: 1.51s\tremaining: 25.6ms\n",
      "295:\tlearn: 0.0007416\ttotal: 1.51s\tremaining: 20.4ms\n",
      "296:\tlearn: 0.0007416\ttotal: 1.52s\tremaining: 15.3ms\n",
      "297:\tlearn: 0.0007416\ttotal: 1.52s\tremaining: 10.2ms\n",
      "298:\tlearn: 0.0007416\ttotal: 1.53s\tremaining: 5.11ms\n",
      "299:\tlearn: 0.0007415\ttotal: 1.53s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91301bfb3d24961a27c8cb3cd07dc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5913720825059625, Recall = 0.9728415079043372, Aging Rate = 0.9764896635589785, Precision = 0.49813200498132004, f1 = 0.6588881262868909\n",
      "Epoch 2: Train Loss = 0.48897989847469675, Recall = 0.984596676124848, Aging Rate = 0.9199432509120389, Precision = 0.5351398986560917, f1 = 0.6934056522980303\n",
      "Epoch 3: Train Loss = 0.41699681106394587, Recall = 0.9558167815160113, Aging Rate = 0.739562221321443, Precision = 0.6462044395724856, f1 = 0.7710922171353826\n",
      "Epoch 4: Train Loss = 0.36978452016962265, Recall = 0.9428455614106201, Aging Rate = 0.6653830563437373, Precision = 0.7084983247030155, f1 = 0.8090434782608696\n",
      "Epoch 5: Train Loss = 0.32659169358669815, Recall = 0.9408188082691528, Aging Rate = 0.6246453182002432, Precision = 0.7530824140168721, f1 = 0.8365471256082176\n",
      "Test Loss = 0.2915375095221694, Recall = 0.9659505472233482, Aging Rate = 0.6254560194568302, precision = 0.772197018794556\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.27309089529403174, Recall = 0.954600729631131, Aging Rate = 0.5818808269152818, Precision = 0.8202716823406478, f1 = 0.8823529411764706\n",
      "Epoch 7: Train Loss = 0.23001561892820266, Recall = 0.9663558978516417, Aging Rate = 0.5575597892176732, Precision = 0.8665939658306071, f1 = 0.9137600613261786\n",
      "Epoch 8: Train Loss = 0.19331442343956748, Recall = 0.9744629104175111, Aging Rate = 0.5458046209971625, Precision = 0.8926847382101746, f1 = 0.931782945736434\n",
      "Epoch 9: Train Loss = 0.16146282535189793, Recall = 0.9825699229833806, Aging Rate = 0.5312119983785974, Precision = 0.9248378481495613, f1 = 0.9528301886792453\n",
      "Epoch 10: Train Loss = 0.13456834775682977, Recall = 0.9866234292663154, Aging Rate = 0.5202675314146737, Precision = 0.9481885469419556, f1 = 0.9670242352006357\n",
      "Test Loss = 0.12108947878479233, Recall = 0.9858127280097284, Aging Rate = 0.506890960680989, precision = 0.9724110355857657\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.11393476427990655, Recall = 0.9898662342926632, Aging Rate = 0.5135792460478313, Precision = 0.9636937647987371, f1 = 0.9766046790641872\n",
      "Epoch 12: Train Loss = 0.0961678314411935, Recall = 0.993109039319011, Aging Rate = 0.5117551682205107, Precision = 0.9702970297029703, f1 = 0.9815705128205129\n",
      "Epoch 13: Train Loss = 0.08268989282831954, Recall = 0.9959464937170652, Aging Rate = 0.5133765707336846, Precision = 0.96999605211212, f1 = 0.9828\n",
      "Epoch 14: Train Loss = 0.0711976958247733, Recall = 0.9963518443453587, Aging Rate = 0.5079043372517228, Precision = 0.9808459696727854, f1 = 0.9885381057711643\n",
      "Epoch 15: Train Loss = 0.06192203072350084, Recall = 0.9975678962302391, Aging Rate = 0.5064856100526955, Precision = 0.9847939175670268, f1 = 0.9911397503020539\n",
      "Test Loss = 0.05550350991606616, Recall = 0.9975678962302391, Aging Rate = 0.5034454803404945, precision = 0.9907407407407407\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05402034360131707, Recall = 0.9987839481151196, Aging Rate = 0.5046615322253749, Precision = 0.989558232931727, f1 = 0.9941496873108735\n",
      "Epoch 17: Train Loss = 0.047519386809788886, Recall = 0.999189298743413, Aging Rate = 0.5040535062829348, Precision = 0.9911540008041817, f1 = 0.9951554299555914\n",
      "Epoch 18: Train Loss = 0.0425041497167446, Recall = 1.0, Aging Rate = 0.503040129712201, Precision = 0.9939564867042707, f1 = 0.9969690846635684\n",
      "Epoch 19: Train Loss = 0.03768894806908943, Recall = 0.9995946493717065, Aging Rate = 0.5024321037697609, Precision = 0.9947559499798305, f1 = 0.9971694298422967\n",
      "Epoch 20: Train Loss = 0.034060779516519105, Recall = 0.9995946493717065, Aging Rate = 0.5018240778273206, Precision = 0.9959612277867528, f1 = 0.9977746307910176\n",
      "Test Loss = 0.031082911311939847, Recall = 1.0, Aging Rate = 0.5016214025131739, precision = 0.9967676767676767\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03097364529786102, Recall = 1.0, Aging Rate = 0.5010133765707336, Precision = 0.9979773462783171, f1 = 0.998987649321725\n",
      "Epoch 22: Train Loss = 0.027944397107280118, Recall = 1.0, Aging Rate = 0.5016214025131739, Precision = 0.9967676767676767, f1 = 0.9983812221772561\n",
      "Epoch 23: Train Loss = 0.025388680899045366, Recall = 1.0, Aging Rate = 0.5014187271990271, Precision = 0.9971705739692805, f1 = 0.9985832827362882\n",
      "Epoch 24: Train Loss = 0.023406827753620397, Recall = 1.0, Aging Rate = 0.5012160518848804, Precision = 0.997573797007683, f1 = 0.9987854251012146\n",
      "Epoch 25: Train Loss = 0.021251813601060682, Recall = 1.0, Aging Rate = 0.500810701256587, Precision = 0.9983812221772562, f1 = 0.9991899554475496\n",
      "Test Loss = 0.019368079349392574, Recall = 1.0, Aging Rate = 0.5006080259424402, precision = 0.9987854251012146\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.019443699529822608, Recall = 1.0, Aging Rate = 0.500810701256587, Precision = 0.9983812221772562, f1 = 0.9991899554475496\n",
      "Epoch 27: Train Loss = 0.017977453600637915, Recall = 1.0, Aging Rate = 0.5006080259424402, Precision = 0.9987854251012146, f1 = 0.9993923435284586\n",
      "Epoch 28: Train Loss = 0.016650449241468863, Recall = 1.0, Aging Rate = 0.5006080259424402, Precision = 0.9987854251012146, f1 = 0.9993923435284586\n",
      "Epoch 29: Train Loss = 0.015390956858161687, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 30: Train Loss = 0.014389782761954855, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Test Loss = 0.013244281629765618, Recall = 1.0, Aging Rate = 0.5002026753141467, precision = 0.9995948136142626\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.013464590791276123, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 32: Train Loss = 0.012532299851834846, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 33: Train Loss = 0.011680678394632538, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 34: Train Loss = 0.011060076738157686, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 35: Train Loss = 0.010509739016466988, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Test Loss = 0.009658608415338086, Recall = 1.0, Aging Rate = 0.5002026753141467, precision = 0.9995948136142626\n",
      "\n",
      "Epoch 36: Train Loss = 0.009802642396157045, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 37: Train Loss = 0.009282918545380868, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 38: Train Loss = 0.008766990778065484, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.008327374179678791, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 40: Train Loss = 0.00790409827329306, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Test Loss = 0.007514395539463472, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.007619341087716707, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 42: Train Loss = 0.007153074619109977, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0068302510948387365, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 44: Train Loss = 0.006585144841787566, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.006270869543876335, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005916658585062866, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.005993394665669213, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.005741840580468495, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.005544014338496053, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.0053376045295486195, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.00523770751590517, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004965142281608974, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.004924494240857302, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.004789862715698443, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0045842125347049486, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004468970843351076, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.004338058309699777, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0040693543054824855, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.004191810033485342, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.004089263876742277, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.004035491388450431, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.00389370500998376, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.003747964698034571, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003613181870991691, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0036676297144361074, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.003560228345754855, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0034784468277431133, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0033955828694043053, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0033149258031637792, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0031763619709688977, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0032464420200368207, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0032016670366845057, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.003194910664765227, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.003057566315202208, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0029845526770367332, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0028829627242248864, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.002937535324217363, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0028987006489285634, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0028756345325139402, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.002781454044408163, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.002736339181073442, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002643008342950503, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0027219319827367643, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0026491604254433427, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.002636789330471961, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0025860908268344972, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.002579733367517053, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0024926353340917902, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0025313395703302783, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0025321344064867988, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.002541073719809684, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.002421188651915167, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.00241727478550026, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022569496464324014, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0023871074604488876, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.002355068071602259, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.002326793927717448, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.002372075584526451, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.002264037409217452, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021508076402045754, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3020720\ttotal: 14.4ms\tremaining: 4.32s\n",
      "1:\tlearn: 0.2142504\ttotal: 20.6ms\tremaining: 3.07s\n",
      "2:\tlearn: 0.1739390\ttotal: 26.7ms\tremaining: 2.64s\n",
      "3:\tlearn: 0.1509862\ttotal: 32.7ms\tremaining: 2.42s\n",
      "4:\tlearn: 0.1352681\ttotal: 38.6ms\tremaining: 2.27s\n",
      "5:\tlearn: 0.1273976\ttotal: 44.3ms\tremaining: 2.17s\n",
      "6:\tlearn: 0.1191621\ttotal: 50.3ms\tremaining: 2.11s\n",
      "7:\tlearn: 0.1144896\ttotal: 56.1ms\tremaining: 2.05s\n",
      "8:\tlearn: 0.1107827\ttotal: 61.7ms\tremaining: 2s\n",
      "9:\tlearn: 0.1066640\ttotal: 68ms\tremaining: 1.97s\n",
      "10:\tlearn: 0.1045516\ttotal: 73.5ms\tremaining: 1.93s\n",
      "11:\tlearn: 0.0995366\ttotal: 78.8ms\tremaining: 1.89s\n",
      "12:\tlearn: 0.0917893\ttotal: 84.1ms\tremaining: 1.86s\n",
      "13:\tlearn: 0.0872510\ttotal: 89.2ms\tremaining: 1.82s\n",
      "14:\tlearn: 0.0814955\ttotal: 94.4ms\tremaining: 1.79s\n",
      "15:\tlearn: 0.0785526\ttotal: 99.5ms\tremaining: 1.76s\n",
      "16:\tlearn: 0.0761927\ttotal: 105ms\tremaining: 1.74s\n",
      "17:\tlearn: 0.0712244\ttotal: 110ms\tremaining: 1.72s\n",
      "18:\tlearn: 0.0672887\ttotal: 115ms\tremaining: 1.7s\n",
      "19:\tlearn: 0.0620436\ttotal: 120ms\tremaining: 1.68s\n",
      "20:\tlearn: 0.0608926\ttotal: 125ms\tremaining: 1.66s\n",
      "21:\tlearn: 0.0590475\ttotal: 130ms\tremaining: 1.65s\n",
      "22:\tlearn: 0.0573498\ttotal: 136ms\tremaining: 1.63s\n",
      "23:\tlearn: 0.0559905\ttotal: 141ms\tremaining: 1.62s\n",
      "24:\tlearn: 0.0549482\ttotal: 146ms\tremaining: 1.6s\n",
      "25:\tlearn: 0.0507438\ttotal: 151ms\tremaining: 1.59s\n",
      "26:\tlearn: 0.0494923\ttotal: 157ms\tremaining: 1.58s\n",
      "27:\tlearn: 0.0472778\ttotal: 167ms\tremaining: 1.63s\n",
      "28:\tlearn: 0.0450650\ttotal: 173ms\tremaining: 1.61s\n",
      "29:\tlearn: 0.0441488\ttotal: 178ms\tremaining: 1.6s\n",
      "30:\tlearn: 0.0418534\ttotal: 184ms\tremaining: 1.59s\n",
      "31:\tlearn: 0.0405034\ttotal: 189ms\tremaining: 1.58s\n",
      "32:\tlearn: 0.0388319\ttotal: 195ms\tremaining: 1.57s\n",
      "33:\tlearn: 0.0364991\ttotal: 201ms\tremaining: 1.57s\n",
      "34:\tlearn: 0.0358199\ttotal: 206ms\tremaining: 1.56s\n",
      "35:\tlearn: 0.0336295\ttotal: 212ms\tremaining: 1.55s\n",
      "36:\tlearn: 0.0326532\ttotal: 217ms\tremaining: 1.54s\n",
      "37:\tlearn: 0.0304340\ttotal: 223ms\tremaining: 1.53s\n",
      "38:\tlearn: 0.0280726\ttotal: 228ms\tremaining: 1.53s\n",
      "39:\tlearn: 0.0271804\ttotal: 233ms\tremaining: 1.52s\n",
      "40:\tlearn: 0.0254537\ttotal: 239ms\tremaining: 1.51s\n",
      "41:\tlearn: 0.0228700\ttotal: 244ms\tremaining: 1.5s\n",
      "42:\tlearn: 0.0213348\ttotal: 249ms\tremaining: 1.49s\n",
      "43:\tlearn: 0.0203228\ttotal: 255ms\tremaining: 1.48s\n",
      "44:\tlearn: 0.0198252\ttotal: 260ms\tremaining: 1.47s\n",
      "45:\tlearn: 0.0189044\ttotal: 266ms\tremaining: 1.47s\n",
      "46:\tlearn: 0.0180331\ttotal: 271ms\tremaining: 1.46s\n",
      "47:\tlearn: 0.0169076\ttotal: 277ms\tremaining: 1.45s\n",
      "48:\tlearn: 0.0162912\ttotal: 282ms\tremaining: 1.45s\n",
      "49:\tlearn: 0.0159140\ttotal: 288ms\tremaining: 1.44s\n",
      "50:\tlearn: 0.0154553\ttotal: 293ms\tremaining: 1.43s\n",
      "51:\tlearn: 0.0149951\ttotal: 299ms\tremaining: 1.42s\n",
      "52:\tlearn: 0.0142292\ttotal: 304ms\tremaining: 1.42s\n",
      "53:\tlearn: 0.0138191\ttotal: 309ms\tremaining: 1.41s\n",
      "54:\tlearn: 0.0131957\ttotal: 315ms\tremaining: 1.4s\n",
      "55:\tlearn: 0.0126273\ttotal: 320ms\tremaining: 1.39s\n",
      "56:\tlearn: 0.0123072\ttotal: 325ms\tremaining: 1.39s\n",
      "57:\tlearn: 0.0120302\ttotal: 331ms\tremaining: 1.38s\n",
      "58:\tlearn: 0.0113467\ttotal: 336ms\tremaining: 1.37s\n",
      "59:\tlearn: 0.0108042\ttotal: 342ms\tremaining: 1.37s\n",
      "60:\tlearn: 0.0103141\ttotal: 348ms\tremaining: 1.36s\n",
      "61:\tlearn: 0.0099418\ttotal: 353ms\tremaining: 1.35s\n",
      "62:\tlearn: 0.0094660\ttotal: 358ms\tremaining: 1.35s\n",
      "63:\tlearn: 0.0091575\ttotal: 364ms\tremaining: 1.34s\n",
      "64:\tlearn: 0.0088923\ttotal: 369ms\tremaining: 1.33s\n",
      "65:\tlearn: 0.0086248\ttotal: 374ms\tremaining: 1.32s\n",
      "66:\tlearn: 0.0082630\ttotal: 379ms\tremaining: 1.32s\n",
      "67:\tlearn: 0.0079342\ttotal: 385ms\tremaining: 1.31s\n",
      "68:\tlearn: 0.0074554\ttotal: 390ms\tremaining: 1.3s\n",
      "69:\tlearn: 0.0072957\ttotal: 395ms\tremaining: 1.3s\n",
      "70:\tlearn: 0.0070608\ttotal: 399ms\tremaining: 1.29s\n",
      "71:\tlearn: 0.0066960\ttotal: 404ms\tremaining: 1.28s\n",
      "72:\tlearn: 0.0064762\ttotal: 409ms\tremaining: 1.27s\n",
      "73:\tlearn: 0.0062854\ttotal: 414ms\tremaining: 1.26s\n",
      "74:\tlearn: 0.0060023\ttotal: 419ms\tremaining: 1.26s\n",
      "75:\tlearn: 0.0058222\ttotal: 424ms\tremaining: 1.25s\n",
      "76:\tlearn: 0.0056479\ttotal: 429ms\tremaining: 1.24s\n",
      "77:\tlearn: 0.0054630\ttotal: 434ms\tremaining: 1.24s\n",
      "78:\tlearn: 0.0051499\ttotal: 439ms\tremaining: 1.23s\n",
      "79:\tlearn: 0.0048815\ttotal: 444ms\tremaining: 1.22s\n",
      "80:\tlearn: 0.0046836\ttotal: 449ms\tremaining: 1.21s\n",
      "81:\tlearn: 0.0045172\ttotal: 454ms\tremaining: 1.21s\n",
      "82:\tlearn: 0.0043152\ttotal: 459ms\tremaining: 1.2s\n",
      "83:\tlearn: 0.0042324\ttotal: 465ms\tremaining: 1.19s\n",
      "84:\tlearn: 0.0041190\ttotal: 470ms\tremaining: 1.19s\n",
      "85:\tlearn: 0.0039345\ttotal: 475ms\tremaining: 1.18s\n",
      "86:\tlearn: 0.0038453\ttotal: 481ms\tremaining: 1.18s\n",
      "87:\tlearn: 0.0037821\ttotal: 486ms\tremaining: 1.17s\n",
      "88:\tlearn: 0.0036722\ttotal: 491ms\tremaining: 1.16s\n",
      "89:\tlearn: 0.0035690\ttotal: 496ms\tremaining: 1.16s\n",
      "90:\tlearn: 0.0035039\ttotal: 501ms\tremaining: 1.15s\n",
      "91:\tlearn: 0.0034471\ttotal: 506ms\tremaining: 1.14s\n",
      "92:\tlearn: 0.0032992\ttotal: 511ms\tremaining: 1.14s\n",
      "93:\tlearn: 0.0031397\ttotal: 516ms\tremaining: 1.13s\n",
      "94:\tlearn: 0.0030480\ttotal: 521ms\tremaining: 1.12s\n",
      "95:\tlearn: 0.0029145\ttotal: 526ms\tremaining: 1.12s\n",
      "96:\tlearn: 0.0028604\ttotal: 531ms\tremaining: 1.11s\n",
      "97:\tlearn: 0.0027809\ttotal: 536ms\tremaining: 1.1s\n",
      "98:\tlearn: 0.0026930\ttotal: 541ms\tremaining: 1.1s\n",
      "99:\tlearn: 0.0026373\ttotal: 546ms\tremaining: 1.09s\n",
      "100:\tlearn: 0.0025748\ttotal: 551ms\tremaining: 1.08s\n",
      "101:\tlearn: 0.0024241\ttotal: 556ms\tremaining: 1.08s\n",
      "102:\tlearn: 0.0023254\ttotal: 561ms\tremaining: 1.07s\n",
      "103:\tlearn: 0.0022726\ttotal: 566ms\tremaining: 1.07s\n",
      "104:\tlearn: 0.0021983\ttotal: 572ms\tremaining: 1.06s\n",
      "105:\tlearn: 0.0021472\ttotal: 577ms\tremaining: 1.05s\n",
      "106:\tlearn: 0.0020816\ttotal: 582ms\tremaining: 1.05s\n",
      "107:\tlearn: 0.0020368\ttotal: 587ms\tremaining: 1.04s\n",
      "108:\tlearn: 0.0019614\ttotal: 593ms\tremaining: 1.04s\n",
      "109:\tlearn: 0.0019060\ttotal: 598ms\tremaining: 1.03s\n",
      "110:\tlearn: 0.0018658\ttotal: 603ms\tremaining: 1.03s\n",
      "111:\tlearn: 0.0018215\ttotal: 608ms\tremaining: 1.02s\n",
      "112:\tlearn: 0.0018215\ttotal: 614ms\tremaining: 1.01s\n",
      "113:\tlearn: 0.0017861\ttotal: 619ms\tremaining: 1.01s\n",
      "114:\tlearn: 0.0017523\ttotal: 624ms\tremaining: 1s\n",
      "115:\tlearn: 0.0016822\ttotal: 629ms\tremaining: 998ms\n",
      "116:\tlearn: 0.0016518\ttotal: 635ms\tremaining: 993ms\n",
      "117:\tlearn: 0.0016517\ttotal: 639ms\tremaining: 986ms\n",
      "118:\tlearn: 0.0016070\ttotal: 644ms\tremaining: 980ms\n",
      "119:\tlearn: 0.0015549\ttotal: 649ms\tremaining: 974ms\n",
      "120:\tlearn: 0.0015160\ttotal: 654ms\tremaining: 968ms\n",
      "121:\tlearn: 0.0015160\ttotal: 659ms\tremaining: 961ms\n",
      "122:\tlearn: 0.0014842\ttotal: 664ms\tremaining: 955ms\n",
      "123:\tlearn: 0.0014842\ttotal: 668ms\tremaining: 949ms\n",
      "124:\tlearn: 0.0014668\ttotal: 673ms\tremaining: 943ms\n",
      "125:\tlearn: 0.0014667\ttotal: 678ms\tremaining: 936ms\n",
      "126:\tlearn: 0.0014666\ttotal: 683ms\tremaining: 930ms\n",
      "127:\tlearn: 0.0014666\ttotal: 687ms\tremaining: 924ms\n",
      "128:\tlearn: 0.0014665\ttotal: 692ms\tremaining: 918ms\n",
      "129:\tlearn: 0.0014386\ttotal: 698ms\tremaining: 912ms\n",
      "130:\tlearn: 0.0013911\ttotal: 703ms\tremaining: 907ms\n",
      "131:\tlearn: 0.0013686\ttotal: 708ms\tremaining: 901ms\n",
      "132:\tlearn: 0.0013272\ttotal: 713ms\tremaining: 895ms\n",
      "133:\tlearn: 0.0013272\ttotal: 718ms\tremaining: 889ms\n",
      "134:\tlearn: 0.0012907\ttotal: 723ms\tremaining: 884ms\n",
      "135:\tlearn: 0.0012906\ttotal: 728ms\tremaining: 878ms\n",
      "136:\tlearn: 0.0012906\ttotal: 733ms\tremaining: 872ms\n",
      "137:\tlearn: 0.0012609\ttotal: 738ms\tremaining: 866ms\n",
      "138:\tlearn: 0.0012608\ttotal: 743ms\tremaining: 861ms\n",
      "139:\tlearn: 0.0012328\ttotal: 748ms\tremaining: 855ms\n",
      "140:\tlearn: 0.0012328\ttotal: 753ms\tremaining: 849ms\n",
      "141:\tlearn: 0.0012090\ttotal: 758ms\tremaining: 844ms\n",
      "142:\tlearn: 0.0012090\ttotal: 763ms\tremaining: 838ms\n",
      "143:\tlearn: 0.0012090\ttotal: 768ms\tremaining: 832ms\n",
      "144:\tlearn: 0.0011840\ttotal: 773ms\tremaining: 826ms\n",
      "145:\tlearn: 0.0011840\ttotal: 778ms\tremaining: 820ms\n",
      "146:\tlearn: 0.0011840\ttotal: 783ms\tremaining: 815ms\n",
      "147:\tlearn: 0.0011656\ttotal: 788ms\tremaining: 809ms\n",
      "148:\tlearn: 0.0011656\ttotal: 793ms\tremaining: 803ms\n",
      "149:\tlearn: 0.0011192\ttotal: 798ms\tremaining: 798ms\n",
      "150:\tlearn: 0.0011192\ttotal: 803ms\tremaining: 792ms\n",
      "151:\tlearn: 0.0011192\ttotal: 807ms\tremaining: 786ms\n",
      "152:\tlearn: 0.0011040\ttotal: 813ms\tremaining: 781ms\n",
      "153:\tlearn: 0.0011039\ttotal: 818ms\tremaining: 775ms\n",
      "154:\tlearn: 0.0010779\ttotal: 823ms\tremaining: 770ms\n",
      "155:\tlearn: 0.0010779\ttotal: 828ms\tremaining: 765ms\n",
      "156:\tlearn: 0.0010679\ttotal: 834ms\tremaining: 759ms\n",
      "157:\tlearn: 0.0010679\ttotal: 839ms\tremaining: 754ms\n",
      "158:\tlearn: 0.0010679\ttotal: 844ms\tremaining: 748ms\n",
      "159:\tlearn: 0.0010679\ttotal: 849ms\tremaining: 743ms\n",
      "160:\tlearn: 0.0010678\ttotal: 854ms\tremaining: 738ms\n",
      "161:\tlearn: 0.0010678\ttotal: 859ms\tremaining: 732ms\n",
      "162:\tlearn: 0.0010429\ttotal: 865ms\tremaining: 727ms\n",
      "163:\tlearn: 0.0010429\ttotal: 870ms\tremaining: 721ms\n",
      "164:\tlearn: 0.0010005\ttotal: 876ms\tremaining: 716ms\n",
      "165:\tlearn: 0.0010005\ttotal: 881ms\tremaining: 711ms\n",
      "166:\tlearn: 0.0009627\ttotal: 886ms\tremaining: 706ms\n",
      "167:\tlearn: 0.0009627\ttotal: 892ms\tremaining: 701ms\n",
      "168:\tlearn: 0.0009627\ttotal: 897ms\tremaining: 695ms\n",
      "169:\tlearn: 0.0009626\ttotal: 902ms\tremaining: 690ms\n",
      "170:\tlearn: 0.0009505\ttotal: 908ms\tremaining: 685ms\n",
      "171:\tlearn: 0.0009505\ttotal: 913ms\tremaining: 679ms\n",
      "172:\tlearn: 0.0009505\ttotal: 918ms\tremaining: 674ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173:\tlearn: 0.0009505\ttotal: 923ms\tremaining: 668ms\n",
      "174:\tlearn: 0.0009505\ttotal: 928ms\tremaining: 663ms\n",
      "175:\tlearn: 0.0009505\ttotal: 934ms\tremaining: 658ms\n",
      "176:\tlearn: 0.0009505\ttotal: 939ms\tremaining: 652ms\n",
      "177:\tlearn: 0.0009504\ttotal: 944ms\tremaining: 647ms\n",
      "178:\tlearn: 0.0009504\ttotal: 950ms\tremaining: 642ms\n",
      "179:\tlearn: 0.0009504\ttotal: 955ms\tremaining: 637ms\n",
      "180:\tlearn: 0.0009252\ttotal: 960ms\tremaining: 631ms\n",
      "181:\tlearn: 0.0009251\ttotal: 966ms\tremaining: 626ms\n",
      "182:\tlearn: 0.0009251\ttotal: 971ms\tremaining: 621ms\n",
      "183:\tlearn: 0.0009251\ttotal: 976ms\tremaining: 616ms\n",
      "184:\tlearn: 0.0008987\ttotal: 982ms\tremaining: 611ms\n",
      "185:\tlearn: 0.0008774\ttotal: 988ms\tremaining: 605ms\n",
      "186:\tlearn: 0.0008774\ttotal: 993ms\tremaining: 600ms\n",
      "187:\tlearn: 0.0008774\ttotal: 998ms\tremaining: 595ms\n",
      "188:\tlearn: 0.0008774\ttotal: 1s\tremaining: 589ms\n",
      "189:\tlearn: 0.0008774\ttotal: 1.01s\tremaining: 584ms\n",
      "190:\tlearn: 0.0008774\ttotal: 1.01s\tremaining: 578ms\n",
      "191:\tlearn: 0.0008774\ttotal: 1.02s\tremaining: 573ms\n",
      "192:\tlearn: 0.0008585\ttotal: 1.02s\tremaining: 568ms\n",
      "193:\tlearn: 0.0008585\ttotal: 1.03s\tremaining: 562ms\n",
      "194:\tlearn: 0.0008585\ttotal: 1.03s\tremaining: 557ms\n",
      "195:\tlearn: 0.0008585\ttotal: 1.04s\tremaining: 551ms\n",
      "196:\tlearn: 0.0008585\ttotal: 1.04s\tremaining: 546ms\n",
      "197:\tlearn: 0.0008584\ttotal: 1.05s\tremaining: 540ms\n",
      "198:\tlearn: 0.0008584\ttotal: 1.05s\tremaining: 535ms\n",
      "199:\tlearn: 0.0008584\ttotal: 1.06s\tremaining: 529ms\n",
      "200:\tlearn: 0.0008584\ttotal: 1.06s\tremaining: 524ms\n",
      "201:\tlearn: 0.0008584\ttotal: 1.07s\tremaining: 518ms\n",
      "202:\tlearn: 0.0008269\ttotal: 1.07s\tremaining: 512ms\n",
      "203:\tlearn: 0.0008269\ttotal: 1.08s\tremaining: 507ms\n",
      "204:\tlearn: 0.0008269\ttotal: 1.08s\tremaining: 501ms\n",
      "205:\tlearn: 0.0008269\ttotal: 1.09s\tremaining: 496ms\n",
      "206:\tlearn: 0.0008268\ttotal: 1.09s\tremaining: 490ms\n",
      "207:\tlearn: 0.0008268\ttotal: 1.09s\tremaining: 485ms\n",
      "208:\tlearn: 0.0008055\ttotal: 1.1s\tremaining: 479ms\n",
      "209:\tlearn: 0.0008055\ttotal: 1.1s\tremaining: 474ms\n",
      "210:\tlearn: 0.0008055\ttotal: 1.11s\tremaining: 468ms\n",
      "211:\tlearn: 0.0008055\ttotal: 1.11s\tremaining: 463ms\n",
      "212:\tlearn: 0.0008054\ttotal: 1.12s\tremaining: 457ms\n",
      "213:\tlearn: 0.0008054\ttotal: 1.12s\tremaining: 452ms\n",
      "214:\tlearn: 0.0008054\ttotal: 1.13s\tremaining: 447ms\n",
      "215:\tlearn: 0.0008054\ttotal: 1.13s\tremaining: 441ms\n",
      "216:\tlearn: 0.0008054\ttotal: 1.14s\tremaining: 436ms\n",
      "217:\tlearn: 0.0008054\ttotal: 1.14s\tremaining: 430ms\n",
      "218:\tlearn: 0.0008054\ttotal: 1.15s\tremaining: 425ms\n",
      "219:\tlearn: 0.0008054\ttotal: 1.15s\tremaining: 420ms\n",
      "220:\tlearn: 0.0008054\ttotal: 1.16s\tremaining: 414ms\n",
      "221:\tlearn: 0.0008053\ttotal: 1.16s\tremaining: 409ms\n",
      "222:\tlearn: 0.0008053\ttotal: 1.17s\tremaining: 404ms\n",
      "223:\tlearn: 0.0008053\ttotal: 1.17s\tremaining: 398ms\n",
      "224:\tlearn: 0.0008053\ttotal: 1.18s\tremaining: 393ms\n",
      "225:\tlearn: 0.0008053\ttotal: 1.18s\tremaining: 387ms\n",
      "226:\tlearn: 0.0008053\ttotal: 1.19s\tremaining: 382ms\n",
      "227:\tlearn: 0.0008053\ttotal: 1.19s\tremaining: 376ms\n",
      "228:\tlearn: 0.0008053\ttotal: 1.2s\tremaining: 371ms\n",
      "229:\tlearn: 0.0008053\ttotal: 1.2s\tremaining: 366ms\n",
      "230:\tlearn: 0.0008053\ttotal: 1.21s\tremaining: 360ms\n",
      "231:\tlearn: 0.0008053\ttotal: 1.21s\tremaining: 355ms\n",
      "232:\tlearn: 0.0008053\ttotal: 1.22s\tremaining: 349ms\n",
      "233:\tlearn: 0.0008053\ttotal: 1.22s\tremaining: 344ms\n",
      "234:\tlearn: 0.0008053\ttotal: 1.22s\tremaining: 339ms\n",
      "235:\tlearn: 0.0008052\ttotal: 1.23s\tremaining: 333ms\n",
      "236:\tlearn: 0.0008052\ttotal: 1.23s\tremaining: 328ms\n",
      "237:\tlearn: 0.0008052\ttotal: 1.24s\tremaining: 323ms\n",
      "238:\tlearn: 0.0008052\ttotal: 1.24s\tremaining: 317ms\n",
      "239:\tlearn: 0.0008052\ttotal: 1.25s\tremaining: 312ms\n",
      "240:\tlearn: 0.0008052\ttotal: 1.25s\tremaining: 307ms\n",
      "241:\tlearn: 0.0008051\ttotal: 1.26s\tremaining: 301ms\n",
      "242:\tlearn: 0.0008051\ttotal: 1.26s\tremaining: 296ms\n",
      "243:\tlearn: 0.0008051\ttotal: 1.27s\tremaining: 291ms\n",
      "244:\tlearn: 0.0008051\ttotal: 1.27s\tremaining: 286ms\n",
      "245:\tlearn: 0.0008051\ttotal: 1.28s\tremaining: 280ms\n",
      "246:\tlearn: 0.0008050\ttotal: 1.28s\tremaining: 275ms\n",
      "247:\tlearn: 0.0008050\ttotal: 1.29s\tremaining: 271ms\n",
      "248:\tlearn: 0.0008050\ttotal: 1.3s\tremaining: 266ms\n",
      "249:\tlearn: 0.0008049\ttotal: 1.3s\tremaining: 261ms\n",
      "250:\tlearn: 0.0008049\ttotal: 1.31s\tremaining: 255ms\n",
      "251:\tlearn: 0.0008049\ttotal: 1.31s\tremaining: 250ms\n",
      "252:\tlearn: 0.0008049\ttotal: 1.32s\tremaining: 245ms\n",
      "253:\tlearn: 0.0008049\ttotal: 1.32s\tremaining: 240ms\n",
      "254:\tlearn: 0.0008049\ttotal: 1.33s\tremaining: 234ms\n",
      "255:\tlearn: 0.0008049\ttotal: 1.33s\tremaining: 229ms\n",
      "256:\tlearn: 0.0008049\ttotal: 1.34s\tremaining: 224ms\n",
      "257:\tlearn: 0.0008049\ttotal: 1.34s\tremaining: 218ms\n",
      "258:\tlearn: 0.0008048\ttotal: 1.35s\tremaining: 213ms\n",
      "259:\tlearn: 0.0008048\ttotal: 1.35s\tremaining: 208ms\n",
      "260:\tlearn: 0.0008048\ttotal: 1.35s\tremaining: 203ms\n",
      "261:\tlearn: 0.0008048\ttotal: 1.36s\tremaining: 197ms\n",
      "262:\tlearn: 0.0008047\ttotal: 1.36s\tremaining: 192ms\n",
      "263:\tlearn: 0.0008047\ttotal: 1.37s\tremaining: 187ms\n",
      "264:\tlearn: 0.0008047\ttotal: 1.37s\tremaining: 182ms\n",
      "265:\tlearn: 0.0008047\ttotal: 1.38s\tremaining: 176ms\n",
      "266:\tlearn: 0.0008047\ttotal: 1.38s\tremaining: 171ms\n",
      "267:\tlearn: 0.0008047\ttotal: 1.39s\tremaining: 166ms\n",
      "268:\tlearn: 0.0008046\ttotal: 1.39s\tremaining: 161ms\n",
      "269:\tlearn: 0.0008046\ttotal: 1.4s\tremaining: 155ms\n",
      "270:\tlearn: 0.0008046\ttotal: 1.4s\tremaining: 150ms\n",
      "271:\tlearn: 0.0008046\ttotal: 1.41s\tremaining: 145ms\n",
      "272:\tlearn: 0.0008046\ttotal: 1.41s\tremaining: 140ms\n",
      "273:\tlearn: 0.0008046\ttotal: 1.42s\tremaining: 135ms\n",
      "274:\tlearn: 0.0008046\ttotal: 1.42s\tremaining: 129ms\n",
      "275:\tlearn: 0.0008046\ttotal: 1.43s\tremaining: 124ms\n",
      "276:\tlearn: 0.0008046\ttotal: 1.43s\tremaining: 119ms\n",
      "277:\tlearn: 0.0008045\ttotal: 1.44s\tremaining: 114ms\n",
      "278:\tlearn: 0.0008045\ttotal: 1.44s\tremaining: 109ms\n",
      "279:\tlearn: 0.0008045\ttotal: 1.45s\tremaining: 103ms\n",
      "280:\tlearn: 0.0008045\ttotal: 1.45s\tremaining: 98.2ms\n",
      "281:\tlearn: 0.0008045\ttotal: 1.46s\tremaining: 93ms\n",
      "282:\tlearn: 0.0008045\ttotal: 1.46s\tremaining: 87.8ms\n",
      "283:\tlearn: 0.0008045\ttotal: 1.47s\tremaining: 82.6ms\n",
      "284:\tlearn: 0.0008044\ttotal: 1.47s\tremaining: 77.4ms\n",
      "285:\tlearn: 0.0008043\ttotal: 1.48s\tremaining: 72.3ms\n",
      "286:\tlearn: 0.0008043\ttotal: 1.48s\tremaining: 67.1ms\n",
      "287:\tlearn: 0.0008043\ttotal: 1.49s\tremaining: 61.9ms\n",
      "288:\tlearn: 0.0008043\ttotal: 1.49s\tremaining: 56.7ms\n",
      "289:\tlearn: 0.0008043\ttotal: 1.5s\tremaining: 51.6ms\n",
      "290:\tlearn: 0.0008043\ttotal: 1.5s\tremaining: 46.4ms\n",
      "291:\tlearn: 0.0008043\ttotal: 1.5s\tremaining: 41.2ms\n",
      "292:\tlearn: 0.0008043\ttotal: 1.51s\tremaining: 36.1ms\n",
      "293:\tlearn: 0.0008042\ttotal: 1.51s\tremaining: 30.9ms\n",
      "294:\tlearn: 0.0008042\ttotal: 1.52s\tremaining: 25.8ms\n",
      "295:\tlearn: 0.0008042\ttotal: 1.52s\tremaining: 20.6ms\n",
      "296:\tlearn: 0.0008042\ttotal: 1.53s\tremaining: 15.5ms\n",
      "297:\tlearn: 0.0008042\ttotal: 1.54s\tremaining: 10.3ms\n",
      "298:\tlearn: 0.0008042\ttotal: 1.54s\tremaining: 5.16ms\n",
      "299:\tlearn: 0.0008042\ttotal: 1.55s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18001a587c8245b8b8f12b17d5203977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6076947777291566, Recall = 0.9019051479529794, Aging Rate = 0.9000810701256587, Precision = 0.5010132852961044, f1 = 0.6441806601042269\n",
      "Epoch 2: Train Loss = 0.5119187136168996, Recall = 0.9983785974868261, Aging Rate = 0.9770976895014187, Precision = 0.5108898568761667, f1 = 0.6759055982436882\n",
      "Epoch 3: Train Loss = 0.4445318347730277, Recall = 0.9728415079043372, Aging Rate = 0.8254965545196595, Precision = 0.5892462558310827, f1 = 0.7339449541284403\n",
      "Epoch 4: Train Loss = 0.39536015250250833, Recall = 0.9436562626672071, Aging Rate = 0.7012565869477098, Precision = 0.6728323699421965, f1 = 0.7855576176817951\n",
      "Epoch 5: Train Loss = 0.3565951732183981, Recall = 0.9485204702067288, Aging Rate = 0.6609241994325091, Precision = 0.7175712971481141, f1 = 0.8170391061452514\n",
      "Test Loss = 0.32021896866685134, Recall = 0.9643291447101743, Aging Rate = 0.6295095257397649, precision = 0.7659368963296845\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.30193880946954155, Recall = 0.9574381840291852, Aging Rate = 0.6082286177543575, Precision = 0.7870709763412196, f1 = 0.8639356254572056\n",
      "Epoch 7: Train Loss = 0.25894455246878, Recall = 0.9590595865423591, Aging Rate = 0.5709363599513579, Precision = 0.8399006034788783, f1 = 0.8955336866010598\n",
      "Epoch 8: Train Loss = 0.2201262177558964, Recall = 0.968382650993109, Aging Rate = 0.5563437373327929, Precision = 0.8703096539162113, f1 = 0.9167306216423637\n",
      "Epoch 9: Train Loss = 0.18746921475667144, Recall = 0.976895014187272, Aging Rate = 0.542359140656668, Precision = 0.9005979073243647, f1 = 0.9371961889947501\n",
      "Epoch 10: Train Loss = 0.15971561662934305, Recall = 0.9801378192136198, Aging Rate = 0.5326307255776247, Precision = 0.9200913242009132, f1 = 0.9491658488714425\n",
      "Test Loss = 0.1445611693662673, Recall = 0.9890555330360762, Aging Rate = 0.5356708552898257, precision = 0.9231933409004919\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.13678153979667151, Recall = 0.9850020267531414, Aging Rate = 0.5226996351844345, Precision = 0.9422256688639007, f1 = 0.9631391200951249\n",
      "Epoch 12: Train Loss = 0.11705344770425333, Recall = 0.9894608836643697, Aging Rate = 0.520064856100527, Precision = 0.95128604832424, f1 = 0.9699980131134511\n",
      "Epoch 13: Train Loss = 0.1016729737782604, Recall = 0.9906769355492501, Aging Rate = 0.5127685447912445, Precision = 0.9660079051383399, f1 = 0.9781869121472885\n",
      "Epoch 14: Train Loss = 0.08832262600777746, Recall = 0.9927036886907175, Aging Rate = 0.5095257397648967, Precision = 0.9741447891805887, f1 = 0.9833366793816503\n",
      "Epoch 15: Train Loss = 0.07708775678541577, Recall = 0.9939197405755978, Aging Rate = 0.5074989866234293, Precision = 0.9792332268370607, f1 = 0.9865218265942466\n",
      "Test Loss = 0.07171664531348348, Recall = 0.9983785974868261, Aging Rate = 0.5129712201053912, precision = 0.973133148952983\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.06776060022936208, Recall = 0.9967571949736522, Aging Rate = 0.5064856100526955, Precision = 0.9839935974389756, f1 = 0.9903342730567861\n",
      "Epoch 17: Train Loss = 0.060348762393323915, Recall = 0.9963518443453587, Aging Rate = 0.5060802594244022, Precision = 0.9843812575090108, f1 = 0.9903303787268332\n",
      "Epoch 18: Train Loss = 0.05292972142233943, Recall = 0.9975678962302391, Aging Rate = 0.5046615322253749, Precision = 0.9883534136546185, f1 = 0.9929392777889853\n",
      "Epoch 19: Train Loss = 0.047219352338373974, Recall = 0.9979732468585326, Aging Rate = 0.5046615322253749, Precision = 0.9887550200803212, f1 = 0.9933427476296146\n",
      "Epoch 20: Train Loss = 0.042982678209594935, Recall = 0.9987839481151196, Aging Rate = 0.5018240778273206, Precision = 0.9951534733441034, f1 = 0.9969654056241151\n",
      "Test Loss = 0.03829063322315054, Recall = 0.999189298743413, Aging Rate = 0.5028374543980543, precision = 0.9935509875050383\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.037968287156101274, Recall = 0.9987839481151196, Aging Rate = 0.503040129712201, Precision = 0.9927477840451249, f1 = 0.9957567185289957\n",
      "Epoch 22: Train Loss = 0.0343755457171265, Recall = 0.9995946493717065, Aging Rate = 0.5024321037697609, Precision = 0.9947559499798305, f1 = 0.9971694298422967\n",
      "Epoch 23: Train Loss = 0.03116211973290768, Recall = 1.0, Aging Rate = 0.5016214025131739, Precision = 0.9967676767676767, f1 = 0.9983812221772561\n",
      "Epoch 24: Train Loss = 0.028156149726214563, Recall = 1.0, Aging Rate = 0.5014187271990271, Precision = 0.9971705739692805, f1 = 0.9985832827362882\n",
      "Epoch 25: Train Loss = 0.025644551163798265, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Test Loss = 0.02347528276550514, Recall = 1.0, Aging Rate = 0.5004053506282935, precision = 0.9991899554475496\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.02350214816083249, Recall = 1.0, Aging Rate = 0.500810701256587, Precision = 0.9983812221772562, f1 = 0.9991899554475496\n",
      "Epoch 27: Train Loss = 0.021541062277376677, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Epoch 28: Train Loss = 0.01979135582393415, Recall = 1.0, Aging Rate = 0.5006080259424402, Precision = 0.9987854251012146, f1 = 0.9993923435284586\n",
      "Epoch 29: Train Loss = 0.018269808754931952, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Epoch 30: Train Loss = 0.01683138839426338, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Test Loss = 0.015495487580919223, Recall = 1.0, Aging Rate = 0.5002026753141467, precision = 0.9995948136142626\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.015804373526686736, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 32: Train Loss = 0.014460530033902113, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 33: Train Loss = 0.013455353368090313, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 34: Train Loss = 0.012666249221525285, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 35: Train Loss = 0.01184395955136347, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Test Loss = 0.011139807872320169, Recall = 1.0, Aging Rate = 0.5002026753141467, precision = 0.9995948136142626\n",
      "\n",
      "Epoch 36: Train Loss = 0.01116215162897308, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 37: Train Loss = 0.010456110694323733, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 38: Train Loss = 0.009812443902588442, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 39: Train Loss = 0.009185821204519872, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.008679567300912747, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.008199307941471124, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.008271518042012838, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.007907072383794143, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.007535894117104799, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0070491312539796135, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.006742694247914172, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006330828533863122, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.006458468068720275, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.006177442477518458, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0059115798807169535, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.005669745070543434, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.005435607534141272, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005148627231842081, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss = 0.0052254897832793475, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.005031888466819148, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.004845371377836712, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004697339718932619, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.004514714159157518, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004255087897358942, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.004373484443462434, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.004227965292500058, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.004117863330904003, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.003994186482998914, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0038335458199970384, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0037442846655963424, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.003748506464647, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0037070429973745204, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.003534474102066725, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.003474501798168749, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.003375608035724536, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003206709645013542, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0032826347887571893, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0032616609510790857, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0031286279642958243, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.00305802588811447, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0030457228592989183, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0028837855698040224, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.002946479926863553, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.002923533152150882, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0028424287293397758, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0027911062308001613, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0027274365403701147, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0025890988843643116, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0026984180170509632, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.002644766567000854, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0026191225955674267, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.002580059691791713, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.002541527771181455, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0024095157616823475, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0026058740764750635, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0024856494557124534, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.00250258768986447, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.002451782150667449, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0023743995912878933, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022427994669364094, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0023475813100482163, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0023305559031405595, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0023099398513199603, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0022984728699208005, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0022759468842639065, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00214971759664042, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3591626\ttotal: 6.45ms\tremaining: 1.93s\n",
      "1:\tlearn: 0.2363130\ttotal: 12.5ms\tremaining: 1.85s\n",
      "2:\tlearn: 0.1805215\ttotal: 18.2ms\tremaining: 1.81s\n",
      "3:\tlearn: 0.1553152\ttotal: 24.1ms\tremaining: 1.78s\n",
      "4:\tlearn: 0.1386584\ttotal: 29.8ms\tremaining: 1.76s\n",
      "5:\tlearn: 0.1302553\ttotal: 35.6ms\tremaining: 1.75s\n",
      "6:\tlearn: 0.1216837\ttotal: 41.5ms\tremaining: 1.74s\n",
      "7:\tlearn: 0.1155692\ttotal: 47.4ms\tremaining: 1.73s\n",
      "8:\tlearn: 0.1099773\ttotal: 53.3ms\tremaining: 1.72s\n",
      "9:\tlearn: 0.1056492\ttotal: 59.3ms\tremaining: 1.72s\n",
      "10:\tlearn: 0.1004436\ttotal: 65.2ms\tremaining: 1.71s\n",
      "11:\tlearn: 0.0950137\ttotal: 71.2ms\tremaining: 1.71s\n",
      "12:\tlearn: 0.0889071\ttotal: 76.8ms\tremaining: 1.7s\n",
      "13:\tlearn: 0.0853518\ttotal: 81.9ms\tremaining: 1.67s\n",
      "14:\tlearn: 0.0820661\ttotal: 87.1ms\tremaining: 1.66s\n",
      "15:\tlearn: 0.0799480\ttotal: 92.3ms\tremaining: 1.64s\n",
      "16:\tlearn: 0.0779839\ttotal: 97.6ms\tremaining: 1.62s\n",
      "17:\tlearn: 0.0755860\ttotal: 103ms\tremaining: 1.61s\n",
      "18:\tlearn: 0.0724712\ttotal: 108ms\tremaining: 1.6s\n",
      "19:\tlearn: 0.0699510\ttotal: 113ms\tremaining: 1.59s\n",
      "20:\tlearn: 0.0686421\ttotal: 119ms\tremaining: 1.58s\n",
      "21:\tlearn: 0.0641608\ttotal: 124ms\tremaining: 1.57s\n",
      "22:\tlearn: 0.0600927\ttotal: 130ms\tremaining: 1.56s\n",
      "23:\tlearn: 0.0562352\ttotal: 135ms\tremaining: 1.55s\n",
      "24:\tlearn: 0.0521492\ttotal: 141ms\tremaining: 1.54s\n",
      "25:\tlearn: 0.0497930\ttotal: 146ms\tremaining: 1.54s\n",
      "26:\tlearn: 0.0482679\ttotal: 151ms\tremaining: 1.53s\n",
      "27:\tlearn: 0.0467389\ttotal: 157ms\tremaining: 1.52s\n",
      "28:\tlearn: 0.0450357\ttotal: 162ms\tremaining: 1.51s\n",
      "29:\tlearn: 0.0434556\ttotal: 168ms\tremaining: 1.51s\n",
      "30:\tlearn: 0.0408614\ttotal: 173ms\tremaining: 1.5s\n",
      "31:\tlearn: 0.0375312\ttotal: 179ms\tremaining: 1.5s\n",
      "32:\tlearn: 0.0360903\ttotal: 184ms\tremaining: 1.49s\n",
      "33:\tlearn: 0.0351550\ttotal: 190ms\tremaining: 1.48s\n",
      "34:\tlearn: 0.0340995\ttotal: 195ms\tremaining: 1.48s\n",
      "35:\tlearn: 0.0325100\ttotal: 200ms\tremaining: 1.47s\n",
      "36:\tlearn: 0.0311085\ttotal: 206ms\tremaining: 1.46s\n",
      "37:\tlearn: 0.0293962\ttotal: 211ms\tremaining: 1.46s\n",
      "38:\tlearn: 0.0284267\ttotal: 216ms\tremaining: 1.45s\n",
      "39:\tlearn: 0.0278907\ttotal: 222ms\tremaining: 1.44s\n",
      "40:\tlearn: 0.0265812\ttotal: 227ms\tremaining: 1.43s\n",
      "41:\tlearn: 0.0252790\ttotal: 232ms\tremaining: 1.43s\n",
      "42:\tlearn: 0.0245255\ttotal: 238ms\tremaining: 1.42s\n",
      "43:\tlearn: 0.0221890\ttotal: 243ms\tremaining: 1.42s\n",
      "44:\tlearn: 0.0213573\ttotal: 249ms\tremaining: 1.41s\n",
      "45:\tlearn: 0.0204257\ttotal: 254ms\tremaining: 1.4s\n",
      "46:\tlearn: 0.0196120\ttotal: 259ms\tremaining: 1.4s\n",
      "47:\tlearn: 0.0189461\ttotal: 265ms\tremaining: 1.39s\n",
      "48:\tlearn: 0.0179286\ttotal: 270ms\tremaining: 1.38s\n",
      "49:\tlearn: 0.0172123\ttotal: 276ms\tremaining: 1.38s\n",
      "50:\tlearn: 0.0165029\ttotal: 281ms\tremaining: 1.37s\n",
      "51:\tlearn: 0.0159712\ttotal: 286ms\tremaining: 1.36s\n",
      "52:\tlearn: 0.0152547\ttotal: 292ms\tremaining: 1.36s\n",
      "53:\tlearn: 0.0143438\ttotal: 297ms\tremaining: 1.35s\n",
      "54:\tlearn: 0.0138246\ttotal: 302ms\tremaining: 1.35s\n",
      "55:\tlearn: 0.0133503\ttotal: 309ms\tremaining: 1.34s\n",
      "56:\tlearn: 0.0126919\ttotal: 314ms\tremaining: 1.34s\n",
      "57:\tlearn: 0.0122608\ttotal: 319ms\tremaining: 1.33s\n",
      "58:\tlearn: 0.0115526\ttotal: 325ms\tremaining: 1.33s\n",
      "59:\tlearn: 0.0113474\ttotal: 330ms\tremaining: 1.32s\n",
      "60:\tlearn: 0.0110169\ttotal: 335ms\tremaining: 1.31s\n",
      "61:\tlearn: 0.0104928\ttotal: 340ms\tremaining: 1.31s\n",
      "62:\tlearn: 0.0098996\ttotal: 346ms\tremaining: 1.3s\n",
      "63:\tlearn: 0.0095048\ttotal: 351ms\tremaining: 1.29s\n",
      "64:\tlearn: 0.0091023\ttotal: 356ms\tremaining: 1.29s\n",
      "65:\tlearn: 0.0089160\ttotal: 361ms\tremaining: 1.28s\n",
      "66:\tlearn: 0.0086223\ttotal: 366ms\tremaining: 1.27s\n",
      "67:\tlearn: 0.0083715\ttotal: 371ms\tremaining: 1.27s\n",
      "68:\tlearn: 0.0078723\ttotal: 376ms\tremaining: 1.26s\n",
      "69:\tlearn: 0.0070860\ttotal: 381ms\tremaining: 1.25s\n",
      "70:\tlearn: 0.0068070\ttotal: 386ms\tremaining: 1.25s\n",
      "71:\tlearn: 0.0065463\ttotal: 391ms\tremaining: 1.24s\n",
      "72:\tlearn: 0.0063177\ttotal: 396ms\tremaining: 1.23s\n",
      "73:\tlearn: 0.0060286\ttotal: 401ms\tremaining: 1.23s\n",
      "74:\tlearn: 0.0058116\ttotal: 406ms\tremaining: 1.22s\n",
      "75:\tlearn: 0.0054325\ttotal: 411ms\tremaining: 1.21s\n",
      "76:\tlearn: 0.0052460\ttotal: 416ms\tremaining: 1.21s\n",
      "77:\tlearn: 0.0051397\ttotal: 421ms\tremaining: 1.2s\n",
      "78:\tlearn: 0.0049136\ttotal: 426ms\tremaining: 1.19s\n",
      "79:\tlearn: 0.0047976\ttotal: 431ms\tremaining: 1.19s\n",
      "80:\tlearn: 0.0046846\ttotal: 436ms\tremaining: 1.18s\n",
      "81:\tlearn: 0.0044685\ttotal: 441ms\tremaining: 1.17s\n",
      "82:\tlearn: 0.0043033\ttotal: 447ms\tremaining: 1.17s\n",
      "83:\tlearn: 0.0041561\ttotal: 452ms\tremaining: 1.16s\n",
      "84:\tlearn: 0.0040551\ttotal: 457ms\tremaining: 1.16s\n",
      "85:\tlearn: 0.0038610\ttotal: 462ms\tremaining: 1.15s\n",
      "86:\tlearn: 0.0037492\ttotal: 468ms\tremaining: 1.15s\n",
      "87:\tlearn: 0.0036882\ttotal: 473ms\tremaining: 1.14s\n",
      "88:\tlearn: 0.0035267\ttotal: 478ms\tremaining: 1.13s\n",
      "89:\tlearn: 0.0034496\ttotal: 483ms\tremaining: 1.13s\n",
      "90:\tlearn: 0.0033583\ttotal: 489ms\tremaining: 1.12s\n",
      "91:\tlearn: 0.0032370\ttotal: 494ms\tremaining: 1.12s\n",
      "92:\tlearn: 0.0031691\ttotal: 499ms\tremaining: 1.11s\n",
      "93:\tlearn: 0.0030803\ttotal: 504ms\tremaining: 1.1s\n",
      "94:\tlearn: 0.0029605\ttotal: 509ms\tremaining: 1.1s\n",
      "95:\tlearn: 0.0028444\ttotal: 514ms\tremaining: 1.09s\n",
      "96:\tlearn: 0.0027635\ttotal: 519ms\tremaining: 1.09s\n",
      "97:\tlearn: 0.0026906\ttotal: 524ms\tremaining: 1.08s\n",
      "98:\tlearn: 0.0026152\ttotal: 529ms\tremaining: 1.07s\n",
      "99:\tlearn: 0.0024976\ttotal: 534ms\tremaining: 1.07s\n",
      "100:\tlearn: 0.0024636\ttotal: 539ms\tremaining: 1.06s\n",
      "101:\tlearn: 0.0024003\ttotal: 544ms\tremaining: 1.06s\n",
      "102:\tlearn: 0.0023182\ttotal: 549ms\tremaining: 1.05s\n",
      "103:\tlearn: 0.0022688\ttotal: 555ms\tremaining: 1.04s\n",
      "104:\tlearn: 0.0021923\ttotal: 560ms\tremaining: 1.04s\n",
      "105:\tlearn: 0.0021315\ttotal: 565ms\tremaining: 1.03s\n",
      "106:\tlearn: 0.0020496\ttotal: 571ms\tremaining: 1.03s\n",
      "107:\tlearn: 0.0020097\ttotal: 576ms\tremaining: 1.02s\n",
      "108:\tlearn: 0.0019666\ttotal: 581ms\tremaining: 1.02s\n",
      "109:\tlearn: 0.0019274\ttotal: 586ms\tremaining: 1.01s\n",
      "110:\tlearn: 0.0018977\ttotal: 591ms\tremaining: 1.01s\n",
      "111:\tlearn: 0.0018487\ttotal: 596ms\tremaining: 1s\n",
      "112:\tlearn: 0.0017927\ttotal: 601ms\tremaining: 995ms\n",
      "113:\tlearn: 0.0017364\ttotal: 607ms\tremaining: 990ms\n",
      "114:\tlearn: 0.0017364\ttotal: 611ms\tremaining: 983ms\n",
      "115:\tlearn: 0.0016680\ttotal: 616ms\tremaining: 978ms\n",
      "116:\tlearn: 0.0016224\ttotal: 622ms\tremaining: 972ms\n",
      "117:\tlearn: 0.0015955\ttotal: 627ms\tremaining: 966ms\n",
      "118:\tlearn: 0.0015183\ttotal: 632ms\tremaining: 961ms\n",
      "119:\tlearn: 0.0015183\ttotal: 636ms\tremaining: 955ms\n",
      "120:\tlearn: 0.0015002\ttotal: 641ms\tremaining: 949ms\n",
      "121:\tlearn: 0.0014335\ttotal: 646ms\tremaining: 943ms\n",
      "122:\tlearn: 0.0014332\ttotal: 651ms\tremaining: 937ms\n",
      "123:\tlearn: 0.0013982\ttotal: 657ms\tremaining: 932ms\n",
      "124:\tlearn: 0.0013605\ttotal: 662ms\tremaining: 926ms\n",
      "125:\tlearn: 0.0013287\ttotal: 667ms\tremaining: 920ms\n",
      "126:\tlearn: 0.0012921\ttotal: 672ms\tremaining: 915ms\n",
      "127:\tlearn: 0.0012493\ttotal: 677ms\tremaining: 910ms\n",
      "128:\tlearn: 0.0012493\ttotal: 682ms\tremaining: 904ms\n",
      "129:\tlearn: 0.0012271\ttotal: 687ms\tremaining: 899ms\n",
      "130:\tlearn: 0.0011791\ttotal: 693ms\tremaining: 894ms\n",
      "131:\tlearn: 0.0011790\ttotal: 698ms\tremaining: 889ms\n",
      "132:\tlearn: 0.0011790\ttotal: 704ms\tremaining: 884ms\n",
      "133:\tlearn: 0.0011790\ttotal: 709ms\tremaining: 878ms\n",
      "134:\tlearn: 0.0011478\ttotal: 715ms\tremaining: 873ms\n",
      "135:\tlearn: 0.0011214\ttotal: 720ms\tremaining: 868ms\n",
      "136:\tlearn: 0.0010816\ttotal: 725ms\tremaining: 863ms\n",
      "137:\tlearn: 0.0010604\ttotal: 731ms\tremaining: 858ms\n",
      "138:\tlearn: 0.0010074\ttotal: 737ms\tremaining: 853ms\n",
      "139:\tlearn: 0.0010074\ttotal: 742ms\tremaining: 848ms\n",
      "140:\tlearn: 0.0010074\ttotal: 747ms\tremaining: 842ms\n",
      "141:\tlearn: 0.0010074\ttotal: 752ms\tremaining: 837ms\n",
      "142:\tlearn: 0.0010074\ttotal: 758ms\tremaining: 832ms\n",
      "143:\tlearn: 0.0010073\ttotal: 763ms\tremaining: 826ms\n",
      "144:\tlearn: 0.0010073\ttotal: 768ms\tremaining: 821ms\n",
      "145:\tlearn: 0.0010073\ttotal: 773ms\tremaining: 816ms\n",
      "146:\tlearn: 0.0010073\ttotal: 778ms\tremaining: 810ms\n",
      "147:\tlearn: 0.0010073\ttotal: 784ms\tremaining: 805ms\n",
      "148:\tlearn: 0.0010072\ttotal: 789ms\tremaining: 799ms\n",
      "149:\tlearn: 0.0009996\ttotal: 794ms\tremaining: 794ms\n",
      "150:\tlearn: 0.0009748\ttotal: 799ms\tremaining: 789ms\n",
      "151:\tlearn: 0.0009457\ttotal: 804ms\tremaining: 783ms\n",
      "152:\tlearn: 0.0009457\ttotal: 809ms\tremaining: 778ms\n",
      "153:\tlearn: 0.0009277\ttotal: 815ms\tremaining: 773ms\n",
      "154:\tlearn: 0.0009277\ttotal: 820ms\tremaining: 767ms\n",
      "155:\tlearn: 0.0009277\ttotal: 825ms\tremaining: 762ms\n",
      "156:\tlearn: 0.0009143\ttotal: 830ms\tremaining: 756ms\n",
      "157:\tlearn: 0.0008852\ttotal: 836ms\tremaining: 751ms\n",
      "158:\tlearn: 0.0008758\ttotal: 841ms\tremaining: 746ms\n",
      "159:\tlearn: 0.0008758\ttotal: 846ms\tremaining: 740ms\n",
      "160:\tlearn: 0.0008758\ttotal: 851ms\tremaining: 735ms\n",
      "161:\tlearn: 0.0008758\ttotal: 856ms\tremaining: 729ms\n",
      "162:\tlearn: 0.0008757\ttotal: 861ms\tremaining: 724ms\n",
      "163:\tlearn: 0.0008757\ttotal: 867ms\tremaining: 719ms\n",
      "164:\tlearn: 0.0008757\ttotal: 872ms\tremaining: 713ms\n",
      "165:\tlearn: 0.0008757\ttotal: 877ms\tremaining: 708ms\n",
      "166:\tlearn: 0.0008757\ttotal: 882ms\tremaining: 702ms\n",
      "167:\tlearn: 0.0008757\ttotal: 891ms\tremaining: 700ms\n",
      "168:\tlearn: 0.0008757\ttotal: 896ms\tremaining: 694ms\n",
      "169:\tlearn: 0.0008757\ttotal: 900ms\tremaining: 689ms\n",
      "170:\tlearn: 0.0008559\ttotal: 905ms\tremaining: 683ms\n",
      "171:\tlearn: 0.0008559\ttotal: 910ms\tremaining: 677ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172:\tlearn: 0.0008559\ttotal: 915ms\tremaining: 672ms\n",
      "173:\tlearn: 0.0008559\ttotal: 920ms\tremaining: 666ms\n",
      "174:\tlearn: 0.0008559\ttotal: 925ms\tremaining: 661ms\n",
      "175:\tlearn: 0.0008559\ttotal: 930ms\tremaining: 655ms\n",
      "176:\tlearn: 0.0008374\ttotal: 935ms\tremaining: 650ms\n",
      "177:\tlearn: 0.0008374\ttotal: 939ms\tremaining: 644ms\n",
      "178:\tlearn: 0.0008374\ttotal: 944ms\tremaining: 638ms\n",
      "179:\tlearn: 0.0008374\ttotal: 949ms\tremaining: 633ms\n",
      "180:\tlearn: 0.0008373\ttotal: 954ms\tremaining: 627ms\n",
      "181:\tlearn: 0.0008088\ttotal: 959ms\tremaining: 622ms\n",
      "182:\tlearn: 0.0008087\ttotal: 964ms\tremaining: 617ms\n",
      "183:\tlearn: 0.0008087\ttotal: 970ms\tremaining: 611ms\n",
      "184:\tlearn: 0.0008087\ttotal: 975ms\tremaining: 606ms\n",
      "185:\tlearn: 0.0008086\ttotal: 979ms\tremaining: 600ms\n",
      "186:\tlearn: 0.0008086\ttotal: 985ms\tremaining: 595ms\n",
      "187:\tlearn: 0.0008086\ttotal: 989ms\tremaining: 589ms\n",
      "188:\tlearn: 0.0008086\ttotal: 994ms\tremaining: 584ms\n",
      "189:\tlearn: 0.0008086\ttotal: 999ms\tremaining: 578ms\n",
      "190:\tlearn: 0.0008085\ttotal: 1s\tremaining: 573ms\n",
      "191:\tlearn: 0.0008085\ttotal: 1.01s\tremaining: 567ms\n",
      "192:\tlearn: 0.0008085\ttotal: 1.01s\tremaining: 562ms\n",
      "193:\tlearn: 0.0008085\ttotal: 1.02s\tremaining: 557ms\n",
      "194:\tlearn: 0.0008085\ttotal: 1.02s\tremaining: 551ms\n",
      "195:\tlearn: 0.0008085\ttotal: 1.03s\tremaining: 546ms\n",
      "196:\tlearn: 0.0008085\ttotal: 1.03s\tremaining: 540ms\n",
      "197:\tlearn: 0.0007898\ttotal: 1.04s\tremaining: 535ms\n",
      "198:\tlearn: 0.0007733\ttotal: 1.04s\tremaining: 530ms\n",
      "199:\tlearn: 0.0007733\ttotal: 1.05s\tremaining: 524ms\n",
      "200:\tlearn: 0.0007733\ttotal: 1.05s\tremaining: 519ms\n",
      "201:\tlearn: 0.0007733\ttotal: 1.06s\tremaining: 513ms\n",
      "202:\tlearn: 0.0007733\ttotal: 1.06s\tremaining: 508ms\n",
      "203:\tlearn: 0.0007733\ttotal: 1.07s\tremaining: 502ms\n",
      "204:\tlearn: 0.0007733\ttotal: 1.07s\tremaining: 497ms\n",
      "205:\tlearn: 0.0007733\ttotal: 1.08s\tremaining: 491ms\n",
      "206:\tlearn: 0.0007733\ttotal: 1.08s\tremaining: 486ms\n",
      "207:\tlearn: 0.0007732\ttotal: 1.08s\tremaining: 480ms\n",
      "208:\tlearn: 0.0007731\ttotal: 1.09s\tremaining: 475ms\n",
      "209:\tlearn: 0.0007731\ttotal: 1.09s\tremaining: 469ms\n",
      "210:\tlearn: 0.0007731\ttotal: 1.1s\tremaining: 464ms\n",
      "211:\tlearn: 0.0007731\ttotal: 1.1s\tremaining: 459ms\n",
      "212:\tlearn: 0.0007731\ttotal: 1.11s\tremaining: 453ms\n",
      "213:\tlearn: 0.0007731\ttotal: 1.11s\tremaining: 448ms\n",
      "214:\tlearn: 0.0007730\ttotal: 1.12s\tremaining: 442ms\n",
      "215:\tlearn: 0.0007730\ttotal: 1.12s\tremaining: 437ms\n",
      "216:\tlearn: 0.0007730\ttotal: 1.13s\tremaining: 432ms\n",
      "217:\tlearn: 0.0007730\ttotal: 1.13s\tremaining: 427ms\n",
      "218:\tlearn: 0.0007730\ttotal: 1.14s\tremaining: 421ms\n",
      "219:\tlearn: 0.0007730\ttotal: 1.14s\tremaining: 416ms\n",
      "220:\tlearn: 0.0007729\ttotal: 1.15s\tremaining: 411ms\n",
      "221:\tlearn: 0.0007729\ttotal: 1.15s\tremaining: 406ms\n",
      "222:\tlearn: 0.0007729\ttotal: 1.16s\tremaining: 400ms\n",
      "223:\tlearn: 0.0007729\ttotal: 1.16s\tremaining: 395ms\n",
      "224:\tlearn: 0.0007729\ttotal: 1.17s\tremaining: 390ms\n",
      "225:\tlearn: 0.0007729\ttotal: 1.17s\tremaining: 385ms\n",
      "226:\tlearn: 0.0007729\ttotal: 1.18s\tremaining: 379ms\n",
      "227:\tlearn: 0.0007729\ttotal: 1.18s\tremaining: 374ms\n",
      "228:\tlearn: 0.0007729\ttotal: 1.19s\tremaining: 368ms\n",
      "229:\tlearn: 0.0007729\ttotal: 1.19s\tremaining: 363ms\n",
      "230:\tlearn: 0.0007729\ttotal: 1.2s\tremaining: 358ms\n",
      "231:\tlearn: 0.0007529\ttotal: 1.2s\tremaining: 352ms\n",
      "232:\tlearn: 0.0007529\ttotal: 1.21s\tremaining: 347ms\n",
      "233:\tlearn: 0.0007528\ttotal: 1.21s\tremaining: 342ms\n",
      "234:\tlearn: 0.0007528\ttotal: 1.22s\tremaining: 337ms\n",
      "235:\tlearn: 0.0007528\ttotal: 1.22s\tremaining: 331ms\n",
      "236:\tlearn: 0.0007528\ttotal: 1.23s\tremaining: 326ms\n",
      "237:\tlearn: 0.0007528\ttotal: 1.23s\tremaining: 321ms\n",
      "238:\tlearn: 0.0007528\ttotal: 1.24s\tremaining: 315ms\n",
      "239:\tlearn: 0.0007528\ttotal: 1.24s\tremaining: 310ms\n",
      "240:\tlearn: 0.0007527\ttotal: 1.25s\tremaining: 305ms\n",
      "241:\tlearn: 0.0007527\ttotal: 1.25s\tremaining: 300ms\n",
      "242:\tlearn: 0.0007527\ttotal: 1.25s\tremaining: 294ms\n",
      "243:\tlearn: 0.0007527\ttotal: 1.26s\tremaining: 289ms\n",
      "244:\tlearn: 0.0007527\ttotal: 1.26s\tremaining: 284ms\n",
      "245:\tlearn: 0.0007527\ttotal: 1.27s\tremaining: 279ms\n",
      "246:\tlearn: 0.0007526\ttotal: 1.27s\tremaining: 274ms\n",
      "247:\tlearn: 0.0007526\ttotal: 1.28s\tremaining: 268ms\n",
      "248:\tlearn: 0.0007526\ttotal: 1.28s\tremaining: 263ms\n",
      "249:\tlearn: 0.0007526\ttotal: 1.29s\tremaining: 258ms\n",
      "250:\tlearn: 0.0007526\ttotal: 1.29s\tremaining: 253ms\n",
      "251:\tlearn: 0.0007525\ttotal: 1.3s\tremaining: 247ms\n",
      "252:\tlearn: 0.0007525\ttotal: 1.3s\tremaining: 242ms\n",
      "253:\tlearn: 0.0007525\ttotal: 1.31s\tremaining: 237ms\n",
      "254:\tlearn: 0.0007525\ttotal: 1.31s\tremaining: 232ms\n",
      "255:\tlearn: 0.0007525\ttotal: 1.32s\tremaining: 227ms\n",
      "256:\tlearn: 0.0007525\ttotal: 1.32s\tremaining: 221ms\n",
      "257:\tlearn: 0.0007525\ttotal: 1.33s\tremaining: 216ms\n",
      "258:\tlearn: 0.0007525\ttotal: 1.33s\tremaining: 211ms\n",
      "259:\tlearn: 0.0007525\ttotal: 1.34s\tremaining: 206ms\n",
      "260:\tlearn: 0.0007525\ttotal: 1.34s\tremaining: 201ms\n",
      "261:\tlearn: 0.0007340\ttotal: 1.35s\tremaining: 196ms\n",
      "262:\tlearn: 0.0007340\ttotal: 1.35s\tremaining: 190ms\n",
      "263:\tlearn: 0.0007340\ttotal: 1.36s\tremaining: 185ms\n",
      "264:\tlearn: 0.0007340\ttotal: 1.36s\tremaining: 180ms\n",
      "265:\tlearn: 0.0007339\ttotal: 1.37s\tremaining: 175ms\n",
      "266:\tlearn: 0.0007339\ttotal: 1.37s\tremaining: 170ms\n",
      "267:\tlearn: 0.0007339\ttotal: 1.38s\tremaining: 165ms\n",
      "268:\tlearn: 0.0007339\ttotal: 1.38s\tremaining: 159ms\n",
      "269:\tlearn: 0.0007339\ttotal: 1.39s\tremaining: 154ms\n",
      "270:\tlearn: 0.0007339\ttotal: 1.39s\tremaining: 149ms\n",
      "271:\tlearn: 0.0007338\ttotal: 1.4s\tremaining: 144ms\n",
      "272:\tlearn: 0.0007338\ttotal: 1.4s\tremaining: 139ms\n",
      "273:\tlearn: 0.0007338\ttotal: 1.41s\tremaining: 134ms\n",
      "274:\tlearn: 0.0007338\ttotal: 1.41s\tremaining: 129ms\n",
      "275:\tlearn: 0.0007338\ttotal: 1.42s\tremaining: 123ms\n",
      "276:\tlearn: 0.0007338\ttotal: 1.42s\tremaining: 118ms\n",
      "277:\tlearn: 0.0007338\ttotal: 1.43s\tremaining: 113ms\n",
      "278:\tlearn: 0.0007338\ttotal: 1.44s\tremaining: 108ms\n",
      "279:\tlearn: 0.0007337\ttotal: 1.44s\tremaining: 103ms\n",
      "280:\tlearn: 0.0007337\ttotal: 1.45s\tremaining: 97.7ms\n",
      "281:\tlearn: 0.0007337\ttotal: 1.45s\tremaining: 92.6ms\n",
      "282:\tlearn: 0.0007337\ttotal: 1.46s\tremaining: 87.5ms\n",
      "283:\tlearn: 0.0007337\ttotal: 1.46s\tremaining: 82.3ms\n",
      "284:\tlearn: 0.0007337\ttotal: 1.47s\tremaining: 77.2ms\n",
      "285:\tlearn: 0.0007337\ttotal: 1.47s\tremaining: 72ms\n",
      "286:\tlearn: 0.0007337\ttotal: 1.48s\tremaining: 66.9ms\n",
      "287:\tlearn: 0.0007337\ttotal: 1.48s\tremaining: 61.7ms\n",
      "288:\tlearn: 0.0007108\ttotal: 1.49s\tremaining: 56.6ms\n",
      "289:\tlearn: 0.0007107\ttotal: 1.49s\tremaining: 51.5ms\n",
      "290:\tlearn: 0.0007107\ttotal: 1.5s\tremaining: 46.3ms\n",
      "291:\tlearn: 0.0007107\ttotal: 1.5s\tremaining: 41.2ms\n",
      "292:\tlearn: 0.0007107\ttotal: 1.51s\tremaining: 36ms\n",
      "293:\tlearn: 0.0007107\ttotal: 1.51s\tremaining: 30.9ms\n",
      "294:\tlearn: 0.0007107\ttotal: 1.52s\tremaining: 25.7ms\n",
      "295:\tlearn: 0.0007107\ttotal: 1.52s\tremaining: 20.6ms\n",
      "296:\tlearn: 0.0007107\ttotal: 1.53s\tremaining: 15.4ms\n",
      "297:\tlearn: 0.0007106\ttotal: 1.53s\tremaining: 10.3ms\n",
      "298:\tlearn: 0.0006943\ttotal: 1.55s\tremaining: 5.19ms\n",
      "299:\tlearn: 0.0006943\ttotal: 1.57s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3760f4a3b5b84c67b7bd12a9cb7de310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5811149738895105, Recall = 0.9833806242399675, Aging Rate = 0.9864207539521687, Precision = 0.49845900965687284, f1 = 0.6615762203436052\n",
      "Epoch 2: Train Loss = 0.47776062406650865, Recall = 0.9805431698419133, Aging Rate = 0.9004864207539521, Precision = 0.5444519468827369, f1 = 0.7001447178002894\n",
      "Epoch 3: Train Loss = 0.4069002857443095, Recall = 0.9517632752330766, Aging Rate = 0.7298338062423997, Precision = 0.6520410996945293, f1 = 0.7738958470665788\n",
      "Epoch 4: Train Loss = 0.35568218592163225, Recall = 0.9347385488447507, Aging Rate = 0.6430887717875963, Precision = 0.726757012291207, f1 = 0.8177304964539007\n",
      "Epoch 5: Train Loss = 0.31442581888486076, Recall = 0.938386704499392, Aging Rate = 0.6055938386704499, Precision = 0.7747657295850067, f1 = 0.848762603116407\n",
      "Test Loss = 0.2780755580945396, Recall = 0.9525739764896636, Aging Rate = 0.5695176327523308, precision = 0.8362989323843416\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2614737701744602, Recall = 0.9602756384272395, Aging Rate = 0.5735711390352655, Precision = 0.8371024734982332, f1 = 0.8944685671134605\n",
      "Epoch 7: Train Loss = 0.22232509877972145, Recall = 0.9691933522496959, Aging Rate = 0.5573571139035266, Precision = 0.8694545454545455, f1 = 0.9166187464059804\n",
      "Epoch 8: Train Loss = 0.18887099809718122, Recall = 0.9760843129306851, Aging Rate = 0.542359140656668, Precision = 0.8998505231689088, f1 = 0.9364184328213104\n",
      "Epoch 9: Train Loss = 0.1594715251413298, Recall = 0.9825699229833806, Aging Rate = 0.5314146736927442, Precision = 0.9244851258581236, f1 = 0.9526429553939869\n",
      "Epoch 10: Train Loss = 0.13554104234846584, Recall = 0.984596676124848, Aging Rate = 0.5218889339278476, Precision = 0.9433009708737864, f1 = 0.9635065450218168\n",
      "Test Loss = 0.12154506280396117, Recall = 0.9886501824077827, Aging Rate = 0.5168220510741791, precision = 0.9564705882352941\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.11620357837399636, Recall = 0.9886501824077827, Aging Rate = 0.5168220510741791, Precision = 0.9564705882352941, f1 = 0.9722941997209488\n",
      "Epoch 12: Train Loss = 0.10055495153796977, Recall = 0.9918929874341306, Aging Rate = 0.5149979732468586, Precision = 0.9630066902794175, f1 = 0.9772364217252396\n",
      "Epoch 13: Train Loss = 0.08748184355299725, Recall = 0.9935143899473045, Aging Rate = 0.5121605188488042, Precision = 0.9699248120300752, f1 = 0.9815778934721666\n",
      "Epoch 14: Train Loss = 0.07599096047610081, Recall = 0.9955411430887718, Aging Rate = 0.5099310903931901, Precision = 0.9761526232114467, f1 = 0.9857515552879792\n",
      "Epoch 15: Train Loss = 0.06688932040090932, Recall = 0.9971625456019457, Aging Rate = 0.5091203891366032, Precision = 0.9792993630573248, f1 = 0.9881502309700743\n",
      "Test Loss = 0.06043031669467579, Recall = 0.9971625456019457, Aging Rate = 0.507701661937576, precision = 0.9820359281437125\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.059805327838412844, Recall = 0.9971625456019457, Aging Rate = 0.5060802594244022, Precision = 0.9851822186623949, f1 = 0.9911361804995971\n",
      "Epoch 17: Train Loss = 0.05270468633143246, Recall = 0.9971625456019457, Aging Rate = 0.5042561815970815, Precision = 0.9887459807073955, f1 = 0.992936427850656\n",
      "Epoch 18: Train Loss = 0.04677644898842403, Recall = 0.9975678962302391, Aging Rate = 0.503850830968788, Precision = 0.9899436846339501, f1 = 0.9937411669695134\n",
      "Epoch 19: Train Loss = 0.04201061607017225, Recall = 0.9987839481151196, Aging Rate = 0.5036481556546413, Precision = 0.9915492957746479, f1 = 0.9951534733441033\n",
      "Epoch 20: Train Loss = 0.03810689394465332, Recall = 0.9987839481151196, Aging Rate = 0.5028374543980543, Precision = 0.9931479242241031, f1 = 0.9959579628132579\n",
      "Test Loss = 0.034424420887091696, Recall = 0.9995946493717065, Aging Rate = 0.5018240778273206, precision = 0.9959612277867528\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03436893508907364, Recall = 0.999189298743413, Aging Rate = 0.5020267531414674, Precision = 0.9951554299555915, f1 = 0.997168284789644\n",
      "Epoch 22: Train Loss = 0.030837300640878574, Recall = 0.9995946493717065, Aging Rate = 0.5014187271990271, Precision = 0.9967663702506063, f1 = 0.9981785063752276\n",
      "Epoch 23: Train Loss = 0.028251356410899285, Recall = 1.0, Aging Rate = 0.5014187271990271, Precision = 0.9971705739692805, f1 = 0.9985832827362882\n",
      "Epoch 24: Train Loss = 0.025913832584614205, Recall = 1.0, Aging Rate = 0.5012160518848804, Precision = 0.997573797007683, f1 = 0.9987854251012146\n",
      "Epoch 25: Train Loss = 0.023484761132449093, Recall = 1.0, Aging Rate = 0.500810701256587, Precision = 0.9983812221772562, f1 = 0.9991899554475496\n",
      "Test Loss = 0.021789422492037237, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.021526565770745083, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 27: Train Loss = 0.01988339049037188, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 28: Train Loss = 0.018510935873013664, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.01697300044146986, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.015833982565852858, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014641932408500162, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.014772786962319154, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.013839903496353792, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.012791546000478042, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.012160488148002814, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.011386065049241152, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01058065914426242, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.010789167455706728, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.010010899436808473, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.009471617821403176, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.008954658410857335, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.008525307075074075, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.007927172800301781, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.008008887994706776, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.007690836268695721, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.007299842409008867, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.006954722453579216, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.00665527035379922, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006357160968049895, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.006361031401026609, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.006131686231730726, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0058841894684268075, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.005773204149445047, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.005437183384883607, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005252729838220973, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.005175663702858874, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.005016217606495514, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.004806079238843449, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004663377778754122, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.004522944105555763, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0042755785837986955, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Train Loss = 0.004339851372179519, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.004207059227037355, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.004093700966465181, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.004042048837587896, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0038901995501303944, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003680276699941024, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.003762688803934633, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0036504088742805724, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0035614381551087194, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.003524620327562468, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0033872446626082007, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033277387816283777, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.003364504989007066, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0033004150242119256, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.003190828535315457, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.003124975883992737, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.003031335613661248, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002885783662976879, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.002999296487185592, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0029300935714126966, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.002882167972725475, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0028171996168118766, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0027683637539822536, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026251298999644132, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3638776\ttotal: 6.23ms\tremaining: 1.86s\n",
      "1:\tlearn: 0.2402764\ttotal: 12.3ms\tremaining: 1.83s\n",
      "2:\tlearn: 0.1878889\ttotal: 18.2ms\tremaining: 1.8s\n",
      "3:\tlearn: 0.1567153\ttotal: 24.2ms\tremaining: 1.79s\n",
      "4:\tlearn: 0.1426989\ttotal: 30.2ms\tremaining: 1.78s\n",
      "5:\tlearn: 0.1310100\ttotal: 36.1ms\tremaining: 1.77s\n",
      "6:\tlearn: 0.1241321\ttotal: 42ms\tremaining: 1.76s\n",
      "7:\tlearn: 0.1191906\ttotal: 47.6ms\tremaining: 1.74s\n",
      "8:\tlearn: 0.1164679\ttotal: 53.2ms\tremaining: 1.72s\n",
      "9:\tlearn: 0.1122206\ttotal: 58.8ms\tremaining: 1.71s\n",
      "10:\tlearn: 0.1080566\ttotal: 64.5ms\tremaining: 1.7s\n",
      "11:\tlearn: 0.0996493\ttotal: 69.7ms\tremaining: 1.67s\n",
      "12:\tlearn: 0.0966160\ttotal: 74.6ms\tremaining: 1.65s\n",
      "13:\tlearn: 0.0906941\ttotal: 79.7ms\tremaining: 1.63s\n",
      "14:\tlearn: 0.0821787\ttotal: 84.7ms\tremaining: 1.61s\n",
      "15:\tlearn: 0.0791789\ttotal: 89.7ms\tremaining: 1.59s\n",
      "16:\tlearn: 0.0747486\ttotal: 94.8ms\tremaining: 1.58s\n",
      "17:\tlearn: 0.0721636\ttotal: 99.8ms\tremaining: 1.56s\n",
      "18:\tlearn: 0.0700813\ttotal: 105ms\tremaining: 1.55s\n",
      "19:\tlearn: 0.0674174\ttotal: 110ms\tremaining: 1.54s\n",
      "20:\tlearn: 0.0651357\ttotal: 115ms\tremaining: 1.53s\n",
      "21:\tlearn: 0.0629068\ttotal: 120ms\tremaining: 1.51s\n",
      "22:\tlearn: 0.0601824\ttotal: 125ms\tremaining: 1.5s\n",
      "23:\tlearn: 0.0562473\ttotal: 130ms\tremaining: 1.49s\n",
      "24:\tlearn: 0.0530609\ttotal: 135ms\tremaining: 1.48s\n",
      "25:\tlearn: 0.0515216\ttotal: 140ms\tremaining: 1.48s\n",
      "26:\tlearn: 0.0501671\ttotal: 145ms\tremaining: 1.47s\n",
      "27:\tlearn: 0.0470979\ttotal: 150ms\tremaining: 1.46s\n",
      "28:\tlearn: 0.0448557\ttotal: 156ms\tremaining: 1.45s\n",
      "29:\tlearn: 0.0423068\ttotal: 161ms\tremaining: 1.45s\n",
      "30:\tlearn: 0.0414076\ttotal: 166ms\tremaining: 1.44s\n",
      "31:\tlearn: 0.0407517\ttotal: 171ms\tremaining: 1.43s\n",
      "32:\tlearn: 0.0398527\ttotal: 176ms\tremaining: 1.43s\n",
      "33:\tlearn: 0.0382622\ttotal: 182ms\tremaining: 1.42s\n",
      "34:\tlearn: 0.0352834\ttotal: 187ms\tremaining: 1.41s\n",
      "35:\tlearn: 0.0338437\ttotal: 192ms\tremaining: 1.41s\n",
      "36:\tlearn: 0.0319581\ttotal: 197ms\tremaining: 1.4s\n",
      "37:\tlearn: 0.0302949\ttotal: 203ms\tremaining: 1.4s\n",
      "38:\tlearn: 0.0281198\ttotal: 208ms\tremaining: 1.39s\n",
      "39:\tlearn: 0.0270791\ttotal: 214ms\tremaining: 1.39s\n",
      "40:\tlearn: 0.0256796\ttotal: 219ms\tremaining: 1.39s\n",
      "41:\tlearn: 0.0250286\ttotal: 225ms\tremaining: 1.38s\n",
      "42:\tlearn: 0.0242881\ttotal: 231ms\tremaining: 1.38s\n",
      "43:\tlearn: 0.0230796\ttotal: 236ms\tremaining: 1.37s\n",
      "44:\tlearn: 0.0212807\ttotal: 242ms\tremaining: 1.37s\n",
      "45:\tlearn: 0.0201490\ttotal: 247ms\tremaining: 1.36s\n",
      "46:\tlearn: 0.0191707\ttotal: 253ms\tremaining: 1.36s\n",
      "47:\tlearn: 0.0178314\ttotal: 258ms\tremaining: 1.35s\n",
      "48:\tlearn: 0.0170028\ttotal: 264ms\tremaining: 1.35s\n",
      "49:\tlearn: 0.0162009\ttotal: 269ms\tremaining: 1.34s\n",
      "50:\tlearn: 0.0154131\ttotal: 274ms\tremaining: 1.34s\n",
      "51:\tlearn: 0.0145757\ttotal: 279ms\tremaining: 1.33s\n",
      "52:\tlearn: 0.0138874\ttotal: 284ms\tremaining: 1.32s\n",
      "53:\tlearn: 0.0129503\ttotal: 289ms\tremaining: 1.32s\n",
      "54:\tlearn: 0.0123066\ttotal: 294ms\tremaining: 1.31s\n",
      "55:\tlearn: 0.0118392\ttotal: 299ms\tremaining: 1.3s\n",
      "56:\tlearn: 0.0112269\ttotal: 305ms\tremaining: 1.3s\n",
      "57:\tlearn: 0.0106191\ttotal: 310ms\tremaining: 1.29s\n",
      "58:\tlearn: 0.0101078\ttotal: 315ms\tremaining: 1.28s\n",
      "59:\tlearn: 0.0093732\ttotal: 320ms\tremaining: 1.28s\n",
      "60:\tlearn: 0.0091140\ttotal: 325ms\tremaining: 1.27s\n",
      "61:\tlearn: 0.0085809\ttotal: 330ms\tremaining: 1.26s\n",
      "62:\tlearn: 0.0083035\ttotal: 335ms\tremaining: 1.26s\n",
      "63:\tlearn: 0.0080348\ttotal: 340ms\tremaining: 1.25s\n",
      "64:\tlearn: 0.0075825\ttotal: 345ms\tremaining: 1.25s\n",
      "65:\tlearn: 0.0072838\ttotal: 349ms\tremaining: 1.24s\n",
      "66:\tlearn: 0.0070101\ttotal: 354ms\tremaining: 1.23s\n",
      "67:\tlearn: 0.0066435\ttotal: 359ms\tremaining: 1.23s\n",
      "68:\tlearn: 0.0064291\ttotal: 365ms\tremaining: 1.22s\n",
      "69:\tlearn: 0.0062559\ttotal: 370ms\tremaining: 1.21s\n",
      "70:\tlearn: 0.0060571\ttotal: 375ms\tremaining: 1.21s\n",
      "71:\tlearn: 0.0058612\ttotal: 380ms\tremaining: 1.2s\n",
      "72:\tlearn: 0.0056583\ttotal: 385ms\tremaining: 1.2s\n",
      "73:\tlearn: 0.0054488\ttotal: 390ms\tremaining: 1.19s\n",
      "74:\tlearn: 0.0051595\ttotal: 395ms\tremaining: 1.18s\n",
      "75:\tlearn: 0.0049950\ttotal: 400ms\tremaining: 1.18s\n",
      "76:\tlearn: 0.0049346\ttotal: 405ms\tremaining: 1.17s\n",
      "77:\tlearn: 0.0046956\ttotal: 411ms\tremaining: 1.17s\n",
      "78:\tlearn: 0.0044975\ttotal: 416ms\tremaining: 1.16s\n",
      "79:\tlearn: 0.0043361\ttotal: 421ms\tremaining: 1.16s\n",
      "80:\tlearn: 0.0040251\ttotal: 426ms\tremaining: 1.15s\n",
      "81:\tlearn: 0.0038738\ttotal: 431ms\tremaining: 1.15s\n",
      "82:\tlearn: 0.0037289\ttotal: 436ms\tremaining: 1.14s\n",
      "83:\tlearn: 0.0035318\ttotal: 441ms\tremaining: 1.13s\n",
      "84:\tlearn: 0.0033528\ttotal: 446ms\tremaining: 1.13s\n",
      "85:\tlearn: 0.0031941\ttotal: 451ms\tremaining: 1.12s\n",
      "86:\tlearn: 0.0030950\ttotal: 456ms\tremaining: 1.11s\n",
      "87:\tlearn: 0.0030176\ttotal: 461ms\tremaining: 1.11s\n",
      "88:\tlearn: 0.0029606\ttotal: 466ms\tremaining: 1.1s\n",
      "89:\tlearn: 0.0028373\ttotal: 470ms\tremaining: 1.1s\n",
      "90:\tlearn: 0.0027085\ttotal: 475ms\tremaining: 1.09s\n",
      "91:\tlearn: 0.0026481\ttotal: 480ms\tremaining: 1.08s\n",
      "92:\tlearn: 0.0025913\ttotal: 485ms\tremaining: 1.08s\n",
      "93:\tlearn: 0.0025217\ttotal: 490ms\tremaining: 1.07s\n",
      "94:\tlearn: 0.0024302\ttotal: 495ms\tremaining: 1.07s\n",
      "95:\tlearn: 0.0023362\ttotal: 500ms\tremaining: 1.06s\n",
      "96:\tlearn: 0.0022644\ttotal: 505ms\tremaining: 1.06s\n",
      "97:\tlearn: 0.0022149\ttotal: 510ms\tremaining: 1.05s\n",
      "98:\tlearn: 0.0021458\ttotal: 515ms\tremaining: 1.04s\n",
      "99:\tlearn: 0.0020694\ttotal: 520ms\tremaining: 1.04s\n",
      "100:\tlearn: 0.0020076\ttotal: 525ms\tremaining: 1.03s\n",
      "101:\tlearn: 0.0019445\ttotal: 530ms\tremaining: 1.03s\n",
      "102:\tlearn: 0.0018727\ttotal: 536ms\tremaining: 1.02s\n",
      "103:\tlearn: 0.0018029\ttotal: 541ms\tremaining: 1.02s\n",
      "104:\tlearn: 0.0017446\ttotal: 546ms\tremaining: 1.01s\n",
      "105:\tlearn: 0.0016913\ttotal: 551ms\tremaining: 1.01s\n",
      "106:\tlearn: 0.0016574\ttotal: 556ms\tremaining: 1s\n",
      "107:\tlearn: 0.0016090\ttotal: 562ms\tremaining: 998ms\n",
      "108:\tlearn: 0.0015504\ttotal: 567ms\tremaining: 993ms\n",
      "109:\tlearn: 0.0015146\ttotal: 572ms\tremaining: 988ms\n",
      "110:\tlearn: 0.0014492\ttotal: 577ms\tremaining: 983ms\n",
      "111:\tlearn: 0.0014084\ttotal: 583ms\tremaining: 978ms\n",
      "112:\tlearn: 0.0013695\ttotal: 588ms\tremaining: 974ms\n",
      "113:\tlearn: 0.0013136\ttotal: 594ms\tremaining: 969ms\n",
      "114:\tlearn: 0.0013135\ttotal: 599ms\tremaining: 963ms\n",
      "115:\tlearn: 0.0012731\ttotal: 604ms\tremaining: 959ms\n",
      "116:\tlearn: 0.0012730\ttotal: 610ms\tremaining: 953ms\n",
      "117:\tlearn: 0.0012730\ttotal: 615ms\tremaining: 949ms\n",
      "118:\tlearn: 0.0012354\ttotal: 620ms\tremaining: 944ms\n",
      "119:\tlearn: 0.0012354\ttotal: 626ms\tremaining: 938ms\n",
      "120:\tlearn: 0.0012353\ttotal: 631ms\tremaining: 933ms\n",
      "121:\tlearn: 0.0012058\ttotal: 636ms\tremaining: 929ms\n",
      "122:\tlearn: 0.0012058\ttotal: 642ms\tremaining: 924ms\n",
      "123:\tlearn: 0.0012058\ttotal: 647ms\tremaining: 918ms\n",
      "124:\tlearn: 0.0012058\ttotal: 651ms\tremaining: 912ms\n",
      "125:\tlearn: 0.0011808\ttotal: 656ms\tremaining: 906ms\n",
      "126:\tlearn: 0.0011418\ttotal: 661ms\tremaining: 900ms\n",
      "127:\tlearn: 0.0011138\ttotal: 666ms\tremaining: 895ms\n",
      "128:\tlearn: 0.0010895\ttotal: 671ms\tremaining: 889ms\n",
      "129:\tlearn: 0.0010895\ttotal: 675ms\tremaining: 883ms\n",
      "130:\tlearn: 0.0010895\ttotal: 680ms\tremaining: 877ms\n",
      "131:\tlearn: 0.0010895\ttotal: 685ms\tremaining: 872ms\n",
      "132:\tlearn: 0.0010416\ttotal: 690ms\tremaining: 867ms\n",
      "133:\tlearn: 0.0010179\ttotal: 695ms\tremaining: 861ms\n",
      "134:\tlearn: 0.0009786\ttotal: 701ms\tremaining: 856ms\n",
      "135:\tlearn: 0.0009396\ttotal: 705ms\tremaining: 851ms\n",
      "136:\tlearn: 0.0009396\ttotal: 710ms\tremaining: 845ms\n",
      "137:\tlearn: 0.0009396\ttotal: 715ms\tremaining: 839ms\n",
      "138:\tlearn: 0.0009395\ttotal: 719ms\tremaining: 833ms\n",
      "139:\tlearn: 0.0009124\ttotal: 725ms\tremaining: 828ms\n",
      "140:\tlearn: 0.0009124\ttotal: 730ms\tremaining: 823ms\n",
      "141:\tlearn: 0.0008911\ttotal: 735ms\tremaining: 817ms\n",
      "142:\tlearn: 0.0008911\ttotal: 739ms\tremaining: 812ms\n",
      "143:\tlearn: 0.0008911\ttotal: 744ms\tremaining: 806ms\n",
      "144:\tlearn: 0.0008911\ttotal: 748ms\tremaining: 800ms\n",
      "145:\tlearn: 0.0008708\ttotal: 753ms\tremaining: 794ms\n",
      "146:\tlearn: 0.0008708\ttotal: 758ms\tremaining: 789ms\n",
      "147:\tlearn: 0.0008708\ttotal: 762ms\tremaining: 783ms\n",
      "148:\tlearn: 0.0008708\ttotal: 767ms\tremaining: 777ms\n",
      "149:\tlearn: 0.0008708\ttotal: 772ms\tremaining: 772ms\n",
      "150:\tlearn: 0.0008707\ttotal: 776ms\tremaining: 766ms\n",
      "151:\tlearn: 0.0008707\ttotal: 782ms\tremaining: 761ms\n",
      "152:\tlearn: 0.0008707\ttotal: 786ms\tremaining: 755ms\n",
      "153:\tlearn: 0.0008707\ttotal: 791ms\tremaining: 750ms\n",
      "154:\tlearn: 0.0008707\ttotal: 796ms\tremaining: 745ms\n",
      "155:\tlearn: 0.0008707\ttotal: 801ms\tremaining: 739ms\n",
      "156:\tlearn: 0.0008707\ttotal: 806ms\tremaining: 734ms\n",
      "157:\tlearn: 0.0008707\ttotal: 810ms\tremaining: 728ms\n",
      "158:\tlearn: 0.0008707\ttotal: 815ms\tremaining: 723ms\n",
      "159:\tlearn: 0.0008707\ttotal: 820ms\tremaining: 718ms\n",
      "160:\tlearn: 0.0008707\ttotal: 825ms\tremaining: 712ms\n",
      "161:\tlearn: 0.0008707\ttotal: 830ms\tremaining: 707ms\n",
      "162:\tlearn: 0.0008707\ttotal: 834ms\tremaining: 701ms\n",
      "163:\tlearn: 0.0008707\ttotal: 839ms\tremaining: 696ms\n",
      "164:\tlearn: 0.0008707\ttotal: 844ms\tremaining: 690ms\n",
      "165:\tlearn: 0.0008707\ttotal: 848ms\tremaining: 685ms\n",
      "166:\tlearn: 0.0008707\ttotal: 853ms\tremaining: 679ms\n",
      "167:\tlearn: 0.0008707\ttotal: 858ms\tremaining: 674ms\n",
      "168:\tlearn: 0.0008707\ttotal: 862ms\tremaining: 668ms\n",
      "169:\tlearn: 0.0008707\ttotal: 867ms\tremaining: 663ms\n",
      "170:\tlearn: 0.0008449\ttotal: 872ms\tremaining: 658ms\n",
      "171:\tlearn: 0.0008449\ttotal: 876ms\tremaining: 652ms\n",
      "172:\tlearn: 0.0008449\ttotal: 881ms\tremaining: 647ms\n",
      "173:\tlearn: 0.0008449\ttotal: 885ms\tremaining: 641ms\n",
      "174:\tlearn: 0.0008449\ttotal: 890ms\tremaining: 636ms\n",
      "175:\tlearn: 0.0008449\ttotal: 911ms\tremaining: 642ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176:\tlearn: 0.0008449\ttotal: 916ms\tremaining: 636ms\n",
      "177:\tlearn: 0.0008449\ttotal: 921ms\tremaining: 631ms\n",
      "178:\tlearn: 0.0008448\ttotal: 926ms\tremaining: 626ms\n",
      "179:\tlearn: 0.0008448\ttotal: 931ms\tremaining: 620ms\n",
      "180:\tlearn: 0.0008448\ttotal: 936ms\tremaining: 615ms\n",
      "181:\tlearn: 0.0008448\ttotal: 941ms\tremaining: 610ms\n",
      "182:\tlearn: 0.0008448\ttotal: 945ms\tremaining: 604ms\n",
      "183:\tlearn: 0.0008447\ttotal: 950ms\tremaining: 599ms\n",
      "184:\tlearn: 0.0008447\ttotal: 955ms\tremaining: 594ms\n",
      "185:\tlearn: 0.0008447\ttotal: 960ms\tremaining: 589ms\n",
      "186:\tlearn: 0.0008276\ttotal: 966ms\tremaining: 584ms\n",
      "187:\tlearn: 0.0008276\ttotal: 971ms\tremaining: 578ms\n",
      "188:\tlearn: 0.0008276\ttotal: 976ms\tremaining: 573ms\n",
      "189:\tlearn: 0.0008276\ttotal: 981ms\tremaining: 568ms\n",
      "190:\tlearn: 0.0008275\ttotal: 986ms\tremaining: 563ms\n",
      "191:\tlearn: 0.0008275\ttotal: 991ms\tremaining: 557ms\n",
      "192:\tlearn: 0.0008275\ttotal: 996ms\tremaining: 552ms\n",
      "193:\tlearn: 0.0008275\ttotal: 1s\tremaining: 547ms\n",
      "194:\tlearn: 0.0008174\ttotal: 1s\tremaining: 541ms\n",
      "195:\tlearn: 0.0008174\ttotal: 1.01s\tremaining: 536ms\n",
      "196:\tlearn: 0.0008174\ttotal: 1.01s\tremaining: 531ms\n",
      "197:\tlearn: 0.0008174\ttotal: 1.02s\tremaining: 525ms\n",
      "198:\tlearn: 0.0008174\ttotal: 1.02s\tremaining: 520ms\n",
      "199:\tlearn: 0.0008174\ttotal: 1.03s\tremaining: 515ms\n",
      "200:\tlearn: 0.0008174\ttotal: 1.03s\tremaining: 510ms\n",
      "201:\tlearn: 0.0008174\ttotal: 1.04s\tremaining: 504ms\n",
      "202:\tlearn: 0.0008174\ttotal: 1.04s\tremaining: 499ms\n",
      "203:\tlearn: 0.0008174\ttotal: 1.05s\tremaining: 494ms\n",
      "204:\tlearn: 0.0008174\ttotal: 1.05s\tremaining: 489ms\n",
      "205:\tlearn: 0.0008174\ttotal: 1.06s\tremaining: 483ms\n",
      "206:\tlearn: 0.0008174\ttotal: 1.06s\tremaining: 478ms\n",
      "207:\tlearn: 0.0008174\ttotal: 1.07s\tremaining: 473ms\n",
      "208:\tlearn: 0.0008174\ttotal: 1.07s\tremaining: 467ms\n",
      "209:\tlearn: 0.0008173\ttotal: 1.08s\tremaining: 462ms\n",
      "210:\tlearn: 0.0008173\ttotal: 1.08s\tremaining: 457ms\n",
      "211:\tlearn: 0.0008173\ttotal: 1.09s\tremaining: 452ms\n",
      "212:\tlearn: 0.0008173\ttotal: 1.09s\tremaining: 446ms\n",
      "213:\tlearn: 0.0008173\ttotal: 1.1s\tremaining: 441ms\n",
      "214:\tlearn: 0.0008173\ttotal: 1.1s\tremaining: 436ms\n",
      "215:\tlearn: 0.0008173\ttotal: 1.11s\tremaining: 431ms\n",
      "216:\tlearn: 0.0007947\ttotal: 1.11s\tremaining: 425ms\n",
      "217:\tlearn: 0.0007947\ttotal: 1.12s\tremaining: 420ms\n",
      "218:\tlearn: 0.0007947\ttotal: 1.12s\tremaining: 415ms\n",
      "219:\tlearn: 0.0007946\ttotal: 1.13s\tremaining: 410ms\n",
      "220:\tlearn: 0.0007946\ttotal: 1.13s\tremaining: 405ms\n",
      "221:\tlearn: 0.0007946\ttotal: 1.14s\tremaining: 399ms\n",
      "222:\tlearn: 0.0007946\ttotal: 1.14s\tremaining: 394ms\n",
      "223:\tlearn: 0.0007946\ttotal: 1.15s\tremaining: 389ms\n",
      "224:\tlearn: 0.0007946\ttotal: 1.15s\tremaining: 384ms\n",
      "225:\tlearn: 0.0007946\ttotal: 1.16s\tremaining: 379ms\n",
      "226:\tlearn: 0.0007946\ttotal: 1.16s\tremaining: 373ms\n",
      "227:\tlearn: 0.0007681\ttotal: 1.17s\tremaining: 368ms\n",
      "228:\tlearn: 0.0007681\ttotal: 1.17s\tremaining: 363ms\n",
      "229:\tlearn: 0.0007681\ttotal: 1.18s\tremaining: 358ms\n",
      "230:\tlearn: 0.0007680\ttotal: 1.18s\tremaining: 353ms\n",
      "231:\tlearn: 0.0007680\ttotal: 1.19s\tremaining: 348ms\n",
      "232:\tlearn: 0.0007680\ttotal: 1.19s\tremaining: 343ms\n",
      "233:\tlearn: 0.0007680\ttotal: 1.2s\tremaining: 338ms\n",
      "234:\tlearn: 0.0007680\ttotal: 1.2s\tremaining: 333ms\n",
      "235:\tlearn: 0.0007680\ttotal: 1.21s\tremaining: 328ms\n",
      "236:\tlearn: 0.0007680\ttotal: 1.21s\tremaining: 323ms\n",
      "237:\tlearn: 0.0007680\ttotal: 1.22s\tremaining: 318ms\n",
      "238:\tlearn: 0.0007679\ttotal: 1.22s\tremaining: 313ms\n",
      "239:\tlearn: 0.0007679\ttotal: 1.23s\tremaining: 307ms\n",
      "240:\tlearn: 0.0007679\ttotal: 1.23s\tremaining: 302ms\n",
      "241:\tlearn: 0.0007679\ttotal: 1.24s\tremaining: 297ms\n",
      "242:\tlearn: 0.0007679\ttotal: 1.25s\tremaining: 292ms\n",
      "243:\tlearn: 0.0007679\ttotal: 1.25s\tremaining: 287ms\n",
      "244:\tlearn: 0.0007679\ttotal: 1.25s\tremaining: 282ms\n",
      "245:\tlearn: 0.0007679\ttotal: 1.26s\tremaining: 277ms\n",
      "246:\tlearn: 0.0007679\ttotal: 1.26s\tremaining: 272ms\n",
      "247:\tlearn: 0.0007678\ttotal: 1.27s\tremaining: 266ms\n",
      "248:\tlearn: 0.0007678\ttotal: 1.27s\tremaining: 261ms\n",
      "249:\tlearn: 0.0007678\ttotal: 1.28s\tremaining: 256ms\n",
      "250:\tlearn: 0.0007678\ttotal: 1.29s\tremaining: 251ms\n",
      "251:\tlearn: 0.0007678\ttotal: 1.29s\tremaining: 246ms\n",
      "252:\tlearn: 0.0007678\ttotal: 1.3s\tremaining: 241ms\n",
      "253:\tlearn: 0.0007678\ttotal: 1.3s\tremaining: 236ms\n",
      "254:\tlearn: 0.0007678\ttotal: 1.31s\tremaining: 231ms\n",
      "255:\tlearn: 0.0007678\ttotal: 1.31s\tremaining: 226ms\n",
      "256:\tlearn: 0.0007677\ttotal: 1.32s\tremaining: 220ms\n",
      "257:\tlearn: 0.0007677\ttotal: 1.32s\tremaining: 215ms\n",
      "258:\tlearn: 0.0007677\ttotal: 1.33s\tremaining: 210ms\n",
      "259:\tlearn: 0.0007677\ttotal: 1.34s\tremaining: 206ms\n",
      "260:\tlearn: 0.0007677\ttotal: 1.35s\tremaining: 201ms\n",
      "261:\tlearn: 0.0007677\ttotal: 1.35s\tremaining: 196ms\n",
      "262:\tlearn: 0.0007677\ttotal: 1.36s\tremaining: 191ms\n",
      "263:\tlearn: 0.0007677\ttotal: 1.36s\tremaining: 186ms\n",
      "264:\tlearn: 0.0007677\ttotal: 1.37s\tremaining: 181ms\n",
      "265:\tlearn: 0.0007676\ttotal: 1.37s\tremaining: 175ms\n",
      "266:\tlearn: 0.0007676\ttotal: 1.38s\tremaining: 170ms\n",
      "267:\tlearn: 0.0007676\ttotal: 1.38s\tremaining: 165ms\n",
      "268:\tlearn: 0.0007675\ttotal: 1.39s\tremaining: 160ms\n",
      "269:\tlearn: 0.0007675\ttotal: 1.39s\tremaining: 155ms\n",
      "270:\tlearn: 0.0007675\ttotal: 1.4s\tremaining: 149ms\n",
      "271:\tlearn: 0.0007675\ttotal: 1.4s\tremaining: 144ms\n",
      "272:\tlearn: 0.0007675\ttotal: 1.41s\tremaining: 139ms\n",
      "273:\tlearn: 0.0007675\ttotal: 1.41s\tremaining: 134ms\n",
      "274:\tlearn: 0.0007675\ttotal: 1.41s\tremaining: 129ms\n",
      "275:\tlearn: 0.0007675\ttotal: 1.42s\tremaining: 123ms\n",
      "276:\tlearn: 0.0007675\ttotal: 1.42s\tremaining: 118ms\n",
      "277:\tlearn: 0.0007675\ttotal: 1.43s\tremaining: 113ms\n",
      "278:\tlearn: 0.0007674\ttotal: 1.43s\tremaining: 108ms\n",
      "279:\tlearn: 0.0007674\ttotal: 1.44s\tremaining: 103ms\n",
      "280:\tlearn: 0.0007674\ttotal: 1.44s\tremaining: 97.6ms\n",
      "281:\tlearn: 0.0007674\ttotal: 1.45s\tremaining: 92.4ms\n",
      "282:\tlearn: 0.0007674\ttotal: 1.45s\tremaining: 87.3ms\n",
      "283:\tlearn: 0.0007674\ttotal: 1.46s\tremaining: 82.1ms\n",
      "284:\tlearn: 0.0007674\ttotal: 1.46s\tremaining: 77ms\n",
      "285:\tlearn: 0.0007674\ttotal: 1.47s\tremaining: 71.8ms\n",
      "286:\tlearn: 0.0007673\ttotal: 1.47s\tremaining: 66.7ms\n",
      "287:\tlearn: 0.0007673\ttotal: 1.48s\tremaining: 61.5ms\n",
      "288:\tlearn: 0.0007673\ttotal: 1.48s\tremaining: 56.4ms\n",
      "289:\tlearn: 0.0007673\ttotal: 1.49s\tremaining: 51.3ms\n",
      "290:\tlearn: 0.0007673\ttotal: 1.49s\tremaining: 46.1ms\n",
      "291:\tlearn: 0.0007673\ttotal: 1.5s\tremaining: 41ms\n",
      "292:\tlearn: 0.0007672\ttotal: 1.5s\tremaining: 35.9ms\n",
      "293:\tlearn: 0.0007671\ttotal: 1.51s\tremaining: 30.7ms\n",
      "294:\tlearn: 0.0007671\ttotal: 1.51s\tremaining: 25.6ms\n",
      "295:\tlearn: 0.0007671\ttotal: 1.51s\tremaining: 20.5ms\n",
      "296:\tlearn: 0.0007671\ttotal: 1.52s\tremaining: 15.4ms\n",
      "297:\tlearn: 0.0007671\ttotal: 1.52s\tremaining: 10.2ms\n",
      "298:\tlearn: 0.0007671\ttotal: 1.53s\tremaining: 5.12ms\n",
      "299:\tlearn: 0.0007671\ttotal: 1.53s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef264c7cc3fd4e24a3baf554b4de0b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5840350100276052, Recall = 0.9987839481151196, Aging Rate = 0.9993919740575598, Precision = 0.4996958020685459, f1 = 0.6661259799945931\n",
      "Epoch 2: Train Loss = 0.48711401898222123, Recall = 0.9918929874341306, Aging Rate = 0.9446696392379408, Precision = 0.5249946363441321, f1 = 0.6865881032547699\n",
      "Epoch 3: Train Loss = 0.4131825848368918, Recall = 0.9574381840291852, Aging Rate = 0.7549655451965951, Precision = 0.6340939597315436, f1 = 0.7629198966408268\n",
      "Epoch 4: Train Loss = 0.3566047845700515, Recall = 0.9363599513579246, Aging Rate = 0.6520064856100527, Precision = 0.7180603046316444, f1 = 0.812807881773399\n",
      "Epoch 5: Train Loss = 0.31250386363492594, Recall = 0.9400081070125659, Aging Rate = 0.6116740980948521, Precision = 0.768389662027833, f1 = 0.8455788514129444\n",
      "Test Loss = 0.2738146880436968, Recall = 0.9618970409404135, Aging Rate = 0.5806647750304013, precision = 0.8282722513089005\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2556365128586927, Recall = 0.9627077421970004, Aging Rate = 0.5709363599513579, Precision = 0.843095491657792, f1 = 0.8989401968205903\n",
      "Epoch 7: Train Loss = 0.2134080711928852, Recall = 0.9708147547628699, Aging Rate = 0.5496554519659506, Precision = 0.8831120943952803, f1 = 0.9248889747055415\n",
      "Epoch 8: Train Loss = 0.17950835047243674, Recall = 0.9748682610458046, Aging Rate = 0.5350628293473855, Precision = 0.9109848484848485, f1 = 0.9418445271196397\n",
      "Epoch 9: Train Loss = 0.1514489122632065, Recall = 0.9813538710985003, Aging Rate = 0.5277665180381029, Precision = 0.9297235023041475, f1 = 0.954841254190495\n",
      "Epoch 10: Train Loss = 0.12869778410790175, Recall = 0.9854073773814349, Aging Rate = 0.5152006485610052, Precision = 0.9563335955940204, f1 = 0.9706528249151527\n",
      "Test Loss = 0.11556172708557548, Recall = 0.992298338062424, Aging Rate = 0.5194568301580867, precision = 0.9551307062036676\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.1108995715603927, Recall = 0.9902715849209567, Aging Rate = 0.516214025131739, Precision = 0.9591676482135846, f1 = 0.974471479856402\n",
      "Epoch 12: Train Loss = 0.09512092101523206, Recall = 0.9906769355492501, Aging Rate = 0.512363194162951, Precision = 0.9667721518987342, f1 = 0.9785785785785786\n",
      "Epoch 13: Train Loss = 0.08356601486973884, Recall = 0.9943250912038913, Aging Rate = 0.5117551682205107, Precision = 0.9714851485148515, f1 = 0.9827724358974358\n",
      "Epoch 14: Train Loss = 0.07274706054359589, Recall = 0.9959464937170652, Aging Rate = 0.5109444669639238, Precision = 0.974613248710829, f1 = 0.9851643945469126\n",
      "Epoch 15: Train Loss = 0.06434492161176589, Recall = 0.9967571949736522, Aging Rate = 0.5087150385083097, Precision = 0.9796812749003984, f1 = 0.9881454691581273\n",
      "Test Loss = 0.057821393765004774, Recall = 0.9971625456019457, Aging Rate = 0.5062829347385488, precision = 0.9847878302642114\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05701572008289013, Recall = 0.9971625456019457, Aging Rate = 0.5070936359951358, Precision = 0.9832134292565947, f1 = 0.9901388609378143\n",
      "Epoch 17: Train Loss = 0.050594503430700224, Recall = 0.9967571949736522, Aging Rate = 0.5054722334819619, Precision = 0.9859663191659984, f1 = 0.9913323926627696\n",
      "Epoch 18: Train Loss = 0.04518441519015561, Recall = 0.9971625456019457, Aging Rate = 0.5046615322253749, Precision = 0.9879518072289156, f1 = 0.9925358079483559\n",
      "Epoch 19: Train Loss = 0.04049120812001478, Recall = 0.9987839481151196, Aging Rate = 0.5032428050263478, Precision = 0.9923479661699557, f1 = 0.9955555555555555\n",
      "Epoch 20: Train Loss = 0.037267133557318095, Recall = 0.9995946493717065, Aging Rate = 0.5042561815970815, Precision = 0.9911575562700965, f1 = 0.9953582240161454\n",
      "Test Loss = 0.03325052826956404, Recall = 1.0, Aging Rate = 0.5034454803404945, precision = 0.9931561996779388\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03306909141300248, Recall = 0.999189298743413, Aging Rate = 0.5024321037697609, Precision = 0.9943525615167407, f1 = 0.9967650626769106\n",
      "Epoch 22: Train Loss = 0.02947000044699642, Recall = 0.9995946493717065, Aging Rate = 0.5024321037697609, Precision = 0.9947559499798305, f1 = 0.9971694298422967\n",
      "Epoch 23: Train Loss = 0.026954780903920065, Recall = 1.0, Aging Rate = 0.5020267531414674, Precision = 0.9959628582963262, f1 = 0.9979773462783171\n",
      "Epoch 24: Train Loss = 0.024487164495176857, Recall = 1.0, Aging Rate = 0.5014187271990271, Precision = 0.9971705739692805, f1 = 0.9985832827362882\n",
      "Epoch 25: Train Loss = 0.022422792456808557, Recall = 1.0, Aging Rate = 0.5012160518848804, Precision = 0.997573797007683, f1 = 0.9987854251012146\n",
      "Test Loss = 0.02051528721495128, Recall = 1.0, Aging Rate = 0.5006080259424402, precision = 0.9987854251012146\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.020509173822550335, Recall = 1.0, Aging Rate = 0.5012160518848804, Precision = 0.997573797007683, f1 = 0.9987854251012146\n",
      "Epoch 27: Train Loss = 0.01902994026445429, Recall = 1.0, Aging Rate = 0.500810701256587, Precision = 0.9983812221772562, f1 = 0.9991899554475496\n",
      "Epoch 28: Train Loss = 0.017720469420479047, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Epoch 29: Train Loss = 0.01633120974149052, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Epoch 30: Train Loss = 0.015197634795868643, Recall = 1.0, Aging Rate = 0.5004053506282935, Precision = 0.9991899554475496, f1 = 0.9995948136142626\n",
      "Test Loss = 0.013849669025495896, Recall = 1.0, Aging Rate = 0.5002026753141467, precision = 0.9995948136142626\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.013945191477232487, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 32: Train Loss = 0.01298292747645495, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 33: Train Loss = 0.012218840552375392, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.011466458070242531, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 35: Train Loss = 0.010815488636255095, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.010124469593085762, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.010195089573018708, Recall = 1.0, Aging Rate = 0.5002026753141467, Precision = 0.9995948136142626, f1 = 0.9997973657548125\n",
      "Epoch 37: Train Loss = 0.009496999009620711, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.009109371667180436, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.008615430715532741, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.008202284764586055, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.007601107508819935, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.007763728016196933, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.007365640807771519, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.007000927671029058, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0066912320777917315, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.00640570388698779, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006083995665196687, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.006236503739929243, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.005866142286362982, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.005658621800686661, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0053871132232955485, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.005159725832032213, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004987154199006009, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.005052148523039431, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.004850896585386897, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.004703754329631867, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.00457247105371592, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.004337956492019342, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004121021538245011, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.004215454982829978, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.004107147238145944, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0039602954592404755, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.003858233750678892, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0037243905093862152, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0035509375820784285, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.003637953469417249, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0035465104544303485, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.003501670643835233, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0033659881723982015, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0032933549591187988, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0031164961238099127, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0032174710706623826, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0031632625508825453, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.003137207302190499, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0030366276910118563, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0029956026048632432, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0028269417116016052, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.002918858371031734, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.002865619589986123, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0028524005716310964, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.002754005713914576, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0027524128557761575, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002558566924346136, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.002656803694774518, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.002627609500923486, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0026058218598382396, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0025681727958437664, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0025388075946801898, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0023941238820182863, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0024938473635004706, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.00247332189790074, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0024034649898710706, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0024005937292631285, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0023663431954546333, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022188547843320495, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3449021\ttotal: 15.2ms\tremaining: 4.56s\n",
      "1:\tlearn: 0.2350707\ttotal: 21.2ms\tremaining: 3.15s\n",
      "2:\tlearn: 0.1832895\ttotal: 27.1ms\tremaining: 2.69s\n",
      "3:\tlearn: 0.1557256\ttotal: 32.9ms\tremaining: 2.44s\n",
      "4:\tlearn: 0.1395972\ttotal: 39ms\tremaining: 2.3s\n",
      "5:\tlearn: 0.1276469\ttotal: 45.1ms\tremaining: 2.21s\n",
      "6:\tlearn: 0.1205076\ttotal: 51ms\tremaining: 2.13s\n",
      "7:\tlearn: 0.1148229\ttotal: 57ms\tremaining: 2.08s\n",
      "8:\tlearn: 0.1099966\ttotal: 63ms\tremaining: 2.04s\n",
      "9:\tlearn: 0.1054702\ttotal: 69.2ms\tremaining: 2s\n",
      "10:\tlearn: 0.1006447\ttotal: 74.4ms\tremaining: 1.95s\n",
      "11:\tlearn: 0.0976137\ttotal: 79.5ms\tremaining: 1.91s\n",
      "12:\tlearn: 0.0936967\ttotal: 84.7ms\tremaining: 1.87s\n",
      "13:\tlearn: 0.0915264\ttotal: 89.7ms\tremaining: 1.83s\n",
      "14:\tlearn: 0.0885964\ttotal: 94.8ms\tremaining: 1.8s\n",
      "15:\tlearn: 0.0860190\ttotal: 100ms\tremaining: 1.77s\n",
      "16:\tlearn: 0.0787217\ttotal: 105ms\tremaining: 1.75s\n",
      "17:\tlearn: 0.0772208\ttotal: 110ms\tremaining: 1.73s\n",
      "18:\tlearn: 0.0716443\ttotal: 116ms\tremaining: 1.72s\n",
      "19:\tlearn: 0.0667827\ttotal: 121ms\tremaining: 1.7s\n",
      "20:\tlearn: 0.0647265\ttotal: 126ms\tremaining: 1.68s\n",
      "21:\tlearn: 0.0634479\ttotal: 132ms\tremaining: 1.67s\n",
      "22:\tlearn: 0.0604605\ttotal: 138ms\tremaining: 1.66s\n",
      "23:\tlearn: 0.0570239\ttotal: 144ms\tremaining: 1.65s\n",
      "24:\tlearn: 0.0539870\ttotal: 149ms\tremaining: 1.64s\n",
      "25:\tlearn: 0.0518756\ttotal: 155ms\tremaining: 1.63s\n",
      "26:\tlearn: 0.0506883\ttotal: 160ms\tremaining: 1.61s\n",
      "27:\tlearn: 0.0481175\ttotal: 165ms\tremaining: 1.6s\n",
      "28:\tlearn: 0.0452215\ttotal: 170ms\tremaining: 1.59s\n",
      "29:\tlearn: 0.0440090\ttotal: 175ms\tremaining: 1.58s\n",
      "30:\tlearn: 0.0415208\ttotal: 181ms\tremaining: 1.57s\n",
      "31:\tlearn: 0.0396265\ttotal: 186ms\tremaining: 1.55s\n",
      "32:\tlearn: 0.0364723\ttotal: 191ms\tremaining: 1.54s\n",
      "33:\tlearn: 0.0348414\ttotal: 196ms\tremaining: 1.53s\n",
      "34:\tlearn: 0.0344182\ttotal: 201ms\tremaining: 1.52s\n",
      "35:\tlearn: 0.0328330\ttotal: 206ms\tremaining: 1.51s\n",
      "36:\tlearn: 0.0308214\ttotal: 212ms\tremaining: 1.5s\n",
      "37:\tlearn: 0.0297191\ttotal: 217ms\tremaining: 1.49s\n",
      "38:\tlearn: 0.0287983\ttotal: 222ms\tremaining: 1.48s\n",
      "39:\tlearn: 0.0269955\ttotal: 227ms\tremaining: 1.47s\n",
      "40:\tlearn: 0.0256763\ttotal: 232ms\tremaining: 1.47s\n",
      "41:\tlearn: 0.0245448\ttotal: 237ms\tremaining: 1.46s\n",
      "42:\tlearn: 0.0222714\ttotal: 242ms\tremaining: 1.45s\n",
      "43:\tlearn: 0.0213254\ttotal: 247ms\tremaining: 1.44s\n",
      "44:\tlearn: 0.0200836\ttotal: 252ms\tremaining: 1.43s\n",
      "45:\tlearn: 0.0189419\ttotal: 257ms\tremaining: 1.42s\n",
      "46:\tlearn: 0.0182752\ttotal: 263ms\tremaining: 1.41s\n",
      "47:\tlearn: 0.0172061\ttotal: 268ms\tremaining: 1.41s\n",
      "48:\tlearn: 0.0163713\ttotal: 273ms\tremaining: 1.4s\n",
      "49:\tlearn: 0.0153874\ttotal: 278ms\tremaining: 1.39s\n",
      "50:\tlearn: 0.0147454\ttotal: 283ms\tremaining: 1.38s\n",
      "51:\tlearn: 0.0138917\ttotal: 289ms\tremaining: 1.38s\n",
      "52:\tlearn: 0.0130621\ttotal: 294ms\tremaining: 1.37s\n",
      "53:\tlearn: 0.0128138\ttotal: 299ms\tremaining: 1.36s\n",
      "54:\tlearn: 0.0121733\ttotal: 304ms\tremaining: 1.35s\n",
      "55:\tlearn: 0.0113919\ttotal: 309ms\tremaining: 1.34s\n",
      "56:\tlearn: 0.0110276\ttotal: 314ms\tremaining: 1.34s\n",
      "57:\tlearn: 0.0104117\ttotal: 319ms\tremaining: 1.33s\n",
      "58:\tlearn: 0.0096967\ttotal: 325ms\tremaining: 1.33s\n",
      "59:\tlearn: 0.0092323\ttotal: 330ms\tremaining: 1.32s\n",
      "60:\tlearn: 0.0088072\ttotal: 352ms\tremaining: 1.38s\n",
      "61:\tlearn: 0.0083477\ttotal: 358ms\tremaining: 1.37s\n",
      "62:\tlearn: 0.0080635\ttotal: 363ms\tremaining: 1.36s\n",
      "63:\tlearn: 0.0077467\ttotal: 369ms\tremaining: 1.36s\n",
      "64:\tlearn: 0.0074785\ttotal: 374ms\tremaining: 1.35s\n",
      "65:\tlearn: 0.0071219\ttotal: 380ms\tremaining: 1.35s\n",
      "66:\tlearn: 0.0069615\ttotal: 386ms\tremaining: 1.34s\n",
      "67:\tlearn: 0.0066966\ttotal: 391ms\tremaining: 1.33s\n",
      "68:\tlearn: 0.0064641\ttotal: 397ms\tremaining: 1.33s\n",
      "69:\tlearn: 0.0062880\ttotal: 402ms\tremaining: 1.32s\n",
      "70:\tlearn: 0.0059955\ttotal: 408ms\tremaining: 1.31s\n",
      "71:\tlearn: 0.0056898\ttotal: 413ms\tremaining: 1.31s\n",
      "72:\tlearn: 0.0054686\ttotal: 419ms\tremaining: 1.3s\n",
      "73:\tlearn: 0.0053080\ttotal: 424ms\tremaining: 1.29s\n",
      "74:\tlearn: 0.0051673\ttotal: 430ms\tremaining: 1.29s\n",
      "75:\tlearn: 0.0049716\ttotal: 435ms\tremaining: 1.28s\n",
      "76:\tlearn: 0.0047429\ttotal: 441ms\tremaining: 1.28s\n",
      "77:\tlearn: 0.0045457\ttotal: 446ms\tremaining: 1.27s\n",
      "78:\tlearn: 0.0042505\ttotal: 452ms\tremaining: 1.26s\n",
      "79:\tlearn: 0.0041050\ttotal: 457ms\tremaining: 1.26s\n",
      "80:\tlearn: 0.0039820\ttotal: 463ms\tremaining: 1.25s\n",
      "81:\tlearn: 0.0039091\ttotal: 468ms\tremaining: 1.24s\n",
      "82:\tlearn: 0.0038308\ttotal: 474ms\tremaining: 1.24s\n",
      "83:\tlearn: 0.0037187\ttotal: 479ms\tremaining: 1.23s\n",
      "84:\tlearn: 0.0035402\ttotal: 485ms\tremaining: 1.23s\n",
      "85:\tlearn: 0.0034831\ttotal: 490ms\tremaining: 1.22s\n",
      "86:\tlearn: 0.0033602\ttotal: 496ms\tremaining: 1.21s\n",
      "87:\tlearn: 0.0031546\ttotal: 502ms\tremaining: 1.21s\n",
      "88:\tlearn: 0.0029661\ttotal: 507ms\tremaining: 1.2s\n",
      "89:\tlearn: 0.0028939\ttotal: 513ms\tremaining: 1.2s\n",
      "90:\tlearn: 0.0028195\ttotal: 518ms\tremaining: 1.19s\n",
      "91:\tlearn: 0.0027435\ttotal: 524ms\tremaining: 1.18s\n",
      "92:\tlearn: 0.0026407\ttotal: 529ms\tremaining: 1.18s\n",
      "93:\tlearn: 0.0025600\ttotal: 534ms\tremaining: 1.17s\n",
      "94:\tlearn: 0.0024693\ttotal: 540ms\tremaining: 1.16s\n",
      "95:\tlearn: 0.0023883\ttotal: 545ms\tremaining: 1.16s\n",
      "96:\tlearn: 0.0023086\ttotal: 551ms\tremaining: 1.15s\n",
      "97:\tlearn: 0.0022599\ttotal: 556ms\tremaining: 1.15s\n",
      "98:\tlearn: 0.0021980\ttotal: 562ms\tremaining: 1.14s\n",
      "99:\tlearn: 0.0021455\ttotal: 568ms\tremaining: 1.14s\n",
      "100:\tlearn: 0.0020598\ttotal: 573ms\tremaining: 1.13s\n",
      "101:\tlearn: 0.0020166\ttotal: 578ms\tremaining: 1.12s\n",
      "102:\tlearn: 0.0019562\ttotal: 584ms\tremaining: 1.12s\n",
      "103:\tlearn: 0.0018969\ttotal: 589ms\tremaining: 1.11s\n",
      "104:\tlearn: 0.0018566\ttotal: 595ms\tremaining: 1.1s\n",
      "105:\tlearn: 0.0018165\ttotal: 600ms\tremaining: 1.1s\n",
      "106:\tlearn: 0.0017846\ttotal: 606ms\tremaining: 1.09s\n",
      "107:\tlearn: 0.0017150\ttotal: 611ms\tremaining: 1.09s\n",
      "108:\tlearn: 0.0016654\ttotal: 617ms\tremaining: 1.08s\n",
      "109:\tlearn: 0.0016350\ttotal: 622ms\tremaining: 1.07s\n",
      "110:\tlearn: 0.0015669\ttotal: 628ms\tremaining: 1.07s\n",
      "111:\tlearn: 0.0015333\ttotal: 633ms\tremaining: 1.06s\n",
      "112:\tlearn: 0.0015063\ttotal: 638ms\tremaining: 1.06s\n",
      "113:\tlearn: 0.0014598\ttotal: 644ms\tremaining: 1.05s\n",
      "114:\tlearn: 0.0014221\ttotal: 649ms\tremaining: 1.04s\n",
      "115:\tlearn: 0.0013871\ttotal: 655ms\tremaining: 1.04s\n",
      "116:\tlearn: 0.0013871\ttotal: 660ms\tremaining: 1.03s\n",
      "117:\tlearn: 0.0013294\ttotal: 665ms\tremaining: 1.03s\n",
      "118:\tlearn: 0.0013294\ttotal: 671ms\tremaining: 1.02s\n",
      "119:\tlearn: 0.0013010\ttotal: 676ms\tremaining: 1.01s\n",
      "120:\tlearn: 0.0012526\ttotal: 682ms\tremaining: 1.01s\n",
      "121:\tlearn: 0.0012177\ttotal: 687ms\tremaining: 1s\n",
      "122:\tlearn: 0.0011944\ttotal: 692ms\tremaining: 997ms\n",
      "123:\tlearn: 0.0011648\ttotal: 698ms\tremaining: 991ms\n",
      "124:\tlearn: 0.0011325\ttotal: 704ms\tremaining: 985ms\n",
      "125:\tlearn: 0.0011325\ttotal: 719ms\tremaining: 993ms\n",
      "126:\tlearn: 0.0011324\ttotal: 724ms\tremaining: 986ms\n",
      "127:\tlearn: 0.0011324\ttotal: 729ms\tremaining: 979ms\n",
      "128:\tlearn: 0.0011324\ttotal: 733ms\tremaining: 972ms\n",
      "129:\tlearn: 0.0011324\ttotal: 738ms\tremaining: 965ms\n",
      "130:\tlearn: 0.0011324\ttotal: 743ms\tremaining: 958ms\n",
      "131:\tlearn: 0.0011324\ttotal: 747ms\tremaining: 951ms\n",
      "132:\tlearn: 0.0010924\ttotal: 753ms\tremaining: 945ms\n",
      "133:\tlearn: 0.0010923\ttotal: 757ms\tremaining: 938ms\n",
      "134:\tlearn: 0.0010739\ttotal: 763ms\tremaining: 932ms\n",
      "135:\tlearn: 0.0010738\ttotal: 768ms\tremaining: 926ms\n",
      "136:\tlearn: 0.0010738\ttotal: 773ms\tremaining: 919ms\n",
      "137:\tlearn: 0.0010738\ttotal: 777ms\tremaining: 912ms\n",
      "138:\tlearn: 0.0010737\ttotal: 782ms\tremaining: 906ms\n",
      "139:\tlearn: 0.0010737\ttotal: 787ms\tremaining: 900ms\n",
      "140:\tlearn: 0.0010737\ttotal: 792ms\tremaining: 893ms\n",
      "141:\tlearn: 0.0010737\ttotal: 797ms\tremaining: 887ms\n",
      "142:\tlearn: 0.0010736\ttotal: 802ms\tremaining: 881ms\n",
      "143:\tlearn: 0.0010736\ttotal: 807ms\tremaining: 874ms\n",
      "144:\tlearn: 0.0010538\ttotal: 811ms\tremaining: 867ms\n",
      "145:\tlearn: 0.0010538\ttotal: 816ms\tremaining: 861ms\n",
      "146:\tlearn: 0.0010295\ttotal: 821ms\tremaining: 855ms\n",
      "147:\tlearn: 0.0010295\ttotal: 826ms\tremaining: 848ms\n",
      "148:\tlearn: 0.0010295\ttotal: 830ms\tremaining: 842ms\n",
      "149:\tlearn: 0.0010094\ttotal: 835ms\tremaining: 835ms\n",
      "150:\tlearn: 0.0010094\ttotal: 840ms\tremaining: 829ms\n",
      "151:\tlearn: 0.0010093\ttotal: 845ms\tremaining: 823ms\n",
      "152:\tlearn: 0.0010092\ttotal: 850ms\tremaining: 816ms\n",
      "153:\tlearn: 0.0009931\ttotal: 855ms\tremaining: 810ms\n",
      "154:\tlearn: 0.0009695\ttotal: 860ms\tremaining: 804ms\n",
      "155:\tlearn: 0.0009695\ttotal: 864ms\tremaining: 798ms\n",
      "156:\tlearn: 0.0009695\ttotal: 869ms\tremaining: 792ms\n",
      "157:\tlearn: 0.0009452\ttotal: 875ms\tremaining: 786ms\n",
      "158:\tlearn: 0.0008933\ttotal: 880ms\tremaining: 780ms\n",
      "159:\tlearn: 0.0008933\ttotal: 885ms\tremaining: 774ms\n",
      "160:\tlearn: 0.0008933\ttotal: 890ms\tremaining: 768ms\n",
      "161:\tlearn: 0.0008933\ttotal: 895ms\tremaining: 763ms\n",
      "162:\tlearn: 0.0008933\ttotal: 900ms\tremaining: 757ms\n",
      "163:\tlearn: 0.0008933\ttotal: 905ms\tremaining: 751ms\n",
      "164:\tlearn: 0.0008932\ttotal: 910ms\tremaining: 745ms\n",
      "165:\tlearn: 0.0008932\ttotal: 915ms\tremaining: 739ms\n",
      "166:\tlearn: 0.0008932\ttotal: 921ms\tremaining: 733ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167:\tlearn: 0.0008932\ttotal: 926ms\tremaining: 727ms\n",
      "168:\tlearn: 0.0008932\ttotal: 931ms\tremaining: 722ms\n",
      "169:\tlearn: 0.0008932\ttotal: 936ms\tremaining: 716ms\n",
      "170:\tlearn: 0.0008932\ttotal: 941ms\tremaining: 710ms\n",
      "171:\tlearn: 0.0008931\ttotal: 946ms\tremaining: 704ms\n",
      "172:\tlearn: 0.0008931\ttotal: 951ms\tremaining: 698ms\n",
      "173:\tlearn: 0.0008930\ttotal: 956ms\tremaining: 692ms\n",
      "174:\tlearn: 0.0008930\ttotal: 961ms\tremaining: 687ms\n",
      "175:\tlearn: 0.0008929\ttotal: 966ms\tremaining: 681ms\n",
      "176:\tlearn: 0.0008929\ttotal: 972ms\tremaining: 675ms\n",
      "177:\tlearn: 0.0008929\ttotal: 977ms\tremaining: 669ms\n",
      "178:\tlearn: 0.0008929\ttotal: 982ms\tremaining: 664ms\n",
      "179:\tlearn: 0.0008676\ttotal: 987ms\tremaining: 658ms\n",
      "180:\tlearn: 0.0008420\ttotal: 992ms\tremaining: 652ms\n",
      "181:\tlearn: 0.0008420\ttotal: 997ms\tremaining: 646ms\n",
      "182:\tlearn: 0.0008420\ttotal: 1s\tremaining: 641ms\n",
      "183:\tlearn: 0.0008420\ttotal: 1.01s\tremaining: 635ms\n",
      "184:\tlearn: 0.0008420\ttotal: 1.01s\tremaining: 629ms\n",
      "185:\tlearn: 0.0008420\ttotal: 1.02s\tremaining: 623ms\n",
      "186:\tlearn: 0.0008419\ttotal: 1.02s\tremaining: 617ms\n",
      "187:\tlearn: 0.0008419\ttotal: 1.03s\tremaining: 611ms\n",
      "188:\tlearn: 0.0008419\ttotal: 1.03s\tremaining: 606ms\n",
      "189:\tlearn: 0.0008180\ttotal: 1.04s\tremaining: 600ms\n",
      "190:\tlearn: 0.0008179\ttotal: 1.04s\tremaining: 594ms\n",
      "191:\tlearn: 0.0008179\ttotal: 1.04s\tremaining: 588ms\n",
      "192:\tlearn: 0.0008179\ttotal: 1.05s\tremaining: 583ms\n",
      "193:\tlearn: 0.0008179\ttotal: 1.05s\tremaining: 577ms\n",
      "194:\tlearn: 0.0008179\ttotal: 1.06s\tremaining: 571ms\n",
      "195:\tlearn: 0.0008179\ttotal: 1.06s\tremaining: 565ms\n",
      "196:\tlearn: 0.0008179\ttotal: 1.07s\tremaining: 560ms\n",
      "197:\tlearn: 0.0008178\ttotal: 1.07s\tremaining: 554ms\n",
      "198:\tlearn: 0.0008178\ttotal: 1.08s\tremaining: 548ms\n",
      "199:\tlearn: 0.0008178\ttotal: 1.08s\tremaining: 542ms\n",
      "200:\tlearn: 0.0008178\ttotal: 1.09s\tremaining: 537ms\n",
      "201:\tlearn: 0.0008178\ttotal: 1.09s\tremaining: 531ms\n",
      "202:\tlearn: 0.0008177\ttotal: 1.1s\tremaining: 525ms\n",
      "203:\tlearn: 0.0008177\ttotal: 1.1s\tremaining: 520ms\n",
      "204:\tlearn: 0.0008177\ttotal: 1.11s\tremaining: 514ms\n",
      "205:\tlearn: 0.0008177\ttotal: 1.11s\tremaining: 509ms\n",
      "206:\tlearn: 0.0008177\ttotal: 1.12s\tremaining: 503ms\n",
      "207:\tlearn: 0.0008177\ttotal: 1.12s\tremaining: 497ms\n",
      "208:\tlearn: 0.0008176\ttotal: 1.13s\tremaining: 492ms\n",
      "209:\tlearn: 0.0008176\ttotal: 1.13s\tremaining: 486ms\n",
      "210:\tlearn: 0.0008176\ttotal: 1.14s\tremaining: 480ms\n",
      "211:\tlearn: 0.0008176\ttotal: 1.14s\tremaining: 475ms\n",
      "212:\tlearn: 0.0008176\ttotal: 1.15s\tremaining: 469ms\n",
      "213:\tlearn: 0.0008176\ttotal: 1.15s\tremaining: 464ms\n",
      "214:\tlearn: 0.0008176\ttotal: 1.16s\tremaining: 458ms\n",
      "215:\tlearn: 0.0008176\ttotal: 1.16s\tremaining: 452ms\n",
      "216:\tlearn: 0.0008176\ttotal: 1.17s\tremaining: 447ms\n",
      "217:\tlearn: 0.0008175\ttotal: 1.17s\tremaining: 441ms\n",
      "218:\tlearn: 0.0008175\ttotal: 1.18s\tremaining: 436ms\n",
      "219:\tlearn: 0.0008175\ttotal: 1.18s\tremaining: 430ms\n",
      "220:\tlearn: 0.0008175\ttotal: 1.19s\tremaining: 425ms\n",
      "221:\tlearn: 0.0008174\ttotal: 1.19s\tremaining: 419ms\n",
      "222:\tlearn: 0.0008174\ttotal: 1.2s\tremaining: 414ms\n",
      "223:\tlearn: 0.0008174\ttotal: 1.2s\tremaining: 409ms\n",
      "224:\tlearn: 0.0008174\ttotal: 1.21s\tremaining: 403ms\n",
      "225:\tlearn: 0.0008173\ttotal: 1.23s\tremaining: 403ms\n",
      "226:\tlearn: 0.0008173\ttotal: 1.24s\tremaining: 397ms\n",
      "227:\tlearn: 0.0008173\ttotal: 1.24s\tremaining: 392ms\n",
      "228:\tlearn: 0.0008173\ttotal: 1.25s\tremaining: 386ms\n",
      "229:\tlearn: 0.0008173\ttotal: 1.25s\tremaining: 381ms\n",
      "230:\tlearn: 0.0008173\ttotal: 1.26s\tremaining: 375ms\n",
      "231:\tlearn: 0.0008173\ttotal: 1.26s\tremaining: 370ms\n",
      "232:\tlearn: 0.0008173\ttotal: 1.27s\tremaining: 364ms\n",
      "233:\tlearn: 0.0008172\ttotal: 1.27s\tremaining: 359ms\n",
      "234:\tlearn: 0.0008172\ttotal: 1.28s\tremaining: 353ms\n",
      "235:\tlearn: 0.0008172\ttotal: 1.28s\tremaining: 348ms\n",
      "236:\tlearn: 0.0008172\ttotal: 1.29s\tremaining: 342ms\n",
      "237:\tlearn: 0.0008172\ttotal: 1.29s\tremaining: 337ms\n",
      "238:\tlearn: 0.0008172\ttotal: 1.3s\tremaining: 332ms\n",
      "239:\tlearn: 0.0008172\ttotal: 1.3s\tremaining: 326ms\n",
      "240:\tlearn: 0.0008171\ttotal: 1.31s\tremaining: 321ms\n",
      "241:\tlearn: 0.0008171\ttotal: 1.31s\tremaining: 315ms\n",
      "242:\tlearn: 0.0008171\ttotal: 1.32s\tremaining: 310ms\n",
      "243:\tlearn: 0.0008171\ttotal: 1.32s\tremaining: 304ms\n",
      "244:\tlearn: 0.0008171\ttotal: 1.33s\tremaining: 299ms\n",
      "245:\tlearn: 0.0008171\ttotal: 1.33s\tremaining: 293ms\n",
      "246:\tlearn: 0.0008171\ttotal: 1.34s\tremaining: 288ms\n",
      "247:\tlearn: 0.0008171\ttotal: 1.35s\tremaining: 282ms\n",
      "248:\tlearn: 0.0008171\ttotal: 1.35s\tremaining: 277ms\n",
      "249:\tlearn: 0.0008171\ttotal: 1.36s\tremaining: 271ms\n",
      "250:\tlearn: 0.0008171\ttotal: 1.36s\tremaining: 266ms\n",
      "251:\tlearn: 0.0008171\ttotal: 1.37s\tremaining: 260ms\n",
      "252:\tlearn: 0.0007875\ttotal: 1.37s\tremaining: 255ms\n",
      "253:\tlearn: 0.0007875\ttotal: 1.38s\tremaining: 249ms\n",
      "254:\tlearn: 0.0007875\ttotal: 1.38s\tremaining: 244ms\n",
      "255:\tlearn: 0.0007875\ttotal: 1.39s\tremaining: 239ms\n",
      "256:\tlearn: 0.0007875\ttotal: 1.39s\tremaining: 233ms\n",
      "257:\tlearn: 0.0007875\ttotal: 1.4s\tremaining: 228ms\n",
      "258:\tlearn: 0.0007875\ttotal: 1.4s\tremaining: 222ms\n",
      "259:\tlearn: 0.0007875\ttotal: 1.41s\tremaining: 217ms\n",
      "260:\tlearn: 0.0007874\ttotal: 1.41s\tremaining: 211ms\n",
      "261:\tlearn: 0.0007874\ttotal: 1.42s\tremaining: 206ms\n",
      "262:\tlearn: 0.0007874\ttotal: 1.42s\tremaining: 200ms\n",
      "263:\tlearn: 0.0007874\ttotal: 1.43s\tremaining: 195ms\n",
      "264:\tlearn: 0.0007874\ttotal: 1.43s\tremaining: 189ms\n",
      "265:\tlearn: 0.0007874\ttotal: 1.44s\tremaining: 184ms\n",
      "266:\tlearn: 0.0007873\ttotal: 1.45s\tremaining: 179ms\n",
      "267:\tlearn: 0.0007873\ttotal: 1.45s\tremaining: 173ms\n",
      "268:\tlearn: 0.0007873\ttotal: 1.46s\tremaining: 168ms\n",
      "269:\tlearn: 0.0007873\ttotal: 1.46s\tremaining: 162ms\n",
      "270:\tlearn: 0.0007872\ttotal: 1.47s\tremaining: 157ms\n",
      "271:\tlearn: 0.0007872\ttotal: 1.47s\tremaining: 152ms\n",
      "272:\tlearn: 0.0007872\ttotal: 1.48s\tremaining: 146ms\n",
      "273:\tlearn: 0.0007872\ttotal: 1.48s\tremaining: 141ms\n",
      "274:\tlearn: 0.0007872\ttotal: 1.49s\tremaining: 135ms\n",
      "275:\tlearn: 0.0007872\ttotal: 1.49s\tremaining: 130ms\n",
      "276:\tlearn: 0.0007872\ttotal: 1.5s\tremaining: 124ms\n",
      "277:\tlearn: 0.0007872\ttotal: 1.5s\tremaining: 119ms\n",
      "278:\tlearn: 0.0007871\ttotal: 1.51s\tremaining: 114ms\n",
      "279:\tlearn: 0.0007871\ttotal: 1.51s\tremaining: 108ms\n",
      "280:\tlearn: 0.0007871\ttotal: 1.52s\tremaining: 103ms\n",
      "281:\tlearn: 0.0007871\ttotal: 1.52s\tremaining: 97.3ms\n",
      "282:\tlearn: 0.0007871\ttotal: 1.53s\tremaining: 91.9ms\n",
      "283:\tlearn: 0.0007871\ttotal: 1.53s\tremaining: 86.5ms\n",
      "284:\tlearn: 0.0007871\ttotal: 1.54s\tremaining: 81ms\n",
      "285:\tlearn: 0.0007871\ttotal: 1.54s\tremaining: 75.6ms\n",
      "286:\tlearn: 0.0007871\ttotal: 1.55s\tremaining: 70.2ms\n",
      "287:\tlearn: 0.0007871\ttotal: 1.55s\tremaining: 64.8ms\n",
      "288:\tlearn: 0.0007645\ttotal: 1.56s\tremaining: 59.4ms\n",
      "289:\tlearn: 0.0007644\ttotal: 1.57s\tremaining: 54ms\n",
      "290:\tlearn: 0.0007644\ttotal: 1.57s\tremaining: 48.6ms\n",
      "291:\tlearn: 0.0007644\ttotal: 1.58s\tremaining: 43.4ms\n",
      "292:\tlearn: 0.0007644\ttotal: 1.59s\tremaining: 37.9ms\n",
      "293:\tlearn: 0.0007644\ttotal: 1.59s\tremaining: 32.5ms\n",
      "294:\tlearn: 0.0007644\ttotal: 1.6s\tremaining: 27.1ms\n",
      "295:\tlearn: 0.0007644\ttotal: 1.6s\tremaining: 21.6ms\n",
      "296:\tlearn: 0.0007644\ttotal: 1.61s\tremaining: 16.2ms\n",
      "297:\tlearn: 0.0007644\ttotal: 1.61s\tremaining: 10.8ms\n",
      "298:\tlearn: 0.0007644\ttotal: 1.62s\tremaining: 5.41ms\n",
      "299:\tlearn: 0.0007644\ttotal: 1.62s\tremaining: 0us\n",
      "Dataset 4:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a6bebebd04b8686d28a47a18f172d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aeca2ab54d04eb9bebecb036d6b841a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6079840324951661, Recall = 0.604982206405694, Aging Rate = 0.4094750889679715, Precision = 0.7387289516567083, f1 = 0.6651993152359991\n",
      "Epoch 2: Train Loss = 0.41783785310928506, Recall = 0.8109430604982206, Aging Rate = 0.4786476868327402, Precision = 0.8471189591078067, f1 = 0.8286363636363636\n",
      "Epoch 3: Train Loss = 0.32003702000578954, Recall = 0.8852313167259787, Aging Rate = 0.5060053380782918, Precision = 0.8747252747252747, f1 = 0.8799469378730931\n",
      "Epoch 4: Train Loss = 0.26915598297458526, Recall = 0.9110320284697508, Aging Rate = 0.5057829181494662, Precision = 0.9006156552330695, f1 = 0.9057938965059709\n",
      "Epoch 5: Train Loss = 0.23149885634934775, Recall = 0.9306049822064056, Aging Rate = 0.5042259786476868, Precision = 0.9228054697838554, f1 = 0.926688815060908\n",
      "Test Loss = 0.19829993411315294, Recall = 0.9430604982206405, Aging Rate = 0.5022241992882562, precision = 0.9388839681133747\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.18797739120564852, Recall = 0.9390569395017794, Aging Rate = 0.49755338078291816, Precision = 0.9436745641484131, f1 = 0.9413600891861763\n",
      "Epoch 7: Train Loss = 0.1606688557252341, Recall = 0.9483985765124555, Aging Rate = 0.49466192170818507, Precision = 0.9586330935251799, f1 = 0.9534883720930233\n",
      "Epoch 8: Train Loss = 0.13875331684575812, Recall = 0.9541814946619217, Aging Rate = 0.49243772241992884, Precision = 0.9688346883468835, f1 = 0.9614522635589423\n",
      "Epoch 9: Train Loss = 0.12169564336379228, Recall = 0.9666370106761566, Aging Rate = 0.4951067615658363, Precision = 0.9761904761904762, f1 = 0.9713902548055432\n",
      "Epoch 10: Train Loss = 0.10656686819214838, Recall = 0.9715302491103203, Aging Rate = 0.4951067615658363, Precision = 0.9811320754716981, f1 = 0.9763075547608404\n",
      "Test Loss = 0.09906340569416823, Recall = 0.9621886120996441, Aging Rate = 0.486432384341637, precision = 0.9890260631001372\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.09311876644867595, Recall = 0.9724199288256228, Aging Rate = 0.4928825622775801, Precision = 0.9864620938628159, f1 = 0.9793906810035843\n",
      "Epoch 12: Train Loss = 0.0837321327303228, Recall = 0.9777580071174378, Aging Rate = 0.4942170818505338, Precision = 0.9891989198919892, f1 = 0.9834451901565996\n",
      "Epoch 13: Train Loss = 0.07529010717820019, Recall = 0.9773131672597865, Aging Rate = 0.4944395017793594, Precision = 0.9883040935672515, f1 = 0.9827779020353389\n",
      "Epoch 14: Train Loss = 0.06793492364289498, Recall = 0.9786476868327402, Aging Rate = 0.4942170818505338, Precision = 0.9900990099009901, f1 = 0.9843400447427294\n",
      "Epoch 15: Train Loss = 0.06172572177075831, Recall = 0.983540925266904, Aging Rate = 0.49532918149466193, Precision = 0.9928154467894028, f1 = 0.9881564245810057\n",
      "Test Loss = 0.056661551904423804, Recall = 0.9817615658362989, Aging Rate = 0.4931049822064057, precision = 0.995489400090212\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05805051238590924, Recall = 0.9822064056939501, Aging Rate = 0.4944395017793594, Precision = 0.9932523616734144, f1 = 0.9876985014538134\n",
      "Epoch 17: Train Loss = 0.052655967953366314, Recall = 0.9844306049822064, Aging Rate = 0.49555160142348753, Precision = 0.9932675044883303, f1 = 0.9888293118856121\n",
      "Epoch 18: Train Loss = 0.05028370211026847, Recall = 0.9857651245551602, Aging Rate = 0.49666370106761565, Precision = 0.9923869234214062, f1 = 0.9890649408614148\n",
      "Epoch 19: Train Loss = 0.045331741805814764, Recall = 0.9879893238434164, Aging Rate = 0.4971085409252669, Precision = 0.9937360178970918, f1 = 0.9908543386125362\n",
      "Epoch 20: Train Loss = 0.04151285104320991, Recall = 0.9879893238434164, Aging Rate = 0.49532918149466193, Precision = 0.997305792546026, f1 = 0.9926256983240224\n",
      "Test Loss = 0.03798224633804844, Recall = 0.9911032028469751, Aging Rate = 0.49755338078291816, precision = 0.9959767545820295\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.039837757596129626, Recall = 0.9902135231316725, Aging Rate = 0.4968861209964413, Precision = 0.9964189794091316, f1 = 0.9933065595716197\n",
      "Epoch 22: Train Loss = 0.03766999049012771, Recall = 0.9902135231316725, Aging Rate = 0.49644128113879005, Precision = 0.9973118279569892, f1 = 0.9937499999999999\n",
      "Epoch 23: Train Loss = 0.03485240581780142, Recall = 0.9919928825622776, Aging Rate = 0.4968861209964413, Precision = 0.9982094897045658, f1 = 0.9950914770191879\n",
      "Epoch 24: Train Loss = 0.03328855839488345, Recall = 0.9933274021352313, Aging Rate = 0.49777580071174377, Precision = 0.9977658623771224, f1 = 0.9955416852429781\n",
      "Epoch 25: Train Loss = 0.031164011026457536, Recall = 0.9942170818505338, Aging Rate = 0.498220640569395, Precision = 0.9977678571428571, f1 = 0.9959893048128342\n",
      "Test Loss = 0.02824382962214883, Recall = 0.9959964412811388, Aging Rate = 0.4984430604982206, precision = 0.9991075412762159\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.029978427001908156, Recall = 0.994661921708185, Aging Rate = 0.49777580071174377, Precision = 0.9991063449508489, f1 = 0.9968791796700847\n",
      "Epoch 27: Train Loss = 0.02833275195357002, Recall = 0.9951067615658363, Aging Rate = 0.498220640569395, Precision = 0.9986607142857142, f1 = 0.996880570409982\n",
      "Epoch 28: Train Loss = 0.027605461420696826, Recall = 0.9942170818505338, Aging Rate = 0.49777580071174377, Precision = 0.9986595174262735, f1 = 0.9964333481943826\n",
      "Epoch 29: Train Loss = 0.026052507823764004, Recall = 0.9955516014234875, Aging Rate = 0.498220640569395, Precision = 0.9991071428571429, f1 = 0.9973262032085563\n",
      "Epoch 30: Train Loss = 0.024816717693973266, Recall = 0.99644128113879, Aging Rate = 0.4984430604982206, Precision = 0.999553770638108, f1 = 0.9979950991312097\n",
      "Test Loss = 0.02301761649840667, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, precision = 0.999554565701559\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.024191391876371728, Recall = 0.9968861209964412, Aging Rate = 0.4988879003558719, Precision = 0.9991083370485956, f1 = 0.9979959919839678\n",
      "Epoch 32: Train Loss = 0.023209273251721445, Recall = 0.9959964412811388, Aging Rate = 0.498220640569395, Precision = 0.9995535714285714, f1 = 0.99777183600713\n",
      "Epoch 33: Train Loss = 0.022506408983331134, Recall = 0.9968861209964412, Aging Rate = 0.4988879003558719, Precision = 0.9991083370485956, f1 = 0.9979959919839678\n",
      "Epoch 34: Train Loss = 0.02222857005841155, Recall = 0.9968861209964412, Aging Rate = 0.4988879003558719, Precision = 0.9991083370485956, f1 = 0.9979959919839678\n",
      "Epoch 35: Train Loss = 0.02151011419731103, Recall = 0.99644128113879, Aging Rate = 0.4984430604982206, Precision = 0.999553770638108, f1 = 0.9979950991312097\n",
      "Test Loss = 0.019494318996566045, Recall = 0.9991103202846975, Aging Rate = 0.5, precision = 0.9991103202846975\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.02131631551807567, Recall = 0.9973309608540926, Aging Rate = 0.4988879003558719, Precision = 0.9995541685242978, f1 = 0.9984413270986416\n",
      "Epoch 37: Train Loss = 0.019958339167553334, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.020019970953570568, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.019300606545869566, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.019306683296410637, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Test Loss = 0.017545443691074636, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.018735169900375753, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.018365853724610127, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01824176085788054, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.017676077743335974, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.018060974597771822, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.017134986506291132, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.01767237894767332, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.01720369837223635, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.01718990085812226, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.01742958172068689, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01711467064890573, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015373254387085972, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.017108421534087735, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.016457146689879087, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016601345140579756, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.01629862822989976, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.016192475360324376, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015039834280057523, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.01619922019064002, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.01622873501938327, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015936482684468036, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.01579048465340799, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015733625647012025, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014386960229044283, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015813840198532755, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.016117323458433364, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01568135824153909, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.01561336553393734, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.01585738657513017, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015601334894520109, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015494099539767592, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01575647817851704, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.01586866384749947, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015632920643294832, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015418629374129704, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014340315962018067, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.014956276240722141, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.015321548910499997, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.015513114432317083, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.01515319519527124, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015529485515507728, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013751685692269183, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.01543802306639342, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.01574823212581173, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.014946168338303253, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.015189248863011068, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.015276551588017533, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013688391591571404, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.014886689693849282, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.015135318789883017, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.015473615318483729, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.015167778145747253, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.015329628506695248, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014242570930130753, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.015313835377105614, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.015107100875882492, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.014881129379904568, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.014761776321988513, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.014965290594779725, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013315981991416726, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014727544099145513, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.014723814612923993, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.01519435294820957, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.01480955795759461, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.01563521479571311, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0157885845200394, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 95.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5648813\ttotal: 5.62ms\tremaining: 837ms\n",
      "1:\tlearn: 0.4498124\ttotal: 11.6ms\tremaining: 857ms\n",
      "2:\tlearn: 0.3404971\ttotal: 18.6ms\tremaining: 911ms\n",
      "3:\tlearn: 0.2655575\ttotal: 25.5ms\tremaining: 930ms\n",
      "4:\tlearn: 0.2041041\ttotal: 32.6ms\tremaining: 947ms\n",
      "5:\tlearn: 0.1678419\ttotal: 38.7ms\tremaining: 930ms\n",
      "6:\tlearn: 0.1394114\ttotal: 45.1ms\tremaining: 921ms\n",
      "7:\tlearn: 0.1155291\ttotal: 51ms\tremaining: 906ms\n",
      "8:\tlearn: 0.1021522\ttotal: 56.9ms\tremaining: 891ms\n",
      "9:\tlearn: 0.0853732\ttotal: 63.2ms\tremaining: 884ms\n",
      "10:\tlearn: 0.0741797\ttotal: 69.9ms\tremaining: 884ms\n",
      "11:\tlearn: 0.0667641\ttotal: 75ms\tremaining: 863ms\n",
      "12:\tlearn: 0.0586243\ttotal: 80ms\tremaining: 843ms\n",
      "13:\tlearn: 0.0512861\ttotal: 84.9ms\tremaining: 825ms\n",
      "14:\tlearn: 0.0444980\ttotal: 90.4ms\tremaining: 814ms\n",
      "15:\tlearn: 0.0381149\ttotal: 95.3ms\tremaining: 798ms\n",
      "16:\tlearn: 0.0337422\ttotal: 100ms\tremaining: 785ms\n",
      "17:\tlearn: 0.0290261\ttotal: 106ms\tremaining: 775ms\n",
      "18:\tlearn: 0.0253285\ttotal: 111ms\tremaining: 764ms\n",
      "19:\tlearn: 0.0228606\ttotal: 115ms\tremaining: 751ms\n",
      "20:\tlearn: 0.0213205\ttotal: 120ms\tremaining: 737ms\n",
      "21:\tlearn: 0.0199532\ttotal: 124ms\tremaining: 720ms\n",
      "22:\tlearn: 0.0189152\ttotal: 128ms\tremaining: 705ms\n",
      "23:\tlearn: 0.0172176\ttotal: 132ms\tremaining: 694ms\n",
      "24:\tlearn: 0.0164645\ttotal: 136ms\tremaining: 679ms\n",
      "25:\tlearn: 0.0148676\ttotal: 141ms\tremaining: 671ms\n",
      "26:\tlearn: 0.0137956\ttotal: 145ms\tremaining: 660ms\n",
      "27:\tlearn: 0.0124950\ttotal: 149ms\tremaining: 651ms\n",
      "28:\tlearn: 0.0116176\ttotal: 154ms\tremaining: 641ms\n",
      "29:\tlearn: 0.0106526\ttotal: 159ms\tremaining: 634ms\n",
      "30:\tlearn: 0.0098613\ttotal: 163ms\tremaining: 626ms\n",
      "31:\tlearn: 0.0092058\ttotal: 168ms\tremaining: 618ms\n",
      "32:\tlearn: 0.0089416\ttotal: 171ms\tremaining: 606ms\n",
      "33:\tlearn: 0.0085777\ttotal: 174ms\tremaining: 594ms\n",
      "34:\tlearn: 0.0080535\ttotal: 179ms\tremaining: 587ms\n",
      "35:\tlearn: 0.0076900\ttotal: 183ms\tremaining: 580ms\n",
      "36:\tlearn: 0.0071086\ttotal: 189ms\tremaining: 576ms\n",
      "37:\tlearn: 0.0067228\ttotal: 192ms\tremaining: 566ms\n",
      "38:\tlearn: 0.0061402\ttotal: 198ms\tremaining: 565ms\n",
      "39:\tlearn: 0.0057397\ttotal: 203ms\tremaining: 558ms\n",
      "40:\tlearn: 0.0053982\ttotal: 208ms\tremaining: 554ms\n",
      "41:\tlearn: 0.0050422\ttotal: 214ms\tremaining: 549ms\n",
      "42:\tlearn: 0.0047096\ttotal: 218ms\tremaining: 543ms\n",
      "43:\tlearn: 0.0043986\ttotal: 224ms\tremaining: 539ms\n",
      "44:\tlearn: 0.0041980\ttotal: 228ms\tremaining: 532ms\n",
      "45:\tlearn: 0.0039868\ttotal: 232ms\tremaining: 525ms\n",
      "46:\tlearn: 0.0037741\ttotal: 238ms\tremaining: 521ms\n",
      "47:\tlearn: 0.0036339\ttotal: 242ms\tremaining: 513ms\n",
      "48:\tlearn: 0.0035174\ttotal: 246ms\tremaining: 506ms\n",
      "49:\tlearn: 0.0034530\ttotal: 250ms\tremaining: 501ms\n",
      "50:\tlearn: 0.0032978\ttotal: 254ms\tremaining: 493ms\n",
      "51:\tlearn: 0.0031896\ttotal: 257ms\tremaining: 485ms\n",
      "52:\tlearn: 0.0030476\ttotal: 262ms\tremaining: 479ms\n",
      "53:\tlearn: 0.0029398\ttotal: 266ms\tremaining: 472ms\n",
      "54:\tlearn: 0.0029001\ttotal: 269ms\tremaining: 464ms\n",
      "55:\tlearn: 0.0027337\ttotal: 274ms\tremaining: 460ms\n",
      "56:\tlearn: 0.0026147\ttotal: 279ms\tremaining: 455ms\n",
      "57:\tlearn: 0.0025420\ttotal: 282ms\tremaining: 448ms\n",
      "58:\tlearn: 0.0024325\ttotal: 287ms\tremaining: 442ms\n",
      "59:\tlearn: 0.0023921\ttotal: 290ms\tremaining: 436ms\n",
      "60:\tlearn: 0.0022605\ttotal: 296ms\tremaining: 432ms\n",
      "61:\tlearn: 0.0021637\ttotal: 300ms\tremaining: 426ms\n",
      "62:\tlearn: 0.0020843\ttotal: 304ms\tremaining: 420ms\n",
      "63:\tlearn: 0.0019897\ttotal: 309ms\tremaining: 415ms\n",
      "64:\tlearn: 0.0019545\ttotal: 312ms\tremaining: 408ms\n",
      "65:\tlearn: 0.0019219\ttotal: 316ms\tremaining: 402ms\n",
      "66:\tlearn: 0.0018762\ttotal: 319ms\tremaining: 395ms\n",
      "67:\tlearn: 0.0018125\ttotal: 323ms\tremaining: 390ms\n",
      "68:\tlearn: 0.0017387\ttotal: 327ms\tremaining: 384ms\n",
      "69:\tlearn: 0.0016637\ttotal: 332ms\tremaining: 379ms\n",
      "70:\tlearn: 0.0015814\ttotal: 336ms\tremaining: 374ms\n",
      "71:\tlearn: 0.0015327\ttotal: 340ms\tremaining: 368ms\n",
      "72:\tlearn: 0.0014846\ttotal: 344ms\tremaining: 363ms\n",
      "73:\tlearn: 0.0014701\ttotal: 347ms\tremaining: 356ms\n",
      "74:\tlearn: 0.0014188\ttotal: 350ms\tremaining: 350ms\n",
      "75:\tlearn: 0.0014188\ttotal: 353ms\tremaining: 344ms\n",
      "76:\tlearn: 0.0013841\ttotal: 357ms\tremaining: 338ms\n",
      "77:\tlearn: 0.0013687\ttotal: 360ms\tremaining: 332ms\n",
      "78:\tlearn: 0.0013381\ttotal: 364ms\tremaining: 327ms\n",
      "79:\tlearn: 0.0013381\ttotal: 366ms\tremaining: 320ms\n",
      "80:\tlearn: 0.0012884\ttotal: 370ms\tremaining: 315ms\n",
      "81:\tlearn: 0.0012884\ttotal: 373ms\tremaining: 309ms\n",
      "82:\tlearn: 0.0012600\ttotal: 376ms\tremaining: 304ms\n",
      "83:\tlearn: 0.0012372\ttotal: 380ms\tremaining: 298ms\n",
      "84:\tlearn: 0.0012176\ttotal: 383ms\tremaining: 293ms\n",
      "85:\tlearn: 0.0012034\ttotal: 386ms\tremaining: 287ms\n",
      "86:\tlearn: 0.0012034\ttotal: 389ms\tremaining: 281ms\n",
      "87:\tlearn: 0.0011838\ttotal: 392ms\tremaining: 276ms\n",
      "88:\tlearn: 0.0011672\ttotal: 395ms\tremaining: 271ms\n",
      "89:\tlearn: 0.0011671\ttotal: 398ms\tremaining: 265ms\n",
      "90:\tlearn: 0.0011439\ttotal: 401ms\tremaining: 260ms\n",
      "91:\tlearn: 0.0010997\ttotal: 405ms\tremaining: 256ms\n",
      "92:\tlearn: 0.0010795\ttotal: 409ms\tremaining: 251ms\n",
      "93:\tlearn: 0.0010795\ttotal: 412ms\tremaining: 245ms\n",
      "94:\tlearn: 0.0010506\ttotal: 415ms\tremaining: 240ms\n",
      "95:\tlearn: 0.0010328\ttotal: 419ms\tremaining: 236ms\n",
      "96:\tlearn: 0.0010328\ttotal: 421ms\tremaining: 230ms\n",
      "97:\tlearn: 0.0010328\ttotal: 424ms\tremaining: 225ms\n",
      "98:\tlearn: 0.0010144\ttotal: 428ms\tremaining: 220ms\n",
      "99:\tlearn: 0.0009983\ttotal: 431ms\tremaining: 216ms\n",
      "100:\tlearn: 0.0009725\ttotal: 435ms\tremaining: 211ms\n",
      "101:\tlearn: 0.0009725\ttotal: 437ms\tremaining: 206ms\n",
      "102:\tlearn: 0.0009607\ttotal: 440ms\tremaining: 201ms\n",
      "103:\tlearn: 0.0009607\ttotal: 443ms\tremaining: 196ms\n",
      "104:\tlearn: 0.0009607\ttotal: 446ms\tremaining: 191ms\n",
      "105:\tlearn: 0.0009607\ttotal: 449ms\tremaining: 186ms\n",
      "106:\tlearn: 0.0009607\ttotal: 452ms\tremaining: 182ms\n",
      "107:\tlearn: 0.0009607\ttotal: 454ms\tremaining: 177ms\n",
      "108:\tlearn: 0.0009606\ttotal: 457ms\tremaining: 172ms\n",
      "109:\tlearn: 0.0009606\ttotal: 459ms\tremaining: 167ms\n",
      "110:\tlearn: 0.0009606\ttotal: 462ms\tremaining: 162ms\n",
      "111:\tlearn: 0.0009606\ttotal: 464ms\tremaining: 158ms\n",
      "112:\tlearn: 0.0009606\ttotal: 467ms\tremaining: 153ms\n",
      "113:\tlearn: 0.0009606\ttotal: 470ms\tremaining: 148ms\n",
      "114:\tlearn: 0.0009606\ttotal: 472ms\tremaining: 144ms\n",
      "115:\tlearn: 0.0009606\ttotal: 475ms\tremaining: 139ms\n",
      "116:\tlearn: 0.0009606\ttotal: 477ms\tremaining: 135ms\n",
      "117:\tlearn: 0.0009606\ttotal: 480ms\tremaining: 130ms\n",
      "118:\tlearn: 0.0009606\ttotal: 482ms\tremaining: 126ms\n",
      "119:\tlearn: 0.0009606\ttotal: 485ms\tremaining: 121ms\n",
      "120:\tlearn: 0.0009606\ttotal: 487ms\tremaining: 117ms\n",
      "121:\tlearn: 0.0009606\ttotal: 490ms\tremaining: 112ms\n",
      "122:\tlearn: 0.0009606\ttotal: 493ms\tremaining: 108ms\n",
      "123:\tlearn: 0.0009606\ttotal: 495ms\tremaining: 104ms\n",
      "124:\tlearn: 0.0009606\ttotal: 498ms\tremaining: 99.6ms\n",
      "125:\tlearn: 0.0009606\ttotal: 501ms\tremaining: 95.3ms\n",
      "126:\tlearn: 0.0009606\ttotal: 503ms\tremaining: 91.2ms\n",
      "127:\tlearn: 0.0009606\ttotal: 506ms\tremaining: 86.9ms\n",
      "128:\tlearn: 0.0009606\ttotal: 508ms\tremaining: 82.8ms\n",
      "129:\tlearn: 0.0009606\ttotal: 511ms\tremaining: 78.6ms\n",
      "130:\tlearn: 0.0009606\ttotal: 513ms\tremaining: 74.4ms\n",
      "131:\tlearn: 0.0009606\ttotal: 516ms\tremaining: 70.3ms\n",
      "132:\tlearn: 0.0009605\ttotal: 518ms\tremaining: 66.3ms\n",
      "133:\tlearn: 0.0009605\ttotal: 521ms\tremaining: 62.2ms\n",
      "134:\tlearn: 0.0009605\ttotal: 523ms\tremaining: 58.1ms\n",
      "135:\tlearn: 0.0009605\ttotal: 525ms\tremaining: 54.1ms\n",
      "136:\tlearn: 0.0009605\ttotal: 528ms\tremaining: 50.1ms\n",
      "137:\tlearn: 0.0009605\ttotal: 530ms\tremaining: 46.1ms\n",
      "138:\tlearn: 0.0009605\ttotal: 532ms\tremaining: 42.1ms\n",
      "139:\tlearn: 0.0009605\ttotal: 535ms\tremaining: 38.2ms\n",
      "140:\tlearn: 0.0009604\ttotal: 537ms\tremaining: 34.3ms\n",
      "141:\tlearn: 0.0009604\ttotal: 540ms\tremaining: 30.4ms\n",
      "142:\tlearn: 0.0009604\ttotal: 542ms\tremaining: 26.5ms\n",
      "143:\tlearn: 0.0009604\ttotal: 545ms\tremaining: 22.7ms\n",
      "144:\tlearn: 0.0009604\ttotal: 547ms\tremaining: 18.9ms\n",
      "145:\tlearn: 0.0009604\ttotal: 549ms\tremaining: 15ms\n",
      "146:\tlearn: 0.0009604\ttotal: 552ms\tremaining: 11.3ms\n",
      "147:\tlearn: 0.0009604\ttotal: 554ms\tremaining: 7.49ms\n",
      "148:\tlearn: 0.0009604\ttotal: 557ms\tremaining: 3.74ms\n",
      "149:\tlearn: 0.0009604\ttotal: 559ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199cc7d11bd740a0ab78d032531bfa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.596420212008447, Recall = 0.6097018246550957, Aging Rate = 0.39052069425901204, Precision = 0.7806267806267806, f1 = 0.6846576711644178\n",
      "Epoch 2: Train Loss = 0.40579729451303437, Recall = 0.8139741878059635, Aging Rate = 0.4766355140186916, Precision = 0.853874883286648, f1 = 0.8334472544998861\n",
      "Epoch 3: Train Loss = 0.30572532895914334, Recall = 0.8914107699154428, Aging Rate = 0.5026702269692924, Precision = 0.8866755201416556, f1 = 0.8890368397691966\n",
      "Epoch 4: Train Loss = 0.24963182931111874, Recall = 0.9167779261237206, Aging Rate = 0.4991099243435692, Precision = 0.9184128399465002, f1 = 0.9175946547884186\n",
      "Epoch 5: Train Loss = 0.21220633556007756, Recall = 0.9279038718291055, Aging Rate = 0.4959946595460614, Precision = 0.9353970390309556, f1 = 0.9316353887399463\n",
      "Test Loss = 0.17922657859500482, Recall = 0.9354695149087673, Aging Rate = 0.49109924343569206, precision = 0.9524241051200725\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1704666772231453, Recall = 0.9439252336448598, Aging Rate = 0.495549621717846, Precision = 0.9524023349797934, f1 = 0.9481448368350469\n",
      "Epoch 7: Train Loss = 0.1436937837983748, Recall = 0.9586114819759679, Aging Rate = 0.4946595460614152, Precision = 0.9689608636977058, f1 = 0.963758389261745\n",
      "Epoch 8: Train Loss = 0.12169262600385405, Recall = 0.9635068980863374, Aging Rate = 0.4915442812639074, Precision = 0.9800814848347669, f1 = 0.9717235188509874\n",
      "Epoch 9: Train Loss = 0.10427712931943885, Recall = 0.9706275033377837, Aging Rate = 0.493324432576769, Precision = 0.9837618403247632, f1 = 0.9771505376344087\n",
      "Epoch 10: Train Loss = 0.09147150772091119, Recall = 0.9737427681352915, Aging Rate = 0.49287939474855363, Precision = 0.9878103837471783, f1 = 0.9807261317794711\n",
      "Test Loss = 0.08136211678843314, Recall = 0.9799732977303071, Aging Rate = 0.4946595460614152, precision = 0.99055330634278\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.0806726402987307, Recall = 0.9764129951045839, Aging Rate = 0.4924343569203382, Precision = 0.991414369633981, f1 = 0.9838565022421525\n",
      "Epoch 12: Train Loss = 0.07085735914034211, Recall = 0.9781931464174455, Aging Rate = 0.4931019136626613, Precision = 0.9918772563176895, f1 = 0.9849876764508179\n",
      "Epoch 13: Train Loss = 0.06321184134295664, Recall = 0.9821984868713841, Aging Rate = 0.4946595460614152, Precision = 0.9928025191183086, f1 = 0.9874720357941834\n",
      "Epoch 14: Train Loss = 0.05742655625516274, Recall = 0.9821984868713841, Aging Rate = 0.49443702714730753, Precision = 0.9932493249324933, f1 = 0.987692996196017\n",
      "Epoch 15: Train Loss = 0.05201945569105927, Recall = 0.9870939029817535, Aging Rate = 0.49666221628838453, Precision = 0.9937275985663082, f1 = 0.9903996427774058\n",
      "Test Loss = 0.04705695586439022, Recall = 0.9884290164663997, Aging Rate = 0.4968847352024922, precision = 0.9946260635915808\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04754789268683342, Recall = 0.9875389408099688, Aging Rate = 0.4962171784601691, Precision = 0.9950672645739911, f1 = 0.9912888094706277\n",
      "Epoch 17: Train Loss = 0.04356453995480238, Recall = 0.9888740542946151, Aging Rate = 0.4964396973742768, Precision = 0.9959659345584939, f1 = 0.9924073246985261\n",
      "Epoch 18: Train Loss = 0.04014876524328814, Recall = 0.9906542056074766, Aging Rate = 0.4975522919448153, Precision = 0.9955277280858676, f1 = 0.99308498773143\n",
      "Epoch 19: Train Loss = 0.03822013764286975, Recall = 0.9915442812639075, Aging Rate = 0.4968847352024922, Precision = 0.9977608598298253, f1 = 0.9946428571428572\n",
      "Epoch 20: Train Loss = 0.036438764739014015, Recall = 0.9942145082331998, Aging Rate = 0.49888740542946153, Precision = 0.9964317573595004, f1 = 0.9953218979728223\n",
      "Test Loss = 0.03266455223978653, Recall = 0.9946595460614153, Aging Rate = 0.49732977303070763, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03321797297094523, Recall = 0.9942145082331998, Aging Rate = 0.4979973297730307, Precision = 0.998212689901698, f1 = 0.9962095875139353\n",
      "Epoch 22: Train Loss = 0.03083444829047071, Recall = 0.9964396973742768, Aging Rate = 0.4982198486871384, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.02951728874128635, Recall = 0.9951045838896306, Aging Rate = 0.4982198486871384, Precision = 0.9986601161232693, f1 = 0.9968791796700848\n",
      "Epoch 24: Train Loss = 0.02770589994002195, Recall = 0.9964396973742768, Aging Rate = 0.4986648865153538, Precision = 0.9991075412762159, f1 = 0.99777183600713\n",
      "Epoch 25: Train Loss = 0.027406826425919042, Recall = 0.9968847352024922, Aging Rate = 0.4986648865153538, Precision = 0.999553770638108, f1 = 0.9982174688057041\n",
      "Test Loss = 0.024553386810346342, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.025120949199399153, Recall = 0.9968847352024922, Aging Rate = 0.4984423676012461, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.024638398906483776, Recall = 0.9982198486871384, Aging Rate = 0.4993324432576769, Precision = 0.999554367201426, f1 = 0.9988866622133155\n",
      "Epoch 28: Train Loss = 0.0238786370464044, Recall = 0.9977748108589231, Aging Rate = 0.49888740542946153, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.022649376484443998, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.021946963671351148, Recall = 0.9982198486871384, Aging Rate = 0.4991099243435692, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.023008611422938988, Recall = 0.9995549621717846, Aging Rate = 0.5002225189141077, precision = 0.9991103202846975\n",
      "\n",
      "Epoch 31: Train Loss = 0.021888702542906105, Recall = 0.9977748108589231, Aging Rate = 0.4991099243435692, Precision = 0.9995541685242978, f1 = 0.9986636971046771\n",
      "Epoch 32: Train Loss = 0.020654737314397514, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.020225211946044198, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.019591251226547403, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.019347730087523374, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01778955972066682, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.019063428766746944, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01871551292180405, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.01834067944884897, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.018063440559159188, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.017769066532340165, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016399472029684888, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.017722255809371028, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 42: Train Loss = 0.017410927447587796, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.017230566338817106, Recall = 0.9982198486871384, Aging Rate = 0.4991099243435692, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.01713137828024178, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016881631063604102, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01858670714642466, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.017051957024805973, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.01680582700588197, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016879376379626297, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.016125394822014177, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016982127855352403, Recall = 0.9982198486871384, Aging Rate = 0.4991099243435692, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015541332746509, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.01631673011231652, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.01603290300383587, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.01643458987297312, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.01582794172981937, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 55: Train Loss = 0.01603255115787639, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016522415211522878, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016364229537138793, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 57: Train Loss = 0.015939409346794307, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015264581623650892, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.01593749533010739, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 60: Train Loss = 0.015110839499822977, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014227617366225336, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.01529855250360624, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.01528899592372726, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015419712627963936, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015286525902075136, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015242863325447734, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Test Loss = 0.013835969145198796, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.014895124782853302, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.015152828700750947, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015749783221380244, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015316165264979384, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 70: Train Loss = 0.015744205187918293, Recall = 0.9995549621717846, Aging Rate = 0.5002225189141077, Precision = 0.9991103202846975, f1 = 0.9993325917686319\n",
      "Test Loss = 0.01488912635091999, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.01495582864068127, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.014767203348202603, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.01501323595066096, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015540021668761492, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014759078137758948, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013874553643833493, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.015152110065808947, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014983794571932125, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.014839143360686013, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Epoch 79: Train Loss = 0.014792975099632143, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.014913120669692848, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013804059624041937, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.014763229179664829, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014888302139145404, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Epoch 83: Train Loss = 0.015066172606815325, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.014748960917440053, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.015110226000091754, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013810211577890818, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.014748840710512124, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.014668195905486344, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.015010903615539318, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.016139532253726434, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 90: Train Loss = 0.014394752750057988, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013447828601231749, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.01484804617304854, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.014724343590700314, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.014776443665421104, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.014719702654468864, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014852053334735065, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01337450962724238, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.014673371180480833, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.014904152364049374, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.014656462217131933, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.014621611062435638, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.01456988543897045, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013633446100623437, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5118339\ttotal: 5.62ms\tremaining: 837ms\n",
      "1:\tlearn: 0.3984615\ttotal: 11.5ms\tremaining: 850ms\n",
      "2:\tlearn: 0.3296941\ttotal: 17.3ms\tremaining: 850ms\n",
      "3:\tlearn: 0.2712707\ttotal: 23ms\tremaining: 840ms\n",
      "4:\tlearn: 0.2253818\ttotal: 29.5ms\tremaining: 855ms\n",
      "5:\tlearn: 0.1826011\ttotal: 35.4ms\tremaining: 850ms\n",
      "6:\tlearn: 0.1455870\ttotal: 41.5ms\tremaining: 848ms\n",
      "7:\tlearn: 0.1200391\ttotal: 48.2ms\tremaining: 855ms\n",
      "8:\tlearn: 0.1051000\ttotal: 53.9ms\tremaining: 845ms\n",
      "9:\tlearn: 0.0915791\ttotal: 59.2ms\tremaining: 829ms\n",
      "10:\tlearn: 0.0741590\ttotal: 66.3ms\tremaining: 838ms\n",
      "11:\tlearn: 0.0714204\ttotal: 69.8ms\tremaining: 803ms\n",
      "12:\tlearn: 0.0625742\ttotal: 75.9ms\tremaining: 800ms\n",
      "13:\tlearn: 0.0574279\ttotal: 80.6ms\tremaining: 783ms\n",
      "14:\tlearn: 0.0496529\ttotal: 85.5ms\tremaining: 770ms\n",
      "15:\tlearn: 0.0452861\ttotal: 89.3ms\tremaining: 748ms\n",
      "16:\tlearn: 0.0424284\ttotal: 93.3ms\tremaining: 730ms\n",
      "17:\tlearn: 0.0363489\ttotal: 98.1ms\tremaining: 719ms\n",
      "18:\tlearn: 0.0324659\ttotal: 103ms\tremaining: 710ms\n",
      "19:\tlearn: 0.0293944\ttotal: 107ms\tremaining: 698ms\n",
      "20:\tlearn: 0.0263648\ttotal: 113ms\tremaining: 691ms\n",
      "21:\tlearn: 0.0242708\ttotal: 117ms\tremaining: 681ms\n",
      "22:\tlearn: 0.0213411\ttotal: 122ms\tremaining: 675ms\n",
      "23:\tlearn: 0.0199733\ttotal: 126ms\tremaining: 663ms\n",
      "24:\tlearn: 0.0185395\ttotal: 131ms\tremaining: 654ms\n",
      "25:\tlearn: 0.0177606\ttotal: 134ms\tremaining: 641ms\n",
      "26:\tlearn: 0.0166615\ttotal: 139ms\tremaining: 631ms\n",
      "27:\tlearn: 0.0149210\ttotal: 143ms\tremaining: 625ms\n",
      "28:\tlearn: 0.0145066\ttotal: 146ms\tremaining: 611ms\n",
      "29:\tlearn: 0.0130557\ttotal: 152ms\tremaining: 608ms\n",
      "30:\tlearn: 0.0122804\ttotal: 156ms\tremaining: 599ms\n",
      "31:\tlearn: 0.0116748\ttotal: 161ms\tremaining: 594ms\n",
      "32:\tlearn: 0.0109043\ttotal: 166ms\tremaining: 587ms\n",
      "33:\tlearn: 0.0099394\ttotal: 171ms\tremaining: 583ms\n",
      "34:\tlearn: 0.0090390\ttotal: 177ms\tremaining: 581ms\n",
      "35:\tlearn: 0.0082501\ttotal: 181ms\tremaining: 575ms\n",
      "36:\tlearn: 0.0077127\ttotal: 186ms\tremaining: 569ms\n",
      "37:\tlearn: 0.0073847\ttotal: 191ms\tremaining: 563ms\n",
      "38:\tlearn: 0.0071071\ttotal: 195ms\tremaining: 556ms\n",
      "39:\tlearn: 0.0068304\ttotal: 199ms\tremaining: 548ms\n",
      "40:\tlearn: 0.0063812\ttotal: 204ms\tremaining: 543ms\n",
      "41:\tlearn: 0.0060849\ttotal: 210ms\tremaining: 539ms\n",
      "42:\tlearn: 0.0055920\ttotal: 215ms\tremaining: 536ms\n",
      "43:\tlearn: 0.0054281\ttotal: 219ms\tremaining: 528ms\n",
      "44:\tlearn: 0.0051324\ttotal: 224ms\tremaining: 523ms\n",
      "45:\tlearn: 0.0049377\ttotal: 228ms\tremaining: 516ms\n",
      "46:\tlearn: 0.0047229\ttotal: 233ms\tremaining: 510ms\n",
      "47:\tlearn: 0.0045239\ttotal: 237ms\tremaining: 503ms\n",
      "48:\tlearn: 0.0042426\ttotal: 243ms\tremaining: 500ms\n",
      "49:\tlearn: 0.0040899\ttotal: 246ms\tremaining: 493ms\n",
      "50:\tlearn: 0.0039225\ttotal: 250ms\tremaining: 486ms\n",
      "51:\tlearn: 0.0038157\ttotal: 254ms\tremaining: 478ms\n",
      "52:\tlearn: 0.0036317\ttotal: 259ms\tremaining: 473ms\n",
      "53:\tlearn: 0.0035183\ttotal: 263ms\tremaining: 467ms\n",
      "54:\tlearn: 0.0034250\ttotal: 266ms\tremaining: 460ms\n",
      "55:\tlearn: 0.0033073\ttotal: 270ms\tremaining: 454ms\n",
      "56:\tlearn: 0.0032334\ttotal: 274ms\tremaining: 448ms\n",
      "57:\tlearn: 0.0030699\ttotal: 279ms\tremaining: 442ms\n",
      "58:\tlearn: 0.0029589\ttotal: 283ms\tremaining: 436ms\n",
      "59:\tlearn: 0.0028562\ttotal: 287ms\tremaining: 430ms\n",
      "60:\tlearn: 0.0027431\ttotal: 291ms\tremaining: 425ms\n",
      "61:\tlearn: 0.0026733\ttotal: 295ms\tremaining: 418ms\n",
      "62:\tlearn: 0.0025720\ttotal: 298ms\tremaining: 412ms\n",
      "63:\tlearn: 0.0024484\ttotal: 303ms\tremaining: 407ms\n",
      "64:\tlearn: 0.0023494\ttotal: 307ms\tremaining: 401ms\n",
      "65:\tlearn: 0.0023116\ttotal: 311ms\tremaining: 395ms\n",
      "66:\tlearn: 0.0022571\ttotal: 314ms\tremaining: 389ms\n",
      "67:\tlearn: 0.0021645\ttotal: 318ms\tremaining: 384ms\n",
      "68:\tlearn: 0.0020685\ttotal: 323ms\tremaining: 379ms\n",
      "69:\tlearn: 0.0020178\ttotal: 327ms\tremaining: 373ms\n",
      "70:\tlearn: 0.0019299\ttotal: 331ms\tremaining: 368ms\n",
      "71:\tlearn: 0.0018839\ttotal: 334ms\tremaining: 362ms\n",
      "72:\tlearn: 0.0018023\ttotal: 338ms\tremaining: 357ms\n",
      "73:\tlearn: 0.0018023\ttotal: 341ms\tremaining: 350ms\n",
      "74:\tlearn: 0.0017799\ttotal: 344ms\tremaining: 344ms\n",
      "75:\tlearn: 0.0017447\ttotal: 347ms\tremaining: 338ms\n",
      "76:\tlearn: 0.0016975\ttotal: 351ms\tremaining: 333ms\n",
      "77:\tlearn: 0.0016662\ttotal: 355ms\tremaining: 327ms\n",
      "78:\tlearn: 0.0016348\ttotal: 358ms\tremaining: 322ms\n",
      "79:\tlearn: 0.0016070\ttotal: 361ms\tremaining: 316ms\n",
      "80:\tlearn: 0.0015691\ttotal: 365ms\tremaining: 311ms\n",
      "81:\tlearn: 0.0015178\ttotal: 368ms\tremaining: 306ms\n",
      "82:\tlearn: 0.0014888\ttotal: 372ms\tremaining: 300ms\n",
      "83:\tlearn: 0.0014490\ttotal: 376ms\tremaining: 295ms\n",
      "84:\tlearn: 0.0014205\ttotal: 379ms\tremaining: 290ms\n",
      "85:\tlearn: 0.0013821\ttotal: 382ms\tremaining: 285ms\n",
      "86:\tlearn: 0.0013406\ttotal: 386ms\tremaining: 280ms\n",
      "87:\tlearn: 0.0013093\ttotal: 390ms\tremaining: 275ms\n",
      "88:\tlearn: 0.0012715\ttotal: 394ms\tremaining: 270ms\n",
      "89:\tlearn: 0.0012373\ttotal: 398ms\tremaining: 266ms\n",
      "90:\tlearn: 0.0011936\ttotal: 402ms\tremaining: 261ms\n",
      "91:\tlearn: 0.0011712\ttotal: 405ms\tremaining: 256ms\n",
      "92:\tlearn: 0.0011712\ttotal: 408ms\tremaining: 250ms\n",
      "93:\tlearn: 0.0011503\ttotal: 411ms\tremaining: 245ms\n",
      "94:\tlearn: 0.0011289\ttotal: 414ms\tremaining: 240ms\n",
      "95:\tlearn: 0.0011289\ttotal: 417ms\tremaining: 235ms\n",
      "96:\tlearn: 0.0011048\ttotal: 420ms\tremaining: 230ms\n",
      "97:\tlearn: 0.0010797\ttotal: 424ms\tremaining: 225ms\n",
      "98:\tlearn: 0.0010489\ttotal: 427ms\tremaining: 220ms\n",
      "99:\tlearn: 0.0010216\ttotal: 431ms\tremaining: 216ms\n",
      "100:\tlearn: 0.0010216\ttotal: 434ms\tremaining: 210ms\n",
      "101:\tlearn: 0.0010215\ttotal: 436ms\tremaining: 205ms\n",
      "102:\tlearn: 0.0010215\ttotal: 439ms\tremaining: 200ms\n",
      "103:\tlearn: 0.0010119\ttotal: 442ms\tremaining: 196ms\n",
      "104:\tlearn: 0.0009876\ttotal: 445ms\tremaining: 191ms\n",
      "105:\tlearn: 0.0009876\ttotal: 449ms\tremaining: 186ms\n",
      "106:\tlearn: 0.0009876\ttotal: 451ms\tremaining: 181ms\n",
      "107:\tlearn: 0.0009577\ttotal: 455ms\tremaining: 177ms\n",
      "108:\tlearn: 0.0009405\ttotal: 459ms\tremaining: 172ms\n",
      "109:\tlearn: 0.0009405\ttotal: 461ms\tremaining: 168ms\n",
      "110:\tlearn: 0.0009217\ttotal: 465ms\tremaining: 163ms\n",
      "111:\tlearn: 0.0009128\ttotal: 468ms\tremaining: 159ms\n",
      "112:\tlearn: 0.0009128\ttotal: 471ms\tremaining: 154ms\n",
      "113:\tlearn: 0.0008885\ttotal: 474ms\tremaining: 150ms\n",
      "114:\tlearn: 0.0008885\ttotal: 477ms\tremaining: 145ms\n",
      "115:\tlearn: 0.0008885\ttotal: 479ms\tremaining: 141ms\n",
      "116:\tlearn: 0.0008885\ttotal: 482ms\tremaining: 136ms\n",
      "117:\tlearn: 0.0008885\ttotal: 484ms\tremaining: 131ms\n",
      "118:\tlearn: 0.0008885\ttotal: 487ms\tremaining: 127ms\n",
      "119:\tlearn: 0.0008885\ttotal: 489ms\tremaining: 122ms\n",
      "120:\tlearn: 0.0008884\ttotal: 492ms\tremaining: 118ms\n",
      "121:\tlearn: 0.0008884\ttotal: 495ms\tremaining: 114ms\n",
      "122:\tlearn: 0.0008884\ttotal: 497ms\tremaining: 109ms\n",
      "123:\tlearn: 0.0008884\ttotal: 500ms\tremaining: 105ms\n",
      "124:\tlearn: 0.0008884\ttotal: 503ms\tremaining: 101ms\n",
      "125:\tlearn: 0.0008884\ttotal: 506ms\tremaining: 96.3ms\n",
      "126:\tlearn: 0.0008884\ttotal: 508ms\tremaining: 92ms\n",
      "127:\tlearn: 0.0008884\ttotal: 510ms\tremaining: 87.7ms\n",
      "128:\tlearn: 0.0008884\ttotal: 513ms\tremaining: 83.5ms\n",
      "129:\tlearn: 0.0008884\ttotal: 515ms\tremaining: 79.3ms\n",
      "130:\tlearn: 0.0008883\ttotal: 518ms\tremaining: 75.1ms\n",
      "131:\tlearn: 0.0008883\ttotal: 520ms\tremaining: 70.9ms\n",
      "132:\tlearn: 0.0008883\ttotal: 523ms\tremaining: 66.8ms\n",
      "133:\tlearn: 0.0008883\ttotal: 525ms\tremaining: 62.7ms\n",
      "134:\tlearn: 0.0008883\ttotal: 528ms\tremaining: 58.6ms\n",
      "135:\tlearn: 0.0008883\ttotal: 530ms\tremaining: 54.5ms\n",
      "136:\tlearn: 0.0008883\ttotal: 532ms\tremaining: 50.5ms\n",
      "137:\tlearn: 0.0008883\ttotal: 535ms\tremaining: 46.5ms\n",
      "138:\tlearn: 0.0008883\ttotal: 537ms\tremaining: 42.5ms\n",
      "139:\tlearn: 0.0008883\ttotal: 540ms\tremaining: 38.6ms\n",
      "140:\tlearn: 0.0008721\ttotal: 543ms\tremaining: 34.7ms\n",
      "141:\tlearn: 0.0008551\ttotal: 546ms\tremaining: 30.8ms\n",
      "142:\tlearn: 0.0008551\ttotal: 549ms\tremaining: 26.9ms\n",
      "143:\tlearn: 0.0008551\ttotal: 551ms\tremaining: 23ms\n",
      "144:\tlearn: 0.0008551\ttotal: 554ms\tremaining: 19.1ms\n",
      "145:\tlearn: 0.0008551\ttotal: 556ms\tremaining: 15.2ms\n",
      "146:\tlearn: 0.0008551\ttotal: 559ms\tremaining: 11.4ms\n",
      "147:\tlearn: 0.0008550\ttotal: 561ms\tremaining: 7.58ms\n",
      "148:\tlearn: 0.0008550\ttotal: 563ms\tremaining: 3.78ms\n",
      "149:\tlearn: 0.0008550\ttotal: 566ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1078e38d167424d8bd8c33f20853cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5939000620940658, Recall = 0.5976858032932799, Aging Rate = 0.3751668891855808, Precision = 0.7965599051008304, f1 = 0.6829392321383169\n",
      "Epoch 2: Train Loss = 0.4063635548459831, Recall = 0.8072986203827325, Aging Rate = 0.4801958166444148, Precision = 0.8405931417979611, f1 = 0.8236095346197503\n",
      "Epoch 3: Train Loss = 0.3118111419383292, Recall = 0.881619937694704, Aging Rate = 0.4991099243435692, Precision = 0.8831921533660276, f1 = 0.8824053452115812\n",
      "Epoch 4: Train Loss = 0.257546755503059, Recall = 0.9127725856697819, Aging Rate = 0.5060080106809078, Precision = 0.9019349164467898, f1 = 0.9073213890732139\n",
      "Epoch 5: Train Loss = 0.22149118587447636, Recall = 0.9327992879394749, Aging Rate = 0.5055629728526925, Precision = 0.9225352112676056, f1 = 0.9276388581544589\n",
      "Test Loss = 0.1912863193669211, Recall = 0.9368046283934134, Aging Rate = 0.49043168669336895, precision = 0.95508166969147\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.18092572902593285, Recall = 0.9439252336448598, Aging Rate = 0.4995549621717846, Precision = 0.9447661469933185, f1 = 0.9443455031166518\n",
      "Epoch 7: Train Loss = 0.15381471966577734, Recall = 0.9514908767245216, Aging Rate = 0.4962171784601691, Precision = 0.9587443946188341, f1 = 0.9551038641947732\n",
      "Epoch 8: Train Loss = 0.13229587320677694, Recall = 0.9603916332888296, Aging Rate = 0.49443702714730753, Precision = 0.9711971197119712, f1 = 0.9657641530543746\n",
      "Epoch 9: Train Loss = 0.1156797567950284, Recall = 0.9719626168224299, Aging Rate = 0.497774810858923, Precision = 0.9763075547608404, f1 = 0.9741302408563782\n",
      "Epoch 10: Train Loss = 0.1011259470285842, Recall = 0.9728526924788607, Aging Rate = 0.49443702714730753, Precision = 0.9837983798379838, f1 = 0.9782949205638846\n",
      "Test Loss = 0.09214605923209872, Recall = 0.9817534490431686, Aging Rate = 0.4997774810858923, precision = 0.9821905609973286\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.08841844371088516, Recall = 0.9790832220738763, Aging Rate = 0.495549621717846, Precision = 0.9878760664571172, f1 = 0.9834599910594547\n",
      "Epoch 12: Train Loss = 0.07900313772410353, Recall = 0.9795282599020917, Aging Rate = 0.4946595460614152, Precision = 0.9901034637876743, f1 = 0.9847874720357942\n",
      "Epoch 13: Train Loss = 0.07080678340224993, Recall = 0.9813084112149533, Aging Rate = 0.49443702714730753, Precision = 0.9923492349234924, f1 = 0.9867979413739092\n",
      "Epoch 14: Train Loss = 0.06426171076777887, Recall = 0.9830885625278148, Aging Rate = 0.49510458388963063, Precision = 0.992808988764045, f1 = 0.9879248658318426\n",
      "Epoch 15: Train Loss = 0.05806570144516922, Recall = 0.9835336003560302, Aging Rate = 0.4946595460614152, Precision = 0.9941520467836257, f1 = 0.988814317673378\n",
      "Test Loss = 0.05283103886913818, Recall = 0.9853137516688919, Aging Rate = 0.49421450823319985, precision = 0.9968482665466006\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05330555803467021, Recall = 0.9857587894971073, Aging Rate = 0.495549621717846, Precision = 0.994611585092052, f1 = 0.9901654000894055\n",
      "Epoch 17: Train Loss = 0.0489474143327264, Recall = 0.9870939029817535, Aging Rate = 0.4953271028037383, Precision = 0.9964061096136568, f1 = 0.9917281466577241\n",
      "Epoch 18: Train Loss = 0.045075868876857335, Recall = 0.9879839786381842, Aging Rate = 0.49510458388963063, Precision = 0.9977528089887641, f1 = 0.9928443649373883\n",
      "Epoch 19: Train Loss = 0.04209355931293052, Recall = 0.9884290164663997, Aging Rate = 0.4953271028037383, Precision = 0.9977538185085355, f1 = 0.9930695282807959\n",
      "Epoch 20: Train Loss = 0.03919389086282672, Recall = 0.9906542056074766, Aging Rate = 0.4964396973742768, Precision = 0.9977588525324966, f1 = 0.994193836534167\n",
      "Test Loss = 0.03610573856729531, Recall = 0.9937694704049844, Aging Rate = 0.4982198486871384, precision = 0.9973202322465387\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03629509290712618, Recall = 0.9902091677792613, Aging Rate = 0.4962171784601691, Precision = 0.9977578475336323, f1 = 0.9939691757873577\n",
      "Epoch 22: Train Loss = 0.034134689322247946, Recall = 0.9928793947485536, Aging Rate = 0.4975522919448153, Precision = 0.9977638640429338, f1 = 0.9953156368503234\n",
      "Epoch 23: Train Loss = 0.032469417355228486, Recall = 0.9933244325767691, Aging Rate = 0.4975522919448153, Precision = 0.998211091234347, f1 = 0.9957617666741022\n",
      "Epoch 24: Train Loss = 0.030246053268394378, Recall = 0.9946595460614153, Aging Rate = 0.4982198486871384, Precision = 0.9982134881643591, f1 = 0.9964333481943825\n",
      "Epoch 25: Train Loss = 0.02876331150559953, Recall = 0.9942145082331998, Aging Rate = 0.4979973297730307, Precision = 0.998212689901698, f1 = 0.9962095875139353\n",
      "Test Loss = 0.026067004656368593, Recall = 0.995549621717846, Aging Rate = 0.4986648865153538, precision = 0.998215082552432\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.027602622696643254, Recall = 0.9946595460614153, Aging Rate = 0.4984423676012461, Precision = 0.9977678571428571, f1 = 0.9962112770225094\n",
      "Epoch 27: Train Loss = 0.027057090082836247, Recall = 0.9942145082331998, Aging Rate = 0.497774810858923, Precision = 0.9986589181940099, f1 = 0.9964317573595004\n",
      "Epoch 28: Train Loss = 0.02526750614179417, Recall = 0.9946595460614153, Aging Rate = 0.4982198486871384, Precision = 0.9982134881643591, f1 = 0.9964333481943825\n",
      "Epoch 29: Train Loss = 0.023969121194852023, Recall = 0.9964396973742768, Aging Rate = 0.49888740542946153, Precision = 0.9986619090098127, f1 = 0.9975495656048118\n",
      "Epoch 30: Train Loss = 0.023249158915090835, Recall = 0.9964396973742768, Aging Rate = 0.49888740542946153, Precision = 0.9986619090098127, f1 = 0.9975495656048118\n",
      "Test Loss = 0.02142316976319261, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.022422585692331295, Recall = 0.9982198486871384, Aging Rate = 0.4993324432576769, Precision = 0.999554367201426, f1 = 0.9988866622133155\n",
      "Epoch 32: Train Loss = 0.021716630262868585, Recall = 0.9973297730307076, Aging Rate = 0.4991099243435692, Precision = 0.9991083370485956, f1 = 0.998218262806236\n",
      "Epoch 33: Train Loss = 0.021354442181311876, Recall = 0.9977748108589231, Aging Rate = 0.4991099243435692, Precision = 0.9995541685242978, f1 = 0.9986636971046771\n",
      "Epoch 34: Train Loss = 0.020453632751195414, Recall = 0.9977748108589231, Aging Rate = 0.49888740542946153, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.02016355878162448, Recall = 0.9977748108589231, Aging Rate = 0.49888740542946153, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01824107223439466, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.019496169146838375, Recall = 0.9982198486871384, Aging Rate = 0.4991099243435692, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.019089304945258814, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.018605744267341774, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.018640546061423523, Recall = 0.9982198486871384, Aging Rate = 0.4991099243435692, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.018655392682427663, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016650280623411173, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.01792530220998331, Recall = 0.9986648865153538, Aging Rate = 0.4995549621717846, Precision = 0.999554565701559, f1 = 0.9991095280498664\n",
      "Epoch 42: Train Loss = 0.01743571574558907, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.017435860396522413, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.017039308903476078, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.017239350680067878, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015834454216583603, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.01689544610942755, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.01684786368187966, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.016452869251585955, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016217116242864024, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01649846789614858, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015599484761416727, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.016840313021615844, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.01627907323148259, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016001546078182934, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 54: Train Loss = 0.016255807005528828, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015611457634919489, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014606377183247074, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015741397523079573, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.01577631843454131, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015388635368905812, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.015310816830716294, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015830901326652255, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014950473997186542, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.01574130691538188, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.016339449639277533, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015031222254866805, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015618111057066897, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.01492228784482772, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013503180403539245, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.015342783247102856, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01527662138638228, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.014701220805001991, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015289476192728328, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.014512506627378647, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013670226234302727, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.014928558682189765, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.015030417805201857, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.014769083373066209, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015008271024009095, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015030597145932353, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013592353553196617, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.01509702767497886, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014503612439593687, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.01574096186927917, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 79: Train Loss = 0.014776601242651715, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.014366050106591392, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014171774701796005, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.014523045480264575, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.015022627190319124, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 83: Train Loss = 0.01599491887378146, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.01624319133894625, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 85: Train Loss = 0.014268701441808175, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013704182137662654, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.014426401362459713, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.014632601661480794, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.014843345338310408, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.01431526540698584, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.01436531470369831, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014392519959103123, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014846770021969947, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.014115019972901583, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.014824245445650778, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.01488094395242091, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014637786430826069, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014018515073522283, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.015546618740297738, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.013968856171317295, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.014414010510183489, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.014275913480727286, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.014278751129427909, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013464786179246017, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.014584082233482034, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.014275504734564421, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103: Train Loss = 0.014514148043419687, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.014387313319634585, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.01534396890404957, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013504628483313796, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.014489617587896549, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.014318262150823659, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.014352554545340323, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.014338925321477423, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.015011927317272545, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014253421357497539, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.014351469239540825, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.014604072360065735, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.014335317963682046, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.014510301241222611, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.014140931102536324, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01273428373416769, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 115.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5534115\ttotal: 6.1ms\tremaining: 909ms\n",
      "1:\tlearn: 0.3983817\ttotal: 12.8ms\tremaining: 948ms\n",
      "2:\tlearn: 0.3283119\ttotal: 18.6ms\tremaining: 911ms\n",
      "3:\tlearn: 0.2497894\ttotal: 24.8ms\tremaining: 906ms\n",
      "4:\tlearn: 0.1976458\ttotal: 31ms\tremaining: 898ms\n",
      "5:\tlearn: 0.1649635\ttotal: 37ms\tremaining: 888ms\n",
      "6:\tlearn: 0.1306357\ttotal: 43.3ms\tremaining: 884ms\n",
      "7:\tlearn: 0.1031121\ttotal: 49.8ms\tremaining: 884ms\n",
      "8:\tlearn: 0.0896255\ttotal: 55.4ms\tremaining: 868ms\n",
      "9:\tlearn: 0.0765062\ttotal: 61.8ms\tremaining: 865ms\n",
      "10:\tlearn: 0.0666219\ttotal: 66.9ms\tremaining: 845ms\n",
      "11:\tlearn: 0.0572806\ttotal: 72.8ms\tremaining: 837ms\n",
      "12:\tlearn: 0.0512592\ttotal: 78.5ms\tremaining: 827ms\n",
      "13:\tlearn: 0.0441471\ttotal: 83.6ms\tremaining: 812ms\n",
      "14:\tlearn: 0.0392328\ttotal: 88.3ms\tremaining: 794ms\n",
      "15:\tlearn: 0.0358890\ttotal: 92.4ms\tremaining: 774ms\n",
      "16:\tlearn: 0.0332438\ttotal: 96.5ms\tremaining: 755ms\n",
      "17:\tlearn: 0.0300080\ttotal: 101ms\tremaining: 741ms\n",
      "18:\tlearn: 0.0276733\ttotal: 105ms\tremaining: 721ms\n",
      "19:\tlearn: 0.0251752\ttotal: 109ms\tremaining: 710ms\n",
      "20:\tlearn: 0.0230762\ttotal: 114ms\tremaining: 698ms\n",
      "21:\tlearn: 0.0212933\ttotal: 117ms\tremaining: 683ms\n",
      "22:\tlearn: 0.0195651\ttotal: 122ms\tremaining: 671ms\n",
      "23:\tlearn: 0.0178968\ttotal: 127ms\tremaining: 665ms\n",
      "24:\tlearn: 0.0165229\ttotal: 131ms\tremaining: 656ms\n",
      "25:\tlearn: 0.0149949\ttotal: 136ms\tremaining: 650ms\n",
      "26:\tlearn: 0.0137029\ttotal: 142ms\tremaining: 646ms\n",
      "27:\tlearn: 0.0127996\ttotal: 146ms\tremaining: 636ms\n",
      "28:\tlearn: 0.0121229\ttotal: 150ms\tremaining: 627ms\n",
      "29:\tlearn: 0.0108728\ttotal: 155ms\tremaining: 621ms\n",
      "30:\tlearn: 0.0103780\ttotal: 160ms\tremaining: 613ms\n",
      "31:\tlearn: 0.0099760\ttotal: 164ms\tremaining: 604ms\n",
      "32:\tlearn: 0.0092479\ttotal: 169ms\tremaining: 598ms\n",
      "33:\tlearn: 0.0089220\ttotal: 172ms\tremaining: 587ms\n",
      "34:\tlearn: 0.0084514\ttotal: 176ms\tremaining: 579ms\n",
      "35:\tlearn: 0.0076386\ttotal: 182ms\tremaining: 577ms\n",
      "36:\tlearn: 0.0073433\ttotal: 186ms\tremaining: 567ms\n",
      "37:\tlearn: 0.0070271\ttotal: 190ms\tremaining: 559ms\n",
      "38:\tlearn: 0.0067633\ttotal: 194ms\tremaining: 551ms\n",
      "39:\tlearn: 0.0063485\ttotal: 198ms\tremaining: 546ms\n",
      "40:\tlearn: 0.0059703\ttotal: 203ms\tremaining: 539ms\n",
      "41:\tlearn: 0.0055792\ttotal: 207ms\tremaining: 533ms\n",
      "42:\tlearn: 0.0051996\ttotal: 212ms\tremaining: 527ms\n",
      "43:\tlearn: 0.0049427\ttotal: 216ms\tremaining: 521ms\n",
      "44:\tlearn: 0.0047893\ttotal: 220ms\tremaining: 514ms\n",
      "45:\tlearn: 0.0046791\ttotal: 223ms\tremaining: 504ms\n",
      "46:\tlearn: 0.0045294\ttotal: 227ms\tremaining: 498ms\n",
      "47:\tlearn: 0.0043781\ttotal: 230ms\tremaining: 490ms\n",
      "48:\tlearn: 0.0041972\ttotal: 234ms\tremaining: 483ms\n",
      "49:\tlearn: 0.0040018\ttotal: 239ms\tremaining: 478ms\n",
      "50:\tlearn: 0.0038844\ttotal: 242ms\tremaining: 470ms\n",
      "51:\tlearn: 0.0037002\ttotal: 246ms\tremaining: 465ms\n",
      "52:\tlearn: 0.0035771\ttotal: 249ms\tremaining: 457ms\n",
      "53:\tlearn: 0.0034226\ttotal: 253ms\tremaining: 450ms\n",
      "54:\tlearn: 0.0033600\ttotal: 256ms\tremaining: 443ms\n",
      "55:\tlearn: 0.0031751\ttotal: 260ms\tremaining: 437ms\n",
      "56:\tlearn: 0.0030201\ttotal: 265ms\tremaining: 432ms\n",
      "57:\tlearn: 0.0028618\ttotal: 269ms\tremaining: 426ms\n",
      "58:\tlearn: 0.0027554\ttotal: 272ms\tremaining: 419ms\n",
      "59:\tlearn: 0.0027084\ttotal: 275ms\tremaining: 412ms\n",
      "60:\tlearn: 0.0026497\ttotal: 278ms\tremaining: 406ms\n",
      "61:\tlearn: 0.0025743\ttotal: 282ms\tremaining: 400ms\n",
      "62:\tlearn: 0.0025283\ttotal: 285ms\tremaining: 393ms\n",
      "63:\tlearn: 0.0024243\ttotal: 289ms\tremaining: 388ms\n",
      "64:\tlearn: 0.0024025\ttotal: 292ms\tremaining: 382ms\n",
      "65:\tlearn: 0.0023623\ttotal: 295ms\tremaining: 375ms\n",
      "66:\tlearn: 0.0022947\ttotal: 298ms\tremaining: 369ms\n",
      "67:\tlearn: 0.0021642\ttotal: 302ms\tremaining: 364ms\n",
      "68:\tlearn: 0.0021283\ttotal: 305ms\tremaining: 358ms\n",
      "69:\tlearn: 0.0020043\ttotal: 310ms\tremaining: 354ms\n",
      "70:\tlearn: 0.0019276\ttotal: 313ms\tremaining: 349ms\n",
      "71:\tlearn: 0.0018701\ttotal: 317ms\tremaining: 343ms\n",
      "72:\tlearn: 0.0018360\ttotal: 320ms\tremaining: 338ms\n",
      "73:\tlearn: 0.0018048\ttotal: 323ms\tremaining: 332ms\n",
      "74:\tlearn: 0.0017605\ttotal: 326ms\tremaining: 326ms\n",
      "75:\tlearn: 0.0016797\ttotal: 330ms\tremaining: 322ms\n",
      "76:\tlearn: 0.0015988\ttotal: 335ms\tremaining: 317ms\n",
      "77:\tlearn: 0.0015735\ttotal: 338ms\tremaining: 312ms\n",
      "78:\tlearn: 0.0015529\ttotal: 341ms\tremaining: 306ms\n",
      "79:\tlearn: 0.0014904\ttotal: 345ms\tremaining: 302ms\n",
      "80:\tlearn: 0.0014456\ttotal: 348ms\tremaining: 297ms\n",
      "81:\tlearn: 0.0014456\ttotal: 351ms\tremaining: 291ms\n",
      "82:\tlearn: 0.0014456\ttotal: 353ms\tremaining: 285ms\n",
      "83:\tlearn: 0.0014143\ttotal: 356ms\tremaining: 280ms\n",
      "84:\tlearn: 0.0013772\ttotal: 359ms\tremaining: 275ms\n",
      "85:\tlearn: 0.0013464\ttotal: 362ms\tremaining: 269ms\n",
      "86:\tlearn: 0.0013055\ttotal: 365ms\tremaining: 264ms\n",
      "87:\tlearn: 0.0013055\ttotal: 367ms\tremaining: 259ms\n",
      "88:\tlearn: 0.0012925\ttotal: 370ms\tremaining: 254ms\n",
      "89:\tlearn: 0.0012722\ttotal: 373ms\tremaining: 249ms\n",
      "90:\tlearn: 0.0012722\ttotal: 375ms\tremaining: 243ms\n",
      "91:\tlearn: 0.0012721\ttotal: 378ms\tremaining: 238ms\n",
      "92:\tlearn: 0.0012721\ttotal: 380ms\tremaining: 233ms\n",
      "93:\tlearn: 0.0012721\ttotal: 382ms\tremaining: 228ms\n",
      "94:\tlearn: 0.0012721\ttotal: 385ms\tremaining: 223ms\n",
      "95:\tlearn: 0.0012559\ttotal: 387ms\tremaining: 218ms\n",
      "96:\tlearn: 0.0012559\ttotal: 390ms\tremaining: 213ms\n",
      "97:\tlearn: 0.0012559\ttotal: 392ms\tremaining: 208ms\n",
      "98:\tlearn: 0.0012292\ttotal: 396ms\tremaining: 204ms\n",
      "99:\tlearn: 0.0011955\ttotal: 398ms\tremaining: 199ms\n",
      "100:\tlearn: 0.0011955\ttotal: 401ms\tremaining: 194ms\n",
      "101:\tlearn: 0.0011792\ttotal: 403ms\tremaining: 190ms\n",
      "102:\tlearn: 0.0011523\ttotal: 407ms\tremaining: 185ms\n",
      "103:\tlearn: 0.0011166\ttotal: 410ms\tremaining: 181ms\n",
      "104:\tlearn: 0.0011028\ttotal: 413ms\tremaining: 177ms\n",
      "105:\tlearn: 0.0011028\ttotal: 416ms\tremaining: 172ms\n",
      "106:\tlearn: 0.0011028\ttotal: 418ms\tremaining: 168ms\n",
      "107:\tlearn: 0.0010921\ttotal: 421ms\tremaining: 164ms\n",
      "108:\tlearn: 0.0010921\ttotal: 423ms\tremaining: 159ms\n",
      "109:\tlearn: 0.0010921\ttotal: 425ms\tremaining: 155ms\n",
      "110:\tlearn: 0.0010920\ttotal: 428ms\tremaining: 150ms\n",
      "111:\tlearn: 0.0010920\ttotal: 431ms\tremaining: 146ms\n",
      "112:\tlearn: 0.0010920\ttotal: 433ms\tremaining: 142ms\n",
      "113:\tlearn: 0.0010920\ttotal: 435ms\tremaining: 138ms\n",
      "114:\tlearn: 0.0010919\ttotal: 438ms\tremaining: 133ms\n",
      "115:\tlearn: 0.0010657\ttotal: 441ms\tremaining: 129ms\n",
      "116:\tlearn: 0.0010657\ttotal: 443ms\tremaining: 125ms\n",
      "117:\tlearn: 0.0010656\ttotal: 446ms\tremaining: 121ms\n",
      "118:\tlearn: 0.0010512\ttotal: 448ms\tremaining: 117ms\n",
      "119:\tlearn: 0.0010512\ttotal: 451ms\tremaining: 113ms\n",
      "120:\tlearn: 0.0010512\ttotal: 453ms\tremaining: 109ms\n",
      "121:\tlearn: 0.0010512\ttotal: 456ms\tremaining: 105ms\n",
      "122:\tlearn: 0.0010512\ttotal: 459ms\tremaining: 101ms\n",
      "123:\tlearn: 0.0010512\ttotal: 461ms\tremaining: 96.7ms\n",
      "124:\tlearn: 0.0010511\ttotal: 464ms\tremaining: 92.8ms\n",
      "125:\tlearn: 0.0010511\ttotal: 466ms\tremaining: 88.9ms\n",
      "126:\tlearn: 0.0010511\ttotal: 469ms\tremaining: 85ms\n",
      "127:\tlearn: 0.0010293\ttotal: 472ms\tremaining: 81.1ms\n",
      "128:\tlearn: 0.0010089\ttotal: 475ms\tremaining: 77.4ms\n",
      "129:\tlearn: 0.0010089\ttotal: 479ms\tremaining: 73.6ms\n",
      "130:\tlearn: 0.0010089\ttotal: 481ms\tremaining: 69.8ms\n",
      "131:\tlearn: 0.0010089\ttotal: 483ms\tremaining: 65.9ms\n",
      "132:\tlearn: 0.0009925\ttotal: 486ms\tremaining: 62.2ms\n",
      "133:\tlearn: 0.0009925\ttotal: 489ms\tremaining: 58.4ms\n",
      "134:\tlearn: 0.0009925\ttotal: 492ms\tremaining: 54.6ms\n",
      "135:\tlearn: 0.0009664\ttotal: 495ms\tremaining: 50.9ms\n",
      "136:\tlearn: 0.0009455\ttotal: 498ms\tremaining: 47.2ms\n",
      "137:\tlearn: 0.0009455\ttotal: 500ms\tremaining: 43.5ms\n",
      "138:\tlearn: 0.0009455\ttotal: 503ms\tremaining: 39.8ms\n",
      "139:\tlearn: 0.0009455\ttotal: 505ms\tremaining: 36.1ms\n",
      "140:\tlearn: 0.0009455\ttotal: 508ms\tremaining: 32.4ms\n",
      "141:\tlearn: 0.0009300\ttotal: 511ms\tremaining: 28.8ms\n",
      "142:\tlearn: 0.0009107\ttotal: 514ms\tremaining: 25.2ms\n",
      "143:\tlearn: 0.0009107\ttotal: 517ms\tremaining: 21.5ms\n",
      "144:\tlearn: 0.0009107\ttotal: 520ms\tremaining: 17.9ms\n",
      "145:\tlearn: 0.0009107\ttotal: 523ms\tremaining: 14.3ms\n",
      "146:\tlearn: 0.0009107\ttotal: 525ms\tremaining: 10.7ms\n",
      "147:\tlearn: 0.0009107\ttotal: 528ms\tremaining: 7.13ms\n",
      "148:\tlearn: 0.0009107\ttotal: 531ms\tremaining: 3.56ms\n",
      "149:\tlearn: 0.0009107\ttotal: 534ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcf85c6f9974c0480d199708adc00e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6278753917669263, Recall = 0.5687583444592791, Aging Rate = 0.38295505117935025, Precision = 0.7425915165601394, f1 = 0.6441532258064516\n",
      "Epoch 2: Train Loss = 0.4206041190217854, Recall = 0.8104138851802403, Aging Rate = 0.4797507788161994, Precision = 0.8446196660482375, f1 = 0.8271632977515331\n",
      "Epoch 3: Train Loss = 0.31297509128855344, Recall = 0.881619937694704, Aging Rate = 0.4984423676012461, Precision = 0.884375, f1 = 0.8829953198127924\n",
      "Epoch 4: Train Loss = 0.2568287772698884, Recall = 0.9190031152647975, Aging Rate = 0.5026702269692924, Precision = 0.9141212926073484, f1 = 0.9165557035064358\n",
      "Epoch 5: Train Loss = 0.21545224844030025, Recall = 0.9368046283934134, Aging Rate = 0.5006675567423231, Precision = 0.9355555555555556, f1 = 0.936179675339115\n",
      "Test Loss = 0.18823700992396317, Recall = 0.9546061415220294, Aging Rate = 0.5129060970182465, precision = 0.93058568329718\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.17540555708292702, Recall = 0.9483756119270138, Aging Rate = 0.4993324432576769, Precision = 0.9496434937611408, f1 = 0.9490091293698508\n",
      "Epoch 7: Train Loss = 0.146514082344893, Recall = 0.9617267467734757, Aging Rate = 0.4995549621717846, Precision = 0.9625835189309577, f1 = 0.9621549421193233\n",
      "Epoch 8: Train Loss = 0.12529121801210713, Recall = 0.9670672007120605, Aging Rate = 0.4964396973742768, Precision = 0.974002689376961, f1 = 0.970522554711925\n",
      "Epoch 9: Train Loss = 0.10986046116711248, Recall = 0.9719626168224299, Aging Rate = 0.49666221628838453, Precision = 0.978494623655914, f1 = 0.9752176825184193\n",
      "Epoch 10: Train Loss = 0.09445566142870683, Recall = 0.9768580329327993, Aging Rate = 0.4964396973742768, Precision = 0.9838637382339758, f1 = 0.98034836980795\n",
      "Test Loss = 0.08685449647257526, Recall = 0.9804183355585224, Aging Rate = 0.4986648865153538, precision = 0.9830432842481035\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.08395186940338223, Recall = 0.9777481085892301, Aging Rate = 0.4953271028037383, Precision = 0.9869721473495059, f1 = 0.9823384752962218\n",
      "Epoch 12: Train Loss = 0.07446547860257298, Recall = 0.9799732977303071, Aging Rate = 0.4948820649755229, Precision = 0.9901079136690647, f1 = 0.9850145381346455\n",
      "Epoch 13: Train Loss = 0.06613725713530036, Recall = 0.9813084112149533, Aging Rate = 0.4939919893190921, Precision = 0.9932432432432432, f1 = 0.9872397582269979\n",
      "Epoch 14: Train Loss = 0.059603309740437156, Recall = 0.9830885625278148, Aging Rate = 0.4946595460614152, Precision = 0.9937022042285201, f1 = 0.9883668903803132\n",
      "Epoch 15: Train Loss = 0.055020371902092434, Recall = 0.9844236760124611, Aging Rate = 0.49421450823319985, Precision = 0.9959477712742009, f1 = 0.990152193375112\n",
      "Test Loss = 0.04890797917640337, Recall = 0.9879839786381842, Aging Rate = 0.495549621717846, precision = 0.9968567579703638\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04967406052560556, Recall = 0.9870939029817535, Aging Rate = 0.49577214063195374, Precision = 0.9955116696588869, f1 = 0.9912849162011174\n",
      "Epoch 17: Train Loss = 0.04598128768600089, Recall = 0.986648865153538, Aging Rate = 0.49510458388963063, Precision = 0.9964044943820225, f1 = 0.9915026833631485\n",
      "Epoch 18: Train Loss = 0.04236273793317129, Recall = 0.9884290164663997, Aging Rate = 0.4953271028037383, Precision = 0.9977538185085355, f1 = 0.9930695282807959\n",
      "Epoch 19: Train Loss = 0.03937948747149668, Recall = 0.9893190921228304, Aging Rate = 0.4962171784601691, Precision = 0.9968609865470852, f1 = 0.9930757203484477\n",
      "Epoch 20: Train Loss = 0.0361086281474877, Recall = 0.9906542056074766, Aging Rate = 0.49666221628838453, Precision = 0.9973118279569892, f1 = 0.9939718687206965\n",
      "Test Loss = 0.03279119215385087, Recall = 0.9937694704049844, Aging Rate = 0.497774810858923, precision = 0.9982118909253465\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03342767093955648, Recall = 0.9924343569203382, Aging Rate = 0.49732977303070763, Precision = 0.9977628635346756, f1 = 0.9950914770191879\n",
      "Epoch 22: Train Loss = 0.031447366272104704, Recall = 0.9933244325767691, Aging Rate = 0.4975522919448153, Precision = 0.998211091234347, f1 = 0.9957617666741022\n",
      "Epoch 23: Train Loss = 0.029835814518939404, Recall = 0.9942145082331998, Aging Rate = 0.4982198486871384, Precision = 0.9977668602054489, f1 = 0.9959875167186802\n",
      "Epoch 24: Train Loss = 0.028246080208591318, Recall = 0.995549621717846, Aging Rate = 0.4984423676012461, Precision = 0.9986607142857142, f1 = 0.9971027412525071\n",
      "Epoch 25: Train Loss = 0.02711085533879906, Recall = 0.9959946595460614, Aging Rate = 0.4986648865153538, Precision = 0.998661311914324, f1 = 0.9973262032085561\n",
      "Test Loss = 0.0245716753935928, Recall = 0.9977748108589231, Aging Rate = 0.4993324432576769, precision = 0.9991087344028521\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.02564825117215017, Recall = 0.9977748108589231, Aging Rate = 0.4995549621717846, Precision = 0.9986636971046771, f1 = 0.9982190560997329\n",
      "Epoch 27: Train Loss = 0.024575800339400794, Recall = 0.995549621717846, Aging Rate = 0.4982198486871384, Precision = 0.9991067440821796, f1 = 0.997325011145787\n",
      "Epoch 28: Train Loss = 0.023728726655543247, Recall = 0.9968847352024922, Aging Rate = 0.4991099243435692, Precision = 0.9986625055728935, f1 = 0.9977728285077951\n",
      "Epoch 29: Train Loss = 0.02297052789875731, Recall = 0.9982198486871384, Aging Rate = 0.4997774810858923, Precision = 0.9986642920747997, f1 = 0.9984420209214333\n",
      "Epoch 30: Train Loss = 0.022336093184148996, Recall = 0.9977748108589231, Aging Rate = 0.4997774810858923, Precision = 0.9982190560997328, f1 = 0.9979968840418428\n",
      "Test Loss = 0.021642727200828847, Recall = 0.9995549621717846, Aging Rate = 0.5004450378282154, precision = 0.9986660738105825\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.022021900506215157, Recall = 0.9986648865153538, Aging Rate = 0.4997774810858923, Precision = 0.9991095280498664, f1 = 0.9988871578010239\n",
      "Epoch 32: Train Loss = 0.020770568998292598, Recall = 0.9982198486871384, Aging Rate = 0.4995549621717846, Precision = 0.9991091314031181, f1 = 0.9986642920747997\n",
      "Epoch 33: Train Loss = 0.020201760505023113, Recall = 0.9977748108589231, Aging Rate = 0.4993324432576769, Precision = 0.9991087344028521, f1 = 0.9984413270986418\n",
      "Epoch 34: Train Loss = 0.019853182952696873, Recall = 0.9991099243435692, Aging Rate = 0.5002225189141077, Precision = 0.9986654804270463, f1 = 0.9988876529477195\n",
      "Epoch 35: Train Loss = 0.020869293225689575, Recall = 0.9986648865153538, Aging Rate = 0.5002225189141077, Precision = 0.998220640569395, f1 = 0.9984427141268075\n",
      "Test Loss = 0.01982259026694998, Recall = 1.0, Aging Rate = 0.5004450378282154, precision = 0.9991107158737217\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.019362488320958604, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 37: Train Loss = 0.018699850045263925, Recall = 0.9986648865153538, Aging Rate = 0.4997774810858923, Precision = 0.9991095280498664, f1 = 0.9988871578010239\n",
      "Epoch 38: Train Loss = 0.017853255690380924, Recall = 0.9995549621717846, Aging Rate = 0.5002225189141077, Precision = 0.9991103202846975, f1 = 0.9993325917686319\n",
      "Epoch 39: Train Loss = 0.017988419063259986, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 40: Train Loss = 0.017692233545645163, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Test Loss = 0.01683581592021887, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.017731929426696973, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.01800063760925464, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 43: Train Loss = 0.01689239579257458, Recall = 0.9991099243435692, Aging Rate = 0.5, Precision = 0.9991099243435692, f1 = 0.9991099243435692\n",
      "Epoch 44: Train Loss = 0.017383757071809128, Recall = 0.9995549621717846, Aging Rate = 0.5004450378282154, Precision = 0.9986660738105825, f1 = 0.9991103202846975\n",
      "Epoch 45: Train Loss = 0.016918781614258496, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Test Loss = 0.015622530524209838, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.01771819752793963, Recall = 0.9995549621717846, Aging Rate = 0.5002225189141077, Precision = 0.9991103202846975, f1 = 0.9993325917686319\n",
      "Epoch 47: Train Loss = 0.016367775479038994, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 48: Train Loss = 0.016687351399987047, Recall = 0.9982198486871384, Aging Rate = 0.4993324432576769, Precision = 0.999554367201426, f1 = 0.9988866622133155\n",
      "Epoch 49: Train Loss = 0.016399928433285933, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.015851289575606997, Recall = 0.9986648865153538, Aging Rate = 0.4995549621717846, Precision = 0.999554565701559, f1 = 0.9991095280498664\n",
      "Test Loss = 0.014828381935175149, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.01601155898457084, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Epoch 52: Train Loss = 0.016191276028241695, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016374909193398, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015952773410437316, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.01591624052673711, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Test Loss = 0.014940862686874392, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016027825382593265, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.015538900838621892, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.01658053635559482, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 59: Train Loss = 0.015632845975730517, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 60: Train Loss = 0.015524150831454931, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Test Loss = 0.014639877956683868, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015623188719964116, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.015154119566477534, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015203908938395617, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.01534964294393738, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Epoch 65: Train Loss = 0.017168013958412113, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014631655491212103, Recall = 0.9982198486871384, Aging Rate = 0.4991099243435692, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.016067293612265724, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01534729106513378, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015270222186491815, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.014915960161143878, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.014895132969539326, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013463844335047362, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.014750572521625809, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.01483223979200954, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.016336405662629915, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Epoch 74: Train Loss = 0.015125539281472238, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015253184074772216, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013703876630078568, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.014710912965055726, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014801940433084169, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.014867294968935215, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.014756663323184836, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.014878376920487438, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014099113431373303, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.01453357436381277, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014609448646344858, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.015410905697270664, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.01495858225233697, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.014974618075696505, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Test Loss = 0.013758317416125156, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.015245684856092154, Recall = 0.9991099243435692, Aging Rate = 0.5, Precision = 0.9991099243435692, f1 = 0.9991099243435692\n",
      "Epoch 87: Train Loss = 0.015374457066141742, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.014495676588126167, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.014652279221784767, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.014894783026843427, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013329132518913012, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014978627168209116, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.014998627805847776, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.014166184621860995, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.014497447299676097, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014620840940732238, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014070869203363465, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 95.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5127387\ttotal: 6.76ms\tremaining: 1.01s\n",
      "1:\tlearn: 0.3842795\ttotal: 14ms\tremaining: 1.04s\n",
      "2:\tlearn: 0.3064320\ttotal: 20.1ms\tremaining: 984ms\n",
      "3:\tlearn: 0.2460386\ttotal: 25.9ms\tremaining: 946ms\n",
      "4:\tlearn: 0.1917558\ttotal: 32.2ms\tremaining: 933ms\n",
      "5:\tlearn: 0.1518048\ttotal: 39ms\tremaining: 936ms\n",
      "6:\tlearn: 0.1322687\ttotal: 43.9ms\tremaining: 897ms\n",
      "7:\tlearn: 0.1115982\ttotal: 49.7ms\tremaining: 882ms\n",
      "8:\tlearn: 0.1002333\ttotal: 54.9ms\tremaining: 860ms\n",
      "9:\tlearn: 0.0833807\ttotal: 61.4ms\tremaining: 860ms\n",
      "10:\tlearn: 0.0727334\ttotal: 67.4ms\tremaining: 852ms\n",
      "11:\tlearn: 0.0643712\ttotal: 72.5ms\tremaining: 834ms\n",
      "12:\tlearn: 0.0578010\ttotal: 77.7ms\tremaining: 819ms\n",
      "13:\tlearn: 0.0509700\ttotal: 83ms\tremaining: 806ms\n",
      "14:\tlearn: 0.0436809\ttotal: 88ms\tremaining: 792ms\n",
      "15:\tlearn: 0.0389181\ttotal: 92.8ms\tremaining: 777ms\n",
      "16:\tlearn: 0.0344294\ttotal: 97.3ms\tremaining: 761ms\n",
      "17:\tlearn: 0.0324926\ttotal: 101ms\tremaining: 742ms\n",
      "18:\tlearn: 0.0307410\ttotal: 106ms\tremaining: 729ms\n",
      "19:\tlearn: 0.0282300\ttotal: 110ms\tremaining: 716ms\n",
      "20:\tlearn: 0.0252701\ttotal: 115ms\tremaining: 706ms\n",
      "21:\tlearn: 0.0235681\ttotal: 119ms\tremaining: 690ms\n",
      "22:\tlearn: 0.0213298\ttotal: 123ms\tremaining: 681ms\n",
      "23:\tlearn: 0.0192571\ttotal: 129ms\tremaining: 677ms\n",
      "24:\tlearn: 0.0172820\ttotal: 134ms\tremaining: 672ms\n",
      "25:\tlearn: 0.0158053\ttotal: 139ms\tremaining: 663ms\n",
      "26:\tlearn: 0.0141313\ttotal: 145ms\tremaining: 658ms\n",
      "27:\tlearn: 0.0131655\ttotal: 149ms\tremaining: 647ms\n",
      "28:\tlearn: 0.0126154\ttotal: 152ms\tremaining: 634ms\n",
      "29:\tlearn: 0.0119020\ttotal: 156ms\tremaining: 624ms\n",
      "30:\tlearn: 0.0110529\ttotal: 160ms\tremaining: 615ms\n",
      "31:\tlearn: 0.0102784\ttotal: 164ms\tremaining: 606ms\n",
      "32:\tlearn: 0.0094680\ttotal: 169ms\tremaining: 598ms\n",
      "33:\tlearn: 0.0090074\ttotal: 173ms\tremaining: 589ms\n",
      "34:\tlearn: 0.0085807\ttotal: 177ms\tremaining: 581ms\n",
      "35:\tlearn: 0.0080925\ttotal: 181ms\tremaining: 573ms\n",
      "36:\tlearn: 0.0078598\ttotal: 184ms\tremaining: 563ms\n",
      "37:\tlearn: 0.0072426\ttotal: 190ms\tremaining: 559ms\n",
      "38:\tlearn: 0.0070967\ttotal: 193ms\tremaining: 549ms\n",
      "39:\tlearn: 0.0066587\ttotal: 198ms\tremaining: 544ms\n",
      "40:\tlearn: 0.0060243\ttotal: 203ms\tremaining: 541ms\n",
      "41:\tlearn: 0.0057918\ttotal: 207ms\tremaining: 532ms\n",
      "42:\tlearn: 0.0053820\ttotal: 212ms\tremaining: 528ms\n",
      "43:\tlearn: 0.0050903\ttotal: 216ms\tremaining: 520ms\n",
      "44:\tlearn: 0.0048111\ttotal: 220ms\tremaining: 514ms\n",
      "45:\tlearn: 0.0045846\ttotal: 224ms\tremaining: 507ms\n",
      "46:\tlearn: 0.0043315\ttotal: 228ms\tremaining: 500ms\n",
      "47:\tlearn: 0.0041859\ttotal: 232ms\tremaining: 494ms\n",
      "48:\tlearn: 0.0040015\ttotal: 237ms\tremaining: 488ms\n",
      "49:\tlearn: 0.0038991\ttotal: 239ms\tremaining: 479ms\n",
      "50:\tlearn: 0.0036338\ttotal: 244ms\tremaining: 473ms\n",
      "51:\tlearn: 0.0034756\ttotal: 247ms\tremaining: 466ms\n",
      "52:\tlearn: 0.0033793\ttotal: 251ms\tremaining: 460ms\n",
      "53:\tlearn: 0.0033110\ttotal: 254ms\tremaining: 452ms\n",
      "54:\tlearn: 0.0031626\ttotal: 258ms\tremaining: 446ms\n",
      "55:\tlearn: 0.0030074\ttotal: 262ms\tremaining: 440ms\n",
      "56:\tlearn: 0.0029099\ttotal: 266ms\tremaining: 434ms\n",
      "57:\tlearn: 0.0027841\ttotal: 270ms\tremaining: 428ms\n",
      "58:\tlearn: 0.0027331\ttotal: 273ms\tremaining: 421ms\n",
      "59:\tlearn: 0.0026411\ttotal: 277ms\tremaining: 415ms\n",
      "60:\tlearn: 0.0025633\ttotal: 281ms\tremaining: 409ms\n",
      "61:\tlearn: 0.0024877\ttotal: 284ms\tremaining: 403ms\n",
      "62:\tlearn: 0.0024236\ttotal: 287ms\tremaining: 397ms\n",
      "63:\tlearn: 0.0023628\ttotal: 291ms\tremaining: 391ms\n",
      "64:\tlearn: 0.0022821\ttotal: 294ms\tremaining: 385ms\n",
      "65:\tlearn: 0.0022154\ttotal: 298ms\tremaining: 379ms\n",
      "66:\tlearn: 0.0021095\ttotal: 302ms\tremaining: 374ms\n",
      "67:\tlearn: 0.0020501\ttotal: 306ms\tremaining: 369ms\n",
      "68:\tlearn: 0.0019949\ttotal: 310ms\tremaining: 363ms\n",
      "69:\tlearn: 0.0019110\ttotal: 313ms\tremaining: 358ms\n",
      "70:\tlearn: 0.0018397\ttotal: 317ms\tremaining: 352ms\n",
      "71:\tlearn: 0.0017876\ttotal: 320ms\tremaining: 347ms\n",
      "72:\tlearn: 0.0017369\ttotal: 324ms\tremaining: 341ms\n",
      "73:\tlearn: 0.0016728\ttotal: 327ms\tremaining: 336ms\n",
      "74:\tlearn: 0.0016367\ttotal: 330ms\tremaining: 330ms\n",
      "75:\tlearn: 0.0015956\ttotal: 334ms\tremaining: 325ms\n",
      "76:\tlearn: 0.0015468\ttotal: 337ms\tremaining: 319ms\n",
      "77:\tlearn: 0.0015160\ttotal: 340ms\tremaining: 314ms\n",
      "78:\tlearn: 0.0014693\ttotal: 343ms\tremaining: 309ms\n",
      "79:\tlearn: 0.0014248\ttotal: 347ms\tremaining: 303ms\n",
      "80:\tlearn: 0.0013791\ttotal: 351ms\tremaining: 299ms\n",
      "81:\tlearn: 0.0013603\ttotal: 353ms\tremaining: 293ms\n",
      "82:\tlearn: 0.0013277\ttotal: 357ms\tremaining: 288ms\n",
      "83:\tlearn: 0.0012956\ttotal: 360ms\tremaining: 283ms\n",
      "84:\tlearn: 0.0012692\ttotal: 363ms\tremaining: 278ms\n",
      "85:\tlearn: 0.0012436\ttotal: 366ms\tremaining: 273ms\n",
      "86:\tlearn: 0.0011898\ttotal: 370ms\tremaining: 268ms\n",
      "87:\tlearn: 0.0011898\ttotal: 373ms\tremaining: 263ms\n",
      "88:\tlearn: 0.0011898\ttotal: 375ms\tremaining: 257ms\n",
      "89:\tlearn: 0.0011898\ttotal: 378ms\tremaining: 252ms\n",
      "90:\tlearn: 0.0011760\ttotal: 380ms\tremaining: 247ms\n",
      "91:\tlearn: 0.0011760\ttotal: 383ms\tremaining: 241ms\n",
      "92:\tlearn: 0.0011760\ttotal: 385ms\tremaining: 236ms\n",
      "93:\tlearn: 0.0011760\ttotal: 388ms\tremaining: 231ms\n",
      "94:\tlearn: 0.0011760\ttotal: 390ms\tremaining: 226ms\n",
      "95:\tlearn: 0.0011760\ttotal: 393ms\tremaining: 221ms\n",
      "96:\tlearn: 0.0011760\ttotal: 395ms\tremaining: 216ms\n",
      "97:\tlearn: 0.0011760\ttotal: 398ms\tremaining: 211ms\n",
      "98:\tlearn: 0.0011760\ttotal: 400ms\tremaining: 206ms\n",
      "99:\tlearn: 0.0011645\ttotal: 403ms\tremaining: 201ms\n",
      "100:\tlearn: 0.0011645\ttotal: 406ms\tremaining: 197ms\n",
      "101:\tlearn: 0.0011462\ttotal: 409ms\tremaining: 192ms\n",
      "102:\tlearn: 0.0011179\ttotal: 412ms\tremaining: 188ms\n",
      "103:\tlearn: 0.0011179\ttotal: 415ms\tremaining: 184ms\n",
      "104:\tlearn: 0.0010910\ttotal: 419ms\tremaining: 179ms\n",
      "105:\tlearn: 0.0010693\ttotal: 422ms\tremaining: 175ms\n",
      "106:\tlearn: 0.0010405\ttotal: 426ms\tremaining: 171ms\n",
      "107:\tlearn: 0.0010405\ttotal: 429ms\tremaining: 167ms\n",
      "108:\tlearn: 0.0010223\ttotal: 432ms\tremaining: 162ms\n",
      "109:\tlearn: 0.0010035\ttotal: 436ms\tremaining: 158ms\n",
      "110:\tlearn: 0.0009849\ttotal: 439ms\tremaining: 154ms\n",
      "111:\tlearn: 0.0009849\ttotal: 442ms\tremaining: 150ms\n",
      "112:\tlearn: 0.0009849\ttotal: 444ms\tremaining: 145ms\n",
      "113:\tlearn: 0.0009849\ttotal: 447ms\tremaining: 141ms\n",
      "114:\tlearn: 0.0009849\ttotal: 450ms\tremaining: 137ms\n",
      "115:\tlearn: 0.0009849\ttotal: 452ms\tremaining: 133ms\n",
      "116:\tlearn: 0.0009849\ttotal: 455ms\tremaining: 128ms\n",
      "117:\tlearn: 0.0009849\ttotal: 458ms\tremaining: 124ms\n",
      "118:\tlearn: 0.0009849\ttotal: 461ms\tremaining: 120ms\n",
      "119:\tlearn: 0.0009848\ttotal: 463ms\tremaining: 116ms\n",
      "120:\tlearn: 0.0009848\ttotal: 466ms\tremaining: 112ms\n",
      "121:\tlearn: 0.0009848\ttotal: 469ms\tremaining: 108ms\n",
      "122:\tlearn: 0.0009848\ttotal: 471ms\tremaining: 103ms\n",
      "123:\tlearn: 0.0009848\ttotal: 474ms\tremaining: 99.3ms\n",
      "124:\tlearn: 0.0009848\ttotal: 477ms\tremaining: 95.3ms\n",
      "125:\tlearn: 0.0009848\ttotal: 479ms\tremaining: 91.3ms\n",
      "126:\tlearn: 0.0009848\ttotal: 482ms\tremaining: 87.3ms\n",
      "127:\tlearn: 0.0009848\ttotal: 485ms\tremaining: 83.3ms\n",
      "128:\tlearn: 0.0009848\ttotal: 487ms\tremaining: 79.4ms\n",
      "129:\tlearn: 0.0009848\ttotal: 490ms\tremaining: 75.4ms\n",
      "130:\tlearn: 0.0009848\ttotal: 493ms\tremaining: 71.5ms\n",
      "131:\tlearn: 0.0009848\ttotal: 496ms\tremaining: 67.6ms\n",
      "132:\tlearn: 0.0009848\ttotal: 498ms\tremaining: 63.7ms\n",
      "133:\tlearn: 0.0009848\ttotal: 501ms\tremaining: 59.8ms\n",
      "134:\tlearn: 0.0009848\ttotal: 503ms\tremaining: 55.9ms\n",
      "135:\tlearn: 0.0009847\ttotal: 506ms\tremaining: 52.1ms\n",
      "136:\tlearn: 0.0009847\ttotal: 509ms\tremaining: 48.3ms\n",
      "137:\tlearn: 0.0009847\ttotal: 511ms\tremaining: 44.5ms\n",
      "138:\tlearn: 0.0009847\ttotal: 514ms\tremaining: 40.7ms\n",
      "139:\tlearn: 0.0009847\ttotal: 517ms\tremaining: 36.9ms\n",
      "140:\tlearn: 0.0009847\ttotal: 519ms\tremaining: 33.1ms\n",
      "141:\tlearn: 0.0009847\ttotal: 522ms\tremaining: 29.4ms\n",
      "142:\tlearn: 0.0009847\ttotal: 524ms\tremaining: 25.7ms\n",
      "143:\tlearn: 0.0009847\ttotal: 527ms\tremaining: 22ms\n",
      "144:\tlearn: 0.0009847\ttotal: 530ms\tremaining: 18.3ms\n",
      "145:\tlearn: 0.0009847\ttotal: 532ms\tremaining: 14.6ms\n",
      "146:\tlearn: 0.0009847\ttotal: 535ms\tremaining: 10.9ms\n",
      "147:\tlearn: 0.0009847\ttotal: 538ms\tremaining: 7.27ms\n",
      "148:\tlearn: 0.0009847\ttotal: 540ms\tremaining: 3.63ms\n",
      "149:\tlearn: 0.0009847\ttotal: 543ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb1ec7a6c034f4da067955c6d81c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6137464534032063, Recall = 0.5398308856252781, Aging Rate = 0.32777036048064084, Precision = 0.823489477257298, f1 = 0.6521505376344086\n",
      "Epoch 2: Train Loss = 0.4259984542064047, Recall = 0.8197596795727636, Aging Rate = 0.48842901646639963, Precision = 0.8391799544419134, f1 = 0.8293561458802342\n",
      "Epoch 3: Train Loss = 0.3135187366590215, Recall = 0.8807298620382733, Aging Rate = 0.4975522919448153, Precision = 0.8850626118067979, f1 = 0.8828909212580861\n",
      "Epoch 4: Train Loss = 0.25051137562269205, Recall = 0.9185580774365821, Aging Rate = 0.5031152647975078, Precision = 0.9128704113224237, f1 = 0.9157054125998225\n",
      "Epoch 5: Train Loss = 0.20725666790948638, Recall = 0.9399198931909212, Aging Rate = 0.5002225189141077, Precision = 0.9395017793594306, f1 = 0.9397107897664072\n",
      "Test Loss = 0.17606425392770958, Recall = 0.9492656875834445, Aging Rate = 0.4939919893190921, precision = 0.9608108108108108\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1632856250697897, Recall = 0.9612817089452603, Aging Rate = 0.5015576323987538, Precision = 0.9582963620230701, f1 = 0.9597867140635415\n",
      "Epoch 7: Train Loss = 0.1355661233737037, Recall = 0.9684023141967067, Aging Rate = 0.4986648865153538, Precision = 0.9709950914770192, f1 = 0.9696969696969697\n",
      "Epoch 8: Train Loss = 0.1150071815677521, Recall = 0.9710725411659991, Aging Rate = 0.4959946595460614, Precision = 0.9789143113503813, f1 = 0.9749776586237712\n",
      "Epoch 9: Train Loss = 0.0994801293572798, Recall = 0.9768580329327993, Aging Rate = 0.49577214063195374, Precision = 0.9851885098743267, f1 = 0.9810055865921788\n",
      "Epoch 10: Train Loss = 0.08522942754915411, Recall = 0.9773030707610146, Aging Rate = 0.4939919893190921, Precision = 0.9891891891891892, f1 = 0.9832102081934184\n",
      "Test Loss = 0.077109492902404, Recall = 0.9773030707610146, Aging Rate = 0.49132176234979974, precision = 0.9945652173913043\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07462358746460453, Recall = 0.9799732977303071, Aging Rate = 0.4939919893190921, Precision = 0.9918918918918919, f1 = 0.9858965748824714\n",
      "Epoch 12: Train Loss = 0.06579859746586073, Recall = 0.9804183355585224, Aging Rate = 0.4931019136626613, Precision = 0.9941335740072202, f1 = 0.9872283217566659\n",
      "Epoch 13: Train Loss = 0.05902703371998146, Recall = 0.9830885625278148, Aging Rate = 0.49421450823319985, Precision = 0.994597028365601, f1 = 0.9888093106535362\n",
      "Epoch 14: Train Loss = 0.05343507028018627, Recall = 0.9857587894971073, Aging Rate = 0.4953271028037383, Precision = 0.995058400718778, f1 = 0.9903867650346524\n",
      "Epoch 15: Train Loss = 0.04968756973796491, Recall = 0.9848687138406764, Aging Rate = 0.4939919893190921, Precision = 0.9968468468468469, f1 = 0.9908215804790687\n",
      "Test Loss = 0.04325418706658381, Recall = 0.9902091677792613, Aging Rate = 0.4962171784601691, precision = 0.9977578475336323\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04433567596837368, Recall = 0.9884290164663997, Aging Rate = 0.4953271028037383, Precision = 0.9977538185085355, f1 = 0.9930695282807959\n",
      "Epoch 17: Train Loss = 0.040611257528762684, Recall = 0.9897641299510458, Aging Rate = 0.4959946595460614, Precision = 0.9977568416330193, f1 = 0.9937444146559427\n",
      "Epoch 18: Train Loss = 0.03730753058974656, Recall = 0.9928793947485536, Aging Rate = 0.497774810858923, Precision = 0.9973178363880196, f1 = 0.9950936663693131\n",
      "Epoch 19: Train Loss = 0.034883789684164244, Recall = 0.9933244325767691, Aging Rate = 0.497774810858923, Precision = 0.997764863656683, f1 = 0.9955396966993756\n",
      "Epoch 20: Train Loss = 0.03273877251062676, Recall = 0.9951045838896306, Aging Rate = 0.4986648865153538, Precision = 0.99776885319054, f1 = 0.9964349376114082\n",
      "Test Loss = 0.029610716242510635, Recall = 0.9946595460614153, Aging Rate = 0.4979973297730307, precision = 0.9986595174262735\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03098655583500252, Recall = 0.9942145082331998, Aging Rate = 0.497774810858923, Precision = 0.9986589181940099, f1 = 0.9964317573595004\n",
      "Epoch 22: Train Loss = 0.029239316214361773, Recall = 0.9946595460614153, Aging Rate = 0.4984423676012461, Precision = 0.9977678571428571, f1 = 0.9962112770225094\n",
      "Epoch 23: Train Loss = 0.027944436271123266, Recall = 0.9964396973742768, Aging Rate = 0.4991099243435692, Precision = 0.9982166740971913, f1 = 0.9973273942093541\n",
      "Epoch 24: Train Loss = 0.02681395239484777, Recall = 0.9959946595460614, Aging Rate = 0.4986648865153538, Precision = 0.998661311914324, f1 = 0.9973262032085561\n",
      "Epoch 25: Train Loss = 0.025175142234989574, Recall = 0.9977748108589231, Aging Rate = 0.4997774810858923, Precision = 0.9982190560997328, f1 = 0.9979968840418428\n",
      "Test Loss = 0.023331680095953097, Recall = 0.9986648865153538, Aging Rate = 0.5, precision = 0.9986648865153538\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.024462111804901255, Recall = 0.9973297730307076, Aging Rate = 0.4993324432576769, Precision = 0.9986631016042781, f1 = 0.9979959919839679\n",
      "Epoch 27: Train Loss = 0.02334465716128674, Recall = 0.9973297730307076, Aging Rate = 0.4995549621717846, Precision = 0.998218262806236, f1 = 0.9977738201246661\n",
      "Epoch 28: Train Loss = 0.022861015346708646, Recall = 0.9973297730307076, Aging Rate = 0.4993324432576769, Precision = 0.9986631016042781, f1 = 0.9979959919839679\n",
      "Epoch 29: Train Loss = 0.02252646865763357, Recall = 0.9982198486871384, Aging Rate = 0.4997774810858923, Precision = 0.9986642920747997, f1 = 0.9984420209214333\n",
      "Epoch 30: Train Loss = 0.021296459441224522, Recall = 0.9986648865153538, Aging Rate = 0.5002225189141077, Precision = 0.998220640569395, f1 = 0.9984427141268075\n",
      "Test Loss = 0.019401670122725544, Recall = 0.9986648865153538, Aging Rate = 0.4995549621717846, precision = 0.999554565701559\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.020466843833811133, Recall = 0.9986648865153538, Aging Rate = 0.5, Precision = 0.9986648865153538, f1 = 0.9986648865153538\n",
      "Epoch 32: Train Loss = 0.01999124325480284, Recall = 0.9982198486871384, Aging Rate = 0.4995549621717846, Precision = 0.9991091314031181, f1 = 0.9986642920747997\n",
      "Epoch 33: Train Loss = 0.019452178915842378, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 34: Train Loss = 0.019143419052608076, Recall = 0.9986648865153538, Aging Rate = 0.4997774810858923, Precision = 0.9991095280498664, f1 = 0.9988871578010239\n",
      "Epoch 35: Train Loss = 0.018733606310698075, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Test Loss = 0.017929778276421678, Recall = 0.9995549621717846, Aging Rate = 0.5, precision = 0.9995549621717846\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.01858590502568786, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 37: Train Loss = 0.018231749981409664, Recall = 0.9991099243435692, Aging Rate = 0.5, Precision = 0.9991099243435692, f1 = 0.9991099243435692\n",
      "Epoch 38: Train Loss = 0.018110012780654466, Recall = 0.9991099243435692, Aging Rate = 0.5002225189141077, Precision = 0.9986654804270463, f1 = 0.9988876529477195\n",
      "Epoch 39: Train Loss = 0.01759605994758921, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01733941339539935, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.017021504542274333, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.017632692493995693, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.017387789053636016, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Epoch 43: Train Loss = 0.01670647133348993, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 44: Train Loss = 0.016580985852404865, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016580187394372822, Recall = 0.9991099243435692, Aging Rate = 0.4997774810858923, Precision = 0.9995547640249333, f1 = 0.9993322946806142\n",
      "Test Loss = 0.015221694913333424, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.01668803629842158, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.016602744508334065, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.01610275192918197, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.015997882866108742, Recall = 1.0, Aging Rate = 0.5002225189141077, Precision = 0.9995551601423488, f1 = 0.999777530589544\n",
      "Epoch 50: Train Loss = 0.01625565369375146, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014612952018058003, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.01563526890528669, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.01582543265126715, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.01574465805605803, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015716524787237233, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015700958126224993, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014514432525785303, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015497463800901697, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.01540355652182777, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015606563980763669, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.015109163398543993, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015518651332484056, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014029452612049112, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.015297050983033673, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.015139357251097321, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01509524981272011, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015308859863148155, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015300010281652676, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014649958579382819, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015486273006878007, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.015050163796369665, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 68: Train Loss = 0.016157040136655303, Recall = 1.0, Aging Rate = 0.5004450378282154, Precision = 0.9991107158737217, f1 = 0.9995551601423488\n",
      "Epoch 69: Train Loss = 0.014940590651570742, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.014936091690740793, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014636870856597045, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.014957873494719526, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 72: Train Loss = 0.014836273156520946, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.015123843141836846, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 74: Train Loss = 0.015680867455349294, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014753538498868, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01364102406683409, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.014703944703447167, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014588685569481475, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.01579569368347572, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Epoch 79: Train Loss = 0.015505586841989608, Recall = 0.9995549621717846, Aging Rate = 0.5002225189141077, Precision = 0.9991103202846975, f1 = 0.9993325917686319\n",
      "Epoch 80: Train Loss = 0.015166215058938365, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013381828809497712, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.014795092989337885, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014836891341670969, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.014599576299826889, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.015639869793404006, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.01442125707476814, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Test Loss = 0.01339989607255536, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.014541719620695075, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.014502243653626165, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.014744782778896594, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.014767858434367375, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.01438605370564518, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013486058789653133, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014881919569600726, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.0146749269434437, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.014519581585520696, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.014355368976048962, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014325381053755044, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Test Loss = 0.015536324875814734, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.01493957341314158, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.015923034961359814, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.014230569778569789, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.01430456605596898, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.014649981624746474, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013409279507775942, Recall = 0.9986648865153538, Aging Rate = 0.4993324432576769, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.015205155125507047, Recall = 0.9991099243435692, Aging Rate = 0.4995549621717846, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: Train Loss = 0.015036285040020598, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.014481189007517306, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.014407686688980766, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.014764695503746051, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013098132636184208, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.014249345310476684, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.014316174620285822, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.014579876156504757, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.01454451827798246, Recall = 0.9995549621717846, Aging Rate = 0.4997774810858923, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.01451604710527106, Recall = 0.9995549621717846, Aging Rate = 0.5, Precision = 0.9995549621717846, f1 = 0.9995549621717846\n",
      "Test Loss = 0.013405012372179885, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 110.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5355742\ttotal: 5.75ms\tremaining: 857ms\n",
      "1:\tlearn: 0.4137733\ttotal: 11ms\tremaining: 816ms\n",
      "2:\tlearn: 0.3356395\ttotal: 16.4ms\tremaining: 803ms\n",
      "3:\tlearn: 0.2647256\ttotal: 22.3ms\tremaining: 814ms\n",
      "4:\tlearn: 0.2067648\ttotal: 28.6ms\tremaining: 830ms\n",
      "5:\tlearn: 0.1758542\ttotal: 34.5ms\tremaining: 828ms\n",
      "6:\tlearn: 0.1382840\ttotal: 40.9ms\tremaining: 835ms\n",
      "7:\tlearn: 0.1205319\ttotal: 46.5ms\tremaining: 825ms\n",
      "8:\tlearn: 0.1020048\ttotal: 51.9ms\tremaining: 814ms\n",
      "9:\tlearn: 0.0865946\ttotal: 57.7ms\tremaining: 808ms\n",
      "10:\tlearn: 0.0731383\ttotal: 63.9ms\tremaining: 807ms\n",
      "11:\tlearn: 0.0630723\ttotal: 69.2ms\tremaining: 795ms\n",
      "12:\tlearn: 0.0568749\ttotal: 74.6ms\tremaining: 786ms\n",
      "13:\tlearn: 0.0488819\ttotal: 80.6ms\tremaining: 783ms\n",
      "14:\tlearn: 0.0429737\ttotal: 85.9ms\tremaining: 773ms\n",
      "15:\tlearn: 0.0393216\ttotal: 90.3ms\tremaining: 756ms\n",
      "16:\tlearn: 0.0335308\ttotal: 96.1ms\tremaining: 752ms\n",
      "17:\tlearn: 0.0303615\ttotal: 101ms\tremaining: 740ms\n",
      "18:\tlearn: 0.0272960\ttotal: 106ms\tremaining: 729ms\n",
      "19:\tlearn: 0.0249130\ttotal: 110ms\tremaining: 715ms\n",
      "20:\tlearn: 0.0225386\ttotal: 115ms\tremaining: 706ms\n",
      "21:\tlearn: 0.0215297\ttotal: 119ms\tremaining: 692ms\n",
      "22:\tlearn: 0.0191087\ttotal: 124ms\tremaining: 684ms\n",
      "23:\tlearn: 0.0182812\ttotal: 128ms\tremaining: 669ms\n",
      "24:\tlearn: 0.0160246\ttotal: 133ms\tremaining: 663ms\n",
      "25:\tlearn: 0.0142073\ttotal: 137ms\tremaining: 655ms\n",
      "26:\tlearn: 0.0131756\ttotal: 141ms\tremaining: 645ms\n",
      "27:\tlearn: 0.0116263\ttotal: 147ms\tremaining: 641ms\n",
      "28:\tlearn: 0.0105927\ttotal: 152ms\tremaining: 634ms\n",
      "29:\tlearn: 0.0099762\ttotal: 156ms\tremaining: 623ms\n",
      "30:\tlearn: 0.0094781\ttotal: 159ms\tremaining: 611ms\n",
      "31:\tlearn: 0.0090314\ttotal: 163ms\tremaining: 601ms\n",
      "32:\tlearn: 0.0084263\ttotal: 167ms\tremaining: 593ms\n",
      "33:\tlearn: 0.0079852\ttotal: 171ms\tremaining: 585ms\n",
      "34:\tlearn: 0.0077593\ttotal: 175ms\tremaining: 575ms\n",
      "35:\tlearn: 0.0073441\ttotal: 179ms\tremaining: 567ms\n",
      "36:\tlearn: 0.0070305\ttotal: 183ms\tremaining: 560ms\n",
      "37:\tlearn: 0.0063113\ttotal: 189ms\tremaining: 556ms\n",
      "38:\tlearn: 0.0061209\ttotal: 192ms\tremaining: 546ms\n",
      "39:\tlearn: 0.0057958\ttotal: 196ms\tremaining: 538ms\n",
      "40:\tlearn: 0.0053870\ttotal: 201ms\tremaining: 533ms\n",
      "41:\tlearn: 0.0051628\ttotal: 204ms\tremaining: 524ms\n",
      "42:\tlearn: 0.0048242\ttotal: 209ms\tremaining: 519ms\n",
      "43:\tlearn: 0.0045283\ttotal: 213ms\tremaining: 513ms\n",
      "44:\tlearn: 0.0042570\ttotal: 217ms\tremaining: 507ms\n",
      "45:\tlearn: 0.0040316\ttotal: 222ms\tremaining: 501ms\n",
      "46:\tlearn: 0.0038356\ttotal: 226ms\tremaining: 494ms\n",
      "47:\tlearn: 0.0036651\ttotal: 229ms\tremaining: 487ms\n",
      "48:\tlearn: 0.0034749\ttotal: 233ms\tremaining: 481ms\n",
      "49:\tlearn: 0.0033159\ttotal: 237ms\tremaining: 474ms\n",
      "50:\tlearn: 0.0032596\ttotal: 240ms\tremaining: 466ms\n",
      "51:\tlearn: 0.0031431\ttotal: 244ms\tremaining: 459ms\n",
      "52:\tlearn: 0.0030142\ttotal: 248ms\tremaining: 453ms\n",
      "53:\tlearn: 0.0028886\ttotal: 252ms\tremaining: 448ms\n",
      "54:\tlearn: 0.0027916\ttotal: 256ms\tremaining: 441ms\n",
      "55:\tlearn: 0.0026929\ttotal: 260ms\tremaining: 436ms\n",
      "56:\tlearn: 0.0026255\ttotal: 263ms\tremaining: 429ms\n",
      "57:\tlearn: 0.0025641\ttotal: 266ms\tremaining: 422ms\n",
      "58:\tlearn: 0.0024801\ttotal: 270ms\tremaining: 416ms\n",
      "59:\tlearn: 0.0023973\ttotal: 273ms\tremaining: 410ms\n",
      "60:\tlearn: 0.0023213\ttotal: 277ms\tremaining: 404ms\n",
      "61:\tlearn: 0.0022458\ttotal: 280ms\tremaining: 398ms\n",
      "62:\tlearn: 0.0021569\ttotal: 284ms\tremaining: 392ms\n",
      "63:\tlearn: 0.0020690\ttotal: 287ms\tremaining: 385ms\n",
      "64:\tlearn: 0.0020221\ttotal: 290ms\tremaining: 379ms\n",
      "65:\tlearn: 0.0019376\ttotal: 294ms\tremaining: 374ms\n",
      "66:\tlearn: 0.0018923\ttotal: 297ms\tremaining: 368ms\n",
      "67:\tlearn: 0.0018734\ttotal: 300ms\tremaining: 362ms\n",
      "68:\tlearn: 0.0017685\ttotal: 305ms\tremaining: 358ms\n",
      "69:\tlearn: 0.0017206\ttotal: 308ms\tremaining: 352ms\n",
      "70:\tlearn: 0.0016950\ttotal: 311ms\tremaining: 346ms\n",
      "71:\tlearn: 0.0016204\ttotal: 315ms\tremaining: 341ms\n",
      "72:\tlearn: 0.0016014\ttotal: 318ms\tremaining: 335ms\n",
      "73:\tlearn: 0.0015768\ttotal: 321ms\tremaining: 330ms\n",
      "74:\tlearn: 0.0015295\ttotal: 325ms\tremaining: 325ms\n",
      "75:\tlearn: 0.0015093\ttotal: 328ms\tremaining: 319ms\n",
      "76:\tlearn: 0.0014707\ttotal: 331ms\tremaining: 314ms\n",
      "77:\tlearn: 0.0014707\ttotal: 334ms\tremaining: 308ms\n",
      "78:\tlearn: 0.0014338\ttotal: 338ms\tremaining: 303ms\n",
      "79:\tlearn: 0.0014084\ttotal: 341ms\tremaining: 298ms\n",
      "80:\tlearn: 0.0013689\ttotal: 344ms\tremaining: 293ms\n",
      "81:\tlearn: 0.0013444\ttotal: 348ms\tremaining: 288ms\n",
      "82:\tlearn: 0.0013225\ttotal: 351ms\tremaining: 283ms\n",
      "83:\tlearn: 0.0012824\ttotal: 354ms\tremaining: 278ms\n",
      "84:\tlearn: 0.0012543\ttotal: 358ms\tremaining: 274ms\n",
      "85:\tlearn: 0.0012265\ttotal: 361ms\tremaining: 269ms\n",
      "86:\tlearn: 0.0011950\ttotal: 365ms\tremaining: 264ms\n",
      "87:\tlearn: 0.0011784\ttotal: 368ms\tremaining: 260ms\n",
      "88:\tlearn: 0.0011513\ttotal: 372ms\tremaining: 255ms\n",
      "89:\tlearn: 0.0011280\ttotal: 375ms\tremaining: 250ms\n",
      "90:\tlearn: 0.0011082\ttotal: 379ms\tremaining: 246ms\n",
      "91:\tlearn: 0.0010842\ttotal: 382ms\tremaining: 241ms\n",
      "92:\tlearn: 0.0010654\ttotal: 385ms\tremaining: 236ms\n",
      "93:\tlearn: 0.0010654\ttotal: 389ms\tremaining: 231ms\n",
      "94:\tlearn: 0.0010452\ttotal: 392ms\tremaining: 227ms\n",
      "95:\tlearn: 0.0010247\ttotal: 396ms\tremaining: 223ms\n",
      "96:\tlearn: 0.0010247\ttotal: 398ms\tremaining: 217ms\n",
      "97:\tlearn: 0.0010247\ttotal: 401ms\tremaining: 213ms\n",
      "98:\tlearn: 0.0010247\ttotal: 404ms\tremaining: 208ms\n",
      "99:\tlearn: 0.0010063\ttotal: 407ms\tremaining: 204ms\n",
      "100:\tlearn: 0.0010063\ttotal: 410ms\tremaining: 199ms\n",
      "101:\tlearn: 0.0010063\ttotal: 413ms\tremaining: 194ms\n",
      "102:\tlearn: 0.0010063\ttotal: 415ms\tremaining: 190ms\n",
      "103:\tlearn: 0.0010062\ttotal: 418ms\tremaining: 185ms\n",
      "104:\tlearn: 0.0009890\ttotal: 422ms\tremaining: 181ms\n",
      "105:\tlearn: 0.0009662\ttotal: 425ms\tremaining: 176ms\n",
      "106:\tlearn: 0.0009512\ttotal: 428ms\tremaining: 172ms\n",
      "107:\tlearn: 0.0009512\ttotal: 431ms\tremaining: 168ms\n",
      "108:\tlearn: 0.0009512\ttotal: 433ms\tremaining: 163ms\n",
      "109:\tlearn: 0.0009512\ttotal: 436ms\tremaining: 159ms\n",
      "110:\tlearn: 0.0009512\ttotal: 439ms\tremaining: 154ms\n",
      "111:\tlearn: 0.0009512\ttotal: 442ms\tremaining: 150ms\n",
      "112:\tlearn: 0.0009512\ttotal: 444ms\tremaining: 146ms\n",
      "113:\tlearn: 0.0009512\ttotal: 447ms\tremaining: 141ms\n",
      "114:\tlearn: 0.0009512\ttotal: 450ms\tremaining: 137ms\n",
      "115:\tlearn: 0.0009512\ttotal: 453ms\tremaining: 133ms\n",
      "116:\tlearn: 0.0009512\ttotal: 455ms\tremaining: 128ms\n",
      "117:\tlearn: 0.0009512\ttotal: 458ms\tremaining: 124ms\n",
      "118:\tlearn: 0.0009512\ttotal: 460ms\tremaining: 120ms\n",
      "119:\tlearn: 0.0009512\ttotal: 463ms\tremaining: 116ms\n",
      "120:\tlearn: 0.0009373\ttotal: 466ms\tremaining: 112ms\n",
      "121:\tlearn: 0.0009373\ttotal: 469ms\tremaining: 108ms\n",
      "122:\tlearn: 0.0009372\ttotal: 472ms\tremaining: 104ms\n",
      "123:\tlearn: 0.0009168\ttotal: 475ms\tremaining: 99.7ms\n",
      "124:\tlearn: 0.0009168\ttotal: 478ms\tremaining: 95.7ms\n",
      "125:\tlearn: 0.0009168\ttotal: 481ms\tremaining: 91.6ms\n",
      "126:\tlearn: 0.0009168\ttotal: 483ms\tremaining: 87.5ms\n",
      "127:\tlearn: 0.0009168\ttotal: 486ms\tremaining: 83.5ms\n",
      "128:\tlearn: 0.0009168\ttotal: 488ms\tremaining: 79.4ms\n",
      "129:\tlearn: 0.0009168\ttotal: 490ms\tremaining: 75.5ms\n",
      "130:\tlearn: 0.0009167\ttotal: 493ms\tremaining: 71.5ms\n",
      "131:\tlearn: 0.0009167\ttotal: 495ms\tremaining: 67.5ms\n",
      "132:\tlearn: 0.0009167\ttotal: 498ms\tremaining: 63.6ms\n",
      "133:\tlearn: 0.0009052\ttotal: 500ms\tremaining: 59.7ms\n",
      "134:\tlearn: 0.0009052\ttotal: 503ms\tremaining: 55.9ms\n",
      "135:\tlearn: 0.0009052\ttotal: 505ms\tremaining: 52ms\n",
      "136:\tlearn: 0.0009052\ttotal: 508ms\tremaining: 48.2ms\n",
      "137:\tlearn: 0.0009052\ttotal: 510ms\tremaining: 44.4ms\n",
      "138:\tlearn: 0.0009052\ttotal: 513ms\tremaining: 40.6ms\n",
      "139:\tlearn: 0.0009052\ttotal: 515ms\tremaining: 36.8ms\n",
      "140:\tlearn: 0.0009052\ttotal: 518ms\tremaining: 33ms\n",
      "141:\tlearn: 0.0009052\ttotal: 520ms\tremaining: 29.3ms\n",
      "142:\tlearn: 0.0009052\ttotal: 523ms\tremaining: 25.6ms\n",
      "143:\tlearn: 0.0009052\ttotal: 525ms\tremaining: 21.9ms\n",
      "144:\tlearn: 0.0009052\ttotal: 527ms\tremaining: 18.2ms\n",
      "145:\tlearn: 0.0009052\ttotal: 530ms\tremaining: 14.5ms\n",
      "146:\tlearn: 0.0009052\ttotal: 532ms\tremaining: 10.9ms\n",
      "147:\tlearn: 0.0009051\ttotal: 535ms\tremaining: 7.23ms\n",
      "148:\tlearn: 0.0009051\ttotal: 537ms\tremaining: 3.6ms\n",
      "149:\tlearn: 0.0009051\ttotal: 540ms\tremaining: 0us\n",
      "Dataset 5:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aec50bf134043fcb34aa9380312e24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea4df6267e34066920df2f53588b953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6229046733119425, Recall = 0.5920107719928187, Aging Rate = 0.39812332439678283, Precision = 0.7401795735129069, f1 = 0.65785536159601\n",
      "Epoch 2: Train Loss = 0.45873985211161017, Recall = 0.7755834829443446, Aging Rate = 0.4542001787310098, Precision = 0.8499754058042303, f1 = 0.811077211922084\n",
      "Epoch 3: Train Loss = 0.35251567201810396, Recall = 0.8411131059245961, Aging Rate = 0.47587131367292224, Precision = 0.8798122065727699, f1 = 0.8600275355667737\n",
      "Epoch 4: Train Loss = 0.2808826142277858, Recall = 0.8837522441651705, Aging Rate = 0.4830205540661305, Precision = 0.9107308048103607, f1 = 0.8970387243735763\n",
      "Epoch 5: Train Loss = 0.23187762882370733, Recall = 0.914721723518851, Aging Rate = 0.4915102770330652, Precision = 0.9263636363636364, f1 = 0.9205058717253838\n",
      "Test Loss = 0.1948453432566572, Recall = 0.9708258527827648, Aging Rate = 0.5178731009830205, precision = 0.9331320103537533\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.17446649784323784, Recall = 0.9510771992818672, Aging Rate = 0.49486148346738157, Precision = 0.9566591422121896, f1 = 0.953860004501463\n",
      "Epoch 7: Train Loss = 0.13994471582986076, Recall = 0.9690305206463196, Aging Rate = 0.4973190348525469, Precision = 0.9699011680143755, f1 = 0.9694656488549618\n",
      "Epoch 8: Train Loss = 0.11380425744737441, Recall = 0.9712746858168761, Aging Rate = 0.4941912421805183, Precision = 0.9783001808318263, f1 = 0.9747747747747747\n",
      "Epoch 9: Train Loss = 0.09270116376546582, Recall = 0.9820466786355476, Aging Rate = 0.4964253798033959, Precision = 0.9846984698469847, f1 = 0.983370786516854\n",
      "Epoch 10: Train Loss = 0.07682797981890532, Recall = 0.9865350089766607, Aging Rate = 0.4970956210902592, Precision = 0.9878651685393258, f1 = 0.9871996406916685\n",
      "Test Loss = 0.06921079204858042, Recall = 0.9964093357271095, Aging Rate = 0.5029043789097408, precision = 0.986228342958685\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06627179432237543, Recall = 0.9887791741472173, Aging Rate = 0.4979892761394102, Precision = 0.9883355764917003, f1 = 0.9885573255553064\n",
      "Epoch 12: Train Loss = 0.055936876910807724, Recall = 0.9923698384201077, Aging Rate = 0.49754244861483465, Precision = 0.9928154467894028, f1 = 0.9925925925925926\n",
      "Epoch 13: Train Loss = 0.046630517112705404, Recall = 0.9964093357271095, Aging Rate = 0.49776586237712245, Precision = 0.9964093357271095, f1 = 0.9964093357271095\n",
      "Epoch 14: Train Loss = 0.04036723446497116, Recall = 0.9973070017953322, Aging Rate = 0.49754244861483465, Precision = 0.9977548271216884, f1 = 0.9975308641975309\n",
      "Epoch 15: Train Loss = 0.03523767386647183, Recall = 0.9977558348294434, Aging Rate = 0.49754244861483465, Precision = 0.9982038616973506, f1 = 0.997979797979798\n",
      "Test Loss = 0.031180938957150864, Recall = 0.9995511669658886, Aging Rate = 0.4984361036639857, precision = 0.9982070820259973\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.030572706816836166, Recall = 0.9995511669658886, Aging Rate = 0.4984361036639857, Precision = 0.9982070820259973, f1 = 0.99887867234806\n",
      "Epoch 17: Train Loss = 0.027030337715090332, Recall = 0.9991023339317774, Aging Rate = 0.4979892761394102, Precision = 0.9986541049798116, f1 = 0.9988781691720889\n",
      "Epoch 18: Train Loss = 0.02382383759069432, Recall = 0.9991023339317774, Aging Rate = 0.4982126899016979, Precision = 0.9982062780269059, f1 = 0.9986541049798117\n",
      "Epoch 19: Train Loss = 0.021323643746295605, Recall = 0.9995511669658886, Aging Rate = 0.4979892761394102, Precision = 0.9991027366532077, f1 = 0.9993269015032532\n",
      "Epoch 20: Train Loss = 0.01894113456562388, Recall = 1.0, Aging Rate = 0.4982126899016979, Precision = 0.9991031390134529, f1 = 0.9995513683266039\n",
      "Test Loss = 0.016986037545418824, Recall = 1.0, Aging Rate = 0.4982126899016979, precision = 0.9991031390134529\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.016961850684512343, Recall = 1.0, Aging Rate = 0.4982126899016979, Precision = 0.9991031390134529, f1 = 0.9995513683266039\n",
      "Epoch 22: Train Loss = 0.015384353425976311, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.013970637597456587, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.012733409099673671, Recall = 1.0, Aging Rate = 0.4979892761394102, Precision = 0.9995513683266039, f1 = 0.9997756338344178\n",
      "Epoch 25: Train Loss = 0.01168446614922185, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.010698853588269372, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.010759922789810916, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.009934838748869084, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.009246900985208844, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.008729316750699464, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.007908684316393357, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0072339911116212205, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.007336413026708274, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.006899167203250357, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.006460358446981621, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0061138320075043405, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.005615086467408969, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005390939688920921, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.005432330392354349, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.005090209843559229, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.004873749759660782, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0046426682363466506, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.004431394152085266, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004260147923405571, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.004194774645379843, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.003979137456552246, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.003860514382336545, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0036839158174359608, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0035170981137396585, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003357257064385298, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.0033937881524250964, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0032933277083798855, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0032159179104517036, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0031001000751269326, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.002942323865089316, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00278540898860999, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0028951512988809897, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0028039885248316385, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0027475119220930167, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Train Loss = 0.002643369737067081, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0025543107372776033, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0024234446069813257, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0024970341190842242, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.00250898799763275, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0024172341949585245, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0023770713139370323, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.002300389146622041, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021693845282843914, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0022438411852444746, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.002215445469761096, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.002210582706992372, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0021843492707622116, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0022309305153827086, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002046324215101959, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.002101668667287913, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0020232622658790584, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.002052190264568758, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0020686271051119703, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0019685902784248085, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002005648046442315, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0019435306848297304, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0018797425556947894, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.001982046264703148, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0018506988339330851, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0018690013182303727, Recall = 1.0, Aging Rate = 0.49776586237712245, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001711935837569183, Recall = 1.0, Aging Rate = 0.49776586237712245, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5120985\ttotal: 12.3ms\tremaining: 2.45s\n",
      "1:\tlearn: 0.4135517\ttotal: 24.2ms\tremaining: 2.4s\n",
      "2:\tlearn: 0.3265254\ttotal: 36.3ms\tremaining: 2.38s\n",
      "3:\tlearn: 0.2693980\ttotal: 48ms\tremaining: 2.35s\n",
      "4:\tlearn: 0.2298629\ttotal: 59.7ms\tremaining: 2.33s\n",
      "5:\tlearn: 0.1998426\ttotal: 71.4ms\tremaining: 2.31s\n",
      "6:\tlearn: 0.1672989\ttotal: 83.4ms\tremaining: 2.3s\n",
      "7:\tlearn: 0.1434606\ttotal: 94.1ms\tremaining: 2.26s\n",
      "8:\tlearn: 0.1340706\ttotal: 104ms\tremaining: 2.21s\n",
      "9:\tlearn: 0.1093879\ttotal: 114ms\tremaining: 2.17s\n",
      "10:\tlearn: 0.0966696\ttotal: 124ms\tremaining: 2.13s\n",
      "11:\tlearn: 0.0865663\ttotal: 134ms\tremaining: 2.1s\n",
      "12:\tlearn: 0.0768784\ttotal: 145ms\tremaining: 2.08s\n",
      "13:\tlearn: 0.0724135\ttotal: 155ms\tremaining: 2.06s\n",
      "14:\tlearn: 0.0670346\ttotal: 166ms\tremaining: 2.05s\n",
      "15:\tlearn: 0.0620271\ttotal: 176ms\tremaining: 2.02s\n",
      "16:\tlearn: 0.0572347\ttotal: 186ms\tremaining: 2s\n",
      "17:\tlearn: 0.0520788\ttotal: 197ms\tremaining: 1.99s\n",
      "18:\tlearn: 0.0468082\ttotal: 207ms\tremaining: 1.98s\n",
      "19:\tlearn: 0.0435155\ttotal: 218ms\tremaining: 1.96s\n",
      "20:\tlearn: 0.0400719\ttotal: 229ms\tremaining: 1.95s\n",
      "21:\tlearn: 0.0354598\ttotal: 240ms\tremaining: 1.94s\n",
      "22:\tlearn: 0.0331739\ttotal: 251ms\tremaining: 1.93s\n",
      "23:\tlearn: 0.0307220\ttotal: 261ms\tremaining: 1.91s\n",
      "24:\tlearn: 0.0279088\ttotal: 271ms\tremaining: 1.89s\n",
      "25:\tlearn: 0.0251570\ttotal: 280ms\tremaining: 1.88s\n",
      "26:\tlearn: 0.0236312\ttotal: 291ms\tremaining: 1.86s\n",
      "27:\tlearn: 0.0221681\ttotal: 301ms\tremaining: 1.85s\n",
      "28:\tlearn: 0.0206503\ttotal: 311ms\tremaining: 1.83s\n",
      "29:\tlearn: 0.0195891\ttotal: 322ms\tremaining: 1.82s\n",
      "30:\tlearn: 0.0182506\ttotal: 332ms\tremaining: 1.81s\n",
      "31:\tlearn: 0.0172210\ttotal: 343ms\tremaining: 1.8s\n",
      "32:\tlearn: 0.0163796\ttotal: 353ms\tremaining: 1.78s\n",
      "33:\tlearn: 0.0156122\ttotal: 362ms\tremaining: 1.77s\n",
      "34:\tlearn: 0.0148977\ttotal: 372ms\tremaining: 1.75s\n",
      "35:\tlearn: 0.0136967\ttotal: 383ms\tremaining: 1.74s\n",
      "36:\tlearn: 0.0125651\ttotal: 394ms\tremaining: 1.74s\n",
      "37:\tlearn: 0.0121341\ttotal: 405ms\tremaining: 1.73s\n",
      "38:\tlearn: 0.0115268\ttotal: 416ms\tremaining: 1.72s\n",
      "39:\tlearn: 0.0111064\ttotal: 425ms\tremaining: 1.7s\n",
      "40:\tlearn: 0.0105473\ttotal: 435ms\tremaining: 1.69s\n",
      "41:\tlearn: 0.0102602\ttotal: 438ms\tremaining: 1.65s\n",
      "42:\tlearn: 0.0095366\ttotal: 448ms\tremaining: 1.63s\n",
      "43:\tlearn: 0.0088722\ttotal: 461ms\tremaining: 1.64s\n",
      "44:\tlearn: 0.0083679\ttotal: 480ms\tremaining: 1.65s\n",
      "45:\tlearn: 0.0077512\ttotal: 496ms\tremaining: 1.66s\n",
      "46:\tlearn: 0.0073093\ttotal: 507ms\tremaining: 1.65s\n",
      "47:\tlearn: 0.0066125\ttotal: 517ms\tremaining: 1.64s\n",
      "48:\tlearn: 0.0063524\ttotal: 527ms\tremaining: 1.62s\n",
      "49:\tlearn: 0.0059437\ttotal: 537ms\tremaining: 1.61s\n",
      "50:\tlearn: 0.0057625\ttotal: 548ms\tremaining: 1.6s\n",
      "51:\tlearn: 0.0055046\ttotal: 558ms\tremaining: 1.59s\n",
      "52:\tlearn: 0.0052110\ttotal: 568ms\tremaining: 1.57s\n",
      "53:\tlearn: 0.0050393\ttotal: 578ms\tremaining: 1.56s\n",
      "54:\tlearn: 0.0047927\ttotal: 588ms\tremaining: 1.55s\n",
      "55:\tlearn: 0.0044214\ttotal: 599ms\tremaining: 1.54s\n",
      "56:\tlearn: 0.0041543\ttotal: 610ms\tremaining: 1.53s\n",
      "57:\tlearn: 0.0039007\ttotal: 621ms\tremaining: 1.52s\n",
      "58:\tlearn: 0.0038254\ttotal: 633ms\tremaining: 1.51s\n",
      "59:\tlearn: 0.0036449\ttotal: 644ms\tremaining: 1.5s\n",
      "60:\tlearn: 0.0035357\ttotal: 656ms\tremaining: 1.49s\n",
      "61:\tlearn: 0.0033631\ttotal: 667ms\tremaining: 1.48s\n",
      "62:\tlearn: 0.0031962\ttotal: 678ms\tremaining: 1.47s\n",
      "63:\tlearn: 0.0030632\ttotal: 689ms\tremaining: 1.46s\n",
      "64:\tlearn: 0.0028045\ttotal: 699ms\tremaining: 1.45s\n",
      "65:\tlearn: 0.0026391\ttotal: 710ms\tremaining: 1.44s\n",
      "66:\tlearn: 0.0025630\ttotal: 721ms\tremaining: 1.43s\n",
      "67:\tlearn: 0.0024547\ttotal: 732ms\tremaining: 1.42s\n",
      "68:\tlearn: 0.0023767\ttotal: 743ms\tremaining: 1.41s\n",
      "69:\tlearn: 0.0022484\ttotal: 754ms\tremaining: 1.4s\n",
      "70:\tlearn: 0.0020918\ttotal: 765ms\tremaining: 1.39s\n",
      "71:\tlearn: 0.0020212\ttotal: 776ms\tremaining: 1.38s\n",
      "72:\tlearn: 0.0018883\ttotal: 786ms\tremaining: 1.37s\n",
      "73:\tlearn: 0.0018148\ttotal: 796ms\tremaining: 1.35s\n",
      "74:\tlearn: 0.0017395\ttotal: 806ms\tremaining: 1.34s\n",
      "75:\tlearn: 0.0016802\ttotal: 817ms\tremaining: 1.33s\n",
      "76:\tlearn: 0.0016174\ttotal: 827ms\tremaining: 1.32s\n",
      "77:\tlearn: 0.0015280\ttotal: 836ms\tremaining: 1.31s\n",
      "78:\tlearn: 0.0014850\ttotal: 846ms\tremaining: 1.29s\n",
      "79:\tlearn: 0.0014291\ttotal: 857ms\tremaining: 1.28s\n",
      "80:\tlearn: 0.0013388\ttotal: 868ms\tremaining: 1.27s\n",
      "81:\tlearn: 0.0012955\ttotal: 878ms\tremaining: 1.26s\n",
      "82:\tlearn: 0.0012590\ttotal: 888ms\tremaining: 1.25s\n",
      "83:\tlearn: 0.0011447\ttotal: 898ms\tremaining: 1.24s\n",
      "84:\tlearn: 0.0011073\ttotal: 908ms\tremaining: 1.23s\n",
      "85:\tlearn: 0.0010505\ttotal: 919ms\tremaining: 1.22s\n",
      "86:\tlearn: 0.0010101\ttotal: 930ms\tremaining: 1.21s\n",
      "87:\tlearn: 0.0009410\ttotal: 941ms\tremaining: 1.2s\n",
      "88:\tlearn: 0.0009082\ttotal: 951ms\tremaining: 1.19s\n",
      "89:\tlearn: 0.0008789\ttotal: 961ms\tremaining: 1.17s\n",
      "90:\tlearn: 0.0008341\ttotal: 972ms\tremaining: 1.16s\n",
      "91:\tlearn: 0.0007795\ttotal: 982ms\tremaining: 1.15s\n",
      "92:\tlearn: 0.0007571\ttotal: 992ms\tremaining: 1.14s\n",
      "93:\tlearn: 0.0007384\ttotal: 1s\tremaining: 1.13s\n",
      "94:\tlearn: 0.0007134\ttotal: 1.01s\tremaining: 1.12s\n",
      "95:\tlearn: 0.0006836\ttotal: 1.02s\tremaining: 1.11s\n",
      "96:\tlearn: 0.0006658\ttotal: 1.03s\tremaining: 1.09s\n",
      "97:\tlearn: 0.0006353\ttotal: 1.04s\tremaining: 1.08s\n",
      "98:\tlearn: 0.0006207\ttotal: 1.05s\tremaining: 1.07s\n",
      "99:\tlearn: 0.0005945\ttotal: 1.06s\tremaining: 1.06s\n",
      "100:\tlearn: 0.0005523\ttotal: 1.07s\tremaining: 1.05s\n",
      "101:\tlearn: 0.0005381\ttotal: 1.08s\tremaining: 1.04s\n",
      "102:\tlearn: 0.0005140\ttotal: 1.1s\tremaining: 1.03s\n",
      "103:\tlearn: 0.0005075\ttotal: 1.11s\tremaining: 1.02s\n",
      "104:\tlearn: 0.0004873\ttotal: 1.12s\tremaining: 1.01s\n",
      "105:\tlearn: 0.0004737\ttotal: 1.13s\tremaining: 1s\n",
      "106:\tlearn: 0.0004587\ttotal: 1.14s\tremaining: 991ms\n",
      "107:\tlearn: 0.0004285\ttotal: 1.15s\tremaining: 980ms\n",
      "108:\tlearn: 0.0004129\ttotal: 1.16s\tremaining: 970ms\n",
      "109:\tlearn: 0.0003988\ttotal: 1.17s\tremaining: 960ms\n",
      "110:\tlearn: 0.0003861\ttotal: 1.18s\tremaining: 949ms\n",
      "111:\tlearn: 0.0003736\ttotal: 1.19s\tremaining: 938ms\n",
      "112:\tlearn: 0.0003654\ttotal: 1.2s\tremaining: 926ms\n",
      "113:\tlearn: 0.0003541\ttotal: 1.21s\tremaining: 915ms\n",
      "114:\tlearn: 0.0003473\ttotal: 1.22s\tremaining: 904ms\n",
      "115:\tlearn: 0.0003373\ttotal: 1.23s\tremaining: 894ms\n",
      "116:\tlearn: 0.0003299\ttotal: 1.24s\tremaining: 883ms\n",
      "117:\tlearn: 0.0003177\ttotal: 1.25s\tremaining: 872ms\n",
      "118:\tlearn: 0.0003112\ttotal: 1.26s\tremaining: 861ms\n",
      "119:\tlearn: 0.0003012\ttotal: 1.27s\tremaining: 850ms\n",
      "120:\tlearn: 0.0002927\ttotal: 1.28s\tremaining: 839ms\n",
      "121:\tlearn: 0.0002735\ttotal: 1.29s\tremaining: 829ms\n",
      "122:\tlearn: 0.0002658\ttotal: 1.31s\tremaining: 818ms\n",
      "123:\tlearn: 0.0002658\ttotal: 1.32s\tremaining: 807ms\n",
      "124:\tlearn: 0.0002605\ttotal: 1.33s\tremaining: 797ms\n",
      "125:\tlearn: 0.0002522\ttotal: 1.34s\tremaining: 786ms\n",
      "126:\tlearn: 0.0002462\ttotal: 1.35s\tremaining: 775ms\n",
      "127:\tlearn: 0.0002462\ttotal: 1.36s\tremaining: 764ms\n",
      "128:\tlearn: 0.0002361\ttotal: 1.37s\tremaining: 754ms\n",
      "129:\tlearn: 0.0002361\ttotal: 1.38s\tremaining: 743ms\n",
      "130:\tlearn: 0.0002361\ttotal: 1.39s\tremaining: 731ms\n",
      "131:\tlearn: 0.0002361\ttotal: 1.4s\tremaining: 720ms\n",
      "132:\tlearn: 0.0002301\ttotal: 1.41s\tremaining: 709ms\n",
      "133:\tlearn: 0.0002301\ttotal: 1.42s\tremaining: 698ms\n",
      "134:\tlearn: 0.0002210\ttotal: 1.43s\tremaining: 688ms\n",
      "135:\tlearn: 0.0002178\ttotal: 1.44s\tremaining: 677ms\n",
      "136:\tlearn: 0.0002110\ttotal: 1.45s\tremaining: 667ms\n",
      "137:\tlearn: 0.0002072\ttotal: 1.47s\tremaining: 659ms\n",
      "138:\tlearn: 0.0002072\ttotal: 1.49s\tremaining: 652ms\n",
      "139:\tlearn: 0.0002072\ttotal: 1.5s\tremaining: 645ms\n",
      "140:\tlearn: 0.0002072\ttotal: 1.52s\tremaining: 637ms\n",
      "141:\tlearn: 0.0002072\ttotal: 1.54s\tremaining: 628ms\n",
      "142:\tlearn: 0.0002008\ttotal: 1.55s\tremaining: 618ms\n",
      "143:\tlearn: 0.0001983\ttotal: 1.56s\tremaining: 607ms\n",
      "144:\tlearn: 0.0001963\ttotal: 1.57s\tremaining: 596ms\n",
      "145:\tlearn: 0.0001963\ttotal: 1.58s\tremaining: 585ms\n",
      "146:\tlearn: 0.0001963\ttotal: 1.59s\tremaining: 574ms\n",
      "147:\tlearn: 0.0001963\ttotal: 1.6s\tremaining: 563ms\n",
      "148:\tlearn: 0.0001963\ttotal: 1.61s\tremaining: 552ms\n",
      "149:\tlearn: 0.0001913\ttotal: 1.62s\tremaining: 540ms\n",
      "150:\tlearn: 0.0001913\ttotal: 1.63s\tremaining: 529ms\n",
      "151:\tlearn: 0.0001882\ttotal: 1.64s\tremaining: 518ms\n",
      "152:\tlearn: 0.0001810\ttotal: 1.65s\tremaining: 507ms\n",
      "153:\tlearn: 0.0001810\ttotal: 1.66s\tremaining: 496ms\n",
      "154:\tlearn: 0.0001810\ttotal: 1.67s\tremaining: 485ms\n",
      "155:\tlearn: 0.0001810\ttotal: 1.68s\tremaining: 474ms\n",
      "156:\tlearn: 0.0001810\ttotal: 1.69s\tremaining: 464ms\n",
      "157:\tlearn: 0.0001770\ttotal: 1.7s\tremaining: 453ms\n",
      "158:\tlearn: 0.0001770\ttotal: 1.71s\tremaining: 442ms\n",
      "159:\tlearn: 0.0001770\ttotal: 1.72s\tremaining: 431ms\n",
      "160:\tlearn: 0.0001770\ttotal: 1.73s\tremaining: 420ms\n",
      "161:\tlearn: 0.0001770\ttotal: 1.74s\tremaining: 409ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162:\tlearn: 0.0001770\ttotal: 1.75s\tremaining: 398ms\n",
      "163:\tlearn: 0.0001770\ttotal: 1.76s\tremaining: 387ms\n",
      "164:\tlearn: 0.0001770\ttotal: 1.77s\tremaining: 376ms\n",
      "165:\tlearn: 0.0001770\ttotal: 1.78s\tremaining: 365ms\n",
      "166:\tlearn: 0.0001770\ttotal: 1.79s\tremaining: 355ms\n",
      "167:\tlearn: 0.0001770\ttotal: 1.8s\tremaining: 344ms\n",
      "168:\tlearn: 0.0001770\ttotal: 1.81s\tremaining: 333ms\n",
      "169:\tlearn: 0.0001770\ttotal: 1.82s\tremaining: 322ms\n",
      "170:\tlearn: 0.0001770\ttotal: 1.83s\tremaining: 311ms\n",
      "171:\tlearn: 0.0001770\ttotal: 1.84s\tremaining: 300ms\n",
      "172:\tlearn: 0.0001770\ttotal: 1.85s\tremaining: 289ms\n",
      "173:\tlearn: 0.0001770\ttotal: 1.86s\tremaining: 279ms\n",
      "174:\tlearn: 0.0001770\ttotal: 1.88s\tremaining: 268ms\n",
      "175:\tlearn: 0.0001739\ttotal: 1.89s\tremaining: 257ms\n",
      "176:\tlearn: 0.0001654\ttotal: 1.9s\tremaining: 247ms\n",
      "177:\tlearn: 0.0001654\ttotal: 1.91s\tremaining: 236ms\n",
      "178:\tlearn: 0.0001601\ttotal: 1.92s\tremaining: 225ms\n",
      "179:\tlearn: 0.0001601\ttotal: 1.93s\tremaining: 215ms\n",
      "180:\tlearn: 0.0001601\ttotal: 1.94s\tremaining: 204ms\n",
      "181:\tlearn: 0.0001601\ttotal: 1.95s\tremaining: 193ms\n",
      "182:\tlearn: 0.0001601\ttotal: 1.96s\tremaining: 182ms\n",
      "183:\tlearn: 0.0001601\ttotal: 1.97s\tremaining: 172ms\n",
      "184:\tlearn: 0.0001601\ttotal: 1.98s\tremaining: 161ms\n",
      "185:\tlearn: 0.0001601\ttotal: 1.99s\tremaining: 150ms\n",
      "186:\tlearn: 0.0001601\ttotal: 2s\tremaining: 139ms\n",
      "187:\tlearn: 0.0001601\ttotal: 2.02s\tremaining: 129ms\n",
      "188:\tlearn: 0.0001601\ttotal: 2.02s\tremaining: 118ms\n",
      "189:\tlearn: 0.0001601\ttotal: 2.04s\tremaining: 107ms\n",
      "190:\tlearn: 0.0001601\ttotal: 2.04s\tremaining: 96.4ms\n",
      "191:\tlearn: 0.0001601\ttotal: 2.06s\tremaining: 85.7ms\n",
      "192:\tlearn: 0.0001601\ttotal: 2.07s\tremaining: 74.9ms\n",
      "193:\tlearn: 0.0001601\ttotal: 2.08s\tremaining: 64.2ms\n",
      "194:\tlearn: 0.0001601\ttotal: 2.08s\tremaining: 53.5ms\n",
      "195:\tlearn: 0.0001601\ttotal: 2.09s\tremaining: 42.7ms\n",
      "196:\tlearn: 0.0001601\ttotal: 2.1s\tremaining: 32.1ms\n",
      "197:\tlearn: 0.0001601\ttotal: 2.11s\tremaining: 21.4ms\n",
      "198:\tlearn: 0.0001601\ttotal: 2.12s\tremaining: 10.7ms\n",
      "199:\tlearn: 0.0001601\ttotal: 2.13s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aed003dbb98450bb4621a245a35e5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6206726944379967, Recall = 0.39829366861248316, Aging Rate = 0.2346368715083799, Precision = 0.8447619047619047, f1 = 0.5413487946292339\n",
      "Epoch 2: Train Loss = 0.4466930445679073, Recall = 0.7786259541984732, Aging Rate = 0.45675977653631283, Precision = 0.8483365949119374, f1 = 0.8119878248653711\n",
      "Epoch 3: Train Loss = 0.3505678570137344, Recall = 0.8343062415806017, Aging Rate = 0.4766480446927374, Precision = 0.8710736052508204, f1 = 0.8522935779816514\n",
      "Epoch 4: Train Loss = 0.2934923571314892, Recall = 0.8706780422092502, Aging Rate = 0.48223463687150836, Precision = 0.8985171455050973, f1 = 0.8843785632839224\n",
      "Epoch 5: Train Loss = 0.24968069568359652, Recall = 0.9012123933542883, Aging Rate = 0.4893854748603352, Precision = 0.9164383561643835, f1 = 0.9087616028978944\n",
      "Test Loss = 0.21220947192367895, Recall = 0.9191737763807813, Aging Rate = 0.4779888268156425, precision = 0.956989247311828\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.19818577435429535, Recall = 0.9375841939829367, Aging Rate = 0.49050279329608937, Precision = 0.95125284738041, f1 = 0.944369063772049\n",
      "Epoch 7: Train Loss = 0.16491457271176344, Recall = 0.953300404131118, Aging Rate = 0.4940782122905028, Precision = 0.9601990049751243, f1 = 0.956737269040108\n",
      "Epoch 8: Train Loss = 0.1389191703689831, Recall = 0.9667714414009879, Aging Rate = 0.4960893854748603, Precision = 0.9698198198198198, f1 = 0.9682932313919497\n",
      "Epoch 9: Train Loss = 0.116446749834375, Recall = 0.9712617871576111, Aging Rate = 0.4949720670391061, Precision = 0.9765237020316027, f1 = 0.9738856371004053\n",
      "Epoch 10: Train Loss = 0.0982711112399341, Recall = 0.9797934440951953, Aging Rate = 0.4963128491620112, Precision = 0.9824403421882035, f1 = 0.9811151079136691\n",
      "Test Loss = 0.08662897905990398, Recall = 0.9869779973057925, Aging Rate = 0.49899441340782125, precision = 0.9843260188087775\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.083750634178436, Recall = 0.9860799281544679, Aging Rate = 0.49743016759776537, Precision = 0.9865229110512129, f1 = 0.9863013698630138\n",
      "Epoch 12: Train Loss = 0.07247830678131327, Recall = 0.9874270318814549, Aging Rate = 0.4987709497206704, Precision = 0.9852150537634409, f1 = 0.9863198026463332\n",
      "Epoch 13: Train Loss = 0.06291998205071721, Recall = 0.9892231701841042, Aging Rate = 0.496536312849162, Precision = 0.9914491449144914, f1 = 0.9903349067206114\n",
      "Epoch 14: Train Loss = 0.05460105631937528, Recall = 0.9896722047597665, Aging Rate = 0.496536312849162, Precision = 0.991899189918992, f1 = 0.9907844459429086\n",
      "Epoch 15: Train Loss = 0.047844260632159324, Recall = 0.9932644813650651, Aging Rate = 0.49743016759776537, Precision = 0.9937106918238994, f1 = 0.9934875364922524\n",
      "Test Loss = 0.0424566541595499, Recall = 0.9941625505163898, Aging Rate = 0.49675977653631287, precision = 0.9959514170040485\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.041690362478767694, Recall = 0.9941625505163898, Aging Rate = 0.496536312849162, Precision = 0.9963996399639964, f1 = 0.99527983816588\n",
      "Epoch 17: Train Loss = 0.03691541403615275, Recall = 0.994611585092052, Aging Rate = 0.49675977653631287, Precision = 0.9964012595591543, f1 = 0.9955056179775281\n",
      "Epoch 18: Train Loss = 0.03349824004666099, Recall = 0.9964077233947014, Aging Rate = 0.4969832402234637, Precision = 0.997751798561151, f1 = 0.9970793080206694\n",
      "Epoch 19: Train Loss = 0.029892669387536343, Recall = 0.9968567579703638, Aging Rate = 0.4976536312849162, Precision = 0.9968567579703638, f1 = 0.9968567579703638\n",
      "Epoch 20: Train Loss = 0.02682510689899908, Recall = 0.9968567579703638, Aging Rate = 0.4969832402234637, Precision = 0.9982014388489209, f1 = 0.9975286452482589\n",
      "Test Loss = 0.024259021360864188, Recall = 0.9950606196677144, Aging Rate = 0.4958659217877095, precision = 0.9986480396575034\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.02414107906210689, Recall = 0.9977548271216884, Aging Rate = 0.4969832402234637, Precision = 0.9991007194244604, f1 = 0.9984273197034375\n",
      "Epoch 22: Train Loss = 0.02191503485820813, Recall = 0.9977548271216884, Aging Rate = 0.49675977653631287, Precision = 0.9995501574448943, f1 = 0.9986516853932584\n",
      "Epoch 23: Train Loss = 0.019920581002724903, Recall = 0.998652896273013, Aging Rate = 0.4972067039106145, Precision = 0.9995505617977528, f1 = 0.9991015274034142\n",
      "Epoch 24: Train Loss = 0.018296103476229328, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Epoch 25: Train Loss = 0.01641835982968521, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Test Loss = 0.014664990342737242, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.015191357472089415, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 27: Train Loss = 0.01396698493427238, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Epoch 28: Train Loss = 0.012849108423392533, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 29: Train Loss = 0.011902605859489913, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 30: Train Loss = 0.011130699088287087, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.010067876540748767, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "\n",
      "Epoch 31: Train Loss = 0.010232437451541758, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 32: Train Loss = 0.009484274420033953, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 33: Train Loss = 0.009035375179129772, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 34: Train Loss = 0.008471507725602422, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 35: Train Loss = 0.00788780132912141, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.007369411227256892, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.007492450753424587, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 37: Train Loss = 0.007043905662237432, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.006594813711025528, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 39: Train Loss = 0.006212911672952621, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 40: Train Loss = 0.005957908991349643, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005510017706924144, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "\n",
      "Epoch 41: Train Loss = 0.005581517582967378, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 42: Train Loss = 0.005232797168486611, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.005016721179500722, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.004958676705709216, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.004535388389820826, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00414557289290528, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.004395341222934092, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.004103641976815695, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.003973691199786503, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.0038528119908989144, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0036470250270261777, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033728170036936606, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0035706927385909595, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0035883987146070715, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.003291028551913423, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0031857019263272844, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.003141577885272866, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002987024514004588, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.003153217579940511, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002969688293029809, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0028190815504952516, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0027622309555013417, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0026855941485956977, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002575225037257265, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.002695184947204823, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.002575463380966356, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0024626926469548953, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.002424121012744707, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0023526701139671177, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002216587994643097, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.002265095640801143, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0022749903895918216, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0023020021550294906, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0022319612869285254, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0022575736817690498, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002081970304521269, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0021355483503543962, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.002065546057556428, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.002022949005242999, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0019764452837095414, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.001996597438613629, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018297411201199172, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.002017086359032289, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0019490162280320538, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0019205417971038285, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0018677758933799394, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.001893545730955191, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001728197646629086, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0018187709231782892, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0018357390320115831, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.001830051013804431, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0018347999376021116, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.001825756972496965, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016824531124113907, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5176584\ttotal: 12.1ms\tremaining: 2.4s\n",
      "1:\tlearn: 0.3938075\ttotal: 24.3ms\tremaining: 2.4s\n",
      "2:\tlearn: 0.3335310\ttotal: 36.7ms\tremaining: 2.41s\n",
      "3:\tlearn: 0.2596231\ttotal: 48.9ms\tremaining: 2.4s\n",
      "4:\tlearn: 0.2246568\ttotal: 62ms\tremaining: 2.42s\n",
      "5:\tlearn: 0.2061880\ttotal: 73.9ms\tremaining: 2.39s\n",
      "6:\tlearn: 0.1678582\ttotal: 85.3ms\tremaining: 2.35s\n",
      "7:\tlearn: 0.1395731\ttotal: 95.7ms\tremaining: 2.3s\n",
      "8:\tlearn: 0.1247930\ttotal: 106ms\tremaining: 2.25s\n",
      "9:\tlearn: 0.1090268\ttotal: 117ms\tremaining: 2.21s\n",
      "10:\tlearn: 0.0981203\ttotal: 126ms\tremaining: 2.17s\n",
      "11:\tlearn: 0.0912440\ttotal: 137ms\tremaining: 2.15s\n",
      "12:\tlearn: 0.0837842\ttotal: 148ms\tremaining: 2.12s\n",
      "13:\tlearn: 0.0749073\ttotal: 158ms\tremaining: 2.1s\n",
      "14:\tlearn: 0.0691582\ttotal: 168ms\tremaining: 2.07s\n",
      "15:\tlearn: 0.0630317\ttotal: 178ms\tremaining: 2.05s\n",
      "16:\tlearn: 0.0591107\ttotal: 189ms\tremaining: 2.04s\n",
      "17:\tlearn: 0.0560446\ttotal: 199ms\tremaining: 2.01s\n",
      "18:\tlearn: 0.0507530\ttotal: 209ms\tremaining: 1.99s\n",
      "19:\tlearn: 0.0463093\ttotal: 219ms\tremaining: 1.97s\n",
      "20:\tlearn: 0.0430301\ttotal: 229ms\tremaining: 1.95s\n",
      "21:\tlearn: 0.0387563\ttotal: 239ms\tremaining: 1.94s\n",
      "22:\tlearn: 0.0372906\ttotal: 250ms\tremaining: 1.92s\n",
      "23:\tlearn: 0.0350268\ttotal: 260ms\tremaining: 1.9s\n",
      "24:\tlearn: 0.0327928\ttotal: 269ms\tremaining: 1.89s\n",
      "25:\tlearn: 0.0289004\ttotal: 280ms\tremaining: 1.87s\n",
      "26:\tlearn: 0.0269935\ttotal: 290ms\tremaining: 1.86s\n",
      "27:\tlearn: 0.0251069\ttotal: 300ms\tremaining: 1.84s\n",
      "28:\tlearn: 0.0236094\ttotal: 310ms\tremaining: 1.83s\n",
      "29:\tlearn: 0.0221740\ttotal: 320ms\tremaining: 1.81s\n",
      "30:\tlearn: 0.0212238\ttotal: 329ms\tremaining: 1.8s\n",
      "31:\tlearn: 0.0192511\ttotal: 340ms\tremaining: 1.78s\n",
      "32:\tlearn: 0.0182844\ttotal: 350ms\tremaining: 1.77s\n",
      "33:\tlearn: 0.0164623\ttotal: 361ms\tremaining: 1.76s\n",
      "34:\tlearn: 0.0151487\ttotal: 371ms\tremaining: 1.75s\n",
      "35:\tlearn: 0.0142048\ttotal: 381ms\tremaining: 1.74s\n",
      "36:\tlearn: 0.0133910\ttotal: 394ms\tremaining: 1.74s\n",
      "37:\tlearn: 0.0121539\ttotal: 411ms\tremaining: 1.75s\n",
      "38:\tlearn: 0.0116778\ttotal: 426ms\tremaining: 1.76s\n",
      "39:\tlearn: 0.0109940\ttotal: 437ms\tremaining: 1.75s\n",
      "40:\tlearn: 0.0098819\ttotal: 447ms\tremaining: 1.73s\n",
      "41:\tlearn: 0.0093939\ttotal: 457ms\tremaining: 1.72s\n",
      "42:\tlearn: 0.0090518\ttotal: 468ms\tremaining: 1.71s\n",
      "43:\tlearn: 0.0087081\ttotal: 479ms\tremaining: 1.7s\n",
      "44:\tlearn: 0.0080685\ttotal: 490ms\tremaining: 1.69s\n",
      "45:\tlearn: 0.0073640\ttotal: 500ms\tremaining: 1.67s\n",
      "46:\tlearn: 0.0070315\ttotal: 510ms\tremaining: 1.66s\n",
      "47:\tlearn: 0.0066258\ttotal: 520ms\tremaining: 1.65s\n",
      "48:\tlearn: 0.0059857\ttotal: 531ms\tremaining: 1.64s\n",
      "49:\tlearn: 0.0056378\ttotal: 541ms\tremaining: 1.62s\n",
      "50:\tlearn: 0.0054752\ttotal: 552ms\tremaining: 1.61s\n",
      "51:\tlearn: 0.0051379\ttotal: 562ms\tremaining: 1.6s\n",
      "52:\tlearn: 0.0049612\ttotal: 572ms\tremaining: 1.59s\n",
      "53:\tlearn: 0.0047638\ttotal: 582ms\tremaining: 1.57s\n",
      "54:\tlearn: 0.0044873\ttotal: 593ms\tremaining: 1.56s\n",
      "55:\tlearn: 0.0043143\ttotal: 602ms\tremaining: 1.55s\n",
      "56:\tlearn: 0.0041402\ttotal: 613ms\tremaining: 1.54s\n",
      "57:\tlearn: 0.0039225\ttotal: 623ms\tremaining: 1.53s\n",
      "58:\tlearn: 0.0037368\ttotal: 633ms\tremaining: 1.51s\n",
      "59:\tlearn: 0.0034505\ttotal: 644ms\tremaining: 1.5s\n",
      "60:\tlearn: 0.0033374\ttotal: 655ms\tremaining: 1.49s\n",
      "61:\tlearn: 0.0031780\ttotal: 664ms\tremaining: 1.48s\n",
      "62:\tlearn: 0.0030325\ttotal: 675ms\tremaining: 1.47s\n",
      "63:\tlearn: 0.0029120\ttotal: 684ms\tremaining: 1.45s\n",
      "64:\tlearn: 0.0027563\ttotal: 694ms\tremaining: 1.44s\n",
      "65:\tlearn: 0.0026433\ttotal: 704ms\tremaining: 1.43s\n",
      "66:\tlearn: 0.0025300\ttotal: 714ms\tremaining: 1.42s\n",
      "67:\tlearn: 0.0024348\ttotal: 724ms\tremaining: 1.41s\n",
      "68:\tlearn: 0.0022166\ttotal: 734ms\tremaining: 1.39s\n",
      "69:\tlearn: 0.0021181\ttotal: 745ms\tremaining: 1.38s\n",
      "70:\tlearn: 0.0020118\ttotal: 755ms\tremaining: 1.37s\n",
      "71:\tlearn: 0.0019082\ttotal: 765ms\tremaining: 1.36s\n",
      "72:\tlearn: 0.0018145\ttotal: 776ms\tremaining: 1.35s\n",
      "73:\tlearn: 0.0018049\ttotal: 777ms\tremaining: 1.32s\n",
      "74:\tlearn: 0.0017247\ttotal: 788ms\tremaining: 1.31s\n",
      "75:\tlearn: 0.0016539\ttotal: 797ms\tremaining: 1.3s\n",
      "76:\tlearn: 0.0014737\ttotal: 808ms\tremaining: 1.29s\n",
      "77:\tlearn: 0.0014232\ttotal: 819ms\tremaining: 1.28s\n",
      "78:\tlearn: 0.0013927\ttotal: 829ms\tremaining: 1.27s\n",
      "79:\tlearn: 0.0013188\ttotal: 839ms\tremaining: 1.26s\n",
      "80:\tlearn: 0.0012638\ttotal: 850ms\tremaining: 1.25s\n",
      "81:\tlearn: 0.0012003\ttotal: 861ms\tremaining: 1.24s\n",
      "82:\tlearn: 0.0011684\ttotal: 872ms\tremaining: 1.23s\n",
      "83:\tlearn: 0.0011245\ttotal: 882ms\tremaining: 1.22s\n",
      "84:\tlearn: 0.0010920\ttotal: 892ms\tremaining: 1.21s\n",
      "85:\tlearn: 0.0010397\ttotal: 902ms\tremaining: 1.2s\n",
      "86:\tlearn: 0.0010002\ttotal: 912ms\tremaining: 1.18s\n",
      "87:\tlearn: 0.0009771\ttotal: 922ms\tremaining: 1.17s\n",
      "88:\tlearn: 0.0009319\ttotal: 932ms\tremaining: 1.16s\n",
      "89:\tlearn: 0.0009042\ttotal: 941ms\tremaining: 1.15s\n",
      "90:\tlearn: 0.0008820\ttotal: 951ms\tremaining: 1.14s\n",
      "91:\tlearn: 0.0008395\ttotal: 961ms\tremaining: 1.13s\n",
      "92:\tlearn: 0.0007994\ttotal: 972ms\tremaining: 1.12s\n",
      "93:\tlearn: 0.0007659\ttotal: 984ms\tremaining: 1.11s\n",
      "94:\tlearn: 0.0007260\ttotal: 995ms\tremaining: 1.1s\n",
      "95:\tlearn: 0.0006934\ttotal: 1s\tremaining: 1.09s\n",
      "96:\tlearn: 0.0006654\ttotal: 1.02s\tremaining: 1.08s\n",
      "97:\tlearn: 0.0006360\ttotal: 1.03s\tremaining: 1.07s\n",
      "98:\tlearn: 0.0006046\ttotal: 1.04s\tremaining: 1.06s\n",
      "99:\tlearn: 0.0005810\ttotal: 1.05s\tremaining: 1.05s\n",
      "100:\tlearn: 0.0005688\ttotal: 1.06s\tremaining: 1.04s\n",
      "101:\tlearn: 0.0005546\ttotal: 1.07s\tremaining: 1.03s\n",
      "102:\tlearn: 0.0005332\ttotal: 1.08s\tremaining: 1.02s\n",
      "103:\tlearn: 0.0005242\ttotal: 1.09s\tremaining: 1s\n",
      "104:\tlearn: 0.0005078\ttotal: 1.1s\tremaining: 995ms\n",
      "105:\tlearn: 0.0004985\ttotal: 1.11s\tremaining: 985ms\n",
      "106:\tlearn: 0.0004836\ttotal: 1.12s\tremaining: 974ms\n",
      "107:\tlearn: 0.0004657\ttotal: 1.13s\tremaining: 963ms\n",
      "108:\tlearn: 0.0004552\ttotal: 1.14s\tremaining: 953ms\n",
      "109:\tlearn: 0.0004439\ttotal: 1.15s\tremaining: 942ms\n",
      "110:\tlearn: 0.0004319\ttotal: 1.16s\tremaining: 932ms\n",
      "111:\tlearn: 0.0004123\ttotal: 1.17s\tremaining: 921ms\n",
      "112:\tlearn: 0.0003976\ttotal: 1.18s\tremaining: 911ms\n",
      "113:\tlearn: 0.0003806\ttotal: 1.19s\tremaining: 900ms\n",
      "114:\tlearn: 0.0003660\ttotal: 1.2s\tremaining: 889ms\n",
      "115:\tlearn: 0.0003578\ttotal: 1.21s\tremaining: 878ms\n",
      "116:\tlearn: 0.0003478\ttotal: 1.22s\tremaining: 867ms\n",
      "117:\tlearn: 0.0003417\ttotal: 1.23s\tremaining: 857ms\n",
      "118:\tlearn: 0.0003303\ttotal: 1.24s\tremaining: 847ms\n",
      "119:\tlearn: 0.0003231\ttotal: 1.25s\tremaining: 836ms\n",
      "120:\tlearn: 0.0003146\ttotal: 1.26s\tremaining: 825ms\n",
      "121:\tlearn: 0.0003043\ttotal: 1.27s\tremaining: 814ms\n",
      "122:\tlearn: 0.0002964\ttotal: 1.28s\tremaining: 804ms\n",
      "123:\tlearn: 0.0002828\ttotal: 1.3s\tremaining: 794ms\n",
      "124:\tlearn: 0.0002727\ttotal: 1.31s\tremaining: 784ms\n",
      "125:\tlearn: 0.0002625\ttotal: 1.32s\tremaining: 773ms\n",
      "126:\tlearn: 0.0002536\ttotal: 1.33s\tremaining: 763ms\n",
      "127:\tlearn: 0.0002435\ttotal: 1.34s\tremaining: 753ms\n",
      "128:\tlearn: 0.0002342\ttotal: 1.35s\tremaining: 743ms\n",
      "129:\tlearn: 0.0002251\ttotal: 1.36s\tremaining: 732ms\n",
      "130:\tlearn: 0.0002178\ttotal: 1.37s\tremaining: 722ms\n",
      "131:\tlearn: 0.0002178\ttotal: 1.38s\tremaining: 711ms\n",
      "132:\tlearn: 0.0002124\ttotal: 1.4s\tremaining: 705ms\n",
      "133:\tlearn: 0.0002000\ttotal: 1.42s\tremaining: 698ms\n",
      "134:\tlearn: 0.0002000\ttotal: 1.44s\tremaining: 691ms\n",
      "135:\tlearn: 0.0001972\ttotal: 1.45s\tremaining: 684ms\n",
      "136:\tlearn: 0.0001972\ttotal: 1.46s\tremaining: 673ms\n",
      "137:\tlearn: 0.0001944\ttotal: 1.47s\tremaining: 662ms\n",
      "138:\tlearn: 0.0001913\ttotal: 1.49s\tremaining: 652ms\n",
      "139:\tlearn: 0.0001837\ttotal: 1.5s\tremaining: 641ms\n",
      "140:\tlearn: 0.0001837\ttotal: 1.5s\tremaining: 630ms\n",
      "141:\tlearn: 0.0001837\ttotal: 1.51s\tremaining: 619ms\n",
      "142:\tlearn: 0.0001837\ttotal: 1.53s\tremaining: 608ms\n",
      "143:\tlearn: 0.0001812\ttotal: 1.54s\tremaining: 598ms\n",
      "144:\tlearn: 0.0001812\ttotal: 1.55s\tremaining: 587ms\n",
      "145:\tlearn: 0.0001812\ttotal: 1.56s\tremaining: 577ms\n",
      "146:\tlearn: 0.0001812\ttotal: 1.57s\tremaining: 566ms\n",
      "147:\tlearn: 0.0001812\ttotal: 1.58s\tremaining: 555ms\n",
      "148:\tlearn: 0.0001745\ttotal: 1.59s\tremaining: 545ms\n",
      "149:\tlearn: 0.0001652\ttotal: 1.6s\tremaining: 534ms\n",
      "150:\tlearn: 0.0001652\ttotal: 1.61s\tremaining: 523ms\n",
      "151:\tlearn: 0.0001652\ttotal: 1.62s\tremaining: 512ms\n",
      "152:\tlearn: 0.0001652\ttotal: 1.63s\tremaining: 501ms\n",
      "153:\tlearn: 0.0001652\ttotal: 1.64s\tremaining: 491ms\n",
      "154:\tlearn: 0.0001652\ttotal: 1.65s\tremaining: 480ms\n",
      "155:\tlearn: 0.0001603\ttotal: 1.66s\tremaining: 470ms\n",
      "156:\tlearn: 0.0001603\ttotal: 1.67s\tremaining: 459ms\n",
      "157:\tlearn: 0.0001603\ttotal: 1.68s\tremaining: 448ms\n",
      "158:\tlearn: 0.0001578\ttotal: 1.7s\tremaining: 437ms\n",
      "159:\tlearn: 0.0001578\ttotal: 1.71s\tremaining: 426ms\n",
      "160:\tlearn: 0.0001578\ttotal: 1.72s\tremaining: 416ms\n",
      "161:\tlearn: 0.0001578\ttotal: 1.73s\tremaining: 405ms\n",
      "162:\tlearn: 0.0001578\ttotal: 1.74s\tremaining: 395ms\n",
      "163:\tlearn: 0.0001578\ttotal: 1.75s\tremaining: 384ms\n",
      "164:\tlearn: 0.0001578\ttotal: 1.76s\tremaining: 373ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165:\tlearn: 0.0001578\ttotal: 1.77s\tremaining: 362ms\n",
      "166:\tlearn: 0.0001578\ttotal: 1.78s\tremaining: 352ms\n",
      "167:\tlearn: 0.0001578\ttotal: 1.79s\tremaining: 341ms\n",
      "168:\tlearn: 0.0001578\ttotal: 1.8s\tremaining: 330ms\n",
      "169:\tlearn: 0.0001578\ttotal: 1.81s\tremaining: 319ms\n",
      "170:\tlearn: 0.0001578\ttotal: 1.82s\tremaining: 309ms\n",
      "171:\tlearn: 0.0001578\ttotal: 1.83s\tremaining: 298ms\n",
      "172:\tlearn: 0.0001578\ttotal: 1.84s\tremaining: 287ms\n",
      "173:\tlearn: 0.0001578\ttotal: 1.85s\tremaining: 276ms\n",
      "174:\tlearn: 0.0001578\ttotal: 1.86s\tremaining: 266ms\n",
      "175:\tlearn: 0.0001578\ttotal: 1.87s\tremaining: 255ms\n",
      "176:\tlearn: 0.0001578\ttotal: 1.88s\tremaining: 244ms\n",
      "177:\tlearn: 0.0001578\ttotal: 1.89s\tremaining: 233ms\n",
      "178:\tlearn: 0.0001578\ttotal: 1.9s\tremaining: 223ms\n",
      "179:\tlearn: 0.0001578\ttotal: 1.91s\tremaining: 212ms\n",
      "180:\tlearn: 0.0001578\ttotal: 1.92s\tremaining: 201ms\n",
      "181:\tlearn: 0.0001578\ttotal: 1.93s\tremaining: 191ms\n",
      "182:\tlearn: 0.0001578\ttotal: 1.94s\tremaining: 180ms\n",
      "183:\tlearn: 0.0001578\ttotal: 1.95s\tremaining: 169ms\n",
      "184:\tlearn: 0.0001578\ttotal: 1.96s\tremaining: 159ms\n",
      "185:\tlearn: 0.0001578\ttotal: 1.97s\tremaining: 148ms\n",
      "186:\tlearn: 0.0001578\ttotal: 1.98s\tremaining: 138ms\n",
      "187:\tlearn: 0.0001578\ttotal: 1.99s\tremaining: 127ms\n",
      "188:\tlearn: 0.0001578\ttotal: 2s\tremaining: 116ms\n",
      "189:\tlearn: 0.0001578\ttotal: 2.01s\tremaining: 106ms\n",
      "190:\tlearn: 0.0001560\ttotal: 2.02s\tremaining: 95.1ms\n",
      "191:\tlearn: 0.0001560\ttotal: 2.03s\tremaining: 84.5ms\n",
      "192:\tlearn: 0.0001560\ttotal: 2.04s\tremaining: 73.9ms\n",
      "193:\tlearn: 0.0001540\ttotal: 2.05s\tremaining: 63.4ms\n",
      "194:\tlearn: 0.0001540\ttotal: 2.06s\tremaining: 52.8ms\n",
      "195:\tlearn: 0.0001540\ttotal: 2.07s\tremaining: 42.2ms\n",
      "196:\tlearn: 0.0001540\ttotal: 2.08s\tremaining: 31.6ms\n",
      "197:\tlearn: 0.0001540\ttotal: 2.09s\tremaining: 21.1ms\n",
      "198:\tlearn: 0.0001540\ttotal: 2.1s\tremaining: 10.5ms\n",
      "199:\tlearn: 0.0001519\ttotal: 2.11s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0c975d8d424d06bcad8b1d99293735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6292398259226837, Recall = 0.46744499326448136, Aging Rate = 0.29720670391061454, Precision = 0.7827067669172932, f1 = 0.5853247118358167\n",
      "Epoch 2: Train Loss = 0.4710157913868654, Recall = 0.7642568477772789, Aging Rate = 0.45631284916201115, Precision = 0.8334965719882468, f1 = 0.7973764347622394\n",
      "Epoch 3: Train Loss = 0.35989061253696847, Recall = 0.8396946564885496, Aging Rate = 0.48089385474860336, Precision = 0.8689591078066915, f1 = 0.8540762731217173\n",
      "Epoch 4: Train Loss = 0.2930502713592359, Recall = 0.874719353390211, Aging Rate = 0.48044692737430167, Precision = 0.906046511627907, f1 = 0.8901073794836646\n",
      "Epoch 5: Train Loss = 0.24027113701711153, Recall = 0.9097440502918724, Aging Rate = 0.4835754189944134, Precision = 0.9362292051756007, f1 = 0.9227966294693691\n",
      "Test Loss = 0.20133168252819744, Recall = 0.9577907498877414, Aging Rate = 0.5032402234636871, precision = 0.9471580817051509\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1862772888477954, Recall = 0.9541984732824428, Aging Rate = 0.4969832402234637, Precision = 0.9554856115107914, f1 = 0.9548416086272747\n",
      "Epoch 7: Train Loss = 0.15220641533422735, Recall = 0.9609339919173776, Aging Rate = 0.495195530726257, Precision = 0.9657039711191335, f1 = 0.9633130767499437\n",
      "Epoch 8: Train Loss = 0.12363740482476837, Recall = 0.9744050291872475, Aging Rate = 0.4943016759776536, Precision = 0.9810126582278481, f1 = 0.9776976796575806\n",
      "Epoch 9: Train Loss = 0.10272909353565238, Recall = 0.979344409519533, Aging Rate = 0.4949720670391061, Precision = 0.9846501128668171, f1 = 0.9819900945520036\n",
      "Epoch 10: Train Loss = 0.08625359482355624, Recall = 0.9842837898518186, Aging Rate = 0.4963128491620112, Precision = 0.9869428185502026, f1 = 0.9856115107913669\n",
      "Test Loss = 0.07498919526458453, Recall = 0.9865289627301302, Aging Rate = 0.4945251396648045, precision = 0.9927699954812472\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.0718972759606452, Recall = 0.9860799281544679, Aging Rate = 0.4943016759776536, Precision = 0.9927667269439421, f1 = 0.9894120297364271\n",
      "Epoch 12: Train Loss = 0.061385318511191694, Recall = 0.9892231701841042, Aging Rate = 0.4956424581005587, Precision = 0.9932371505861136, f1 = 0.9912260967379078\n",
      "Epoch 13: Train Loss = 0.05237227165998693, Recall = 0.9919173776380781, Aging Rate = 0.4958659217877095, Precision = 0.9954934655250113, f1 = 0.9937022042285201\n",
      "Epoch 14: Train Loss = 0.045294019367608275, Recall = 0.994611585092052, Aging Rate = 0.4969832402234637, Precision = 0.9959532374100719, f1 = 0.9952819591103123\n",
      "Epoch 15: Train Loss = 0.039420968878535585, Recall = 0.995958688819039, Aging Rate = 0.49743016759776537, Precision = 0.9964061096136568, f1 = 0.996182348978217\n",
      "Test Loss = 0.03502989342842022, Recall = 0.998652896273013, Aging Rate = 0.49854748603351956, precision = 0.9968623935454953\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03466816706822238, Recall = 0.997305792546026, Aging Rate = 0.4981005586592179, Precision = 0.9964109466128309, f1 = 0.9968581687612209\n",
      "Epoch 17: Train Loss = 0.031046875516712334, Recall = 0.998652896273013, Aging Rate = 0.4983240223463687, Precision = 0.9973094170403587, f1 = 0.9979807045097598\n",
      "Epoch 18: Train Loss = 0.027284743743818566, Recall = 0.9982038616973506, Aging Rate = 0.4983240223463687, Precision = 0.9968609865470852, f1 = 0.9975319721785955\n",
      "Epoch 19: Train Loss = 0.024148150500139045, Recall = 0.9991019308486754, Aging Rate = 0.4981005586592179, Precision = 0.9982054733064154, f1 = 0.998653500897666\n",
      "Epoch 20: Train Loss = 0.021688762496137087, Recall = 0.998652896273013, Aging Rate = 0.4976536312849162, Precision = 0.998652896273013, f1 = 0.998652896273013\n",
      "Test Loss = 0.019605598772288034, Recall = 0.9991019308486754, Aging Rate = 0.4972067039106145, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.019708873095958594, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Epoch 22: Train Loss = 0.017770628364725486, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 23: Train Loss = 0.016128043879926536, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 24: Train Loss = 0.014769886988917543, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 25: Train Loss = 0.013742964577408477, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.012291654529226892, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.012392519562812157, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 27: Train Loss = 0.011211418443302203, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.010449825789800237, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 29: Train Loss = 0.009731974090402686, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 30: Train Loss = 0.009038175146756226, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.008233215060792656, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "\n",
      "Epoch 31: Train Loss = 0.00859653200610806, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 32: Train Loss = 0.008001517947013984, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 33: Train Loss = 0.007500498664861951, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.0069905585275349, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.006546506308352148, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00600841690583269, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.006149293045362447, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.005928886208225396, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.0056312342598909436, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.005327783728944523, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.005009814281869867, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004745742180931002, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.004697760968526315, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.004575983502137261, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.004360579831848265, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.004224860864852727, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.003928270984763623, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0038196725898822924, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.0038183997523601495, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.003712053292446546, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0034918858290925703, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0034038064352564473, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0032901078464681877, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003127977842069621, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.00321005916992962, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.0030648403170430126, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0031387535101271044, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.002882558738438194, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.002808418278348263, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026165755977073696, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0027771895402241187, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002663381667287406, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.002649461184758381, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.002541134032113842, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0025389287487588117, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002347700192551415, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0024397101450201603, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.002383538516493275, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.002424890010008932, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.002307183035550504, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0022610409243563035, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021790853142738343, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0022422512316576846, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0022315248762720457, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.002159290401337866, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0021762803424794914, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0020657643403691455, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019369136065494415, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0020727688605850303, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0020086756040076804, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.002050870188912533, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0020153728763276303, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0019390045539814168, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018482394431920607, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0019506110333936423, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.001956618752554267, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.001969981221976435, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0018839691320236501, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0018920071579661617, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001743972605463721, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0019071973894603093, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0017984732776859453, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0018564924222609683, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0019006614147180786, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0018610156066299485, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017521130696053528, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5067609\ttotal: 11.6ms\tremaining: 2.3s\n",
      "1:\tlearn: 0.4184714\ttotal: 23.4ms\tremaining: 2.32s\n",
      "2:\tlearn: 0.3157484\ttotal: 35.3ms\tremaining: 2.32s\n",
      "3:\tlearn: 0.2715129\ttotal: 47.2ms\tremaining: 2.31s\n",
      "4:\tlearn: 0.2347590\ttotal: 58.9ms\tremaining: 2.3s\n",
      "5:\tlearn: 0.2062593\ttotal: 70.8ms\tremaining: 2.29s\n",
      "6:\tlearn: 0.1709385\ttotal: 82.9ms\tremaining: 2.29s\n",
      "7:\tlearn: 0.1472099\ttotal: 92.7ms\tremaining: 2.22s\n",
      "8:\tlearn: 0.1313059\ttotal: 103ms\tremaining: 2.18s\n",
      "9:\tlearn: 0.1190415\ttotal: 112ms\tremaining: 2.13s\n",
      "10:\tlearn: 0.1072787\ttotal: 123ms\tremaining: 2.11s\n",
      "11:\tlearn: 0.0955967\ttotal: 132ms\tremaining: 2.08s\n",
      "12:\tlearn: 0.0901431\ttotal: 143ms\tremaining: 2.06s\n",
      "13:\tlearn: 0.0835391\ttotal: 154ms\tremaining: 2.05s\n",
      "14:\tlearn: 0.0764446\ttotal: 164ms\tremaining: 2.02s\n",
      "15:\tlearn: 0.0671548\ttotal: 175ms\tremaining: 2.01s\n",
      "16:\tlearn: 0.0631302\ttotal: 185ms\tremaining: 1.99s\n",
      "17:\tlearn: 0.0607038\ttotal: 196ms\tremaining: 1.99s\n",
      "18:\tlearn: 0.0558285\ttotal: 207ms\tremaining: 1.98s\n",
      "19:\tlearn: 0.0493650\ttotal: 219ms\tremaining: 1.97s\n",
      "20:\tlearn: 0.0471653\ttotal: 229ms\tremaining: 1.96s\n",
      "21:\tlearn: 0.0437611\ttotal: 240ms\tremaining: 1.94s\n",
      "22:\tlearn: 0.0393807\ttotal: 250ms\tremaining: 1.93s\n",
      "23:\tlearn: 0.0376746\ttotal: 261ms\tremaining: 1.91s\n",
      "24:\tlearn: 0.0347404\ttotal: 271ms\tremaining: 1.89s\n",
      "25:\tlearn: 0.0308048\ttotal: 281ms\tremaining: 1.88s\n",
      "26:\tlearn: 0.0286512\ttotal: 291ms\tremaining: 1.87s\n",
      "27:\tlearn: 0.0266658\ttotal: 301ms\tremaining: 1.85s\n",
      "28:\tlearn: 0.0238274\ttotal: 311ms\tremaining: 1.83s\n",
      "29:\tlearn: 0.0226224\ttotal: 320ms\tremaining: 1.81s\n",
      "30:\tlearn: 0.0211328\ttotal: 330ms\tremaining: 1.8s\n",
      "31:\tlearn: 0.0197312\ttotal: 340ms\tremaining: 1.78s\n",
      "32:\tlearn: 0.0187867\ttotal: 351ms\tremaining: 1.77s\n",
      "33:\tlearn: 0.0172306\ttotal: 361ms\tremaining: 1.76s\n",
      "34:\tlearn: 0.0159518\ttotal: 371ms\tremaining: 1.75s\n",
      "35:\tlearn: 0.0147434\ttotal: 380ms\tremaining: 1.73s\n",
      "36:\tlearn: 0.0140237\ttotal: 391ms\tremaining: 1.72s\n",
      "37:\tlearn: 0.0127818\ttotal: 411ms\tremaining: 1.75s\n",
      "38:\tlearn: 0.0122977\ttotal: 427ms\tremaining: 1.76s\n",
      "39:\tlearn: 0.0119144\ttotal: 439ms\tremaining: 1.75s\n",
      "40:\tlearn: 0.0110478\ttotal: 450ms\tremaining: 1.74s\n",
      "41:\tlearn: 0.0106670\ttotal: 460ms\tremaining: 1.73s\n",
      "42:\tlearn: 0.0096679\ttotal: 471ms\tremaining: 1.72s\n",
      "43:\tlearn: 0.0092109\ttotal: 481ms\tremaining: 1.71s\n",
      "44:\tlearn: 0.0088447\ttotal: 491ms\tremaining: 1.69s\n",
      "45:\tlearn: 0.0079445\ttotal: 501ms\tremaining: 1.68s\n",
      "46:\tlearn: 0.0075902\ttotal: 511ms\tremaining: 1.66s\n",
      "47:\tlearn: 0.0067217\ttotal: 521ms\tremaining: 1.65s\n",
      "48:\tlearn: 0.0059841\ttotal: 531ms\tremaining: 1.64s\n",
      "49:\tlearn: 0.0054979\ttotal: 541ms\tremaining: 1.62s\n",
      "50:\tlearn: 0.0053213\ttotal: 552ms\tremaining: 1.61s\n",
      "51:\tlearn: 0.0049412\ttotal: 563ms\tremaining: 1.6s\n",
      "52:\tlearn: 0.0047969\ttotal: 574ms\tremaining: 1.59s\n",
      "53:\tlearn: 0.0046415\ttotal: 586ms\tremaining: 1.58s\n",
      "54:\tlearn: 0.0042584\ttotal: 596ms\tremaining: 1.57s\n",
      "55:\tlearn: 0.0040290\ttotal: 608ms\tremaining: 1.56s\n",
      "56:\tlearn: 0.0037639\ttotal: 619ms\tremaining: 1.55s\n",
      "57:\tlearn: 0.0035064\ttotal: 629ms\tremaining: 1.54s\n",
      "58:\tlearn: 0.0033170\ttotal: 641ms\tremaining: 1.53s\n",
      "59:\tlearn: 0.0032472\ttotal: 651ms\tremaining: 1.52s\n",
      "60:\tlearn: 0.0029522\ttotal: 663ms\tremaining: 1.51s\n",
      "61:\tlearn: 0.0028893\ttotal: 673ms\tremaining: 1.5s\n",
      "62:\tlearn: 0.0027670\ttotal: 684ms\tremaining: 1.49s\n",
      "63:\tlearn: 0.0025247\ttotal: 695ms\tremaining: 1.48s\n",
      "64:\tlearn: 0.0024178\ttotal: 705ms\tremaining: 1.47s\n",
      "65:\tlearn: 0.0023078\ttotal: 715ms\tremaining: 1.45s\n",
      "66:\tlearn: 0.0022033\ttotal: 726ms\tremaining: 1.44s\n",
      "67:\tlearn: 0.0020611\ttotal: 737ms\tremaining: 1.43s\n",
      "68:\tlearn: 0.0019291\ttotal: 749ms\tremaining: 1.42s\n",
      "69:\tlearn: 0.0018309\ttotal: 759ms\tremaining: 1.41s\n",
      "70:\tlearn: 0.0017673\ttotal: 770ms\tremaining: 1.4s\n",
      "71:\tlearn: 0.0017081\ttotal: 781ms\tremaining: 1.39s\n",
      "72:\tlearn: 0.0015982\ttotal: 791ms\tremaining: 1.38s\n",
      "73:\tlearn: 0.0015171\ttotal: 802ms\tremaining: 1.36s\n",
      "74:\tlearn: 0.0014341\ttotal: 813ms\tremaining: 1.35s\n",
      "75:\tlearn: 0.0014127\ttotal: 824ms\tremaining: 1.34s\n",
      "76:\tlearn: 0.0013361\ttotal: 834ms\tremaining: 1.33s\n",
      "77:\tlearn: 0.0012928\ttotal: 844ms\tremaining: 1.32s\n",
      "78:\tlearn: 0.0012281\ttotal: 854ms\tremaining: 1.31s\n",
      "79:\tlearn: 0.0011733\ttotal: 864ms\tremaining: 1.3s\n",
      "80:\tlearn: 0.0011242\ttotal: 875ms\tremaining: 1.28s\n",
      "81:\tlearn: 0.0010985\ttotal: 885ms\tremaining: 1.27s\n",
      "82:\tlearn: 0.0010466\ttotal: 896ms\tremaining: 1.26s\n",
      "83:\tlearn: 0.0010018\ttotal: 906ms\tremaining: 1.25s\n",
      "84:\tlearn: 0.0009703\ttotal: 916ms\tremaining: 1.24s\n",
      "85:\tlearn: 0.0009420\ttotal: 926ms\tremaining: 1.23s\n",
      "86:\tlearn: 0.0009101\ttotal: 936ms\tremaining: 1.22s\n",
      "87:\tlearn: 0.0008821\ttotal: 947ms\tremaining: 1.21s\n",
      "88:\tlearn: 0.0008241\ttotal: 957ms\tremaining: 1.19s\n",
      "89:\tlearn: 0.0008014\ttotal: 967ms\tremaining: 1.18s\n",
      "90:\tlearn: 0.0007626\ttotal: 977ms\tremaining: 1.17s\n",
      "91:\tlearn: 0.0007261\ttotal: 988ms\tremaining: 1.16s\n",
      "92:\tlearn: 0.0007139\ttotal: 998ms\tremaining: 1.15s\n",
      "93:\tlearn: 0.0006664\ttotal: 1.01s\tremaining: 1.14s\n",
      "94:\tlearn: 0.0006517\ttotal: 1.02s\tremaining: 1.13s\n",
      "95:\tlearn: 0.0006283\ttotal: 1.03s\tremaining: 1.11s\n",
      "96:\tlearn: 0.0005972\ttotal: 1.04s\tremaining: 1.1s\n",
      "97:\tlearn: 0.0005793\ttotal: 1.05s\tremaining: 1.09s\n",
      "98:\tlearn: 0.0005459\ttotal: 1.06s\tremaining: 1.08s\n",
      "99:\tlearn: 0.0005391\ttotal: 1.07s\tremaining: 1.07s\n",
      "100:\tlearn: 0.0005288\ttotal: 1.08s\tremaining: 1.06s\n",
      "101:\tlearn: 0.0005160\ttotal: 1.09s\tremaining: 1.05s\n",
      "102:\tlearn: 0.0004930\ttotal: 1.1s\tremaining: 1.04s\n",
      "103:\tlearn: 0.0004850\ttotal: 1.11s\tremaining: 1.02s\n",
      "104:\tlearn: 0.0004730\ttotal: 1.12s\tremaining: 1.01s\n",
      "105:\tlearn: 0.0004599\ttotal: 1.13s\tremaining: 1s\n",
      "106:\tlearn: 0.0004455\ttotal: 1.14s\tremaining: 991ms\n",
      "107:\tlearn: 0.0004322\ttotal: 1.15s\tremaining: 981ms\n",
      "108:\tlearn: 0.0004222\ttotal: 1.16s\tremaining: 970ms\n",
      "109:\tlearn: 0.0004086\ttotal: 1.17s\tremaining: 959ms\n",
      "110:\tlearn: 0.0003948\ttotal: 1.18s\tremaining: 948ms\n",
      "111:\tlearn: 0.0003865\ttotal: 1.19s\tremaining: 936ms\n",
      "112:\tlearn: 0.0003781\ttotal: 1.2s\tremaining: 925ms\n",
      "113:\tlearn: 0.0003596\ttotal: 1.21s\tremaining: 914ms\n",
      "114:\tlearn: 0.0003488\ttotal: 1.22s\tremaining: 903ms\n",
      "115:\tlearn: 0.0003444\ttotal: 1.23s\tremaining: 892ms\n",
      "116:\tlearn: 0.0003364\ttotal: 1.24s\tremaining: 881ms\n",
      "117:\tlearn: 0.0003221\ttotal: 1.25s\tremaining: 871ms\n",
      "118:\tlearn: 0.0003221\ttotal: 1.26s\tremaining: 860ms\n",
      "119:\tlearn: 0.0003114\ttotal: 1.27s\tremaining: 849ms\n",
      "120:\tlearn: 0.0003069\ttotal: 1.28s\tremaining: 838ms\n",
      "121:\tlearn: 0.0002932\ttotal: 1.29s\tremaining: 827ms\n",
      "122:\tlearn: 0.0002932\ttotal: 1.3s\tremaining: 815ms\n",
      "123:\tlearn: 0.0002856\ttotal: 1.31s\tremaining: 805ms\n",
      "124:\tlearn: 0.0002775\ttotal: 1.32s\tremaining: 794ms\n",
      "125:\tlearn: 0.0002661\ttotal: 1.33s\tremaining: 783ms\n",
      "126:\tlearn: 0.0002661\ttotal: 1.34s\tremaining: 771ms\n",
      "127:\tlearn: 0.0002594\ttotal: 1.35s\tremaining: 760ms\n",
      "128:\tlearn: 0.0002511\ttotal: 1.36s\tremaining: 749ms\n",
      "129:\tlearn: 0.0002485\ttotal: 1.37s\tremaining: 738ms\n",
      "130:\tlearn: 0.0002422\ttotal: 1.39s\tremaining: 730ms\n",
      "131:\tlearn: 0.0002321\ttotal: 1.4s\tremaining: 723ms\n",
      "132:\tlearn: 0.0002321\ttotal: 1.42s\tremaining: 715ms\n",
      "133:\tlearn: 0.0002287\ttotal: 1.44s\tremaining: 708ms\n",
      "134:\tlearn: 0.0002287\ttotal: 1.45s\tremaining: 700ms\n",
      "135:\tlearn: 0.0002222\ttotal: 1.47s\tremaining: 690ms\n",
      "136:\tlearn: 0.0002222\ttotal: 1.48s\tremaining: 679ms\n",
      "137:\tlearn: 0.0002222\ttotal: 1.49s\tremaining: 669ms\n",
      "138:\tlearn: 0.0002222\ttotal: 1.5s\tremaining: 658ms\n",
      "139:\tlearn: 0.0002222\ttotal: 1.51s\tremaining: 647ms\n",
      "140:\tlearn: 0.0002222\ttotal: 1.52s\tremaining: 636ms\n",
      "141:\tlearn: 0.0002222\ttotal: 1.53s\tremaining: 625ms\n",
      "142:\tlearn: 0.0002222\ttotal: 1.54s\tremaining: 614ms\n",
      "143:\tlearn: 0.0002222\ttotal: 1.55s\tremaining: 603ms\n",
      "144:\tlearn: 0.0002222\ttotal: 1.56s\tremaining: 592ms\n",
      "145:\tlearn: 0.0002166\ttotal: 1.57s\tremaining: 581ms\n",
      "146:\tlearn: 0.0002090\ttotal: 1.58s\tremaining: 571ms\n",
      "147:\tlearn: 0.0002090\ttotal: 1.59s\tremaining: 559ms\n",
      "148:\tlearn: 0.0002090\ttotal: 1.6s\tremaining: 548ms\n",
      "149:\tlearn: 0.0002050\ttotal: 1.61s\tremaining: 537ms\n",
      "150:\tlearn: 0.0002023\ttotal: 1.62s\tremaining: 526ms\n",
      "151:\tlearn: 0.0002023\ttotal: 1.63s\tremaining: 515ms\n",
      "152:\tlearn: 0.0002023\ttotal: 1.64s\tremaining: 504ms\n",
      "153:\tlearn: 0.0001949\ttotal: 1.65s\tremaining: 493ms\n",
      "154:\tlearn: 0.0001949\ttotal: 1.66s\tremaining: 482ms\n",
      "155:\tlearn: 0.0001949\ttotal: 1.67s\tremaining: 472ms\n",
      "156:\tlearn: 0.0001949\ttotal: 1.68s\tremaining: 461ms\n",
      "157:\tlearn: 0.0001949\ttotal: 1.69s\tremaining: 450ms\n",
      "158:\tlearn: 0.0001949\ttotal: 1.7s\tremaining: 439ms\n",
      "159:\tlearn: 0.0001949\ttotal: 1.71s\tremaining: 428ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160:\tlearn: 0.0001907\ttotal: 1.72s\tremaining: 417ms\n",
      "161:\tlearn: 0.0001907\ttotal: 1.73s\tremaining: 406ms\n",
      "162:\tlearn: 0.0001907\ttotal: 1.74s\tremaining: 396ms\n",
      "163:\tlearn: 0.0001830\ttotal: 1.75s\tremaining: 385ms\n",
      "164:\tlearn: 0.0001830\ttotal: 1.76s\tremaining: 374ms\n",
      "165:\tlearn: 0.0001830\ttotal: 1.77s\tremaining: 363ms\n",
      "166:\tlearn: 0.0001830\ttotal: 1.78s\tremaining: 352ms\n",
      "167:\tlearn: 0.0001830\ttotal: 1.79s\tremaining: 341ms\n",
      "168:\tlearn: 0.0001830\ttotal: 1.8s\tremaining: 331ms\n",
      "169:\tlearn: 0.0001830\ttotal: 1.81s\tremaining: 320ms\n",
      "170:\tlearn: 0.0001830\ttotal: 1.82s\tremaining: 309ms\n",
      "171:\tlearn: 0.0001830\ttotal: 1.83s\tremaining: 298ms\n",
      "172:\tlearn: 0.0001830\ttotal: 1.84s\tremaining: 287ms\n",
      "173:\tlearn: 0.0001830\ttotal: 1.85s\tremaining: 277ms\n",
      "174:\tlearn: 0.0001830\ttotal: 1.86s\tremaining: 266ms\n",
      "175:\tlearn: 0.0001830\ttotal: 1.87s\tremaining: 255ms\n",
      "176:\tlearn: 0.0001830\ttotal: 1.88s\tremaining: 244ms\n",
      "177:\tlearn: 0.0001807\ttotal: 1.89s\tremaining: 233ms\n",
      "178:\tlearn: 0.0001807\ttotal: 1.9s\tremaining: 223ms\n",
      "179:\tlearn: 0.0001807\ttotal: 1.91s\tremaining: 212ms\n",
      "180:\tlearn: 0.0001807\ttotal: 1.92s\tremaining: 202ms\n",
      "181:\tlearn: 0.0001807\ttotal: 1.93s\tremaining: 191ms\n",
      "182:\tlearn: 0.0001807\ttotal: 1.94s\tremaining: 180ms\n",
      "183:\tlearn: 0.0001807\ttotal: 1.95s\tremaining: 170ms\n",
      "184:\tlearn: 0.0001807\ttotal: 1.96s\tremaining: 159ms\n",
      "185:\tlearn: 0.0001806\ttotal: 1.97s\tremaining: 148ms\n",
      "186:\tlearn: 0.0001806\ttotal: 1.98s\tremaining: 138ms\n",
      "187:\tlearn: 0.0001806\ttotal: 1.99s\tremaining: 127ms\n",
      "188:\tlearn: 0.0001806\ttotal: 2s\tremaining: 116ms\n",
      "189:\tlearn: 0.0001806\ttotal: 2.01s\tremaining: 106ms\n",
      "190:\tlearn: 0.0001806\ttotal: 2.02s\tremaining: 95.2ms\n",
      "191:\tlearn: 0.0001806\ttotal: 2.03s\tremaining: 84.6ms\n",
      "192:\tlearn: 0.0001806\ttotal: 2.04s\tremaining: 74ms\n",
      "193:\tlearn: 0.0001806\ttotal: 2.05s\tremaining: 63.4ms\n",
      "194:\tlearn: 0.0001806\ttotal: 2.06s\tremaining: 52.8ms\n",
      "195:\tlearn: 0.0001806\ttotal: 2.07s\tremaining: 42.3ms\n",
      "196:\tlearn: 0.0001806\ttotal: 2.08s\tremaining: 31.7ms\n",
      "197:\tlearn: 0.0001806\ttotal: 2.09s\tremaining: 21.1ms\n",
      "198:\tlearn: 0.0001789\ttotal: 2.1s\tremaining: 10.6ms\n",
      "199:\tlearn: 0.0001789\ttotal: 2.11s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857b6468283f4d30a7c9be702a56038f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6304474763896878, Recall = 0.3592276605298608, Aging Rate = 0.21273743016759777, Precision = 0.8403361344537815, f1 = 0.5033029254482542\n",
      "Epoch 2: Train Loss = 0.45968786740436235, Recall = 0.7674000898069151, Aging Rate = 0.4507262569832402, Precision = 0.8472979672781359, f1 = 0.8053722902921772\n",
      "Epoch 3: Train Loss = 0.35454070650665453, Recall = 0.8414907947911989, Aging Rate = 0.47910614525139666, Precision = 0.8740671641791045, f1 = 0.8574696865705788\n",
      "Epoch 4: Train Loss = 0.28854920549765645, Recall = 0.8796587337224966, Aging Rate = 0.4829050279329609, Precision = 0.906524757056918, f1 = 0.8928896991795806\n",
      "Epoch 5: Train Loss = 0.24119450524865582, Recall = 0.9097440502918724, Aging Rate = 0.4911731843575419, Precision = 0.921747042766151, f1 = 0.9157062146892655\n",
      "Test Loss = 0.20645510909277634, Recall = 0.9137853614728334, Aging Rate = 0.47307262569832403, precision = 0.9612659423712802\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.19087741826166654, Recall = 0.9317467444993265, Aging Rate = 0.4864804469273743, Precision = 0.9531465319246669, f1 = 0.9423251589464123\n",
      "Epoch 7: Train Loss = 0.1601525950931304, Recall = 0.9524023349797934, Aging Rate = 0.49206703910614524, Precision = 0.9632152588555858, f1 = 0.9577782795213367\n",
      "Epoch 8: Train Loss = 0.13124260825984305, Recall = 0.9681185451279749, Aging Rate = 0.4956424581005587, Precision = 0.9720468890892696, f1 = 0.9700787401574803\n",
      "Epoch 9: Train Loss = 0.10974122930338928, Recall = 0.9721598563089358, Aging Rate = 0.4936312849162011, Precision = 0.9800814848347669, f1 = 0.9761045987376015\n",
      "Epoch 10: Train Loss = 0.09277083331313213, Recall = 0.9802424786708577, Aging Rate = 0.4960893854748603, Precision = 0.9833333333333333, f1 = 0.9817854733528221\n",
      "Test Loss = 0.0846576080641933, Recall = 0.9914683430624158, Aging Rate = 0.503463687150838, precision = 0.9800266311584553\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07874982115276699, Recall = 0.9883251010327795, Aging Rate = 0.4987709497206704, Precision = 0.9861111111111112, f1 = 0.9872168647678852\n",
      "Epoch 12: Train Loss = 0.06663103152420267, Recall = 0.9919173776380781, Aging Rate = 0.4983240223463687, Precision = 0.9905829596412556, f1 = 0.991249719542293\n",
      "Epoch 13: Train Loss = 0.05819603210804183, Recall = 0.9905702739110912, Aging Rate = 0.4972067039106145, Precision = 0.9914606741573033, f1 = 0.991015274034142\n",
      "Epoch 14: Train Loss = 0.04991854906248647, Recall = 0.9932644813650651, Aging Rate = 0.4972067039106145, Precision = 0.9941573033707866, f1 = 0.9937106918238993\n",
      "Epoch 15: Train Loss = 0.04376330770677028, Recall = 0.9941625505163898, Aging Rate = 0.4976536312849162, Precision = 0.9941625505163898, f1 = 0.9941625505163898\n",
      "Test Loss = 0.03956999325802207, Recall = 0.9982038616973506, Aging Rate = 0.49966480446927375, precision = 0.9941860465116279\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03813368651454009, Recall = 0.9968567579703638, Aging Rate = 0.49787709497206706, Precision = 0.9964093357271095, f1 = 0.9966329966329966\n",
      "Epoch 17: Train Loss = 0.033925334140242146, Recall = 0.995958688819039, Aging Rate = 0.49675977653631287, Precision = 0.9977507872244714, f1 = 0.9968539325842697\n",
      "Epoch 18: Train Loss = 0.03054120563053885, Recall = 0.9982038616973506, Aging Rate = 0.4983240223463687, Precision = 0.9968609865470852, f1 = 0.9975319721785955\n",
      "Epoch 19: Train Loss = 0.02700300135842249, Recall = 0.9991019308486754, Aging Rate = 0.4983240223463687, Precision = 0.9977578475336323, f1 = 0.9984294368409243\n",
      "Epoch 20: Train Loss = 0.02441033887838185, Recall = 0.9995509654243376, Aging Rate = 0.4981005586592179, Precision = 0.9986541049798116, f1 = 0.9991023339317774\n",
      "Test Loss = 0.021759865518721788, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, precision = 0.9995509654243376\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.022124967717008885, Recall = 0.9995509654243376, Aging Rate = 0.49787709497206706, Precision = 0.9991023339317774, f1 = 0.9993265993265993\n",
      "Epoch 22: Train Loss = 0.01974285100675162, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Epoch 23: Train Loss = 0.017986357407280187, Recall = 0.9995509654243376, Aging Rate = 0.49787709497206706, Precision = 0.9991023339317774, f1 = 0.9993265993265993\n",
      "Epoch 24: Train Loss = 0.016787975994276935, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 25: Train Loss = 0.01500482620182317, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.013651205708361204, Recall = 1.0, Aging Rate = 0.4981005586592179, precision = 0.9991027366532077\n",
      "\n",
      "Epoch 26: Train Loss = 0.013873922462840487, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 27: Train Loss = 0.01277436549676447, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 28: Train Loss = 0.011709539967916864, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 29: Train Loss = 0.011062904096640356, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 30: Train Loss = 0.01021720542296684, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.009338345959699354, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.009416667053569628, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 32: Train Loss = 0.008743095581716332, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 33: Train Loss = 0.008267751821009807, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 34: Train Loss = 0.007807157839018872, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 35: Train Loss = 0.007204864403992558, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.0065212810841810435, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.006725467275411889, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 37: Train Loss = 0.006269263261252775, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 38: Train Loss = 0.005960859792962587, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 39: Train Loss = 0.005596781475833674, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.005346337276817867, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0049108757337753, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.005093184465323913, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.004859105324965782, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.004556258087821679, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.004342810421110282, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.004227458266385274, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004082509849595124, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.003983134750391268, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0038823191731044366, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0037011055892864084, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.003529305237569123, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss = 0.0034098028151224777, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003134687860449516, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0032796910125502078, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0031902389566013435, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0030350966378005855, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0030045061771804727, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0030655743766513617, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0027585640969015036, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0028582349299406374, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0027217343804951987, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.002651336458537665, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0025486121041825116, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.002517582795754408, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0023969384604367798, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.002507431867620882, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0023790163209131145, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.002303646649416099, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0023208217249985515, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.002276992142439804, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002083737877634609, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0022072870427659354, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0021332790522040103, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0021209832628169122, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.002108755699884375, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0020577180316911064, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019095421426372822, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0020509792874208207, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0020522178986562447, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0019798809151014302, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.001997291054212431, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.001973942229966503, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017571419453009047, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0018725955216526236, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0018653614246988846, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0018410160546529226, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0018507037609399365, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0018433465595310294, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018205573595019692, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0018242685659138184, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0017650764218567969, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0018429428950347548, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.001778861356784678, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0017323565966704622, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016089045894514332, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5232577\ttotal: 11.9ms\tremaining: 2.36s\n",
      "1:\tlearn: 0.4217341\ttotal: 23.6ms\tremaining: 2.33s\n",
      "2:\tlearn: 0.3304336\ttotal: 35.5ms\tremaining: 2.33s\n",
      "3:\tlearn: 0.2729652\ttotal: 47.5ms\tremaining: 2.33s\n",
      "4:\tlearn: 0.2340125\ttotal: 59.3ms\tremaining: 2.31s\n",
      "5:\tlearn: 0.2025759\ttotal: 71.3ms\tremaining: 2.31s\n",
      "6:\tlearn: 0.1703873\ttotal: 83.3ms\tremaining: 2.3s\n",
      "7:\tlearn: 0.1469643\ttotal: 93.3ms\tremaining: 2.24s\n",
      "8:\tlearn: 0.1264073\ttotal: 103ms\tremaining: 2.19s\n",
      "9:\tlearn: 0.1104515\ttotal: 113ms\tremaining: 2.15s\n",
      "10:\tlearn: 0.0993568\ttotal: 123ms\tremaining: 2.11s\n",
      "11:\tlearn: 0.0920043\ttotal: 133ms\tremaining: 2.08s\n",
      "12:\tlearn: 0.0819001\ttotal: 143ms\tremaining: 2.05s\n",
      "13:\tlearn: 0.0780919\ttotal: 153ms\tremaining: 2.03s\n",
      "14:\tlearn: 0.0657685\ttotal: 163ms\tremaining: 2.01s\n",
      "15:\tlearn: 0.0608241\ttotal: 173ms\tremaining: 1.99s\n",
      "16:\tlearn: 0.0561318\ttotal: 183ms\tremaining: 1.97s\n",
      "17:\tlearn: 0.0516311\ttotal: 193ms\tremaining: 1.95s\n",
      "18:\tlearn: 0.0463465\ttotal: 204ms\tremaining: 1.94s\n",
      "19:\tlearn: 0.0442385\ttotal: 214ms\tremaining: 1.93s\n",
      "20:\tlearn: 0.0413438\ttotal: 225ms\tremaining: 1.92s\n",
      "21:\tlearn: 0.0387420\ttotal: 236ms\tremaining: 1.91s\n",
      "22:\tlearn: 0.0360390\ttotal: 246ms\tremaining: 1.89s\n",
      "23:\tlearn: 0.0347242\ttotal: 256ms\tremaining: 1.88s\n",
      "24:\tlearn: 0.0300242\ttotal: 266ms\tremaining: 1.86s\n",
      "25:\tlearn: 0.0283807\ttotal: 277ms\tremaining: 1.85s\n",
      "26:\tlearn: 0.0273835\ttotal: 288ms\tremaining: 1.85s\n",
      "27:\tlearn: 0.0258583\ttotal: 299ms\tremaining: 1.84s\n",
      "28:\tlearn: 0.0248720\ttotal: 310ms\tremaining: 1.82s\n",
      "29:\tlearn: 0.0230069\ttotal: 320ms\tremaining: 1.81s\n",
      "30:\tlearn: 0.0204285\ttotal: 331ms\tremaining: 1.8s\n",
      "31:\tlearn: 0.0193345\ttotal: 341ms\tremaining: 1.79s\n",
      "32:\tlearn: 0.0179483\ttotal: 351ms\tremaining: 1.78s\n",
      "33:\tlearn: 0.0166417\ttotal: 362ms\tremaining: 1.77s\n",
      "34:\tlearn: 0.0162076\ttotal: 372ms\tremaining: 1.75s\n",
      "35:\tlearn: 0.0154513\ttotal: 383ms\tremaining: 1.74s\n",
      "36:\tlearn: 0.0146840\ttotal: 395ms\tremaining: 1.74s\n",
      "37:\tlearn: 0.0141481\ttotal: 414ms\tremaining: 1.76s\n",
      "38:\tlearn: 0.0135786\ttotal: 431ms\tremaining: 1.78s\n",
      "39:\tlearn: 0.0129048\ttotal: 443ms\tremaining: 1.77s\n",
      "40:\tlearn: 0.0120027\ttotal: 453ms\tremaining: 1.75s\n",
      "41:\tlearn: 0.0112638\ttotal: 462ms\tremaining: 1.74s\n",
      "42:\tlearn: 0.0100720\ttotal: 472ms\tremaining: 1.72s\n",
      "43:\tlearn: 0.0091448\ttotal: 483ms\tremaining: 1.71s\n",
      "44:\tlearn: 0.0088789\ttotal: 495ms\tremaining: 1.71s\n",
      "45:\tlearn: 0.0085325\ttotal: 505ms\tremaining: 1.69s\n",
      "46:\tlearn: 0.0081440\ttotal: 515ms\tremaining: 1.68s\n",
      "47:\tlearn: 0.0074184\ttotal: 525ms\tremaining: 1.66s\n",
      "48:\tlearn: 0.0071305\ttotal: 535ms\tremaining: 1.65s\n",
      "49:\tlearn: 0.0065625\ttotal: 545ms\tremaining: 1.63s\n",
      "50:\tlearn: 0.0063211\ttotal: 555ms\tremaining: 1.62s\n",
      "51:\tlearn: 0.0060574\ttotal: 565ms\tremaining: 1.61s\n",
      "52:\tlearn: 0.0057486\ttotal: 576ms\tremaining: 1.6s\n",
      "53:\tlearn: 0.0053170\ttotal: 587ms\tremaining: 1.59s\n",
      "54:\tlearn: 0.0051326\ttotal: 598ms\tremaining: 1.57s\n",
      "55:\tlearn: 0.0049766\ttotal: 609ms\tremaining: 1.56s\n",
      "56:\tlearn: 0.0047130\ttotal: 618ms\tremaining: 1.55s\n",
      "57:\tlearn: 0.0046258\ttotal: 629ms\tremaining: 1.54s\n",
      "58:\tlearn: 0.0043795\ttotal: 639ms\tremaining: 1.53s\n",
      "59:\tlearn: 0.0041518\ttotal: 650ms\tremaining: 1.52s\n",
      "60:\tlearn: 0.0040208\ttotal: 660ms\tremaining: 1.5s\n",
      "61:\tlearn: 0.0038739\ttotal: 671ms\tremaining: 1.49s\n",
      "62:\tlearn: 0.0036547\ttotal: 681ms\tremaining: 1.48s\n",
      "63:\tlearn: 0.0034849\ttotal: 691ms\tremaining: 1.47s\n",
      "64:\tlearn: 0.0033124\ttotal: 701ms\tremaining: 1.46s\n",
      "65:\tlearn: 0.0031031\ttotal: 711ms\tremaining: 1.44s\n",
      "66:\tlearn: 0.0029322\ttotal: 721ms\tremaining: 1.43s\n",
      "67:\tlearn: 0.0027677\ttotal: 733ms\tremaining: 1.42s\n",
      "68:\tlearn: 0.0026958\ttotal: 744ms\tremaining: 1.41s\n",
      "69:\tlearn: 0.0026253\ttotal: 755ms\tremaining: 1.4s\n",
      "70:\tlearn: 0.0024843\ttotal: 767ms\tremaining: 1.39s\n",
      "71:\tlearn: 0.0023813\ttotal: 778ms\tremaining: 1.38s\n",
      "72:\tlearn: 0.0023347\ttotal: 790ms\tremaining: 1.37s\n",
      "73:\tlearn: 0.0022416\ttotal: 801ms\tremaining: 1.36s\n",
      "74:\tlearn: 0.0020678\ttotal: 812ms\tremaining: 1.35s\n",
      "75:\tlearn: 0.0020123\ttotal: 823ms\tremaining: 1.34s\n",
      "76:\tlearn: 0.0019339\ttotal: 834ms\tremaining: 1.33s\n",
      "77:\tlearn: 0.0018543\ttotal: 845ms\tremaining: 1.32s\n",
      "78:\tlearn: 0.0017749\ttotal: 855ms\tremaining: 1.31s\n",
      "79:\tlearn: 0.0016963\ttotal: 866ms\tremaining: 1.3s\n",
      "80:\tlearn: 0.0016005\ttotal: 877ms\tremaining: 1.29s\n",
      "81:\tlearn: 0.0015559\ttotal: 887ms\tremaining: 1.28s\n",
      "82:\tlearn: 0.0014755\ttotal: 898ms\tremaining: 1.27s\n",
      "83:\tlearn: 0.0013854\ttotal: 910ms\tremaining: 1.26s\n",
      "84:\tlearn: 0.0013594\ttotal: 921ms\tremaining: 1.25s\n",
      "85:\tlearn: 0.0013297\ttotal: 931ms\tremaining: 1.23s\n",
      "86:\tlearn: 0.0012720\ttotal: 942ms\tremaining: 1.22s\n",
      "87:\tlearn: 0.0012242\ttotal: 952ms\tremaining: 1.21s\n",
      "88:\tlearn: 0.0011759\ttotal: 963ms\tremaining: 1.2s\n",
      "89:\tlearn: 0.0011212\ttotal: 975ms\tremaining: 1.19s\n",
      "90:\tlearn: 0.0010829\ttotal: 986ms\tremaining: 1.18s\n",
      "91:\tlearn: 0.0010394\ttotal: 996ms\tremaining: 1.17s\n",
      "92:\tlearn: 0.0009796\ttotal: 1.01s\tremaining: 1.16s\n",
      "93:\tlearn: 0.0009537\ttotal: 1.02s\tremaining: 1.15s\n",
      "94:\tlearn: 0.0009039\ttotal: 1.03s\tremaining: 1.14s\n",
      "95:\tlearn: 0.0008764\ttotal: 1.04s\tremaining: 1.12s\n",
      "96:\tlearn: 0.0008491\ttotal: 1.05s\tremaining: 1.11s\n",
      "97:\tlearn: 0.0007943\ttotal: 1.06s\tremaining: 1.1s\n",
      "98:\tlearn: 0.0007444\ttotal: 1.07s\tremaining: 1.09s\n",
      "99:\tlearn: 0.0007279\ttotal: 1.08s\tremaining: 1.08s\n",
      "100:\tlearn: 0.0007121\ttotal: 1.09s\tremaining: 1.07s\n",
      "101:\tlearn: 0.0006634\ttotal: 1.1s\tremaining: 1.06s\n",
      "102:\tlearn: 0.0006440\ttotal: 1.11s\tremaining: 1.04s\n",
      "103:\tlearn: 0.0006245\ttotal: 1.12s\tremaining: 1.03s\n",
      "104:\tlearn: 0.0005998\ttotal: 1.13s\tremaining: 1.02s\n",
      "105:\tlearn: 0.0005810\ttotal: 1.14s\tremaining: 1.01s\n",
      "106:\tlearn: 0.0005670\ttotal: 1.15s\tremaining: 1000ms\n",
      "107:\tlearn: 0.0005499\ttotal: 1.16s\tremaining: 989ms\n",
      "108:\tlearn: 0.0005273\ttotal: 1.17s\tremaining: 977ms\n",
      "109:\tlearn: 0.0005099\ttotal: 1.18s\tremaining: 966ms\n",
      "110:\tlearn: 0.0004991\ttotal: 1.19s\tremaining: 955ms\n",
      "111:\tlearn: 0.0004775\ttotal: 1.2s\tremaining: 944ms\n",
      "112:\tlearn: 0.0004614\ttotal: 1.21s\tremaining: 933ms\n",
      "113:\tlearn: 0.0004486\ttotal: 1.22s\tremaining: 922ms\n",
      "114:\tlearn: 0.0004366\ttotal: 1.23s\tremaining: 911ms\n",
      "115:\tlearn: 0.0004216\ttotal: 1.24s\tremaining: 900ms\n",
      "116:\tlearn: 0.0004084\ttotal: 1.25s\tremaining: 889ms\n",
      "117:\tlearn: 0.0003927\ttotal: 1.26s\tremaining: 878ms\n",
      "118:\tlearn: 0.0003777\ttotal: 1.27s\tremaining: 867ms\n",
      "119:\tlearn: 0.0003677\ttotal: 1.29s\tremaining: 857ms\n",
      "120:\tlearn: 0.0003601\ttotal: 1.3s\tremaining: 846ms\n",
      "121:\tlearn: 0.0003467\ttotal: 1.31s\tremaining: 835ms\n",
      "122:\tlearn: 0.0003396\ttotal: 1.31s\tremaining: 824ms\n",
      "123:\tlearn: 0.0003333\ttotal: 1.32s\tremaining: 812ms\n",
      "124:\tlearn: 0.0003242\ttotal: 1.33s\tremaining: 801ms\n",
      "125:\tlearn: 0.0003011\ttotal: 1.34s\tremaining: 790ms\n",
      "126:\tlearn: 0.0002951\ttotal: 1.35s\tremaining: 779ms\n",
      "127:\tlearn: 0.0002885\ttotal: 1.37s\tremaining: 769ms\n",
      "128:\tlearn: 0.0002764\ttotal: 1.38s\tremaining: 758ms\n",
      "129:\tlearn: 0.0002662\ttotal: 1.39s\tremaining: 747ms\n",
      "130:\tlearn: 0.0002577\ttotal: 1.4s\tremaining: 740ms\n",
      "131:\tlearn: 0.0002512\ttotal: 1.42s\tremaining: 734ms\n",
      "132:\tlearn: 0.0002512\ttotal: 1.44s\tremaining: 727ms\n",
      "133:\tlearn: 0.0002512\ttotal: 1.46s\tremaining: 720ms\n",
      "134:\tlearn: 0.0002435\ttotal: 1.48s\tremaining: 712ms\n",
      "135:\tlearn: 0.0002394\ttotal: 1.49s\tremaining: 701ms\n",
      "136:\tlearn: 0.0002394\ttotal: 1.5s\tremaining: 690ms\n",
      "137:\tlearn: 0.0002394\ttotal: 1.51s\tremaining: 679ms\n",
      "138:\tlearn: 0.0002394\ttotal: 1.52s\tremaining: 668ms\n",
      "139:\tlearn: 0.0002394\ttotal: 1.53s\tremaining: 656ms\n",
      "140:\tlearn: 0.0002334\ttotal: 1.54s\tremaining: 645ms\n",
      "141:\tlearn: 0.0002227\ttotal: 1.55s\tremaining: 634ms\n",
      "142:\tlearn: 0.0002227\ttotal: 1.56s\tremaining: 623ms\n",
      "143:\tlearn: 0.0002227\ttotal: 1.57s\tremaining: 612ms\n",
      "144:\tlearn: 0.0002227\ttotal: 1.58s\tremaining: 601ms\n",
      "145:\tlearn: 0.0002227\ttotal: 1.59s\tremaining: 589ms\n",
      "146:\tlearn: 0.0002227\ttotal: 1.6s\tremaining: 578ms\n",
      "147:\tlearn: 0.0002227\ttotal: 1.61s\tremaining: 567ms\n",
      "148:\tlearn: 0.0002227\ttotal: 1.62s\tremaining: 556ms\n",
      "149:\tlearn: 0.0002183\ttotal: 1.63s\tremaining: 545ms\n",
      "150:\tlearn: 0.0002121\ttotal: 1.64s\tremaining: 534ms\n",
      "151:\tlearn: 0.0002121\ttotal: 1.66s\tremaining: 523ms\n",
      "152:\tlearn: 0.0002121\ttotal: 1.67s\tremaining: 512ms\n",
      "153:\tlearn: 0.0002050\ttotal: 1.68s\tremaining: 501ms\n",
      "154:\tlearn: 0.0002050\ttotal: 1.69s\tremaining: 489ms\n",
      "155:\tlearn: 0.0002050\ttotal: 1.7s\tremaining: 478ms\n",
      "156:\tlearn: 0.0002050\ttotal: 1.71s\tremaining: 467ms\n",
      "157:\tlearn: 0.0002050\ttotal: 1.72s\tremaining: 456ms\n",
      "158:\tlearn: 0.0002022\ttotal: 1.73s\tremaining: 445ms\n",
      "159:\tlearn: 0.0002022\ttotal: 1.74s\tremaining: 434ms\n",
      "160:\tlearn: 0.0002022\ttotal: 1.75s\tremaining: 423ms\n",
      "161:\tlearn: 0.0002022\ttotal: 1.75s\tremaining: 412ms\n",
      "162:\tlearn: 0.0002022\ttotal: 1.76s\tremaining: 401ms\n",
      "163:\tlearn: 0.0002022\ttotal: 1.77s\tremaining: 390ms\n",
      "164:\tlearn: 0.0002022\ttotal: 1.79s\tremaining: 379ms\n",
      "165:\tlearn: 0.0002022\ttotal: 1.8s\tremaining: 368ms\n",
      "166:\tlearn: 0.0002022\ttotal: 1.81s\tremaining: 357ms\n",
      "167:\tlearn: 0.0002022\ttotal: 1.82s\tremaining: 346ms\n",
      "168:\tlearn: 0.0002022\ttotal: 1.83s\tremaining: 335ms\n",
      "169:\tlearn: 0.0002022\ttotal: 1.84s\tremaining: 324ms\n",
      "170:\tlearn: 0.0001995\ttotal: 1.85s\tremaining: 313ms\n",
      "171:\tlearn: 0.0001995\ttotal: 1.86s\tremaining: 302ms\n",
      "172:\tlearn: 0.0001995\ttotal: 1.87s\tremaining: 291ms\n",
      "173:\tlearn: 0.0001995\ttotal: 1.88s\tremaining: 280ms\n",
      "174:\tlearn: 0.0001995\ttotal: 1.89s\tremaining: 269ms\n",
      "175:\tlearn: 0.0001995\ttotal: 1.9s\tremaining: 259ms\n",
      "176:\tlearn: 0.0001995\ttotal: 1.91s\tremaining: 248ms\n",
      "177:\tlearn: 0.0001995\ttotal: 1.92s\tremaining: 237ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178:\tlearn: 0.0001995\ttotal: 1.93s\tremaining: 226ms\n",
      "179:\tlearn: 0.0001995\ttotal: 1.94s\tremaining: 215ms\n",
      "180:\tlearn: 0.0001995\ttotal: 1.95s\tremaining: 204ms\n",
      "181:\tlearn: 0.0001995\ttotal: 1.96s\tremaining: 193ms\n",
      "182:\tlearn: 0.0001995\ttotal: 1.96s\tremaining: 183ms\n",
      "183:\tlearn: 0.0001995\ttotal: 1.97s\tremaining: 172ms\n",
      "184:\tlearn: 0.0001995\ttotal: 1.99s\tremaining: 161ms\n",
      "185:\tlearn: 0.0001995\ttotal: 2s\tremaining: 150ms\n",
      "186:\tlearn: 0.0001995\ttotal: 2s\tremaining: 139ms\n",
      "187:\tlearn: 0.0001995\ttotal: 2.01s\tremaining: 129ms\n",
      "188:\tlearn: 0.0001995\ttotal: 2.02s\tremaining: 118ms\n",
      "189:\tlearn: 0.0001995\ttotal: 2.03s\tremaining: 107ms\n",
      "190:\tlearn: 0.0001953\ttotal: 2.04s\tremaining: 96.2ms\n",
      "191:\tlearn: 0.0001953\ttotal: 2.05s\tremaining: 85.5ms\n",
      "192:\tlearn: 0.0001953\ttotal: 2.06s\tremaining: 74.8ms\n",
      "193:\tlearn: 0.0001953\ttotal: 2.07s\tremaining: 64.1ms\n",
      "194:\tlearn: 0.0001953\ttotal: 2.08s\tremaining: 53.4ms\n",
      "195:\tlearn: 0.0001953\ttotal: 2.09s\tremaining: 42.8ms\n",
      "196:\tlearn: 0.0001953\ttotal: 2.1s\tremaining: 32.1ms\n",
      "197:\tlearn: 0.0001953\ttotal: 2.12s\tremaining: 21.4ms\n",
      "198:\tlearn: 0.0001953\ttotal: 2.13s\tremaining: 10.7ms\n",
      "199:\tlearn: 0.0001953\ttotal: 2.13s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabf6d1af5fc4cfaaa1bec642d1c288d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.631189254489025, Recall = 0.38302649303996406, Aging Rate = 0.2353072625698324, Precision = 0.8100664767331434, f1 = 0.5201219512195122\n",
      "Epoch 2: Train Loss = 0.46455547543211356, Recall = 0.7691962281095645, Aging Rate = 0.4583240223463687, Precision = 0.8352023403217943, f1 = 0.8008415147265078\n",
      "Epoch 3: Train Loss = 0.3583036278412995, Recall = 0.8446340368208352, Aging Rate = 0.4773184357541899, Precision = 0.8806179775280899, f1 = 0.862250744900298\n",
      "Epoch 4: Train Loss = 0.29474729770055696, Recall = 0.8877413560844185, Aging Rate = 0.4922905027932961, Precision = 0.8974126191556968, f1 = 0.8925507900677201\n",
      "Epoch 5: Train Loss = 0.2433431536658516, Recall = 0.9097440502918724, Aging Rate = 0.4940782122905028, Precision = 0.9163274536408865, f1 = 0.9130238846327174\n",
      "Test Loss = 0.21760733745617575, Recall = 0.8643915581499776, Aging Rate = 0.44379888268156426, precision = 0.9692849949647533\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1943390188370337, Recall = 0.9317467444993265, Aging Rate = 0.4893854748603352, Precision = 0.9474885844748858, f1 = 0.9395517319447589\n",
      "Epoch 7: Train Loss = 0.15678562187282732, Recall = 0.960035922766053, Aging Rate = 0.4972067039106145, Precision = 0.9608988764044943, f1 = 0.9604672057502245\n",
      "Epoch 8: Train Loss = 0.12900879472494126, Recall = 0.9739559946115851, Aging Rate = 0.5003351955307263, Precision = 0.968736042876284, f1 = 0.9713390058217644\n",
      "Epoch 9: Train Loss = 0.10706636254181409, Recall = 0.9820386169735069, Aging Rate = 0.5001117318435754, Precision = 0.9772117962466488, f1 = 0.979619260918253\n",
      "Epoch 10: Train Loss = 0.08812791363820018, Recall = 0.9847328244274809, Aging Rate = 0.49675977653631287, Precision = 0.9865047233468286, f1 = 0.9856179775280899\n",
      "Test Loss = 0.0781364549355134, Recall = 0.9869779973057925, Aging Rate = 0.4954189944134078, precision = 0.9914298601714028\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07431456334431079, Recall = 0.9878760664571172, Aging Rate = 0.49675977653631287, Precision = 0.9896536212325686, f1 = 0.9887640449438203\n",
      "Epoch 12: Train Loss = 0.06287367037721187, Recall = 0.9905702739110912, Aging Rate = 0.4972067039106145, Precision = 0.9914606741573033, f1 = 0.991015274034142\n",
      "Epoch 13: Train Loss = 0.05384650400314251, Recall = 0.9937135159407274, Aging Rate = 0.49675977653631287, Precision = 0.9955015744489428, f1 = 0.9946067415730337\n",
      "Epoch 14: Train Loss = 0.04628261871927277, Recall = 0.995958688819039, Aging Rate = 0.4969832402234637, Precision = 0.9973021582733813, f1 = 0.9966299707930801\n",
      "Epoch 15: Train Loss = 0.040330966016766744, Recall = 0.9964077233947014, Aging Rate = 0.4972067039106145, Precision = 0.9973033707865169, f1 = 0.9968553459119497\n",
      "Test Loss = 0.03559596509240859, Recall = 0.9982038616973506, Aging Rate = 0.49743016759776537, precision = 0.9986522911051213\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03536376551251505, Recall = 0.9977548271216884, Aging Rate = 0.49743016759776537, Precision = 0.9982030548068284, f1 = 0.9979788906355267\n",
      "Epoch 17: Train Loss = 0.031049700653253322, Recall = 0.9982038616973506, Aging Rate = 0.4972067039106145, Precision = 0.9991011235955056, f1 = 0.9986522911051212\n",
      "Epoch 18: Train Loss = 0.027595032799843305, Recall = 0.9991019308486754, Aging Rate = 0.49787709497206706, Precision = 0.998653500897666, f1 = 0.9988776655443322\n",
      "Epoch 19: Train Loss = 0.024486730503719613, Recall = 0.998652896273013, Aging Rate = 0.4976536312849162, Precision = 0.998652896273013, f1 = 0.998652896273013\n",
      "Epoch 20: Train Loss = 0.021943027259334506, Recall = 0.998652896273013, Aging Rate = 0.4976536312849162, Precision = 0.998652896273013, f1 = 0.998652896273013\n",
      "Test Loss = 0.01983129741800897, Recall = 1.0, Aging Rate = 0.4983240223463687, precision = 0.9986547085201793\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.01980743609618541, Recall = 1.0, Aging Rate = 0.4983240223463687, Precision = 0.9986547085201793, f1 = 0.9993269015032533\n",
      "Epoch 22: Train Loss = 0.0180319907378884, Recall = 0.9991019308486754, Aging Rate = 0.4976536312849162, Precision = 0.9991019308486754, f1 = 0.9991019308486754\n",
      "Epoch 23: Train Loss = 0.01655211420448799, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Epoch 24: Train Loss = 0.015033437816956857, Recall = 0.9995509654243376, Aging Rate = 0.49787709497206706, Precision = 0.9991023339317774, f1 = 0.9993265993265993\n",
      "Epoch 25: Train Loss = 0.013820343812167978, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Test Loss = 0.012830490011659415, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.012955843337587805, Recall = 0.9995509654243376, Aging Rate = 0.49787709497206706, Precision = 0.9991023339317774, f1 = 0.9993265993265993\n",
      "Epoch 27: Train Loss = 0.011789584223638867, Recall = 0.9995509654243376, Aging Rate = 0.4976536312849162, Precision = 0.9995509654243376, f1 = 0.9995509654243376\n",
      "Epoch 28: Train Loss = 0.0108063115679019, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 29: Train Loss = 0.010021654517915328, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 30: Train Loss = 0.009289617876433794, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.008481304130449308, Recall = 1.0, Aging Rate = 0.49787709497206706, precision = 0.9995511669658886\n",
      "\n",
      "Epoch 31: Train Loss = 0.008668547076636187, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 32: Train Loss = 0.008112860267367942, Recall = 1.0, Aging Rate = 0.4981005586592179, Precision = 0.9991027366532077, f1 = 0.9995511669658886\n",
      "Epoch 33: Train Loss = 0.0075945225349684665, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 34: Train Loss = 0.007324435853675091, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 35: Train Loss = 0.006757106009725086, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Test Loss = 0.006099508488602811, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.006281982598066663, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 37: Train Loss = 0.00602380978432614, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 38: Train Loss = 0.005774478919325428, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 39: Train Loss = 0.0053155955844480735, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.005008168668387322, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004662381584844526, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.004805791172401865, Recall = 1.0, Aging Rate = 0.49787709497206706, Precision = 0.9995511669658886, f1 = 0.9997755331088665\n",
      "Epoch 42: Train Loss = 0.004733845148048255, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.004332620773214701, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.004203802807069275, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0039647859593010485, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0037108997416854237, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.0038716996316511872, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0038063119555818304, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0035942058154907306, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.003398256010828571, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.00325949590528311, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0030234802040328694, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0031458416364972567, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.003117352270744949, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0029838230087312575, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0028533242729247947, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.002789310577309914, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026027373091302105, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0026914466181821996, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002628730730564651, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.002633186588636157, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0025001849588162824, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0024896221452876508, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022614485679933144, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.002549749466294017, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.002322789608506101, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0022915864517319136, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0022520610095423193, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.00217581596587103, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020840154451906017, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.002239312079615433, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0021025463951980516, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.002102008254980075, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.002086157089296999, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.002094715700755364, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019449470215346977, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.001982972778057336, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.001956307867765635, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0019323167132276147, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0019687322383569604, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.002060429685509779, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018224293552339078, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0019196283356832892, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.001840820782375652, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.001854568581284133, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0018259734688222658, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0018192503706671755, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016681917056482931, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0018375177762611618, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0018320258635007801, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0017529921189949498, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0017483476184142034, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.001739472716255894, Recall = 1.0, Aging Rate = 0.4976536312849162, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016117779125443884, Recall = 1.0, Aging Rate = 0.4976536312849162, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5044185\ttotal: 12.8ms\tremaining: 2.55s\n",
      "1:\tlearn: 0.3898962\ttotal: 25.2ms\tremaining: 2.49s\n",
      "2:\tlearn: 0.3136641\ttotal: 37.6ms\tremaining: 2.47s\n",
      "3:\tlearn: 0.2742691\ttotal: 50.3ms\tremaining: 2.46s\n",
      "4:\tlearn: 0.2324626\ttotal: 62.8ms\tremaining: 2.45s\n",
      "5:\tlearn: 0.2108851\ttotal: 74.5ms\tremaining: 2.41s\n",
      "6:\tlearn: 0.1708617\ttotal: 84.9ms\tremaining: 2.34s\n",
      "7:\tlearn: 0.1465456\ttotal: 95.6ms\tremaining: 2.29s\n",
      "8:\tlearn: 0.1256149\ttotal: 107ms\tremaining: 2.27s\n",
      "9:\tlearn: 0.1193405\ttotal: 118ms\tremaining: 2.24s\n",
      "10:\tlearn: 0.1062297\ttotal: 129ms\tremaining: 2.21s\n",
      "11:\tlearn: 0.0980763\ttotal: 140ms\tremaining: 2.19s\n",
      "12:\tlearn: 0.0847796\ttotal: 151ms\tremaining: 2.17s\n",
      "13:\tlearn: 0.0761944\ttotal: 162ms\tremaining: 2.15s\n",
      "14:\tlearn: 0.0680486\ttotal: 173ms\tremaining: 2.13s\n",
      "15:\tlearn: 0.0633000\ttotal: 184ms\tremaining: 2.11s\n",
      "16:\tlearn: 0.0590499\ttotal: 195ms\tremaining: 2.1s\n",
      "17:\tlearn: 0.0552358\ttotal: 205ms\tremaining: 2.07s\n",
      "18:\tlearn: 0.0513687\ttotal: 216ms\tremaining: 2.06s\n",
      "19:\tlearn: 0.0458885\ttotal: 228ms\tremaining: 2.05s\n",
      "20:\tlearn: 0.0428128\ttotal: 238ms\tremaining: 2.03s\n",
      "21:\tlearn: 0.0398812\ttotal: 249ms\tremaining: 2.01s\n",
      "22:\tlearn: 0.0371047\ttotal: 260ms\tremaining: 2s\n",
      "23:\tlearn: 0.0355333\ttotal: 271ms\tremaining: 1.99s\n",
      "24:\tlearn: 0.0330481\ttotal: 281ms\tremaining: 1.97s\n",
      "25:\tlearn: 0.0315689\ttotal: 291ms\tremaining: 1.95s\n",
      "26:\tlearn: 0.0295675\ttotal: 302ms\tremaining: 1.93s\n",
      "27:\tlearn: 0.0283075\ttotal: 313ms\tremaining: 1.92s\n",
      "28:\tlearn: 0.0260188\ttotal: 323ms\tremaining: 1.9s\n",
      "29:\tlearn: 0.0241266\ttotal: 333ms\tremaining: 1.89s\n",
      "30:\tlearn: 0.0222947\ttotal: 343ms\tremaining: 1.87s\n",
      "31:\tlearn: 0.0205689\ttotal: 354ms\tremaining: 1.86s\n",
      "32:\tlearn: 0.0198228\ttotal: 364ms\tremaining: 1.84s\n",
      "33:\tlearn: 0.0190225\ttotal: 375ms\tremaining: 1.83s\n",
      "34:\tlearn: 0.0179836\ttotal: 386ms\tremaining: 1.82s\n",
      "35:\tlearn: 0.0169916\ttotal: 397ms\tremaining: 1.81s\n",
      "36:\tlearn: 0.0163619\ttotal: 409ms\tremaining: 1.8s\n",
      "37:\tlearn: 0.0157848\ttotal: 420ms\tremaining: 1.79s\n",
      "38:\tlearn: 0.0143556\ttotal: 431ms\tremaining: 1.78s\n",
      "39:\tlearn: 0.0130766\ttotal: 443ms\tremaining: 1.77s\n",
      "40:\tlearn: 0.0121941\ttotal: 462ms\tremaining: 1.79s\n",
      "41:\tlearn: 0.0113691\ttotal: 482ms\tremaining: 1.81s\n",
      "42:\tlearn: 0.0106230\ttotal: 495ms\tremaining: 1.81s\n",
      "43:\tlearn: 0.0104030\ttotal: 506ms\tremaining: 1.79s\n",
      "44:\tlearn: 0.0100659\ttotal: 516ms\tremaining: 1.78s\n",
      "45:\tlearn: 0.0095924\ttotal: 528ms\tremaining: 1.77s\n",
      "46:\tlearn: 0.0088134\ttotal: 540ms\tremaining: 1.76s\n",
      "47:\tlearn: 0.0080831\ttotal: 552ms\tremaining: 1.75s\n",
      "48:\tlearn: 0.0073777\ttotal: 564ms\tremaining: 1.74s\n",
      "49:\tlearn: 0.0069697\ttotal: 576ms\tremaining: 1.73s\n",
      "50:\tlearn: 0.0065933\ttotal: 587ms\tremaining: 1.71s\n",
      "51:\tlearn: 0.0060181\ttotal: 599ms\tremaining: 1.7s\n",
      "52:\tlearn: 0.0057009\ttotal: 611ms\tremaining: 1.69s\n",
      "53:\tlearn: 0.0052270\ttotal: 623ms\tremaining: 1.68s\n",
      "54:\tlearn: 0.0049499\ttotal: 634ms\tremaining: 1.67s\n",
      "55:\tlearn: 0.0047887\ttotal: 646ms\tremaining: 1.66s\n",
      "56:\tlearn: 0.0046104\ttotal: 657ms\tremaining: 1.65s\n",
      "57:\tlearn: 0.0043479\ttotal: 668ms\tremaining: 1.64s\n",
      "58:\tlearn: 0.0041448\ttotal: 680ms\tremaining: 1.62s\n",
      "59:\tlearn: 0.0038073\ttotal: 691ms\tremaining: 1.61s\n",
      "60:\tlearn: 0.0034760\ttotal: 703ms\tremaining: 1.6s\n",
      "61:\tlearn: 0.0033452\ttotal: 714ms\tremaining: 1.59s\n",
      "62:\tlearn: 0.0031214\ttotal: 725ms\tremaining: 1.58s\n",
      "63:\tlearn: 0.0030275\ttotal: 736ms\tremaining: 1.56s\n",
      "64:\tlearn: 0.0028792\ttotal: 747ms\tremaining: 1.55s\n",
      "65:\tlearn: 0.0027864\ttotal: 757ms\tremaining: 1.54s\n",
      "66:\tlearn: 0.0026665\ttotal: 768ms\tremaining: 1.52s\n",
      "67:\tlearn: 0.0025929\ttotal: 779ms\tremaining: 1.51s\n",
      "68:\tlearn: 0.0024914\ttotal: 790ms\tremaining: 1.5s\n",
      "69:\tlearn: 0.0023335\ttotal: 802ms\tremaining: 1.49s\n",
      "70:\tlearn: 0.0022415\ttotal: 812ms\tremaining: 1.48s\n",
      "71:\tlearn: 0.0020877\ttotal: 824ms\tremaining: 1.46s\n",
      "72:\tlearn: 0.0019473\ttotal: 835ms\tremaining: 1.45s\n",
      "73:\tlearn: 0.0018565\ttotal: 846ms\tremaining: 1.44s\n",
      "74:\tlearn: 0.0017921\ttotal: 858ms\tremaining: 1.43s\n",
      "75:\tlearn: 0.0017479\ttotal: 869ms\tremaining: 1.42s\n",
      "76:\tlearn: 0.0016910\ttotal: 879ms\tremaining: 1.4s\n",
      "77:\tlearn: 0.0016212\ttotal: 891ms\tremaining: 1.39s\n",
      "78:\tlearn: 0.0015715\ttotal: 901ms\tremaining: 1.38s\n",
      "79:\tlearn: 0.0014812\ttotal: 911ms\tremaining: 1.37s\n",
      "80:\tlearn: 0.0013706\ttotal: 922ms\tremaining: 1.35s\n",
      "81:\tlearn: 0.0013082\ttotal: 933ms\tremaining: 1.34s\n",
      "82:\tlearn: 0.0012621\ttotal: 945ms\tremaining: 1.33s\n",
      "83:\tlearn: 0.0011610\ttotal: 957ms\tremaining: 1.32s\n",
      "84:\tlearn: 0.0011204\ttotal: 968ms\tremaining: 1.31s\n",
      "85:\tlearn: 0.0010816\ttotal: 979ms\tremaining: 1.3s\n",
      "86:\tlearn: 0.0010401\ttotal: 989ms\tremaining: 1.28s\n",
      "87:\tlearn: 0.0010400\ttotal: 993ms\tremaining: 1.26s\n",
      "88:\tlearn: 0.0010081\ttotal: 1s\tremaining: 1.25s\n",
      "89:\tlearn: 0.0009858\ttotal: 1.01s\tremaining: 1.24s\n",
      "90:\tlearn: 0.0009589\ttotal: 1.03s\tremaining: 1.23s\n",
      "91:\tlearn: 0.0009352\ttotal: 1.04s\tremaining: 1.22s\n",
      "92:\tlearn: 0.0009175\ttotal: 1.05s\tremaining: 1.21s\n",
      "93:\tlearn: 0.0008907\ttotal: 1.06s\tremaining: 1.2s\n",
      "94:\tlearn: 0.0008569\ttotal: 1.07s\tremaining: 1.19s\n",
      "95:\tlearn: 0.0008275\ttotal: 1.08s\tremaining: 1.17s\n",
      "96:\tlearn: 0.0007813\ttotal: 1.09s\tremaining: 1.16s\n",
      "97:\tlearn: 0.0007652\ttotal: 1.1s\tremaining: 1.15s\n",
      "98:\tlearn: 0.0007355\ttotal: 1.11s\tremaining: 1.14s\n",
      "99:\tlearn: 0.0007121\ttotal: 1.13s\tremaining: 1.13s\n",
      "100:\tlearn: 0.0006752\ttotal: 1.14s\tremaining: 1.11s\n",
      "101:\tlearn: 0.0006577\ttotal: 1.15s\tremaining: 1.1s\n",
      "102:\tlearn: 0.0006193\ttotal: 1.16s\tremaining: 1.09s\n",
      "103:\tlearn: 0.0005900\ttotal: 1.17s\tremaining: 1.08s\n",
      "104:\tlearn: 0.0005801\ttotal: 1.18s\tremaining: 1.07s\n",
      "105:\tlearn: 0.0005654\ttotal: 1.19s\tremaining: 1.06s\n",
      "106:\tlearn: 0.0005516\ttotal: 1.2s\tremaining: 1.04s\n",
      "107:\tlearn: 0.0005194\ttotal: 1.21s\tremaining: 1.03s\n",
      "108:\tlearn: 0.0005072\ttotal: 1.23s\tremaining: 1.02s\n",
      "109:\tlearn: 0.0004884\ttotal: 1.24s\tremaining: 1.01s\n",
      "110:\tlearn: 0.0004744\ttotal: 1.25s\tremaining: 1s\n",
      "111:\tlearn: 0.0004636\ttotal: 1.26s\tremaining: 990ms\n",
      "112:\tlearn: 0.0004486\ttotal: 1.27s\tremaining: 979ms\n",
      "113:\tlearn: 0.0004359\ttotal: 1.28s\tremaining: 968ms\n",
      "114:\tlearn: 0.0004281\ttotal: 1.29s\tremaining: 956ms\n",
      "115:\tlearn: 0.0004077\ttotal: 1.3s\tremaining: 945ms\n",
      "116:\tlearn: 0.0003938\ttotal: 1.32s\tremaining: 934ms\n",
      "117:\tlearn: 0.0003752\ttotal: 1.33s\tremaining: 922ms\n",
      "118:\tlearn: 0.0003597\ttotal: 1.34s\tremaining: 911ms\n",
      "119:\tlearn: 0.0003527\ttotal: 1.35s\tremaining: 900ms\n",
      "120:\tlearn: 0.0003527\ttotal: 1.36s\tremaining: 888ms\n",
      "121:\tlearn: 0.0003378\ttotal: 1.37s\tremaining: 877ms\n",
      "122:\tlearn: 0.0003280\ttotal: 1.38s\tremaining: 866ms\n",
      "123:\tlearn: 0.0003200\ttotal: 1.39s\tremaining: 854ms\n",
      "124:\tlearn: 0.0003106\ttotal: 1.4s\tremaining: 843ms\n",
      "125:\tlearn: 0.0002984\ttotal: 1.42s\tremaining: 832ms\n",
      "126:\tlearn: 0.0002929\ttotal: 1.43s\tremaining: 822ms\n",
      "127:\tlearn: 0.0002838\ttotal: 1.45s\tremaining: 815ms\n",
      "128:\tlearn: 0.0002745\ttotal: 1.47s\tremaining: 809ms\n",
      "129:\tlearn: 0.0002687\ttotal: 1.49s\tremaining: 802ms\n",
      "130:\tlearn: 0.0002687\ttotal: 1.51s\tremaining: 796ms\n",
      "131:\tlearn: 0.0002610\ttotal: 1.53s\tremaining: 787ms\n",
      "132:\tlearn: 0.0002503\ttotal: 1.54s\tremaining: 777ms\n",
      "133:\tlearn: 0.0002401\ttotal: 1.55s\tremaining: 766ms\n",
      "134:\tlearn: 0.0002401\ttotal: 1.57s\tremaining: 755ms\n",
      "135:\tlearn: 0.0002354\ttotal: 1.58s\tremaining: 743ms\n",
      "136:\tlearn: 0.0002354\ttotal: 1.59s\tremaining: 732ms\n",
      "137:\tlearn: 0.0002323\ttotal: 1.6s\tremaining: 720ms\n",
      "138:\tlearn: 0.0002323\ttotal: 1.61s\tremaining: 708ms\n",
      "139:\tlearn: 0.0002213\ttotal: 1.63s\tremaining: 696ms\n",
      "140:\tlearn: 0.0002191\ttotal: 1.64s\tremaining: 684ms\n",
      "141:\tlearn: 0.0002191\ttotal: 1.65s\tremaining: 672ms\n",
      "142:\tlearn: 0.0002191\ttotal: 1.66s\tremaining: 660ms\n",
      "143:\tlearn: 0.0002191\ttotal: 1.67s\tremaining: 648ms\n",
      "144:\tlearn: 0.0002191\ttotal: 1.68s\tremaining: 636ms\n",
      "145:\tlearn: 0.0002141\ttotal: 1.69s\tremaining: 624ms\n",
      "146:\tlearn: 0.0002105\ttotal: 1.7s\tremaining: 612ms\n",
      "147:\tlearn: 0.0002063\ttotal: 1.71s\tremaining: 600ms\n",
      "148:\tlearn: 0.0002063\ttotal: 1.72s\tremaining: 588ms\n",
      "149:\tlearn: 0.0001974\ttotal: 1.73s\tremaining: 577ms\n",
      "150:\tlearn: 0.0001974\ttotal: 1.74s\tremaining: 565ms\n",
      "151:\tlearn: 0.0001933\ttotal: 1.75s\tremaining: 553ms\n",
      "152:\tlearn: 0.0001875\ttotal: 1.76s\tremaining: 541ms\n",
      "153:\tlearn: 0.0001875\ttotal: 1.77s\tremaining: 529ms\n",
      "154:\tlearn: 0.0001875\ttotal: 1.78s\tremaining: 518ms\n",
      "155:\tlearn: 0.0001875\ttotal: 1.79s\tremaining: 506ms\n",
      "156:\tlearn: 0.0001875\ttotal: 1.8s\tremaining: 494ms\n",
      "157:\tlearn: 0.0001875\ttotal: 1.81s\tremaining: 482ms\n",
      "158:\tlearn: 0.0001875\ttotal: 1.82s\tremaining: 471ms\n",
      "159:\tlearn: 0.0001875\ttotal: 1.84s\tremaining: 459ms\n",
      "160:\tlearn: 0.0001875\ttotal: 1.85s\tremaining: 447ms\n",
      "161:\tlearn: 0.0001875\ttotal: 1.86s\tremaining: 436ms\n",
      "162:\tlearn: 0.0001875\ttotal: 1.87s\tremaining: 424ms\n",
      "163:\tlearn: 0.0001875\ttotal: 1.88s\tremaining: 413ms\n",
      "164:\tlearn: 0.0001875\ttotal: 1.89s\tremaining: 401ms\n",
      "165:\tlearn: 0.0001875\ttotal: 1.9s\tremaining: 389ms\n",
      "166:\tlearn: 0.0001875\ttotal: 1.91s\tremaining: 378ms\n",
      "167:\tlearn: 0.0001875\ttotal: 1.92s\tremaining: 366ms\n",
      "168:\tlearn: 0.0001875\ttotal: 1.93s\tremaining: 355ms\n",
      "169:\tlearn: 0.0001875\ttotal: 1.94s\tremaining: 343ms\n",
      "170:\tlearn: 0.0001828\ttotal: 1.95s\tremaining: 331ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171:\tlearn: 0.0001828\ttotal: 1.96s\tremaining: 320ms\n",
      "172:\tlearn: 0.0001828\ttotal: 1.97s\tremaining: 308ms\n",
      "173:\tlearn: 0.0001828\ttotal: 1.99s\tremaining: 297ms\n",
      "174:\tlearn: 0.0001828\ttotal: 2s\tremaining: 285ms\n",
      "175:\tlearn: 0.0001828\ttotal: 2.01s\tremaining: 274ms\n",
      "176:\tlearn: 0.0001828\ttotal: 2.02s\tremaining: 262ms\n",
      "177:\tlearn: 0.0001828\ttotal: 2.03s\tremaining: 251ms\n",
      "178:\tlearn: 0.0001828\ttotal: 2.04s\tremaining: 239ms\n",
      "179:\tlearn: 0.0001828\ttotal: 2.05s\tremaining: 228ms\n",
      "180:\tlearn: 0.0001828\ttotal: 2.06s\tremaining: 216ms\n",
      "181:\tlearn: 0.0001803\ttotal: 2.07s\tremaining: 205ms\n",
      "182:\tlearn: 0.0001803\ttotal: 2.08s\tremaining: 194ms\n",
      "183:\tlearn: 0.0001803\ttotal: 2.1s\tremaining: 182ms\n",
      "184:\tlearn: 0.0001803\ttotal: 2.11s\tremaining: 171ms\n",
      "185:\tlearn: 0.0001803\ttotal: 2.12s\tremaining: 159ms\n",
      "186:\tlearn: 0.0001803\ttotal: 2.13s\tremaining: 148ms\n",
      "187:\tlearn: 0.0001803\ttotal: 2.14s\tremaining: 136ms\n",
      "188:\tlearn: 0.0001803\ttotal: 2.15s\tremaining: 125ms\n",
      "189:\tlearn: 0.0001803\ttotal: 2.16s\tremaining: 114ms\n",
      "190:\tlearn: 0.0001803\ttotal: 2.17s\tremaining: 102ms\n",
      "191:\tlearn: 0.0001803\ttotal: 2.18s\tremaining: 90.7ms\n",
      "192:\tlearn: 0.0001803\ttotal: 2.19s\tremaining: 79.3ms\n",
      "193:\tlearn: 0.0001803\ttotal: 2.2s\tremaining: 68ms\n",
      "194:\tlearn: 0.0001803\ttotal: 2.21s\tremaining: 56.6ms\n",
      "195:\tlearn: 0.0001803\ttotal: 2.22s\tremaining: 45.3ms\n",
      "196:\tlearn: 0.0001803\ttotal: 2.23s\tremaining: 34ms\n",
      "197:\tlearn: 0.0001803\ttotal: 2.24s\tremaining: 22.6ms\n",
      "198:\tlearn: 0.0001803\ttotal: 2.25s\tremaining: 11.3ms\n",
      "199:\tlearn: 0.0001803\ttotal: 2.26s\tremaining: 0us\n",
      "Dataset 6:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c4708702a84f26bd6735adf51f61cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719e9d073e8f4dce8ea3790b096198ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.47258465734864313, Recall = 0.9730025538124772, Aging Rate = 0.8807376227700942, Precision = 0.6069640418752845, f1 = 0.7475823405746321\n",
      "Epoch 2: Train Loss = 0.30919351901408787, Recall = 0.9584093396570594, Aging Rate = 0.6865103227099619, Precision = 0.7670072992700729, f1 = 0.852092118066818\n",
      "Epoch 3: Train Loss = 0.24483353499002794, Recall = 0.9664356074425392, Aging Rate = 0.6500300661455202, Precision = 0.816836262719704, f1 = 0.8853609625668449\n",
      "Epoch 4: Train Loss = 0.20103830876869389, Recall = 0.9693542502736228, Aging Rate = 0.6237722990579274, Precision = 0.8537917737789203, f1 = 0.9079104732615754\n",
      "Epoch 5: Train Loss = 0.16509832341450206, Recall = 0.9784750091207588, Aging Rate = 0.6059330527159752, Precision = 0.8871981475355607, f1 = 0.9306037473976405\n",
      "Test Loss = 0.1351995619585505, Recall = 0.9879605983217804, Aging Rate = 0.6007215874924835, precision = 0.9035702369035702\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.12512899897014432, Recall = 0.9861364465523531, Aging Rate = 0.5892964521948286, Precision = 0.9193877551020408, f1 = 0.9515930293962331\n",
      "Epoch 7: Train Loss = 0.10044259842471003, Recall = 0.9883254286756659, Aging Rate = 0.5776708759270395, Precision = 0.9399722414989591, f1 = 0.9635425929219279\n",
      "Epoch 8: Train Loss = 0.08282678602026254, Recall = 0.9937978839839474, Aging Rate = 0.5736620565243535, Precision = 0.9517819706498952, f1 = 0.9723362484383367\n",
      "Epoch 9: Train Loss = 0.06826609270197281, Recall = 0.9930682232761766, Aging Rate = 0.566446181599519, Precision = 0.9631988676574664, f1 = 0.977905514639842\n",
      "Epoch 10: Train Loss = 0.05760638358765505, Recall = 0.9952572053994893, Aging Rate = 0.5620364802565645, Precision = 0.9728958630527818, f1 = 0.9839495040577096\n",
      "Test Loss = 0.05222129368156607, Recall = 0.9981758482305728, Aging Rate = 0.5656444177189818, precision = 0.969525159461375\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04935222145302139, Recall = 0.996716526815031, Aging Rate = 0.5600320705552215, Precision = 0.9778095919828204, f1 = 0.9871725383920505\n",
      "Epoch 12: Train Loss = 0.043580457919118014, Recall = 0.9959868661072602, Aging Rate = 0.5578272198837443, Precision = 0.9809558030901905, f1 = 0.9884141926140478\n",
      "Epoch 13: Train Loss = 0.03794804585794811, Recall = 0.9974461875228019, Aging Rate = 0.5568250150330728, Precision = 0.9841612670986322, f1 = 0.9907591955064322\n",
      "Epoch 14: Train Loss = 0.03242044094322732, Recall = 0.9981758482305728, Aging Rate = 0.5552214872719984, Precision = 0.9877256317689531, f1 = 0.9929232444202505\n",
      "Epoch 15: Train Loss = 0.02874147616017675, Recall = 0.9981758482305728, Aging Rate = 0.5538184004810583, Precision = 0.990228013029316, f1 = 0.994186046511628\n",
      "Test Loss = 0.02513230150077536, Recall = 0.9992703392922291, Aging Rate = 0.5522148727199839, precision = 0.9941923774954627\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.02648695881518997, Recall = 0.9989055089383436, Aging Rate = 0.5534175185407897, Precision = 0.9916696848967765, f1 = 0.9952744456561251\n",
      "Epoch 17: Train Loss = 0.022802578495906122, Recall = 0.9989055089383436, Aging Rate = 0.5522148727199839, Precision = 0.9938294010889293, f1 = 0.9963609898107714\n",
      "Epoch 18: Train Loss = 0.02077119437017041, Recall = 0.9996351696461145, Aging Rate = 0.5530166366005211, Precision = 0.9931134469010511, f1 = 0.9963636363636365\n",
      "Epoch 19: Train Loss = 0.019329338146705538, Recall = 0.9992703392922291, Aging Rate = 0.5536179595109241, Precision = 0.9916727009413469, f1 = 0.995457023441759\n",
      "Epoch 20: Train Loss = 0.016933730565045848, Recall = 1.0, Aging Rate = 0.5522148727199839, Precision = 0.9949183303085299, f1 = 0.99745269286754\n",
      "Test Loss = 0.01653517292432396, Recall = 1.0, Aging Rate = 0.5530166366005211, precision = 0.9934758970641536\n",
      "\n",
      "Epoch 21: Train Loss = 0.01599814891644378, Recall = 1.0, Aging Rate = 0.5520144317498497, Precision = 0.995279593318809, f1 = 0.9976342129208371\n",
      "Epoch 22: Train Loss = 0.014242275572841322, Recall = 1.0, Aging Rate = 0.5514131088394468, Precision = 0.9963649581970193, f1 = 0.9981791697013839\n",
      "Epoch 23: Train Loss = 0.01297587146392657, Recall = 1.0, Aging Rate = 0.5510122268991782, Precision = 0.9970898508548564, f1 = 0.9985428051001822\n",
      "Epoch 24: Train Loss = 0.011923642916424549, Recall = 1.0, Aging Rate = 0.5510122268991782, Precision = 0.9970898508548564, f1 = 0.9985428051001822\n",
      "Epoch 25: Train Loss = 0.01107521836736812, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Test Loss = 0.009228031905631182, Recall = 1.0, Aging Rate = 0.5504109039887753, precision = 0.9981791697013839\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.010087211111007532, Recall = 1.0, Aging Rate = 0.5504109039887753, Precision = 0.9981791697013839, f1 = 0.9990887552396573\n",
      "Epoch 27: Train Loss = 0.009723363627150795, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Epoch 28: Train Loss = 0.008929255916715553, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Epoch 29: Train Loss = 0.008199939819746874, Recall = 1.0, Aging Rate = 0.5504109039887753, Precision = 0.9981791697013839, f1 = 0.9990887552396573\n",
      "Epoch 30: Train Loss = 0.008262825712365038, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Test Loss = 0.006980241361074336, Recall = 1.0, Aging Rate = 0.5500100220485067, precision = 0.9989067055393586\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.007285410190913225, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 32: Train Loss = 0.006810830654162002, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 33: Train Loss = 0.0064727052760893454, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 34: Train Loss = 0.006255101088793248, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Epoch 35: Train Loss = 0.005686252179771578, Recall = 1.0, Aging Rate = 0.5504109039887753, Precision = 0.9981791697013839, f1 = 0.9990887552396573\n",
      "Test Loss = 0.005235094243672143, Recall = 1.0, Aging Rate = 0.5500100220485067, precision = 0.9989067055393586\n",
      "\n",
      "Epoch 36: Train Loss = 0.0053879909895361965, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 37: Train Loss = 0.005144481644574624, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 38: Train Loss = 0.0048382438947576385, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 39: Train Loss = 0.0047273220778415856, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 40: Train Loss = 0.004469534788615994, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Test Loss = 0.0034955010673002837, Recall = 1.0, Aging Rate = 0.550210463018641, precision = 0.9985428051001821\n",
      "\n",
      "Epoch 41: Train Loss = 0.0044178089276191365, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 42: Train Loss = 0.004402031062820866, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 43: Train Loss = 0.00398838350390293, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 44: Train Loss = 0.004367740059718636, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 45: Train Loss = 0.0037742877046031235, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.00290854332479877, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.003262800482311935, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 47: Train Loss = 0.0036274391074516857, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.0035087060506981465, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 49: Train Loss = 0.0032231516057674324, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 50: Train Loss = 0.0030214877724576584, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.002527974323905507, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.0031372951129445795, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 52: Train Loss = 0.0030311593796802493, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0031944959915205192, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004045331618237302, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, Precision = 0.9992706053975201, f1 = 0.9994528542768557\n",
      "Epoch 55: Train Loss = 0.002894463342351073, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.002552234520963013, Recall = 1.0, Aging Rate = 0.5500100220485067, precision = 0.9989067055393586\n",
      "\n",
      "Epoch 56: Train Loss = 0.0026571510737102145, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002821173265111168, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 58: Train Loss = 0.002946643119906672, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 59: Train Loss = 0.0025547693037051063, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0030899258675766814, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.0025495969871970783, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0026496477833170408, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.00267914093825141, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 63: Train Loss = 0.002562711828687514, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.002551542472756621, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0023676067872639723, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0020942131105357157, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0025098479978626718, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0025717330805129974, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.003998151226546022, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 69: Train Loss = 0.0033239832886116307, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 70: Train Loss = 0.002226283004584706, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019580867416084434, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0022000596631470658, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0022508762458570437, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0026441691059239866, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 74: Train Loss = 0.002830266317818967, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 75: Train Loss = 0.0022015456256499048, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018689180101294314, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.002078843570436993, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.00228132890562769, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.002301686622970005, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0023366075070383723, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0024681286821931474, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002070134367069872, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.002446199253001369, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0027887936699989863, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0024184164363203176, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.002227063389689344, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0025005218395386486, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021024257788241603, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0021258032866069473, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.002332421964366533, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0022539785127145217, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0023851353061618984, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0031447836906018396, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0022341876551290494, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.002448901483584864, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.0023375651427892056, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.00221713635518678, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 94: Train Loss = 0.002227650229782904, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.002172956038094679, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017715094397165588, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.002112981275432999, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.002173675479043307, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.0023856313548466733, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0023747945702239934, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0025760413804232453, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.002577131435767527, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5883018\ttotal: 36.7ms\tremaining: 9.13s\n",
      "1:\tlearn: 0.5155153\ttotal: 52.1ms\tremaining: 6.46s\n",
      "2:\tlearn: 0.4607837\ttotal: 57.4ms\tremaining: 4.73s\n",
      "3:\tlearn: 0.4376417\ttotal: 67.8ms\tremaining: 4.17s\n",
      "4:\tlearn: 0.4110264\ttotal: 83.7ms\tremaining: 4.1s\n",
      "5:\tlearn: 0.3880931\ttotal: 98.3ms\tremaining: 4s\n",
      "6:\tlearn: 0.3708456\ttotal: 128ms\tremaining: 4.46s\n",
      "7:\tlearn: 0.3504697\ttotal: 161ms\tremaining: 4.86s\n",
      "8:\tlearn: 0.3378983\ttotal: 167ms\tremaining: 4.46s\n",
      "9:\tlearn: 0.3223934\ttotal: 172ms\tremaining: 4.14s\n",
      "10:\tlearn: 0.3089371\ttotal: 178ms\tremaining: 3.87s\n",
      "11:\tlearn: 0.2916163\ttotal: 183ms\tremaining: 3.63s\n",
      "12:\tlearn: 0.2851361\ttotal: 188ms\tremaining: 3.43s\n",
      "13:\tlearn: 0.2716080\ttotal: 193ms\tremaining: 3.25s\n",
      "14:\tlearn: 0.2588500\ttotal: 207ms\tremaining: 3.24s\n",
      "15:\tlearn: 0.2511530\ttotal: 222ms\tremaining: 3.25s\n",
      "16:\tlearn: 0.2452768\ttotal: 227ms\tremaining: 3.11s\n",
      "17:\tlearn: 0.2363856\ttotal: 232ms\tremaining: 2.99s\n",
      "18:\tlearn: 0.2263752\ttotal: 237ms\tremaining: 2.88s\n",
      "19:\tlearn: 0.2185028\ttotal: 241ms\tremaining: 2.77s\n",
      "20:\tlearn: 0.2115874\ttotal: 246ms\tremaining: 2.68s\n",
      "21:\tlearn: 0.2059781\ttotal: 253ms\tremaining: 2.62s\n",
      "22:\tlearn: 0.2004188\ttotal: 285ms\tremaining: 2.81s\n",
      "23:\tlearn: 0.1953997\ttotal: 316ms\tremaining: 2.98s\n",
      "24:\tlearn: 0.1885296\ttotal: 333ms\tremaining: 3s\n",
      "25:\tlearn: 0.1847183\ttotal: 338ms\tremaining: 2.91s\n",
      "26:\tlearn: 0.1778960\ttotal: 349ms\tremaining: 2.88s\n",
      "27:\tlearn: 0.1736879\ttotal: 381ms\tremaining: 3.02s\n",
      "28:\tlearn: 0.1681162\ttotal: 386ms\tremaining: 2.94s\n",
      "29:\tlearn: 0.1661196\ttotal: 392ms\tremaining: 2.87s\n",
      "30:\tlearn: 0.1611426\ttotal: 397ms\tremaining: 2.8s\n",
      "31:\tlearn: 0.1550443\ttotal: 402ms\tremaining: 2.74s\n",
      "32:\tlearn: 0.1525338\ttotal: 406ms\tremaining: 2.67s\n",
      "33:\tlearn: 0.1464621\ttotal: 445ms\tremaining: 2.82s\n",
      "34:\tlearn: 0.1438447\ttotal: 465ms\tremaining: 2.85s\n",
      "35:\tlearn: 0.1414612\ttotal: 486ms\tremaining: 2.89s\n",
      "36:\tlearn: 0.1358522\ttotal: 502ms\tremaining: 2.89s\n",
      "37:\tlearn: 0.1325301\ttotal: 507ms\tremaining: 2.83s\n",
      "38:\tlearn: 0.1302011\ttotal: 512ms\tremaining: 2.77s\n",
      "39:\tlearn: 0.1264408\ttotal: 552ms\tremaining: 2.9s\n",
      "40:\tlearn: 0.1231302\ttotal: 583ms\tremaining: 2.97s\n",
      "41:\tlearn: 0.1212879\ttotal: 596ms\tremaining: 2.95s\n",
      "42:\tlearn: 0.1170386\ttotal: 605ms\tremaining: 2.91s\n",
      "43:\tlearn: 0.1128641\ttotal: 626ms\tremaining: 2.93s\n",
      "44:\tlearn: 0.1079821\ttotal: 631ms\tremaining: 2.88s\n",
      "45:\tlearn: 0.1064536\ttotal: 636ms\tremaining: 2.82s\n",
      "46:\tlearn: 0.1041870\ttotal: 656ms\tremaining: 2.83s\n",
      "47:\tlearn: 0.1003440\ttotal: 661ms\tremaining: 2.78s\n",
      "48:\tlearn: 0.0962368\ttotal: 667ms\tremaining: 2.73s\n",
      "49:\tlearn: 0.0938964\ttotal: 672ms\tremaining: 2.69s\n",
      "50:\tlearn: 0.0925185\ttotal: 677ms\tremaining: 2.64s\n",
      "51:\tlearn: 0.0909451\ttotal: 682ms\tremaining: 2.6s\n",
      "52:\tlearn: 0.0878776\ttotal: 687ms\tremaining: 2.55s\n",
      "53:\tlearn: 0.0860704\ttotal: 691ms\tremaining: 2.51s\n",
      "54:\tlearn: 0.0849715\ttotal: 703ms\tremaining: 2.49s\n",
      "55:\tlearn: 0.0832413\ttotal: 708ms\tremaining: 2.45s\n",
      "56:\tlearn: 0.0820456\ttotal: 719ms\tremaining: 2.43s\n",
      "57:\tlearn: 0.0804579\ttotal: 724ms\tremaining: 2.4s\n",
      "58:\tlearn: 0.0796246\ttotal: 729ms\tremaining: 2.36s\n",
      "59:\tlearn: 0.0784503\ttotal: 733ms\tremaining: 2.32s\n",
      "60:\tlearn: 0.0768050\ttotal: 738ms\tremaining: 2.29s\n",
      "61:\tlearn: 0.0745555\ttotal: 750ms\tremaining: 2.27s\n",
      "62:\tlearn: 0.0726045\ttotal: 755ms\tremaining: 2.24s\n",
      "63:\tlearn: 0.0709490\ttotal: 760ms\tremaining: 2.21s\n",
      "64:\tlearn: 0.0688457\ttotal: 764ms\tremaining: 2.17s\n",
      "65:\tlearn: 0.0680394\ttotal: 769ms\tremaining: 2.14s\n",
      "66:\tlearn: 0.0670970\ttotal: 774ms\tremaining: 2.11s\n",
      "67:\tlearn: 0.0658420\ttotal: 779ms\tremaining: 2.08s\n",
      "68:\tlearn: 0.0651943\ttotal: 784ms\tremaining: 2.06s\n",
      "69:\tlearn: 0.0639246\ttotal: 789ms\tremaining: 2.03s\n",
      "70:\tlearn: 0.0621623\ttotal: 793ms\tremaining: 2s\n",
      "71:\tlearn: 0.0612878\ttotal: 799ms\tremaining: 1.97s\n",
      "72:\tlearn: 0.0610819\ttotal: 812ms\tremaining: 1.97s\n",
      "73:\tlearn: 0.0593013\ttotal: 817ms\tremaining: 1.94s\n",
      "74:\tlearn: 0.0578705\ttotal: 822ms\tremaining: 1.92s\n",
      "75:\tlearn: 0.0568143\ttotal: 827ms\tremaining: 1.89s\n",
      "76:\tlearn: 0.0562318\ttotal: 843ms\tremaining: 1.89s\n",
      "77:\tlearn: 0.0555474\ttotal: 874ms\tremaining: 1.93s\n",
      "78:\tlearn: 0.0547869\ttotal: 905ms\tremaining: 1.96s\n",
      "79:\tlearn: 0.0537831\ttotal: 937ms\tremaining: 1.99s\n",
      "80:\tlearn: 0.0529053\ttotal: 952ms\tremaining: 1.99s\n",
      "81:\tlearn: 0.0522901\ttotal: 984ms\tremaining: 2.02s\n",
      "82:\tlearn: 0.0510750\ttotal: 1.03s\tremaining: 2.07s\n",
      "83:\tlearn: 0.0503482\ttotal: 1.05s\tremaining: 2.07s\n",
      "84:\tlearn: 0.0495548\ttotal: 1.06s\tremaining: 2.06s\n",
      "85:\tlearn: 0.0487527\ttotal: 1.08s\tremaining: 2.05s\n",
      "86:\tlearn: 0.0477472\ttotal: 1.09s\tremaining: 2.05s\n",
      "87:\tlearn: 0.0469265\ttotal: 1.12s\tremaining: 2.07s\n",
      "88:\tlearn: 0.0461802\ttotal: 1.13s\tremaining: 2.04s\n",
      "89:\tlearn: 0.0451029\ttotal: 1.13s\tremaining: 2.02s\n",
      "90:\tlearn: 0.0448075\ttotal: 1.14s\tremaining: 1.99s\n",
      "91:\tlearn: 0.0444887\ttotal: 1.14s\tremaining: 1.96s\n",
      "92:\tlearn: 0.0435327\ttotal: 1.15s\tremaining: 1.94s\n",
      "93:\tlearn: 0.0429757\ttotal: 1.15s\tremaining: 1.91s\n",
      "94:\tlearn: 0.0421471\ttotal: 1.18s\tremaining: 1.93s\n",
      "95:\tlearn: 0.0416923\ttotal: 1.2s\tremaining: 1.93s\n",
      "96:\tlearn: 0.0411501\ttotal: 1.22s\tremaining: 1.92s\n",
      "97:\tlearn: 0.0407463\ttotal: 1.22s\tremaining: 1.89s\n",
      "98:\tlearn: 0.0400730\ttotal: 1.23s\tremaining: 1.87s\n",
      "99:\tlearn: 0.0392719\ttotal: 1.23s\tremaining: 1.84s\n",
      "100:\tlearn: 0.0384949\ttotal: 1.24s\tremaining: 1.82s\n",
      "101:\tlearn: 0.0380424\ttotal: 1.24s\tremaining: 1.8s\n",
      "102:\tlearn: 0.0375534\ttotal: 1.24s\tremaining: 1.78s\n",
      "103:\tlearn: 0.0369163\ttotal: 1.25s\tremaining: 1.75s\n",
      "104:\tlearn: 0.0363775\ttotal: 1.27s\tremaining: 1.75s\n",
      "105:\tlearn: 0.0363470\ttotal: 1.28s\tremaining: 1.74s\n",
      "106:\tlearn: 0.0358032\ttotal: 1.29s\tremaining: 1.72s\n",
      "107:\tlearn: 0.0355740\ttotal: 1.29s\tremaining: 1.7s\n",
      "108:\tlearn: 0.0351850\ttotal: 1.3s\tremaining: 1.69s\n",
      "109:\tlearn: 0.0348883\ttotal: 1.34s\tremaining: 1.7s\n",
      "110:\tlearn: 0.0342387\ttotal: 1.37s\tremaining: 1.72s\n",
      "111:\tlearn: 0.0342176\ttotal: 1.38s\tremaining: 1.71s\n",
      "112:\tlearn: 0.0341611\ttotal: 1.41s\tremaining: 1.71s\n",
      "113:\tlearn: 0.0338440\ttotal: 1.42s\tremaining: 1.69s\n",
      "114:\tlearn: 0.0334742\ttotal: 1.42s\tremaining: 1.67s\n",
      "115:\tlearn: 0.0334742\ttotal: 1.42s\tremaining: 1.64s\n",
      "116:\tlearn: 0.0331953\ttotal: 1.43s\tremaining: 1.62s\n",
      "117:\tlearn: 0.0329971\ttotal: 1.43s\tremaining: 1.6s\n",
      "118:\tlearn: 0.0326603\ttotal: 1.44s\tremaining: 1.58s\n",
      "119:\tlearn: 0.0320769\ttotal: 1.44s\tremaining: 1.56s\n",
      "120:\tlearn: 0.0317538\ttotal: 1.46s\tremaining: 1.56s\n",
      "121:\tlearn: 0.0314922\ttotal: 1.47s\tremaining: 1.54s\n",
      "122:\tlearn: 0.0311095\ttotal: 1.47s\tremaining: 1.52s\n",
      "123:\tlearn: 0.0306831\ttotal: 1.48s\tremaining: 1.5s\n",
      "124:\tlearn: 0.0302374\ttotal: 1.49s\tremaining: 1.49s\n",
      "125:\tlearn: 0.0298225\ttotal: 1.5s\tremaining: 1.47s\n",
      "126:\tlearn: 0.0293104\ttotal: 1.5s\tremaining: 1.45s\n",
      "127:\tlearn: 0.0292947\ttotal: 1.51s\tremaining: 1.44s\n",
      "128:\tlearn: 0.0292946\ttotal: 1.52s\tremaining: 1.42s\n",
      "129:\tlearn: 0.0291597\ttotal: 1.52s\tremaining: 1.41s\n",
      "130:\tlearn: 0.0287417\ttotal: 1.53s\tremaining: 1.39s\n",
      "131:\tlearn: 0.0286969\ttotal: 1.53s\tremaining: 1.37s\n",
      "132:\tlearn: 0.0283305\ttotal: 1.54s\tremaining: 1.35s\n",
      "133:\tlearn: 0.0281453\ttotal: 1.56s\tremaining: 1.35s\n",
      "134:\tlearn: 0.0277045\ttotal: 1.58s\tremaining: 1.35s\n",
      "135:\tlearn: 0.0273791\ttotal: 1.6s\tremaining: 1.34s\n",
      "136:\tlearn: 0.0268857\ttotal: 1.61s\tremaining: 1.33s\n",
      "137:\tlearn: 0.0268856\ttotal: 1.62s\tremaining: 1.32s\n",
      "138:\tlearn: 0.0268856\ttotal: 1.64s\tremaining: 1.31s\n",
      "139:\tlearn: 0.0268855\ttotal: 1.67s\tremaining: 1.31s\n",
      "140:\tlearn: 0.0264257\ttotal: 1.69s\tremaining: 1.31s\n",
      "141:\tlearn: 0.0262144\ttotal: 1.72s\tremaining: 1.31s\n",
      "142:\tlearn: 0.0260636\ttotal: 1.75s\tremaining: 1.31s\n",
      "143:\tlearn: 0.0259315\ttotal: 1.77s\tremaining: 1.3s\n",
      "144:\tlearn: 0.0256612\ttotal: 1.8s\tremaining: 1.3s\n",
      "145:\tlearn: 0.0254051\ttotal: 1.82s\tremaining: 1.29s\n",
      "146:\tlearn: 0.0250792\ttotal: 1.83s\tremaining: 1.28s\n",
      "147:\tlearn: 0.0249047\ttotal: 1.85s\tremaining: 1.27s\n",
      "148:\tlearn: 0.0247360\ttotal: 1.85s\tremaining: 1.26s\n",
      "149:\tlearn: 0.0243322\ttotal: 1.86s\tremaining: 1.24s\n",
      "150:\tlearn: 0.0239952\ttotal: 1.86s\tremaining: 1.22s\n",
      "151:\tlearn: 0.0238448\ttotal: 1.87s\tremaining: 1.2s\n",
      "152:\tlearn: 0.0235573\ttotal: 1.88s\tremaining: 1.19s\n",
      "153:\tlearn: 0.0231285\ttotal: 1.88s\tremaining: 1.17s\n",
      "154:\tlearn: 0.0228527\ttotal: 1.89s\tremaining: 1.16s\n",
      "155:\tlearn: 0.0228526\ttotal: 1.91s\tremaining: 1.15s\n",
      "156:\tlearn: 0.0227114\ttotal: 1.94s\tremaining: 1.15s\n",
      "157:\tlearn: 0.0224858\ttotal: 1.95s\tremaining: 1.13s\n",
      "158:\tlearn: 0.0224131\ttotal: 1.95s\tremaining: 1.12s\n",
      "159:\tlearn: 0.0222234\ttotal: 1.96s\tremaining: 1.1s\n",
      "160:\tlearn: 0.0221746\ttotal: 1.96s\tremaining: 1.08s\n",
      "161:\tlearn: 0.0220821\ttotal: 1.97s\tremaining: 1.07s\n",
      "162:\tlearn: 0.0220327\ttotal: 1.97s\tremaining: 1.05s\n",
      "163:\tlearn: 0.0216914\ttotal: 1.98s\tremaining: 1.04s\n",
      "164:\tlearn: 0.0215207\ttotal: 2s\tremaining: 1.03s\n",
      "165:\tlearn: 0.0212751\ttotal: 2.02s\tremaining: 1.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166:\tlearn: 0.0209966\ttotal: 2.03s\tremaining: 1.01s\n",
      "167:\tlearn: 0.0208302\ttotal: 2.05s\tremaining: 1s\n",
      "168:\tlearn: 0.0206747\ttotal: 2.06s\tremaining: 990ms\n",
      "169:\tlearn: 0.0205224\ttotal: 2.08s\tremaining: 980ms\n",
      "170:\tlearn: 0.0202840\ttotal: 2.1s\tremaining: 969ms\n",
      "171:\tlearn: 0.0199373\ttotal: 2.11s\tremaining: 958ms\n",
      "172:\tlearn: 0.0198578\ttotal: 2.12s\tremaining: 942ms\n",
      "173:\tlearn: 0.0197136\ttotal: 2.12s\tremaining: 927ms\n",
      "174:\tlearn: 0.0194405\ttotal: 2.13s\tremaining: 911ms\n",
      "175:\tlearn: 0.0192345\ttotal: 2.13s\tremaining: 896ms\n",
      "176:\tlearn: 0.0190863\ttotal: 2.14s\tremaining: 881ms\n",
      "177:\tlearn: 0.0189483\ttotal: 2.14s\tremaining: 866ms\n",
      "178:\tlearn: 0.0188720\ttotal: 2.16s\tremaining: 856ms\n",
      "179:\tlearn: 0.0187315\ttotal: 2.16s\tremaining: 841ms\n",
      "180:\tlearn: 0.0185622\ttotal: 2.17s\tremaining: 826ms\n",
      "181:\tlearn: 0.0185621\ttotal: 2.17s\tremaining: 811ms\n",
      "182:\tlearn: 0.0184635\ttotal: 2.18s\tremaining: 797ms\n",
      "183:\tlearn: 0.0183388\ttotal: 2.18s\tremaining: 782ms\n",
      "184:\tlearn: 0.0181248\ttotal: 2.19s\tremaining: 768ms\n",
      "185:\tlearn: 0.0181248\ttotal: 2.22s\tremaining: 764ms\n",
      "186:\tlearn: 0.0180163\ttotal: 2.22s\tremaining: 750ms\n",
      "187:\tlearn: 0.0179067\ttotal: 2.23s\tremaining: 735ms\n",
      "188:\tlearn: 0.0178247\ttotal: 2.23s\tremaining: 721ms\n",
      "189:\tlearn: 0.0177324\ttotal: 2.24s\tremaining: 707ms\n",
      "190:\tlearn: 0.0176625\ttotal: 2.24s\tremaining: 693ms\n",
      "191:\tlearn: 0.0175862\ttotal: 2.25s\tremaining: 679ms\n",
      "192:\tlearn: 0.0174734\ttotal: 2.28s\tremaining: 674ms\n",
      "193:\tlearn: 0.0173934\ttotal: 2.3s\tremaining: 664ms\n",
      "194:\tlearn: 0.0171649\ttotal: 2.33s\tremaining: 657ms\n",
      "195:\tlearn: 0.0169776\ttotal: 2.36s\tremaining: 651ms\n",
      "196:\tlearn: 0.0168058\ttotal: 2.38s\tremaining: 640ms\n",
      "197:\tlearn: 0.0166140\ttotal: 2.41s\tremaining: 633ms\n",
      "198:\tlearn: 0.0165271\ttotal: 2.42s\tremaining: 622ms\n",
      "199:\tlearn: 0.0163169\ttotal: 2.46s\tremaining: 614ms\n",
      "200:\tlearn: 0.0162063\ttotal: 2.5s\tremaining: 611ms\n",
      "201:\tlearn: 0.0160277\ttotal: 2.54s\tremaining: 603ms\n",
      "202:\tlearn: 0.0158004\ttotal: 2.55s\tremaining: 591ms\n",
      "203:\tlearn: 0.0157037\ttotal: 2.56s\tremaining: 577ms\n",
      "204:\tlearn: 0.0154402\ttotal: 2.56s\tremaining: 563ms\n",
      "205:\tlearn: 0.0152745\ttotal: 2.58s\tremaining: 552ms\n",
      "206:\tlearn: 0.0151762\ttotal: 2.59s\tremaining: 537ms\n",
      "207:\tlearn: 0.0150613\ttotal: 2.6s\tremaining: 525ms\n",
      "208:\tlearn: 0.0149510\ttotal: 2.61s\tremaining: 513ms\n",
      "209:\tlearn: 0.0148777\ttotal: 2.63s\tremaining: 501ms\n",
      "210:\tlearn: 0.0148042\ttotal: 2.63s\tremaining: 487ms\n",
      "211:\tlearn: 0.0148042\ttotal: 2.64s\tremaining: 473ms\n",
      "212:\tlearn: 0.0147384\ttotal: 2.65s\tremaining: 460ms\n",
      "213:\tlearn: 0.0146177\ttotal: 2.65s\tremaining: 446ms\n",
      "214:\tlearn: 0.0145474\ttotal: 2.65s\tremaining: 432ms\n",
      "215:\tlearn: 0.0143869\ttotal: 2.66s\tremaining: 419ms\n",
      "216:\tlearn: 0.0143521\ttotal: 2.66s\tremaining: 405ms\n",
      "217:\tlearn: 0.0142434\ttotal: 2.67s\tremaining: 392ms\n",
      "218:\tlearn: 0.0141704\ttotal: 2.68s\tremaining: 379ms\n",
      "219:\tlearn: 0.0140551\ttotal: 2.68s\tremaining: 366ms\n",
      "220:\tlearn: 0.0139633\ttotal: 2.69s\tremaining: 353ms\n",
      "221:\tlearn: 0.0139633\ttotal: 2.71s\tremaining: 341ms\n",
      "222:\tlearn: 0.0138749\ttotal: 2.74s\tremaining: 332ms\n",
      "223:\tlearn: 0.0137231\ttotal: 2.77s\tremaining: 322ms\n",
      "224:\tlearn: 0.0135963\ttotal: 2.79s\tremaining: 310ms\n",
      "225:\tlearn: 0.0135549\ttotal: 2.8s\tremaining: 297ms\n",
      "226:\tlearn: 0.0134607\ttotal: 2.81s\tremaining: 284ms\n",
      "227:\tlearn: 0.0133834\ttotal: 2.81s\tremaining: 271ms\n",
      "228:\tlearn: 0.0133119\ttotal: 2.81s\tremaining: 258ms\n",
      "229:\tlearn: 0.0131696\ttotal: 2.83s\tremaining: 246ms\n",
      "230:\tlearn: 0.0131295\ttotal: 2.83s\tremaining: 233ms\n",
      "231:\tlearn: 0.0129771\ttotal: 2.84s\tremaining: 220ms\n",
      "232:\tlearn: 0.0129427\ttotal: 2.85s\tremaining: 208ms\n",
      "233:\tlearn: 0.0128719\ttotal: 2.85s\tremaining: 195ms\n",
      "234:\tlearn: 0.0128176\ttotal: 2.86s\tremaining: 183ms\n",
      "235:\tlearn: 0.0127166\ttotal: 2.87s\tremaining: 170ms\n",
      "236:\tlearn: 0.0126285\ttotal: 2.87s\tremaining: 157ms\n",
      "237:\tlearn: 0.0125770\ttotal: 2.88s\tremaining: 145ms\n",
      "238:\tlearn: 0.0124912\ttotal: 2.88s\tremaining: 133ms\n",
      "239:\tlearn: 0.0123870\ttotal: 2.88s\tremaining: 120ms\n",
      "240:\tlearn: 0.0123335\ttotal: 2.89s\tremaining: 108ms\n",
      "241:\tlearn: 0.0122616\ttotal: 2.91s\tremaining: 96.1ms\n",
      "242:\tlearn: 0.0122616\ttotal: 2.91s\tremaining: 83.8ms\n",
      "243:\tlearn: 0.0121958\ttotal: 2.91s\tremaining: 71.7ms\n",
      "244:\tlearn: 0.0121532\ttotal: 2.92s\tremaining: 59.6ms\n",
      "245:\tlearn: 0.0120573\ttotal: 2.95s\tremaining: 48ms\n",
      "246:\tlearn: 0.0119391\ttotal: 2.97s\tremaining: 36ms\n",
      "247:\tlearn: 0.0118833\ttotal: 2.98s\tremaining: 24.1ms\n",
      "248:\tlearn: 0.0118832\ttotal: 3.02s\tremaining: 12.1ms\n",
      "249:\tlearn: 0.0118460\ttotal: 3.02s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedb3e19d8cd43f18f3b87ca6e080cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.4821039306111507, Recall = 0.9821233126596133, Aging Rate = 0.9174183203046703, Precision = 0.5881581822154249, f1 = 0.735720142115332\n",
      "Epoch 2: Train Loss = 0.3101193755118396, Recall = 0.9624224735497994, Aging Rate = 0.6867107636800962, Precision = 0.7699941622883829, f1 = 0.8555213231717205\n",
      "Epoch 3: Train Loss = 0.24383057199611194, Recall = 0.967894928858081, Aging Rate = 0.6462216877129685, Precision = 0.8228908188585607, f1 = 0.8895222129086336\n",
      "Epoch 4: Train Loss = 0.2067677453951789, Recall = 0.9708135716891645, Aging Rate = 0.6275806774904791, Precision = 0.8498882146279144, f1 = 0.9063351498637602\n",
      "Epoch 5: Train Loss = 0.17387803069728963, Recall = 0.974461875228019, Aging Rate = 0.6117458408498697, Precision = 0.8751638269986893, f1 = 0.9221474192991541\n",
      "Test Loss = 0.1450389976861315, Recall = 0.9865012769062386, Aging Rate = 0.6055321707757065, precision = 0.895067858325058\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.13696667121894662, Recall = 0.9810288215979569, Aging Rate = 0.5925035077169774, Precision = 0.9096752368064953, f1 = 0.944005616991399\n",
      "Epoch 7: Train Loss = 0.11551666526188471, Recall = 0.9832178037212697, Aging Rate = 0.5840849869713369, Precision = 0.9248455730954015, f1 = 0.9531388152077808\n",
      "Epoch 8: Train Loss = 0.09704855788110085, Recall = 0.9890550893834367, Aging Rate = 0.578673080777711, Precision = 0.9390370626948389, f1 = 0.9633972992181946\n",
      "Epoch 9: Train Loss = 0.08422073213233096, Recall = 0.990149580445093, Aging Rate = 0.5738624974944879, Precision = 0.9479566887879847, f1 = 0.9685938615274804\n",
      "Epoch 10: Train Loss = 0.07257527032317129, Recall = 0.9912440715067493, Aging Rate = 0.5670475045099218, Precision = 0.9604100388829975, f1 = 0.9755834829443447\n",
      "Test Loss = 0.06263214478440587, Recall = 0.9959868661072602, Aging Rate = 0.571858087793145, precision = 0.9568874868559412\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.061996414308397196, Recall = 0.9937978839839474, Aging Rate = 0.5650430948085788, Precision = 0.9663001064207166, f1 = 0.9798561151079137\n",
      "Epoch 12: Train Loss = 0.05448831035405559, Recall = 0.9945275446917183, Aging Rate = 0.5638404489877731, Precision = 0.9690721649484536, f1 = 0.9816348577601729\n",
      "Epoch 13: Train Loss = 0.04857983238999555, Recall = 0.9945275446917183, Aging Rate = 0.5582281018240128, Precision = 0.9788150807899462, f1 = 0.9866087585957294\n",
      "Epoch 14: Train Loss = 0.04273724637583636, Recall = 0.9963516964611455, Aging Rate = 0.5584285427941471, Precision = 0.9802584350323044, f1 = 0.9882395512936493\n",
      "Epoch 15: Train Loss = 0.0387634462465066, Recall = 0.9970813571689164, Aging Rate = 0.5560232511525356, Precision = 0.9852198990627253, f1 = 0.9911151405258386\n",
      "Test Loss = 0.032241847858917115, Recall = 0.9970813571689164, Aging Rate = 0.5522148727199839, precision = 0.9920145190562614\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03412552376528449, Recall = 0.9952572053994893, Aging Rate = 0.5536179595109241, Precision = 0.9876900796524257, f1 = 0.991459204070507\n",
      "Epoch 17: Train Loss = 0.03126627153263921, Recall = 0.9974461875228019, Aging Rate = 0.5548206053317298, Precision = 0.9877167630057804, f1 = 0.9925576329642403\n",
      "Epoch 18: Train Loss = 0.02784414648245083, Recall = 0.9974461875228019, Aging Rate = 0.5538184004810583, Precision = 0.9895041621425986, f1 = 0.9934593023255814\n",
      "Epoch 19: Train Loss = 0.02605094417808684, Recall = 0.9978110178766874, Aging Rate = 0.5530166366005211, Precision = 0.9913011960855382, f1 = 0.9945454545454545\n",
      "Epoch 20: Train Loss = 0.022945065485057044, Recall = 0.9985406785844583, Aging Rate = 0.5524153136901183, Precision = 0.9931059506531205, f1 = 0.99581589958159\n",
      "Test Loss = 0.02271202045212255, Recall = 0.9981758482305728, Aging Rate = 0.550210463018641, precision = 0.9967213114754099\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.021983260654624626, Recall = 0.9985406785844583, Aging Rate = 0.5530166366005211, Precision = 0.9920260964117433, f1 = 0.9952727272727273\n",
      "Epoch 22: Train Loss = 0.01967560064617308, Recall = 0.9992703392922291, Aging Rate = 0.5534175185407897, Precision = 0.9920318725099602, f1 = 0.9956379498364232\n",
      "Epoch 23: Train Loss = 0.017062412826898795, Recall = 0.9992703392922291, Aging Rate = 0.5526157546602526, Precision = 0.9934711643090316, f1 = 0.9963623135685704\n",
      "Epoch 24: Train Loss = 0.016288489926368218, Recall = 0.9996351696461145, Aging Rate = 0.5524153136901183, Precision = 0.9941944847605225, f1 = 0.9969074040385664\n",
      "Epoch 25: Train Loss = 0.015603268932716166, Recall = 0.9996351696461145, Aging Rate = 0.5520144317498497, Precision = 0.9949164851125636, f1 = 0.997270245677889\n",
      "Test Loss = 0.012473717949034504, Recall = 0.9996351696461145, Aging Rate = 0.5508117859290439, precision = 0.9970887918486172\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.014209833474951878, Recall = 0.9996351696461145, Aging Rate = 0.5512126678693124, Precision = 0.9963636363636363, f1 = 0.9979967219085777\n",
      "Epoch 27: Train Loss = 0.01550263580501208, Recall = 0.9989055089383436, Aging Rate = 0.5518139907797154, Precision = 0.9945513984743916, f1 = 0.9967236985802693\n",
      "Epoch 28: Train Loss = 0.01229987308023825, Recall = 0.9996351696461145, Aging Rate = 0.5516135498095811, Precision = 0.9956395348837209, f1 = 0.9976333515383214\n",
      "Epoch 29: Train Loss = 0.011226206354268223, Recall = 1.0, Aging Rate = 0.5510122268991782, Precision = 0.9970898508548564, f1 = 0.9985428051001822\n",
      "Epoch 30: Train Loss = 0.010352237752333916, Recall = 1.0, Aging Rate = 0.5508117859290439, Precision = 0.99745269286754, f1 = 0.9987247221716159\n",
      "Test Loss = 0.009298275839819261, Recall = 1.0, Aging Rate = 0.5512126678693124, precision = 0.9967272727272727\n",
      "\n",
      "Epoch 31: Train Loss = 0.010280243469843494, Recall = 0.9996351696461145, Aging Rate = 0.550210463018641, Precision = 0.9981785063752276, f1 = 0.998906306963179\n",
      "Epoch 32: Train Loss = 0.009207975136479791, Recall = 1.0, Aging Rate = 0.5510122268991782, Precision = 0.9970898508548564, f1 = 0.9985428051001822\n",
      "Epoch 33: Train Loss = 0.008870721538826497, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Epoch 34: Train Loss = 0.008158895964955466, Recall = 1.0, Aging Rate = 0.5504109039887753, Precision = 0.9981791697013839, f1 = 0.9990887552396573\n",
      "Epoch 35: Train Loss = 0.008032270301451488, Recall = 1.0, Aging Rate = 0.5508117859290439, Precision = 0.99745269286754, f1 = 0.9987247221716159\n",
      "Test Loss = 0.006350284412904221, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, precision = 0.9992706053975201\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.007598548834498671, Recall = 0.9996351696461145, Aging Rate = 0.5498095810783724, Precision = 0.998906306963179, f1 = 0.99927060539752\n",
      "Epoch 37: Train Loss = 0.0066905540993221005, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 38: Train Loss = 0.006914976313911822, Recall = 0.9996351696461145, Aging Rate = 0.550210463018641, Precision = 0.9981785063752276, f1 = 0.998906306963179\n",
      "Epoch 39: Train Loss = 0.006452264985072246, Recall = 0.9996351696461145, Aging Rate = 0.5500100220485067, Precision = 0.9985422740524781, f1 = 0.9990884229717412\n",
      "Epoch 40: Train Loss = 0.0067969369432762175, Recall = 0.9992703392922291, Aging Rate = 0.5500100220485067, Precision = 0.9981778425655977, f1 = 0.9987237921604376\n",
      "Test Loss = 0.005233287324477043, Recall = 1.0, Aging Rate = 0.5500100220485067, precision = 0.9989067055393586\n",
      "\n",
      "Epoch 41: Train Loss = 0.006028479545536429, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 42: Train Loss = 0.005738704561560713, Recall = 1.0, Aging Rate = 0.5504109039887753, Precision = 0.9981791697013839, f1 = 0.9990887552396573\n",
      "Epoch 43: Train Loss = 0.00575590090582356, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, Precision = 0.9992706053975201, f1 = 0.9994528542768557\n",
      "Epoch 44: Train Loss = 0.005025534063947642, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 45: Train Loss = 0.00491257569134552, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Test Loss = 0.004382358121869185, Recall = 1.0, Aging Rate = 0.5500100220485067, precision = 0.9989067055393586\n",
      "\n",
      "Epoch 46: Train Loss = 0.004709767427752427, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.0042748377056585915, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 48: Train Loss = 0.004433122315210895, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 49: Train Loss = 0.0045226787396729605, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 50: Train Loss = 0.004234264518550021, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.004765034567935836, Recall = 1.0, Aging Rate = 0.550210463018641, precision = 0.9985428051001821\n",
      "\n",
      "Epoch 51: Train Loss = 0.004176058195537715, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 52: Train Loss = 0.0038123975897039475, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 53: Train Loss = 0.0036531581372484835, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 54: Train Loss = 0.004033315980926045, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 55: Train Loss = 0.003994182972314004, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Test Loss = 0.0027615789999902744, Recall = 1.0, Aging Rate = 0.5498095810783724, precision = 0.999270871308786\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.0036000381781082875, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 57: Train Loss = 0.003230952400427426, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 58: Train Loss = 0.003183491708362454, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 59: Train Loss = 0.0032570865536812378, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 60: Train Loss = 0.0032510533566254577, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0030710912809129836, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.0033114959627142003, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 62: Train Loss = 0.003566652830732334, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 63: Train Loss = 0.002974169939936722, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 64: Train Loss = 0.002903882290535239, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 65: Train Loss = 0.0030086111392333766, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.003758497907922731, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 66: Train Loss = 0.0036023725701563583, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 67: Train Loss = 0.003400996806782122, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 68: Train Loss = 0.00232088922637394, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 69: Train Loss = 0.002683372691132376, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 70: Train Loss = 0.004100851633792359, Recall = 0.9996351696461145, Aging Rate = 0.5500100220485067, Precision = 0.9985422740524781, f1 = 0.9990884229717412\n",
      "Test Loss = 0.0027203791583280605, Recall = 1.0, Aging Rate = 0.5498095810783724, precision = 0.999270871308786\n",
      "\n",
      "Epoch 71: Train Loss = 0.003119737017017898, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 72: Train Loss = 0.0026673406116495654, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 73: Train Loss = 0.002342004202540361, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0024949006688116332, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.003184479530613237, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.0029867971038767286, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 76: Train Loss = 0.003144228414694543, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 77: Train Loss = 0.0024509258698212836, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 78: Train Loss = 0.002544306939703745, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 79: Train Loss = 0.0026524684900232367, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 80: Train Loss = 0.0030315572758630666, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020881220959468563, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.0026524604624295956, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 82: Train Loss = 0.0027697638540005462, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 83: Train Loss = 0.003108964256937045, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, Precision = 0.9992706053975201, f1 = 0.9994528542768557\n",
      "Epoch 84: Train Loss = 0.0027895474675965325, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 85: Train Loss = 0.0023626632247165692, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0018741315387347688, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.002230811323281258, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.002284932602397608, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.002418659361880452, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.002489462103205588, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0038742581707908717, Recall = 0.9996351696461145, Aging Rate = 0.5498095810783724, Precision = 0.998906306963179, f1 = 0.99927060539752\n",
      "Test Loss = 0.0027376908576909336, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 91: Train Loss = 0.0028513712264051578, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 92: Train Loss = 0.0021160382437330253, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.0022517636736017112, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0022414633858395265, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.002437079309544282, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021164459405272023, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 96: Train Loss = 0.002306366228557508, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: Train Loss = 0.00225458267199648, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.002229507169274639, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0022693571542559752, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0027177309721284917, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0029266486657489564, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 101: Train Loss = 0.002889747269739845, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 102: Train Loss = 0.002441454750177144, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.002316995412844916, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0022359537733556345, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 105: Train Loss = 0.003100468329472983, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.002693739963020016, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.003445208718885228, Recall = 0.9996351696461145, Aging Rate = 0.5492082581679696, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.002443529801182994, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.0022542118615685317, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.002042585665754538, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.0021981632314899803, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019945179113048943, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0022570224513189063, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 112: Train Loss = 0.0024839793951216076, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 113: Train Loss = 0.0021846689187056717, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.002319706899041523, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.002497794224628064, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0020306483257859108, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.0023221377283148058, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 117: Train Loss = 0.0029006695966513505, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.002439582779901623, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.002096057363464162, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.0021504395956797595, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001870297639684684, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.002265524574532869, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 122: Train Loss = 0.002237590405706217, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.0025018346221280493, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.0030867288794124335, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 125: Train Loss = 0.002464024667161608, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.001905110686133347, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 126: Train Loss = 0.0025738279616520313, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 127: Train Loss = 0.002738607363154864, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 128: Train Loss = 0.002460513603526335, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.0023710586287052867, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.0023379578411760766, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019807994068318543, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 130.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6074565\ttotal: 25.2ms\tremaining: 6.28s\n",
      "1:\tlearn: 0.5413743\ttotal: 39.8ms\tremaining: 4.93s\n",
      "2:\tlearn: 0.4899953\ttotal: 55.5ms\tremaining: 4.57s\n",
      "3:\tlearn: 0.4597689\ttotal: 71.2ms\tremaining: 4.38s\n",
      "4:\tlearn: 0.4350615\ttotal: 103ms\tremaining: 5.02s\n",
      "5:\tlearn: 0.4116907\ttotal: 118ms\tremaining: 4.8s\n",
      "6:\tlearn: 0.3832564\ttotal: 134ms\tremaining: 4.66s\n",
      "7:\tlearn: 0.3681215\ttotal: 150ms\tremaining: 4.54s\n",
      "8:\tlearn: 0.3499491\ttotal: 155ms\tremaining: 4.14s\n",
      "9:\tlearn: 0.3342547\ttotal: 166ms\tremaining: 3.99s\n",
      "10:\tlearn: 0.3213385\ttotal: 172ms\tremaining: 3.74s\n",
      "11:\tlearn: 0.3001336\ttotal: 181ms\tremaining: 3.6s\n",
      "12:\tlearn: 0.2845811\ttotal: 213ms\tremaining: 3.88s\n",
      "13:\tlearn: 0.2760095\ttotal: 260ms\tremaining: 4.39s\n",
      "14:\tlearn: 0.2663769\ttotal: 308ms\tremaining: 4.83s\n",
      "15:\tlearn: 0.2575447\ttotal: 340ms\tremaining: 4.97s\n",
      "16:\tlearn: 0.2501982\ttotal: 371ms\tremaining: 5.09s\n",
      "17:\tlearn: 0.2411903\ttotal: 377ms\tremaining: 4.85s\n",
      "18:\tlearn: 0.2348276\ttotal: 381ms\tremaining: 4.64s\n",
      "19:\tlearn: 0.2288105\ttotal: 386ms\tremaining: 4.44s\n",
      "20:\tlearn: 0.2200748\ttotal: 391ms\tremaining: 4.26s\n",
      "21:\tlearn: 0.2132537\ttotal: 418ms\tremaining: 4.33s\n",
      "22:\tlearn: 0.2040438\ttotal: 448ms\tremaining: 4.42s\n",
      "23:\tlearn: 0.1925561\ttotal: 453ms\tremaining: 4.26s\n",
      "24:\tlearn: 0.1850522\ttotal: 458ms\tremaining: 4.12s\n",
      "25:\tlearn: 0.1788410\ttotal: 462ms\tremaining: 3.98s\n",
      "26:\tlearn: 0.1728064\ttotal: 467ms\tremaining: 3.86s\n",
      "27:\tlearn: 0.1694939\ttotal: 472ms\tremaining: 3.74s\n",
      "28:\tlearn: 0.1646495\ttotal: 477ms\tremaining: 3.63s\n",
      "29:\tlearn: 0.1576943\ttotal: 529ms\tremaining: 3.88s\n",
      "30:\tlearn: 0.1567790\ttotal: 533ms\tremaining: 3.77s\n",
      "31:\tlearn: 0.1517705\ttotal: 538ms\tremaining: 3.66s\n",
      "32:\tlearn: 0.1480684\ttotal: 554ms\tremaining: 3.64s\n",
      "33:\tlearn: 0.1448324\ttotal: 570ms\tremaining: 3.62s\n",
      "34:\tlearn: 0.1429915\ttotal: 602ms\tremaining: 3.7s\n",
      "35:\tlearn: 0.1397206\ttotal: 607ms\tremaining: 3.61s\n",
      "36:\tlearn: 0.1343360\ttotal: 612ms\tremaining: 3.52s\n",
      "37:\tlearn: 0.1299361\ttotal: 616ms\tremaining: 3.44s\n",
      "38:\tlearn: 0.1266919\ttotal: 621ms\tremaining: 3.36s\n",
      "39:\tlearn: 0.1231539\ttotal: 626ms\tremaining: 3.28s\n",
      "40:\tlearn: 0.1200797\ttotal: 633ms\tremaining: 3.23s\n",
      "41:\tlearn: 0.1191223\ttotal: 656ms\tremaining: 3.25s\n",
      "42:\tlearn: 0.1142374\ttotal: 662ms\tremaining: 3.19s\n",
      "43:\tlearn: 0.1112079\ttotal: 667ms\tremaining: 3.12s\n",
      "44:\tlearn: 0.1089748\ttotal: 671ms\tremaining: 3.06s\n",
      "45:\tlearn: 0.1066531\ttotal: 676ms\tremaining: 3s\n",
      "46:\tlearn: 0.1042069\ttotal: 681ms\tremaining: 2.94s\n",
      "47:\tlearn: 0.1029413\ttotal: 695ms\tremaining: 2.92s\n",
      "48:\tlearn: 0.1000261\ttotal: 699ms\tremaining: 2.87s\n",
      "49:\tlearn: 0.0987925\ttotal: 704ms\tremaining: 2.82s\n",
      "50:\tlearn: 0.0950809\ttotal: 709ms\tremaining: 2.77s\n",
      "51:\tlearn: 0.0938466\ttotal: 714ms\tremaining: 2.72s\n",
      "52:\tlearn: 0.0904493\ttotal: 726ms\tremaining: 2.7s\n",
      "53:\tlearn: 0.0874948\ttotal: 731ms\tremaining: 2.65s\n",
      "54:\tlearn: 0.0861445\ttotal: 736ms\tremaining: 2.61s\n",
      "55:\tlearn: 0.0836962\ttotal: 741ms\tremaining: 2.57s\n",
      "56:\tlearn: 0.0805268\ttotal: 745ms\tremaining: 2.52s\n",
      "57:\tlearn: 0.0780393\ttotal: 772ms\tremaining: 2.56s\n",
      "58:\tlearn: 0.0754739\ttotal: 788ms\tremaining: 2.55s\n",
      "59:\tlearn: 0.0732605\ttotal: 820ms\tremaining: 2.6s\n",
      "60:\tlearn: 0.0719771\ttotal: 836ms\tremaining: 2.59s\n",
      "61:\tlearn: 0.0695451\ttotal: 868ms\tremaining: 2.63s\n",
      "62:\tlearn: 0.0684942\ttotal: 899ms\tremaining: 2.67s\n",
      "63:\tlearn: 0.0681198\ttotal: 964ms\tremaining: 2.8s\n",
      "64:\tlearn: 0.0675555\ttotal: 979ms\tremaining: 2.79s\n",
      "65:\tlearn: 0.0667148\ttotal: 983ms\tremaining: 2.74s\n",
      "66:\tlearn: 0.0655494\ttotal: 1.01s\tremaining: 2.75s\n",
      "67:\tlearn: 0.0641212\ttotal: 1.01s\tremaining: 2.71s\n",
      "68:\tlearn: 0.0627878\ttotal: 1.02s\tremaining: 2.67s\n",
      "69:\tlearn: 0.0621375\ttotal: 1.02s\tremaining: 2.63s\n",
      "70:\tlearn: 0.0607312\ttotal: 1.03s\tremaining: 2.59s\n",
      "71:\tlearn: 0.0602002\ttotal: 1.04s\tremaining: 2.57s\n",
      "72:\tlearn: 0.0593913\ttotal: 1.04s\tremaining: 2.53s\n",
      "73:\tlearn: 0.0589948\ttotal: 1.05s\tremaining: 2.49s\n",
      "74:\tlearn: 0.0577330\ttotal: 1.05s\tremaining: 2.46s\n",
      "75:\tlearn: 0.0564766\ttotal: 1.06s\tremaining: 2.42s\n",
      "76:\tlearn: 0.0563915\ttotal: 1.06s\tremaining: 2.39s\n",
      "77:\tlearn: 0.0554310\ttotal: 1.07s\tremaining: 2.36s\n",
      "78:\tlearn: 0.0547931\ttotal: 1.07s\tremaining: 2.33s\n",
      "79:\tlearn: 0.0534433\ttotal: 1.08s\tremaining: 2.3s\n",
      "80:\tlearn: 0.0531479\ttotal: 1.1s\tremaining: 2.29s\n",
      "81:\tlearn: 0.0523053\ttotal: 1.11s\tremaining: 2.28s\n",
      "82:\tlearn: 0.0513370\ttotal: 1.14s\tremaining: 2.3s\n",
      "83:\tlearn: 0.0504294\ttotal: 1.16s\tremaining: 2.29s\n",
      "84:\tlearn: 0.0497457\ttotal: 1.17s\tremaining: 2.28s\n",
      "85:\tlearn: 0.0483174\ttotal: 1.18s\tremaining: 2.25s\n",
      "86:\tlearn: 0.0471332\ttotal: 1.18s\tremaining: 2.22s\n",
      "87:\tlearn: 0.0461852\ttotal: 1.19s\tremaining: 2.19s\n",
      "88:\tlearn: 0.0459586\ttotal: 1.19s\tremaining: 2.16s\n",
      "89:\tlearn: 0.0456510\ttotal: 1.2s\tremaining: 2.13s\n",
      "90:\tlearn: 0.0452640\ttotal: 1.22s\tremaining: 2.13s\n",
      "91:\tlearn: 0.0445767\ttotal: 1.22s\tremaining: 2.1s\n",
      "92:\tlearn: 0.0441450\ttotal: 1.23s\tremaining: 2.08s\n",
      "93:\tlearn: 0.0441138\ttotal: 1.23s\tremaining: 2.05s\n",
      "94:\tlearn: 0.0432716\ttotal: 1.24s\tremaining: 2.02s\n",
      "95:\tlearn: 0.0426098\ttotal: 1.25s\tremaining: 2.01s\n",
      "96:\tlearn: 0.0416521\ttotal: 1.26s\tremaining: 1.98s\n",
      "97:\tlearn: 0.0408585\ttotal: 1.26s\tremaining: 1.96s\n",
      "98:\tlearn: 0.0400090\ttotal: 1.27s\tremaining: 1.93s\n",
      "99:\tlearn: 0.0393600\ttotal: 1.27s\tremaining: 1.91s\n",
      "100:\tlearn: 0.0390218\ttotal: 1.3s\tremaining: 1.92s\n",
      "101:\tlearn: 0.0387934\ttotal: 1.3s\tremaining: 1.89s\n",
      "102:\tlearn: 0.0381772\ttotal: 1.31s\tremaining: 1.87s\n",
      "103:\tlearn: 0.0377932\ttotal: 1.31s\tremaining: 1.84s\n",
      "104:\tlearn: 0.0377543\ttotal: 1.32s\tremaining: 1.82s\n",
      "105:\tlearn: 0.0370971\ttotal: 1.34s\tremaining: 1.83s\n",
      "106:\tlearn: 0.0366190\ttotal: 1.35s\tremaining: 1.8s\n",
      "107:\tlearn: 0.0357494\ttotal: 1.35s\tremaining: 1.78s\n",
      "108:\tlearn: 0.0353244\ttotal: 1.36s\tremaining: 1.76s\n",
      "109:\tlearn: 0.0345970\ttotal: 1.36s\tremaining: 1.74s\n",
      "110:\tlearn: 0.0345816\ttotal: 1.37s\tremaining: 1.72s\n",
      "111:\tlearn: 0.0341794\ttotal: 1.38s\tremaining: 1.69s\n",
      "112:\tlearn: 0.0341793\ttotal: 1.38s\tremaining: 1.67s\n",
      "113:\tlearn: 0.0337192\ttotal: 1.38s\tremaining: 1.65s\n",
      "114:\tlearn: 0.0329912\ttotal: 1.39s\tremaining: 1.63s\n",
      "115:\tlearn: 0.0324474\ttotal: 1.4s\tremaining: 1.61s\n",
      "116:\tlearn: 0.0321393\ttotal: 1.4s\tremaining: 1.59s\n",
      "117:\tlearn: 0.0316240\ttotal: 1.41s\tremaining: 1.57s\n",
      "118:\tlearn: 0.0314989\ttotal: 1.41s\tremaining: 1.55s\n",
      "119:\tlearn: 0.0314610\ttotal: 1.42s\tremaining: 1.53s\n",
      "120:\tlearn: 0.0310007\ttotal: 1.44s\tremaining: 1.53s\n",
      "121:\tlearn: 0.0307385\ttotal: 1.44s\tremaining: 1.51s\n",
      "122:\tlearn: 0.0305810\ttotal: 1.45s\tremaining: 1.49s\n",
      "123:\tlearn: 0.0302041\ttotal: 1.45s\tremaining: 1.48s\n",
      "124:\tlearn: 0.0298246\ttotal: 1.46s\tremaining: 1.46s\n",
      "125:\tlearn: 0.0296369\ttotal: 1.47s\tremaining: 1.45s\n",
      "126:\tlearn: 0.0296368\ttotal: 1.47s\tremaining: 1.42s\n",
      "127:\tlearn: 0.0291937\ttotal: 1.49s\tremaining: 1.42s\n",
      "128:\tlearn: 0.0290636\ttotal: 1.49s\tremaining: 1.4s\n",
      "129:\tlearn: 0.0287854\ttotal: 1.5s\tremaining: 1.39s\n",
      "130:\tlearn: 0.0287854\ttotal: 1.53s\tremaining: 1.39s\n",
      "131:\tlearn: 0.0287107\ttotal: 1.56s\tremaining: 1.4s\n",
      "132:\tlearn: 0.0286873\ttotal: 1.58s\tremaining: 1.39s\n",
      "133:\tlearn: 0.0283229\ttotal: 1.58s\tremaining: 1.37s\n",
      "134:\tlearn: 0.0280562\ttotal: 1.59s\tremaining: 1.35s\n",
      "135:\tlearn: 0.0277969\ttotal: 1.59s\tremaining: 1.34s\n",
      "136:\tlearn: 0.0275136\ttotal: 1.6s\tremaining: 1.32s\n",
      "137:\tlearn: 0.0273823\ttotal: 1.6s\tremaining: 1.3s\n",
      "138:\tlearn: 0.0270747\ttotal: 1.61s\tremaining: 1.28s\n",
      "139:\tlearn: 0.0267379\ttotal: 1.64s\tremaining: 1.29s\n",
      "140:\tlearn: 0.0266645\ttotal: 1.65s\tremaining: 1.27s\n",
      "141:\tlearn: 0.0263477\ttotal: 1.66s\tremaining: 1.26s\n",
      "142:\tlearn: 0.0261550\ttotal: 1.67s\tremaining: 1.25s\n",
      "143:\tlearn: 0.0259043\ttotal: 1.68s\tremaining: 1.24s\n",
      "144:\tlearn: 0.0254341\ttotal: 1.68s\tremaining: 1.22s\n",
      "145:\tlearn: 0.0251564\ttotal: 1.69s\tremaining: 1.2s\n",
      "146:\tlearn: 0.0250939\ttotal: 1.69s\tremaining: 1.19s\n",
      "147:\tlearn: 0.0250767\ttotal: 1.7s\tremaining: 1.17s\n",
      "148:\tlearn: 0.0247237\ttotal: 1.7s\tremaining: 1.15s\n",
      "149:\tlearn: 0.0243044\ttotal: 1.71s\tremaining: 1.14s\n",
      "150:\tlearn: 0.0240669\ttotal: 1.71s\tremaining: 1.12s\n",
      "151:\tlearn: 0.0238282\ttotal: 1.72s\tremaining: 1.11s\n",
      "152:\tlearn: 0.0236725\ttotal: 1.73s\tremaining: 1.09s\n",
      "153:\tlearn: 0.0236500\ttotal: 1.75s\tremaining: 1.09s\n",
      "154:\tlearn: 0.0233667\ttotal: 1.78s\tremaining: 1.09s\n",
      "155:\tlearn: 0.0229684\ttotal: 1.79s\tremaining: 1.08s\n",
      "156:\tlearn: 0.0229584\ttotal: 1.79s\tremaining: 1.06s\n",
      "157:\tlearn: 0.0228910\ttotal: 1.8s\tremaining: 1.05s\n",
      "158:\tlearn: 0.0228300\ttotal: 1.83s\tremaining: 1.05s\n",
      "159:\tlearn: 0.0227145\ttotal: 1.84s\tremaining: 1.03s\n",
      "160:\tlearn: 0.0226447\ttotal: 1.85s\tremaining: 1.02s\n",
      "161:\tlearn: 0.0224907\ttotal: 1.85s\tremaining: 1.01s\n",
      "162:\tlearn: 0.0224785\ttotal: 1.86s\tremaining: 991ms\n",
      "163:\tlearn: 0.0222710\ttotal: 1.88s\tremaining: 984ms\n",
      "164:\tlearn: 0.0222710\ttotal: 1.88s\tremaining: 967ms\n",
      "165:\tlearn: 0.0221391\ttotal: 1.88s\tremaining: 953ms\n",
      "166:\tlearn: 0.0218190\ttotal: 1.91s\tremaining: 948ms\n",
      "167:\tlearn: 0.0215845\ttotal: 1.94s\tremaining: 947ms\n",
      "168:\tlearn: 0.0215845\ttotal: 1.95s\tremaining: 935ms\n",
      "169:\tlearn: 0.0214517\ttotal: 1.99s\tremaining: 935ms\n",
      "170:\tlearn: 0.0212526\ttotal: 1.99s\tremaining: 920ms\n",
      "171:\tlearn: 0.0209712\ttotal: 2s\tremaining: 908ms\n",
      "172:\tlearn: 0.0206953\ttotal: 2.05s\tremaining: 913ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173:\tlearn: 0.0206953\ttotal: 2.06s\tremaining: 901ms\n",
      "174:\tlearn: 0.0206272\ttotal: 2.07s\tremaining: 886ms\n",
      "175:\tlearn: 0.0204303\ttotal: 2.08s\tremaining: 874ms\n",
      "176:\tlearn: 0.0204299\ttotal: 2.09s\tremaining: 864ms\n",
      "177:\tlearn: 0.0202839\ttotal: 2.1s\tremaining: 849ms\n",
      "178:\tlearn: 0.0200435\ttotal: 2.11s\tremaining: 836ms\n",
      "179:\tlearn: 0.0198481\ttotal: 2.14s\tremaining: 832ms\n",
      "180:\tlearn: 0.0196728\ttotal: 2.15s\tremaining: 818ms\n",
      "181:\tlearn: 0.0195429\ttotal: 2.15s\tremaining: 803ms\n",
      "182:\tlearn: 0.0193652\ttotal: 2.15s\tremaining: 789ms\n",
      "183:\tlearn: 0.0192178\ttotal: 2.16s\tremaining: 775ms\n",
      "184:\tlearn: 0.0190925\ttotal: 2.16s\tremaining: 760ms\n",
      "185:\tlearn: 0.0190528\ttotal: 2.17s\tremaining: 746ms\n",
      "186:\tlearn: 0.0188228\ttotal: 2.2s\tremaining: 740ms\n",
      "187:\tlearn: 0.0186914\ttotal: 2.22s\tremaining: 732ms\n",
      "188:\tlearn: 0.0185907\ttotal: 2.22s\tremaining: 718ms\n",
      "189:\tlearn: 0.0184648\ttotal: 2.23s\tremaining: 704ms\n",
      "190:\tlearn: 0.0183093\ttotal: 2.23s\tremaining: 690ms\n",
      "191:\tlearn: 0.0182509\ttotal: 2.24s\tremaining: 676ms\n",
      "192:\tlearn: 0.0181115\ttotal: 2.24s\tremaining: 662ms\n",
      "193:\tlearn: 0.0179521\ttotal: 2.25s\tremaining: 649ms\n",
      "194:\tlearn: 0.0178097\ttotal: 2.3s\tremaining: 648ms\n",
      "195:\tlearn: 0.0174751\ttotal: 2.36s\tremaining: 651ms\n",
      "196:\tlearn: 0.0174751\ttotal: 2.39s\tremaining: 642ms\n",
      "197:\tlearn: 0.0174521\ttotal: 2.42s\tremaining: 636ms\n",
      "198:\tlearn: 0.0172368\ttotal: 2.43s\tremaining: 622ms\n",
      "199:\tlearn: 0.0170728\ttotal: 2.43s\tremaining: 608ms\n",
      "200:\tlearn: 0.0168795\ttotal: 2.44s\tremaining: 594ms\n",
      "201:\tlearn: 0.0168510\ttotal: 2.45s\tremaining: 583ms\n",
      "202:\tlearn: 0.0168089\ttotal: 2.46s\tremaining: 569ms\n",
      "203:\tlearn: 0.0166663\ttotal: 2.46s\tremaining: 555ms\n",
      "204:\tlearn: 0.0165149\ttotal: 2.47s\tremaining: 542ms\n",
      "205:\tlearn: 0.0164130\ttotal: 2.47s\tremaining: 528ms\n",
      "206:\tlearn: 0.0162468\ttotal: 2.49s\tremaining: 517ms\n",
      "207:\tlearn: 0.0161710\ttotal: 2.5s\tremaining: 504ms\n",
      "208:\tlearn: 0.0159490\ttotal: 2.51s\tremaining: 493ms\n",
      "209:\tlearn: 0.0158840\ttotal: 2.53s\tremaining: 482ms\n",
      "210:\tlearn: 0.0157335\ttotal: 2.56s\tremaining: 473ms\n",
      "211:\tlearn: 0.0156541\ttotal: 2.57s\tremaining: 460ms\n",
      "212:\tlearn: 0.0154966\ttotal: 2.57s\tremaining: 447ms\n",
      "213:\tlearn: 0.0154524\ttotal: 2.58s\tremaining: 433ms\n",
      "214:\tlearn: 0.0152927\ttotal: 2.58s\tremaining: 420ms\n",
      "215:\tlearn: 0.0151794\ttotal: 2.59s\tremaining: 408ms\n",
      "216:\tlearn: 0.0149401\ttotal: 2.61s\tremaining: 396ms\n",
      "217:\tlearn: 0.0148952\ttotal: 2.61s\tremaining: 383ms\n",
      "218:\tlearn: 0.0147803\ttotal: 2.62s\tremaining: 370ms\n",
      "219:\tlearn: 0.0146107\ttotal: 2.62s\tremaining: 357ms\n",
      "220:\tlearn: 0.0145162\ttotal: 2.63s\tremaining: 345ms\n",
      "221:\tlearn: 0.0144605\ttotal: 2.63s\tremaining: 332ms\n",
      "222:\tlearn: 0.0143982\ttotal: 2.64s\tremaining: 319ms\n",
      "223:\tlearn: 0.0143580\ttotal: 2.64s\tremaining: 307ms\n",
      "224:\tlearn: 0.0143389\ttotal: 2.67s\tremaining: 296ms\n",
      "225:\tlearn: 0.0142198\ttotal: 2.67s\tremaining: 284ms\n",
      "226:\tlearn: 0.0140756\ttotal: 2.68s\tremaining: 272ms\n",
      "227:\tlearn: 0.0139865\ttotal: 2.69s\tremaining: 259ms\n",
      "228:\tlearn: 0.0138474\ttotal: 2.69s\tremaining: 247ms\n",
      "229:\tlearn: 0.0137395\ttotal: 2.7s\tremaining: 235ms\n",
      "230:\tlearn: 0.0136307\ttotal: 2.7s\tremaining: 222ms\n",
      "231:\tlearn: 0.0135566\ttotal: 2.71s\tremaining: 210ms\n",
      "232:\tlearn: 0.0135566\ttotal: 2.75s\tremaining: 200ms\n",
      "233:\tlearn: 0.0135565\ttotal: 2.75s\tremaining: 188ms\n",
      "234:\tlearn: 0.0134359\ttotal: 2.76s\tremaining: 176ms\n",
      "235:\tlearn: 0.0133426\ttotal: 2.77s\tremaining: 164ms\n",
      "236:\tlearn: 0.0132973\ttotal: 2.77s\tremaining: 152ms\n",
      "237:\tlearn: 0.0132136\ttotal: 2.78s\tremaining: 140ms\n",
      "238:\tlearn: 0.0131280\ttotal: 2.78s\tremaining: 128ms\n",
      "239:\tlearn: 0.0130828\ttotal: 2.79s\tremaining: 116ms\n",
      "240:\tlearn: 0.0130193\ttotal: 2.81s\tremaining: 105ms\n",
      "241:\tlearn: 0.0129273\ttotal: 2.81s\tremaining: 93ms\n",
      "242:\tlearn: 0.0128637\ttotal: 2.82s\tremaining: 81.2ms\n",
      "243:\tlearn: 0.0127971\ttotal: 2.82s\tremaining: 69.4ms\n",
      "244:\tlearn: 0.0126948\ttotal: 2.83s\tremaining: 57.7ms\n",
      "245:\tlearn: 0.0126946\ttotal: 2.83s\tremaining: 46ms\n",
      "246:\tlearn: 0.0126300\ttotal: 2.84s\tremaining: 34.5ms\n",
      "247:\tlearn: 0.0125887\ttotal: 2.84s\tremaining: 22.9ms\n",
      "248:\tlearn: 0.0125593\ttotal: 2.85s\tremaining: 11.4ms\n",
      "249:\tlearn: 0.0125549\ttotal: 2.87s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3f41721a9e4b2a931eddbb93e3e522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.47624112708364513, Recall = 0.9865012769062386, Aging Rate = 0.9170174383644016, Precision = 0.5910382513661202, f1 = 0.7392017495899399\n",
      "Epoch 2: Train Loss = 0.31392737541487375, Recall = 0.964611455673112, Aging Rate = 0.691320905993185, Precision = 0.7665990142070165, f1 = 0.8542810985460421\n",
      "Epoch 3: Train Loss = 0.25133727152795826, Recall = 0.9642466253192266, Aging Rate = 0.6440168370414913, Precision = 0.8225957049486461, f1 = 0.8878065166274772\n",
      "Epoch 4: Train Loss = 0.21366581595548528, Recall = 0.9675300985041956, Aging Rate = 0.6275806774904791, Precision = 0.8470137336314276, f1 = 0.9032697547683924\n",
      "Epoch 5: Train Loss = 0.18210080651893815, Recall = 0.9726377234585918, Aging Rate = 0.6111445179394668, Precision = 0.8743850442768121, f1 = 0.9208981001727116\n",
      "Test Loss = 0.1486406336120528, Recall = 0.9850419554906968, Aging Rate = 0.604529965925035, precision = 0.8952254641909815\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14029575550716275, Recall = 0.9828529733673842, Aging Rate = 0.5941070354780517, Precision = 0.9089068825910931, f1 = 0.9444347063978965\n",
      "Epoch 7: Train Loss = 0.11357385269874895, Recall = 0.9861364465523531, Aging Rate = 0.5848867508518741, Precision = 0.9263193968471556, f1 = 0.955292454497261\n",
      "Epoch 8: Train Loss = 0.09595470525597637, Recall = 0.990149580445093, Aging Rate = 0.5770695530166366, Precision = 0.9426884334838486, f1 = 0.9658362989323844\n",
      "Epoch 9: Train Loss = 0.07994531649079918, Recall = 0.9923385625684057, Aging Rate = 0.5702545600320705, Precision = 0.9560632688927944, f1 = 0.9738632295023273\n",
      "Epoch 10: Train Loss = 0.06877052694548644, Recall = 0.9912440715067493, Aging Rate = 0.5652435357787132, Precision = 0.9634751773049646, f1 = 0.9771623808667507\n",
      "Test Loss = 0.05946238839236833, Recall = 0.9956220357533747, Aging Rate = 0.565844858689116, precision = 0.9667020899752037\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.058998858905234014, Recall = 0.9941627143378329, Aging Rate = 0.5630386851072359, Precision = 0.9700961196155216, f1 = 0.981981981981982\n",
      "Epoch 12: Train Loss = 0.052459135668762556, Recall = 0.9937978839839474, Aging Rate = 0.5620364802565645, Precision = 0.9714693295292439, f1 = 0.9825067628494137\n",
      "Epoch 13: Train Loss = 0.04458884930353493, Recall = 0.9945275446917183, Aging Rate = 0.5572258969733414, Precision = 0.9805755395683453, f1 = 0.9875022640825937\n",
      "Epoch 14: Train Loss = 0.03871743398608211, Recall = 0.996716526815031, Aging Rate = 0.5572258969733414, Precision = 0.9827338129496402, f1 = 0.9896757833725773\n",
      "Epoch 15: Train Loss = 0.034427236961258705, Recall = 0.9970813571689164, Aging Rate = 0.5558228101824013, Precision = 0.9855751893256401, f1 = 0.9912948857453754\n",
      "Test Loss = 0.032801914357931755, Recall = 0.9959868661072602, Aging Rate = 0.550210463018641, precision = 0.994535519125683\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03208187974632491, Recall = 0.996716526815031, Aging Rate = 0.5548206053317298, Precision = 0.9869942196531792, f1 = 0.9918315483753857\n",
      "Epoch 17: Train Loss = 0.027654779875211567, Recall = 0.9974461875228019, Aging Rate = 0.5536179595109241, Precision = 0.9898624185372918, f1 = 0.9936398328184627\n",
      "Epoch 18: Train Loss = 0.02536611267888019, Recall = 0.9981758482305728, Aging Rate = 0.5526157546602526, Precision = 0.9923830250272034, f1 = 0.9952710076391414\n",
      "Epoch 19: Train Loss = 0.022929908331027368, Recall = 0.9989055089383436, Aging Rate = 0.5540188414511926, Precision = 0.9905933429811867, f1 = 0.9947320617620345\n",
      "Epoch 20: Train Loss = 0.021197979125844067, Recall = 0.9981758482305728, Aging Rate = 0.5522148727199839, Precision = 0.993103448275862, f1 = 0.9956331877729258\n",
      "Test Loss = 0.019076981418033382, Recall = 1.0, Aging Rate = 0.5536179595109241, precision = 0.9923968139029689\n",
      "\n",
      "Epoch 21: Train Loss = 0.01839148409262906, Recall = 0.9992703392922291, Aging Rate = 0.5522148727199839, Precision = 0.9941923774954627, f1 = 0.9967248908296942\n",
      "Epoch 22: Train Loss = 0.01681478318024115, Recall = 1.0, Aging Rate = 0.5520144317498497, Precision = 0.995279593318809, f1 = 0.9976342129208371\n",
      "Epoch 23: Train Loss = 0.015563951486019363, Recall = 0.9996351696461145, Aging Rate = 0.5514131088394468, Precision = 0.9960014540167212, f1 = 0.9978150036416606\n",
      "Epoch 24: Train Loss = 0.01437409058458525, Recall = 1.0, Aging Rate = 0.5518139907797154, Precision = 0.9956411187795132, f1 = 0.9978157990535128\n",
      "Epoch 25: Train Loss = 0.01323825581901643, Recall = 0.9996351696461145, Aging Rate = 0.5508117859290439, Precision = 0.9970887918486172, f1 = 0.998360357077792\n",
      "Test Loss = 0.01142410203978034, Recall = 1.0, Aging Rate = 0.5516135498095811, precision = 0.9960029069767442\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.012584140422530166, Recall = 0.9996351696461145, Aging Rate = 0.5510122268991782, Precision = 0.9967260822117133, f1 = 0.9981785063752276\n",
      "Epoch 27: Train Loss = 0.010996274273386338, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 28: Train Loss = 0.010482279511008432, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 29: Train Loss = 0.009878677624957454, Recall = 0.9996351696461145, Aging Rate = 0.550210463018641, Precision = 0.9981785063752276, f1 = 0.998906306963179\n",
      "Epoch 30: Train Loss = 0.009671238856511458, Recall = 1.0, Aging Rate = 0.5512126678693124, Precision = 0.9967272727272727, f1 = 0.9983609542888363\n",
      "Test Loss = 0.008570585753708946, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, precision = 0.9992706053975201\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.008848471624157332, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 32: Train Loss = 0.00785202250249742, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 33: Train Loss = 0.008237093486615, Recall = 0.9992703392922291, Aging Rate = 0.5498095810783724, Precision = 0.998541742617572, f1 = 0.99890590809628\n",
      "Epoch 34: Train Loss = 0.007310317019314264, Recall = 1.0, Aging Rate = 0.5504109039887753, Precision = 0.9981791697013839, f1 = 0.9990887552396573\n",
      "Epoch 35: Train Loss = 0.006887998819458722, Recall = 0.9996351696461145, Aging Rate = 0.5500100220485067, Precision = 0.9985422740524781, f1 = 0.9990884229717412\n",
      "Test Loss = 0.006057125161739623, Recall = 0.9996351696461145, Aging Rate = 0.5500100220485067, precision = 0.9985422740524781\n",
      "\n",
      "Epoch 36: Train Loss = 0.006277793405626449, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 37: Train Loss = 0.006385511741931458, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 38: Train Loss = 0.005740387618726899, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 39: Train Loss = 0.005530164589947487, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 40: Train Loss = 0.005329389139856059, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Test Loss = 0.004585847949266052, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, precision = 0.9992706053975201\n",
      "\n",
      "Epoch 41: Train Loss = 0.005221579543459174, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 42: Train Loss = 0.0048579579617960295, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 43: Train Loss = 0.004981261032517185, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 44: Train Loss = 0.004556847363635734, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 45: Train Loss = 0.004283124588306349, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.003989574702717566, Recall = 1.0, Aging Rate = 0.5498095810783724, precision = 0.999270871308786\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.004428542999964746, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.0037669615528510246, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.004070105741618511, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 49: Train Loss = 0.0036403375251584935, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 50: Train Loss = 0.003270797032730736, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.003064424805729102, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.0038940800489410893, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 52: Train Loss = 0.003873291497743573, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0033792324719241562, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 54: Train Loss = 0.0034704549467299557, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 55: Train Loss = 0.0034037259831840886, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.002868576321161359, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 56: Train Loss = 0.0032705962772033685, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 57: Train Loss = 0.0030933807192781073, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 58: Train Loss = 0.002942540717900537, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0027014760759948085, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0028797228875971164, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0026909451053688206, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 61: Train Loss = 0.0030942033831829847, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.003073102167708955, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 63: Train Loss = 0.0026719147236978547, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0027673857328201026, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 65: Train Loss = 0.005040977839779988, Recall = 0.9996351696461145, Aging Rate = 0.5498095810783724, Precision = 0.998906306963179, f1 = 0.99927060539752\n",
      "Test Loss = 0.0026862660338473985, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 66: Train Loss = 0.002765472453726184, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 67: Train Loss = 0.0026133533038526003, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 68: Train Loss = 0.002698800473030239, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.002626491132667215, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 70: Train Loss = 0.0023481157915130942, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002072486786435379, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 71: Train Loss = 0.002542179604217755, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 72: Train Loss = 0.0025274342160349376, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.002976772369057878, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.00374568747138124, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 75: Train Loss = 0.0025251154192764587, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0033984416839218643, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.002599064494065207, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0037695639292131024, Recall = 0.9996351696461145, Aging Rate = 0.5498095810783724, Precision = 0.998906306963179, f1 = 0.99927060539752\n",
      "Epoch 78: Train Loss = 0.0025718799578207012, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 79: Train Loss = 0.0020901411957982806, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0021827290320954136, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001965154962416116, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0024722683467743754, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0021309585065834224, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0025682255048105054, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0026824804474008834, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.002110607867848033, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019215037276672294, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0022609810733205944, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.002643708816728926, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 88: Train Loss = 0.0025754404315113796, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0022584280092427856, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0022582302344341795, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020192581322996178, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.0029689085727311504, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 92: Train Loss = 0.0022677430870169375, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.0031379555020637035, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 94: Train Loss = 0.0024447352866707327, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.00299035932181381, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.0028006984707301097, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0025792676640983793, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.002080470585047909, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.002154781096124408, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0023463994079584204, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0025913720302674663, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002121539833803513, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6030416\ttotal: 34ms\tremaining: 8.46s\n",
      "1:\tlearn: 0.5300836\ttotal: 49.5ms\tremaining: 6.13s\n",
      "2:\tlearn: 0.4939809\ttotal: 64.3ms\tremaining: 5.3s\n",
      "3:\tlearn: 0.4559135\ttotal: 94.4ms\tremaining: 5.8s\n",
      "4:\tlearn: 0.4343364\ttotal: 110ms\tremaining: 5.39s\n",
      "5:\tlearn: 0.4015080\ttotal: 140ms\tremaining: 5.7s\n",
      "6:\tlearn: 0.3670953\ttotal: 188ms\tremaining: 6.51s\n",
      "7:\tlearn: 0.3468942\ttotal: 204ms\tremaining: 6.16s\n",
      "8:\tlearn: 0.3295347\ttotal: 236ms\tremaining: 6.33s\n",
      "9:\tlearn: 0.3178788\ttotal: 267ms\tremaining: 6.41s\n",
      "10:\tlearn: 0.3059428\ttotal: 283ms\tremaining: 6.14s\n",
      "11:\tlearn: 0.2982641\ttotal: 299ms\tremaining: 5.93s\n",
      "12:\tlearn: 0.2809024\ttotal: 314ms\tremaining: 5.73s\n",
      "13:\tlearn: 0.2685918\ttotal: 319ms\tremaining: 5.38s\n",
      "14:\tlearn: 0.2569669\ttotal: 324ms\tremaining: 5.08s\n",
      "15:\tlearn: 0.2473021\ttotal: 329ms\tremaining: 4.81s\n",
      "16:\tlearn: 0.2389179\ttotal: 334ms\tremaining: 4.58s\n",
      "17:\tlearn: 0.2275139\ttotal: 339ms\tremaining: 4.37s\n",
      "18:\tlearn: 0.2200319\ttotal: 345ms\tremaining: 4.2s\n",
      "19:\tlearn: 0.2146122\ttotal: 361ms\tremaining: 4.16s\n",
      "20:\tlearn: 0.2097881\ttotal: 366ms\tremaining: 3.99s\n",
      "21:\tlearn: 0.2012178\ttotal: 371ms\tremaining: 3.84s\n",
      "22:\tlearn: 0.1920443\ttotal: 392ms\tremaining: 3.87s\n",
      "23:\tlearn: 0.1889833\ttotal: 424ms\tremaining: 3.99s\n",
      "24:\tlearn: 0.1837354\ttotal: 440ms\tremaining: 3.96s\n",
      "25:\tlearn: 0.1744059\ttotal: 480ms\tremaining: 4.13s\n",
      "26:\tlearn: 0.1704423\ttotal: 485ms\tremaining: 4s\n",
      "27:\tlearn: 0.1658338\ttotal: 490ms\tremaining: 3.88s\n",
      "28:\tlearn: 0.1592746\ttotal: 495ms\tremaining: 3.77s\n",
      "29:\tlearn: 0.1548571\ttotal: 500ms\tremaining: 3.67s\n",
      "30:\tlearn: 0.1502170\ttotal: 505ms\tremaining: 3.56s\n",
      "31:\tlearn: 0.1453068\ttotal: 509ms\tremaining: 3.47s\n",
      "32:\tlearn: 0.1433436\ttotal: 517ms\tremaining: 3.4s\n",
      "33:\tlearn: 0.1410745\ttotal: 522ms\tremaining: 3.32s\n",
      "34:\tlearn: 0.1392065\ttotal: 533ms\tremaining: 3.27s\n",
      "35:\tlearn: 0.1347289\ttotal: 538ms\tremaining: 3.2s\n",
      "36:\tlearn: 0.1311723\ttotal: 543ms\tremaining: 3.12s\n",
      "37:\tlearn: 0.1293436\ttotal: 563ms\tremaining: 3.14s\n",
      "38:\tlearn: 0.1264512\ttotal: 568ms\tremaining: 3.07s\n",
      "39:\tlearn: 0.1234135\ttotal: 573ms\tremaining: 3.01s\n",
      "40:\tlearn: 0.1198058\ttotal: 577ms\tremaining: 2.94s\n",
      "41:\tlearn: 0.1160432\ttotal: 593ms\tremaining: 2.94s\n",
      "42:\tlearn: 0.1139328\ttotal: 598ms\tremaining: 2.88s\n",
      "43:\tlearn: 0.1122002\ttotal: 602ms\tremaining: 2.82s\n",
      "44:\tlearn: 0.1068069\ttotal: 607ms\tremaining: 2.76s\n",
      "45:\tlearn: 0.1034597\ttotal: 612ms\tremaining: 2.71s\n",
      "46:\tlearn: 0.1000335\ttotal: 624ms\tremaining: 2.7s\n",
      "47:\tlearn: 0.0970141\ttotal: 629ms\tremaining: 2.65s\n",
      "48:\tlearn: 0.0951101\ttotal: 634ms\tremaining: 2.6s\n",
      "49:\tlearn: 0.0939995\ttotal: 639ms\tremaining: 2.55s\n",
      "50:\tlearn: 0.0921672\ttotal: 643ms\tremaining: 2.51s\n",
      "51:\tlearn: 0.0903750\ttotal: 648ms\tremaining: 2.47s\n",
      "52:\tlearn: 0.0892411\ttotal: 653ms\tremaining: 2.42s\n",
      "53:\tlearn: 0.0864889\ttotal: 657ms\tremaining: 2.38s\n",
      "54:\tlearn: 0.0843021\ttotal: 687ms\tremaining: 2.44s\n",
      "55:\tlearn: 0.0813347\ttotal: 692ms\tremaining: 2.4s\n",
      "56:\tlearn: 0.0789800\ttotal: 697ms\tremaining: 2.36s\n",
      "57:\tlearn: 0.0780822\ttotal: 701ms\tremaining: 2.32s\n",
      "58:\tlearn: 0.0754223\ttotal: 706ms\tremaining: 2.29s\n",
      "59:\tlearn: 0.0752248\ttotal: 711ms\tremaining: 2.25s\n",
      "60:\tlearn: 0.0730227\ttotal: 719ms\tremaining: 2.23s\n",
      "61:\tlearn: 0.0709630\ttotal: 734ms\tremaining: 2.23s\n",
      "62:\tlearn: 0.0701374\ttotal: 749ms\tremaining: 2.22s\n",
      "63:\tlearn: 0.0701274\ttotal: 762ms\tremaining: 2.21s\n",
      "64:\tlearn: 0.0682648\ttotal: 779ms\tremaining: 2.22s\n",
      "65:\tlearn: 0.0677218\ttotal: 794ms\tremaining: 2.21s\n",
      "66:\tlearn: 0.0661402\ttotal: 810ms\tremaining: 2.21s\n",
      "67:\tlearn: 0.0657626\ttotal: 841ms\tremaining: 2.25s\n",
      "68:\tlearn: 0.0647418\ttotal: 857ms\tremaining: 2.25s\n",
      "69:\tlearn: 0.0627197\ttotal: 888ms\tremaining: 2.28s\n",
      "70:\tlearn: 0.0613495\ttotal: 904ms\tremaining: 2.28s\n",
      "71:\tlearn: 0.0601765\ttotal: 920ms\tremaining: 2.27s\n",
      "72:\tlearn: 0.0599491\ttotal: 936ms\tremaining: 2.27s\n",
      "73:\tlearn: 0.0589864\ttotal: 952ms\tremaining: 2.26s\n",
      "74:\tlearn: 0.0579192\ttotal: 967ms\tremaining: 2.26s\n",
      "75:\tlearn: 0.0571354\ttotal: 999ms\tremaining: 2.29s\n",
      "76:\tlearn: 0.0560351\ttotal: 1.03s\tremaining: 2.31s\n",
      "77:\tlearn: 0.0560299\ttotal: 1.03s\tremaining: 2.28s\n",
      "78:\tlearn: 0.0551829\ttotal: 1.04s\tremaining: 2.24s\n",
      "79:\tlearn: 0.0549695\ttotal: 1.04s\tremaining: 2.21s\n",
      "80:\tlearn: 0.0539626\ttotal: 1.05s\tremaining: 2.18s\n",
      "81:\tlearn: 0.0527281\ttotal: 1.05s\tremaining: 2.15s\n",
      "82:\tlearn: 0.0520213\ttotal: 1.06s\tremaining: 2.13s\n",
      "83:\tlearn: 0.0516439\ttotal: 1.07s\tremaining: 2.12s\n",
      "84:\tlearn: 0.0505414\ttotal: 1.09s\tremaining: 2.12s\n",
      "85:\tlearn: 0.0494970\ttotal: 1.09s\tremaining: 2.09s\n",
      "86:\tlearn: 0.0492553\ttotal: 1.1s\tremaining: 2.06s\n",
      "87:\tlearn: 0.0484743\ttotal: 1.1s\tremaining: 2.03s\n",
      "88:\tlearn: 0.0483338\ttotal: 1.11s\tremaining: 2.01s\n",
      "89:\tlearn: 0.0474273\ttotal: 1.11s\tremaining: 1.98s\n",
      "90:\tlearn: 0.0465731\ttotal: 1.12s\tremaining: 1.96s\n",
      "91:\tlearn: 0.0465731\ttotal: 1.13s\tremaining: 1.95s\n",
      "92:\tlearn: 0.0461565\ttotal: 1.14s\tremaining: 1.92s\n",
      "93:\tlearn: 0.0456190\ttotal: 1.17s\tremaining: 1.94s\n",
      "94:\tlearn: 0.0449003\ttotal: 1.18s\tremaining: 1.93s\n",
      "95:\tlearn: 0.0445228\ttotal: 1.2s\tremaining: 1.92s\n",
      "96:\tlearn: 0.0435376\ttotal: 1.23s\tremaining: 1.94s\n",
      "97:\tlearn: 0.0432826\ttotal: 1.24s\tremaining: 1.93s\n",
      "98:\tlearn: 0.0432563\ttotal: 1.29s\tremaining: 1.97s\n",
      "99:\tlearn: 0.0423464\ttotal: 1.31s\tremaining: 1.96s\n",
      "100:\tlearn: 0.0416547\ttotal: 1.31s\tremaining: 1.94s\n",
      "101:\tlearn: 0.0411623\ttotal: 1.32s\tremaining: 1.91s\n",
      "102:\tlearn: 0.0404486\ttotal: 1.32s\tremaining: 1.89s\n",
      "103:\tlearn: 0.0404486\ttotal: 1.32s\tremaining: 1.86s\n",
      "104:\tlearn: 0.0399038\ttotal: 1.33s\tremaining: 1.83s\n",
      "105:\tlearn: 0.0397843\ttotal: 1.33s\tremaining: 1.81s\n",
      "106:\tlearn: 0.0393381\ttotal: 1.37s\tremaining: 1.83s\n",
      "107:\tlearn: 0.0387583\ttotal: 1.38s\tremaining: 1.82s\n",
      "108:\tlearn: 0.0383089\ttotal: 1.39s\tremaining: 1.79s\n",
      "109:\tlearn: 0.0374405\ttotal: 1.4s\tremaining: 1.78s\n",
      "110:\tlearn: 0.0368953\ttotal: 1.41s\tremaining: 1.77s\n",
      "111:\tlearn: 0.0363995\ttotal: 1.42s\tremaining: 1.75s\n",
      "112:\tlearn: 0.0359037\ttotal: 1.42s\tremaining: 1.73s\n",
      "113:\tlearn: 0.0354173\ttotal: 1.43s\tremaining: 1.7s\n",
      "114:\tlearn: 0.0347860\ttotal: 1.43s\tremaining: 1.68s\n",
      "115:\tlearn: 0.0343813\ttotal: 1.45s\tremaining: 1.67s\n",
      "116:\tlearn: 0.0336167\ttotal: 1.45s\tremaining: 1.65s\n",
      "117:\tlearn: 0.0335764\ttotal: 1.48s\tremaining: 1.65s\n",
      "118:\tlearn: 0.0331463\ttotal: 1.51s\tremaining: 1.66s\n",
      "119:\tlearn: 0.0325458\ttotal: 1.54s\tremaining: 1.67s\n",
      "120:\tlearn: 0.0325158\ttotal: 1.57s\tremaining: 1.68s\n",
      "121:\tlearn: 0.0325118\ttotal: 1.62s\tremaining: 1.7s\n",
      "122:\tlearn: 0.0325049\ttotal: 1.65s\tremaining: 1.7s\n",
      "123:\tlearn: 0.0323417\ttotal: 1.68s\tremaining: 1.71s\n",
      "124:\tlearn: 0.0323200\ttotal: 1.7s\tremaining: 1.7s\n",
      "125:\tlearn: 0.0322804\ttotal: 1.72s\tremaining: 1.69s\n",
      "126:\tlearn: 0.0319940\ttotal: 1.73s\tremaining: 1.68s\n",
      "127:\tlearn: 0.0319940\ttotal: 1.74s\tremaining: 1.66s\n",
      "128:\tlearn: 0.0317718\ttotal: 1.76s\tremaining: 1.65s\n",
      "129:\tlearn: 0.0317436\ttotal: 1.77s\tremaining: 1.63s\n",
      "130:\tlearn: 0.0314575\ttotal: 1.77s\tremaining: 1.61s\n",
      "131:\tlearn: 0.0311016\ttotal: 1.78s\tremaining: 1.59s\n",
      "132:\tlearn: 0.0307238\ttotal: 1.81s\tremaining: 1.59s\n",
      "133:\tlearn: 0.0302818\ttotal: 1.86s\tremaining: 1.61s\n",
      "134:\tlearn: 0.0299078\ttotal: 1.88s\tremaining: 1.6s\n",
      "135:\tlearn: 0.0295536\ttotal: 1.89s\tremaining: 1.58s\n",
      "136:\tlearn: 0.0292319\ttotal: 1.89s\tremaining: 1.56s\n",
      "137:\tlearn: 0.0288689\ttotal: 1.9s\tremaining: 1.54s\n",
      "138:\tlearn: 0.0288616\ttotal: 1.93s\tremaining: 1.54s\n",
      "139:\tlearn: 0.0288428\ttotal: 1.95s\tremaining: 1.53s\n",
      "140:\tlearn: 0.0285250\ttotal: 1.96s\tremaining: 1.52s\n",
      "141:\tlearn: 0.0284180\ttotal: 1.99s\tremaining: 1.52s\n",
      "142:\tlearn: 0.0282211\ttotal: 2.01s\tremaining: 1.5s\n",
      "143:\tlearn: 0.0282060\ttotal: 2.02s\tremaining: 1.48s\n",
      "144:\tlearn: 0.0279087\ttotal: 2.02s\tremaining: 1.46s\n",
      "145:\tlearn: 0.0276233\ttotal: 2.02s\tremaining: 1.44s\n",
      "146:\tlearn: 0.0271783\ttotal: 2.04s\tremaining: 1.43s\n",
      "147:\tlearn: 0.0269045\ttotal: 2.05s\tremaining: 1.41s\n",
      "148:\tlearn: 0.0268709\ttotal: 2.05s\tremaining: 1.39s\n",
      "149:\tlearn: 0.0266056\ttotal: 2.06s\tremaining: 1.37s\n",
      "150:\tlearn: 0.0262625\ttotal: 2.06s\tremaining: 1.35s\n",
      "151:\tlearn: 0.0261783\ttotal: 2.07s\tremaining: 1.34s\n",
      "152:\tlearn: 0.0259212\ttotal: 2.08s\tremaining: 1.32s\n",
      "153:\tlearn: 0.0259119\ttotal: 2.08s\tremaining: 1.3s\n",
      "154:\tlearn: 0.0258958\ttotal: 2.09s\tremaining: 1.28s\n",
      "155:\tlearn: 0.0258877\ttotal: 2.09s\tremaining: 1.26s\n",
      "156:\tlearn: 0.0258877\ttotal: 2.09s\tremaining: 1.24s\n",
      "157:\tlearn: 0.0258859\ttotal: 2.1s\tremaining: 1.22s\n",
      "158:\tlearn: 0.0255310\ttotal: 2.12s\tremaining: 1.21s\n",
      "159:\tlearn: 0.0255199\ttotal: 2.12s\tremaining: 1.19s\n",
      "160:\tlearn: 0.0253068\ttotal: 2.13s\tremaining: 1.18s\n",
      "161:\tlearn: 0.0250904\ttotal: 2.13s\tremaining: 1.16s\n",
      "162:\tlearn: 0.0249069\ttotal: 2.14s\tremaining: 1.14s\n",
      "163:\tlearn: 0.0249066\ttotal: 2.15s\tremaining: 1.13s\n",
      "164:\tlearn: 0.0247684\ttotal: 2.15s\tremaining: 1.11s\n",
      "165:\tlearn: 0.0246494\ttotal: 2.18s\tremaining: 1.1s\n",
      "166:\tlearn: 0.0242232\ttotal: 2.23s\tremaining: 1.11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167:\tlearn: 0.0242232\ttotal: 2.27s\tremaining: 1.11s\n",
      "168:\tlearn: 0.0242231\ttotal: 2.31s\tremaining: 1.1s\n",
      "169:\tlearn: 0.0242156\ttotal: 2.34s\tremaining: 1.1s\n",
      "170:\tlearn: 0.0242088\ttotal: 2.35s\tremaining: 1.09s\n",
      "171:\tlearn: 0.0241994\ttotal: 2.37s\tremaining: 1.07s\n",
      "172:\tlearn: 0.0239497\ttotal: 2.39s\tremaining: 1.06s\n",
      "173:\tlearn: 0.0236513\ttotal: 2.4s\tremaining: 1.05s\n",
      "174:\tlearn: 0.0236512\ttotal: 2.43s\tremaining: 1.04s\n",
      "175:\tlearn: 0.0232631\ttotal: 2.45s\tremaining: 1.03s\n",
      "176:\tlearn: 0.0231456\ttotal: 2.45s\tremaining: 1.01s\n",
      "177:\tlearn: 0.0229901\ttotal: 2.46s\tremaining: 995ms\n",
      "178:\tlearn: 0.0228185\ttotal: 2.46s\tremaining: 978ms\n",
      "179:\tlearn: 0.0224745\ttotal: 2.47s\tremaining: 961ms\n",
      "180:\tlearn: 0.0221730\ttotal: 2.48s\tremaining: 945ms\n",
      "181:\tlearn: 0.0219737\ttotal: 2.48s\tremaining: 928ms\n",
      "182:\tlearn: 0.0219635\ttotal: 2.49s\tremaining: 913ms\n",
      "183:\tlearn: 0.0216840\ttotal: 2.51s\tremaining: 900ms\n",
      "184:\tlearn: 0.0216839\ttotal: 2.56s\tremaining: 899ms\n",
      "185:\tlearn: 0.0215256\ttotal: 2.58s\tremaining: 889ms\n",
      "186:\tlearn: 0.0215256\ttotal: 2.59s\tremaining: 872ms\n",
      "187:\tlearn: 0.0214722\ttotal: 2.6s\tremaining: 858ms\n",
      "188:\tlearn: 0.0214011\ttotal: 2.62s\tremaining: 845ms\n",
      "189:\tlearn: 0.0212642\ttotal: 2.63s\tremaining: 832ms\n",
      "190:\tlearn: 0.0210233\ttotal: 2.67s\tremaining: 823ms\n",
      "191:\tlearn: 0.0208181\ttotal: 2.7s\tremaining: 815ms\n",
      "192:\tlearn: 0.0207575\ttotal: 2.7s\tremaining: 798ms\n",
      "193:\tlearn: 0.0207575\ttotal: 2.71s\tremaining: 781ms\n",
      "194:\tlearn: 0.0207573\ttotal: 2.71s\tremaining: 764ms\n",
      "195:\tlearn: 0.0204531\ttotal: 2.73s\tremaining: 752ms\n",
      "196:\tlearn: 0.0204530\ttotal: 2.73s\tremaining: 735ms\n",
      "197:\tlearn: 0.0204529\ttotal: 2.73s\tremaining: 718ms\n",
      "198:\tlearn: 0.0203482\ttotal: 2.74s\tremaining: 702ms\n",
      "199:\tlearn: 0.0203480\ttotal: 2.74s\tremaining: 686ms\n",
      "200:\tlearn: 0.0203382\ttotal: 2.81s\tremaining: 684ms\n",
      "201:\tlearn: 0.0203182\ttotal: 2.81s\tremaining: 668ms\n",
      "202:\tlearn: 0.0203182\ttotal: 2.82s\tremaining: 654ms\n",
      "203:\tlearn: 0.0203181\ttotal: 2.83s\tremaining: 637ms\n",
      "204:\tlearn: 0.0201795\ttotal: 2.83s\tremaining: 622ms\n",
      "205:\tlearn: 0.0201795\ttotal: 2.83s\tremaining: 605ms\n",
      "206:\tlearn: 0.0201476\ttotal: 2.85s\tremaining: 593ms\n",
      "207:\tlearn: 0.0200345\ttotal: 2.86s\tremaining: 577ms\n",
      "208:\tlearn: 0.0198086\ttotal: 2.86s\tremaining: 562ms\n",
      "209:\tlearn: 0.0198046\ttotal: 2.87s\tremaining: 546ms\n",
      "210:\tlearn: 0.0197974\ttotal: 2.87s\tremaining: 531ms\n",
      "211:\tlearn: 0.0196959\ttotal: 2.88s\tremaining: 516ms\n",
      "212:\tlearn: 0.0196959\ttotal: 2.88s\tremaining: 500ms\n",
      "213:\tlearn: 0.0196294\ttotal: 2.9s\tremaining: 488ms\n",
      "214:\tlearn: 0.0196036\ttotal: 2.9s\tremaining: 473ms\n",
      "215:\tlearn: 0.0196033\ttotal: 2.91s\tremaining: 458ms\n",
      "216:\tlearn: 0.0195024\ttotal: 2.91s\tremaining: 443ms\n",
      "217:\tlearn: 0.0193117\ttotal: 2.92s\tremaining: 429ms\n",
      "218:\tlearn: 0.0190998\ttotal: 2.93s\tremaining: 415ms\n",
      "219:\tlearn: 0.0188476\ttotal: 2.94s\tremaining: 400ms\n",
      "220:\tlearn: 0.0188317\ttotal: 2.94s\tremaining: 386ms\n",
      "221:\tlearn: 0.0187322\ttotal: 2.95s\tremaining: 372ms\n",
      "222:\tlearn: 0.0186061\ttotal: 2.96s\tremaining: 359ms\n",
      "223:\tlearn: 0.0184236\ttotal: 2.99s\tremaining: 347ms\n",
      "224:\tlearn: 0.0184236\ttotal: 3.02s\tremaining: 336ms\n",
      "225:\tlearn: 0.0182159\ttotal: 3.04s\tremaining: 323ms\n",
      "226:\tlearn: 0.0182159\ttotal: 3.05s\tremaining: 310ms\n",
      "227:\tlearn: 0.0180768\ttotal: 3.09s\tremaining: 298ms\n",
      "228:\tlearn: 0.0180481\ttotal: 3.12s\tremaining: 286ms\n",
      "229:\tlearn: 0.0180481\ttotal: 3.13s\tremaining: 272ms\n",
      "230:\tlearn: 0.0180481\ttotal: 3.17s\tremaining: 260ms\n",
      "231:\tlearn: 0.0180481\ttotal: 3.18s\tremaining: 247ms\n",
      "232:\tlearn: 0.0179103\ttotal: 3.21s\tremaining: 235ms\n",
      "233:\tlearn: 0.0178342\ttotal: 3.25s\tremaining: 222ms\n",
      "234:\tlearn: 0.0177892\ttotal: 3.27s\tremaining: 209ms\n",
      "235:\tlearn: 0.0177892\ttotal: 3.27s\tremaining: 194ms\n",
      "236:\tlearn: 0.0177440\ttotal: 3.28s\tremaining: 180ms\n",
      "237:\tlearn: 0.0176322\ttotal: 3.29s\tremaining: 166ms\n",
      "238:\tlearn: 0.0176322\ttotal: 3.3s\tremaining: 152ms\n",
      "239:\tlearn: 0.0174580\ttotal: 3.32s\tremaining: 138ms\n",
      "240:\tlearn: 0.0173056\ttotal: 3.33s\tremaining: 124ms\n",
      "241:\tlearn: 0.0173056\ttotal: 3.33s\tremaining: 110ms\n",
      "242:\tlearn: 0.0173028\ttotal: 3.34s\tremaining: 96.1ms\n",
      "243:\tlearn: 0.0172707\ttotal: 3.35s\tremaining: 82.4ms\n",
      "244:\tlearn: 0.0172344\ttotal: 3.36s\tremaining: 68.5ms\n",
      "245:\tlearn: 0.0170838\ttotal: 3.36s\tremaining: 54.7ms\n",
      "246:\tlearn: 0.0168811\ttotal: 3.37s\tremaining: 40.9ms\n",
      "247:\tlearn: 0.0168075\ttotal: 3.37s\tremaining: 27.2ms\n",
      "248:\tlearn: 0.0168075\ttotal: 3.37s\tremaining: 13.6ms\n",
      "249:\tlearn: 0.0168022\ttotal: 3.4s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0bf2ce1ed94b8cb33dfbb656f8995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.4941816312214444, Recall = 0.9813936519518424, Aging Rate = 0.924032872319102, Precision = 0.5835140997830802, f1 = 0.7318732145286355\n",
      "Epoch 2: Train Loss = 0.32513982812336584, Recall = 0.961327982488143, Aging Rate = 0.6987372218881539, Precision = 0.7558806655192197, f1 = 0.8463144371286334\n",
      "Epoch 3: Train Loss = 0.25669191375383754, Recall = 0.9668004377964247, Aging Rate = 0.6540388855482061, Precision = 0.8121360711002146, f1 = 0.882744836775483\n",
      "Epoch 4: Train Loss = 0.21411041355988478, Recall = 0.9686245895658518, Aging Rate = 0.6285828823411506, Precision = 0.8466198979591837, f1 = 0.9035222052067381\n",
      "Epoch 5: Train Loss = 0.18081833702644812, Recall = 0.97117840204305, Aging Rate = 0.6109440769693325, Precision = 0.8733595800524935, f1 = 0.9196752461565038\n",
      "Test Loss = 0.1475657185229349, Recall = 0.977745348412988, Aging Rate = 0.5888955702545601, precision = 0.9121851599727706\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.13814374965784137, Recall = 0.9788398394746443, Aging Rate = 0.5892964521948286, Precision = 0.9125850340136055, f1 = 0.9445520154902306\n",
      "Epoch 7: Train Loss = 0.11175540168906864, Recall = 0.9839474644290405, Aging Rate = 0.5792744036881139, Precision = 0.9332179930795848, f1 = 0.9579115610015982\n",
      "Epoch 8: Train Loss = 0.08907687461526725, Recall = 0.990149580445093, Aging Rate = 0.5722589697334135, Precision = 0.9506129597197899, f1 = 0.9699785561115082\n",
      "Epoch 9: Train Loss = 0.0752745050900349, Recall = 0.9912440715067493, Aging Rate = 0.5668470635397875, Precision = 0.9607496463932107, f1 = 0.97575866403304\n",
      "Epoch 10: Train Loss = 0.060610485301810575, Recall = 0.9930682232761766, Aging Rate = 0.5600320705552215, Precision = 0.9742304939155333, f1 = 0.9835591689250226\n",
      "Test Loss = 0.05075197073555012, Recall = 0.9952572053994893, Aging Rate = 0.5568250150330728, precision = 0.9820014398848093\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05125716403066981, Recall = 0.9952572053994893, Aging Rate = 0.5586289837642814, Precision = 0.9788302834589164, f1 = 0.9869753979739507\n",
      "Epoch 12: Train Loss = 0.043874918210838575, Recall = 0.9948923750456038, Aging Rate = 0.5558228101824013, Precision = 0.9834114677244861, f1 = 0.9891186071817193\n",
      "Epoch 13: Train Loss = 0.036934787395756896, Recall = 0.9981758482305728, Aging Rate = 0.5558228101824013, Precision = 0.9866570501262171, f1 = 0.9923830250272035\n",
      "Epoch 14: Train Loss = 0.032151801173763206, Recall = 0.9981758482305728, Aging Rate = 0.5548206053317298, Precision = 0.9884393063583815, f1 = 0.9932837175530949\n",
      "Epoch 15: Train Loss = 0.028143418638835, Recall = 0.9978110178766874, Aging Rate = 0.5530166366005211, Precision = 0.9913011960855382, f1 = 0.9945454545454545\n",
      "Test Loss = 0.024068102915953314, Recall = 0.9989055089383436, Aging Rate = 0.5518139907797154, precision = 0.9945513984743916\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.02467352944120278, Recall = 0.9989055089383436, Aging Rate = 0.5528161956303869, Precision = 0.9927483683828862, f1 = 0.9958174213493362\n",
      "Epoch 17: Train Loss = 0.022268367068620655, Recall = 0.9985406785844583, Aging Rate = 0.5518139907797154, Precision = 0.9941881583726844, f1 = 0.9963596650891882\n",
      "Epoch 18: Train Loss = 0.019704580336688194, Recall = 1.0, Aging Rate = 0.5524153136901183, Precision = 0.9945573294629898, f1 = 0.9972712388575588\n",
      "Epoch 19: Train Loss = 0.0174395003964414, Recall = 1.0, Aging Rate = 0.5520144317498497, Precision = 0.995279593318809, f1 = 0.9976342129208371\n",
      "Epoch 20: Train Loss = 0.016216939873265362, Recall = 0.9989055089383436, Aging Rate = 0.5508117859290439, Precision = 0.9963609898107715, f1 = 0.997631626890144\n",
      "Test Loss = 0.013861003673455654, Recall = 1.0, Aging Rate = 0.5508117859290439, precision = 0.99745269286754\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.014547317524400078, Recall = 0.9992703392922291, Aging Rate = 0.5506113449589096, Precision = 0.9970877320713506, f1 = 0.9981778425655977\n",
      "Epoch 22: Train Loss = 0.012746251982521571, Recall = 1.0, Aging Rate = 0.5514131088394468, Precision = 0.9963649581970193, f1 = 0.9981791697013839\n",
      "Epoch 23: Train Loss = 0.012168445244527074, Recall = 0.9996351696461145, Aging Rate = 0.5512126678693124, Precision = 0.9963636363636363, f1 = 0.9979967219085777\n",
      "Epoch 24: Train Loss = 0.01051441874294595, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 25: Train Loss = 0.00980124733057773, Recall = 1.0, Aging Rate = 0.5506113449589096, Precision = 0.9978157990535129, f1 = 0.9989067055393587\n",
      "Test Loss = 0.008236511659357263, Recall = 1.0, Aging Rate = 0.5498095810783724, precision = 0.999270871308786\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.008971440090782348, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 27: Train Loss = 0.008081698792642715, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 28: Train Loss = 0.007411564940183094, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 29: Train Loss = 0.006968034326240173, Recall = 1.0, Aging Rate = 0.550210463018641, Precision = 0.9985428051001821, f1 = 0.999270871308786\n",
      "Epoch 30: Train Loss = 0.006555230877390007, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.00549616498068441, Recall = 1.0, Aging Rate = 0.5498095810783724, precision = 0.999270871308786\n",
      "\n",
      "Epoch 31: Train Loss = 0.006374112149675508, Recall = 1.0, Aging Rate = 0.5500100220485067, Precision = 0.9989067055393586, f1 = 0.9994530537830446\n",
      "Epoch 32: Train Loss = 0.006077756300474496, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 33: Train Loss = 0.005870904945401599, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 34: Train Loss = 0.0053821958142113765, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 35: Train Loss = 0.004695136421321807, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Test Loss = 0.003855488058661239, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.004619320421131493, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 37: Train Loss = 0.004412679154622615, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 38: Train Loss = 0.0042568699152564105, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 39: Train Loss = 0.0038440494048996255, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 40: Train Loss = 0.003876159290422264, Recall = 0.9996351696461145, Aging Rate = 0.5494086991381039, Precision = 0.9996351696461145, f1 = 0.9996351696461145\n",
      "Test Loss = 0.0034307536217694857, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 41: Train Loss = 0.003969021441236118, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 42: Train Loss = 0.003472156134503301, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 43: Train Loss = 0.003282412139545416, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 44: Train Loss = 0.003123451532843814, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0034704489072855346, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.002768112768579254, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 46: Train Loss = 0.003320184320948404, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 47: Train Loss = 0.0031673509620486987, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.0029902987742725976, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 49: Train Loss = 0.0029402504533030485, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 50: Train Loss = 0.002923601601548354, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.002336463902916966, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.0027371607073088952, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0030850768800526947, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 53: Train Loss = 0.0026956560267712326, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 54: Train Loss = 0.0029587909863676705, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 55: Train Loss = 0.0029269958822749965, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0027169961153551427, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.0025605992556397097, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002406243276594725, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 58: Train Loss = 0.0023897641355582113, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0027796375093129266, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0029622547857789667, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022356633060438363, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 61: Train Loss = 0.0022734954679387814, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0024721164342298234, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 63: Train Loss = 0.0024815851795987583, Recall = 1.0, Aging Rate = 0.5498095810783724, Precision = 0.999270871308786, f1 = 0.99963530269876\n",
      "Epoch 64: Train Loss = 0.00245140858298977, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 65: Train Loss = 0.00241270958265638, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.002567524000478501, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 66: Train Loss = 0.0037249338777984482, Recall = 0.9996351696461145, Aging Rate = 0.5496091401082381, Precision = 0.9992706053975201, f1 = 0.9994528542768557\n",
      "Epoch 67: Train Loss = 0.0026054429203028443, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 68: Train Loss = 0.0023724237820887733, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0024202119603478306, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0024258477357491526, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0018632269189373118, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.002321325442261843, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.002223058149616551, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0021526015860283, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0020771840236588975, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.002451408381695568, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002069914602476855, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0021698589535274386, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.00233162744668917, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 78: Train Loss = 0.0025365636218245865, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.002271159256064186, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 80: Train Loss = 0.002144048874161721, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001766939821523702, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.002329379076906867, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0023075551387108525, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 83: Train Loss = 0.0022406215648063978, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.002100958371521223, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.002433529685182905, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.003028714306132469, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 86: Train Loss = 0.0030537298791846763, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 87: Train Loss = 0.0022572110242593574, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.002187319688233335, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 89: Train Loss = 0.002044331140504091, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0022620482224977403, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018735404829318247, Recall = 1.0, Aging Rate = 0.5496091401082381, precision = 0.99963530269876\n",
      "\n",
      "Epoch 91: Train Loss = 0.002517771832340659, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 92: Train Loss = 0.002175851017647862, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 93: Train Loss = 0.0022952518729975765, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0023864334143538046, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 95: Train Loss = 0.0020376769201620954, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Test Loss = 0.0017029837017576849, Recall = 1.0, Aging Rate = 0.5494086991381039, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0020008867381203685, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0025459494344956574, Recall = 1.0, Aging Rate = 0.5496091401082381, Precision = 0.99963530269876, f1 = 0.9998176180922853\n",
      "Epoch 98: Train Loss = 0.0021178298292638268, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.002151393452926195, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.002686822567070506, Recall = 1.0, Aging Rate = 0.5494086991381039, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00244226603107749, Recall = 1.0, Aging Rate = 0.5498095810783724, precision = 0.999270871308786\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6054946\ttotal: 25.3ms\tremaining: 6.3s\n",
      "1:\tlearn: 0.5260557\ttotal: 41ms\tremaining: 5.08s\n",
      "2:\tlearn: 0.4950122\ttotal: 56.9ms\tremaining: 4.68s\n",
      "3:\tlearn: 0.4573964\ttotal: 72.6ms\tremaining: 4.46s\n",
      "4:\tlearn: 0.4304743\ttotal: 87.9ms\tremaining: 4.31s\n",
      "5:\tlearn: 0.4076756\ttotal: 120ms\tremaining: 4.87s\n",
      "6:\tlearn: 0.3812348\ttotal: 152ms\tremaining: 5.26s\n",
      "7:\tlearn: 0.3673042\ttotal: 183ms\tremaining: 5.54s\n",
      "8:\tlearn: 0.3432766\ttotal: 199ms\tremaining: 5.33s\n",
      "9:\tlearn: 0.3257710\ttotal: 215ms\tremaining: 5.16s\n",
      "10:\tlearn: 0.3102137\ttotal: 231ms\tremaining: 5.01s\n",
      "11:\tlearn: 0.2966259\ttotal: 278ms\tremaining: 5.52s\n",
      "12:\tlearn: 0.2891921\ttotal: 310ms\tremaining: 5.65s\n",
      "13:\tlearn: 0.2802935\ttotal: 341ms\tremaining: 5.75s\n",
      "14:\tlearn: 0.2682555\ttotal: 356ms\tremaining: 5.57s\n",
      "15:\tlearn: 0.2611964\ttotal: 387ms\tremaining: 5.65s\n",
      "16:\tlearn: 0.2438969\ttotal: 392ms\tremaining: 5.37s\n",
      "17:\tlearn: 0.2358875\ttotal: 396ms\tremaining: 5.11s\n",
      "18:\tlearn: 0.2257568\ttotal: 402ms\tremaining: 4.88s\n",
      "19:\tlearn: 0.2146168\ttotal: 406ms\tremaining: 4.67s\n",
      "20:\tlearn: 0.2103096\ttotal: 411ms\tremaining: 4.49s\n",
      "21:\tlearn: 0.2025362\ttotal: 443ms\tremaining: 4.59s\n",
      "22:\tlearn: 0.1985427\ttotal: 449ms\tremaining: 4.43s\n",
      "23:\tlearn: 0.1925743\ttotal: 463ms\tremaining: 4.36s\n",
      "24:\tlearn: 0.1877089\ttotal: 479ms\tremaining: 4.31s\n",
      "25:\tlearn: 0.1816976\ttotal: 511ms\tremaining: 4.4s\n",
      "26:\tlearn: 0.1728208\ttotal: 516ms\tremaining: 4.26s\n",
      "27:\tlearn: 0.1675773\ttotal: 520ms\tremaining: 4.13s\n",
      "28:\tlearn: 0.1640041\ttotal: 525ms\tremaining: 4s\n",
      "29:\tlearn: 0.1604121\ttotal: 530ms\tremaining: 3.88s\n",
      "30:\tlearn: 0.1564763\ttotal: 553ms\tremaining: 3.9s\n",
      "31:\tlearn: 0.1527247\ttotal: 563ms\tremaining: 3.83s\n",
      "32:\tlearn: 0.1493865\ttotal: 568ms\tremaining: 3.73s\n",
      "33:\tlearn: 0.1452717\ttotal: 587ms\tremaining: 3.73s\n",
      "34:\tlearn: 0.1389807\ttotal: 619ms\tremaining: 3.8s\n",
      "35:\tlearn: 0.1359902\ttotal: 667ms\tremaining: 3.96s\n",
      "36:\tlearn: 0.1333092\ttotal: 672ms\tremaining: 3.87s\n",
      "37:\tlearn: 0.1281189\ttotal: 682ms\tremaining: 3.81s\n",
      "38:\tlearn: 0.1259127\ttotal: 687ms\tremaining: 3.72s\n",
      "39:\tlearn: 0.1239794\ttotal: 693ms\tremaining: 3.64s\n",
      "40:\tlearn: 0.1187938\ttotal: 698ms\tremaining: 3.56s\n",
      "41:\tlearn: 0.1147154\ttotal: 703ms\tremaining: 3.48s\n",
      "42:\tlearn: 0.1120499\ttotal: 708ms\tremaining: 3.41s\n",
      "43:\tlearn: 0.1089481\ttotal: 759ms\tremaining: 3.56s\n",
      "44:\tlearn: 0.1045178\ttotal: 764ms\tremaining: 3.48s\n",
      "45:\tlearn: 0.1024122\ttotal: 769ms\tremaining: 3.41s\n",
      "46:\tlearn: 0.0990410\ttotal: 774ms\tremaining: 3.34s\n",
      "47:\tlearn: 0.0976003\ttotal: 791ms\tremaining: 3.33s\n",
      "48:\tlearn: 0.0940887\ttotal: 795ms\tremaining: 3.26s\n",
      "49:\tlearn: 0.0920624\ttotal: 800ms\tremaining: 3.2s\n",
      "50:\tlearn: 0.0903686\ttotal: 805ms\tremaining: 3.14s\n",
      "51:\tlearn: 0.0879618\ttotal: 810ms\tremaining: 3.08s\n",
      "52:\tlearn: 0.0853280\ttotal: 815ms\tremaining: 3.03s\n",
      "53:\tlearn: 0.0836032\ttotal: 819ms\tremaining: 2.97s\n",
      "54:\tlearn: 0.0809325\ttotal: 837ms\tremaining: 2.97s\n",
      "55:\tlearn: 0.0789560\ttotal: 869ms\tremaining: 3.01s\n",
      "56:\tlearn: 0.0777251\ttotal: 874ms\tremaining: 2.96s\n",
      "57:\tlearn: 0.0766377\ttotal: 885ms\tremaining: 2.93s\n",
      "58:\tlearn: 0.0745367\ttotal: 935ms\tremaining: 3.02s\n",
      "59:\tlearn: 0.0729227\ttotal: 950ms\tremaining: 3.01s\n",
      "60:\tlearn: 0.0710949\ttotal: 961ms\tremaining: 2.98s\n",
      "61:\tlearn: 0.0693133\ttotal: 987ms\tremaining: 2.99s\n",
      "62:\tlearn: 0.0680258\ttotal: 997ms\tremaining: 2.96s\n",
      "63:\tlearn: 0.0669855\ttotal: 1.01s\tremaining: 2.93s\n",
      "64:\tlearn: 0.0653335\ttotal: 1.01s\tremaining: 2.89s\n",
      "65:\tlearn: 0.0645785\ttotal: 1.02s\tremaining: 2.84s\n",
      "66:\tlearn: 0.0631928\ttotal: 1.02s\tremaining: 2.8s\n",
      "67:\tlearn: 0.0617690\ttotal: 1.04s\tremaining: 2.79s\n",
      "68:\tlearn: 0.0605765\ttotal: 1.04s\tremaining: 2.74s\n",
      "69:\tlearn: 0.0591531\ttotal: 1.05s\tremaining: 2.7s\n",
      "70:\tlearn: 0.0584858\ttotal: 1.05s\tremaining: 2.66s\n",
      "71:\tlearn: 0.0579058\ttotal: 1.08s\tremaining: 2.68s\n",
      "72:\tlearn: 0.0563223\ttotal: 1.1s\tremaining: 2.67s\n",
      "73:\tlearn: 0.0548955\ttotal: 1.1s\tremaining: 2.63s\n",
      "74:\tlearn: 0.0536654\ttotal: 1.11s\tremaining: 2.59s\n",
      "75:\tlearn: 0.0526461\ttotal: 1.11s\tremaining: 2.55s\n",
      "76:\tlearn: 0.0518738\ttotal: 1.12s\tremaining: 2.51s\n",
      "77:\tlearn: 0.0510395\ttotal: 1.12s\tremaining: 2.48s\n",
      "78:\tlearn: 0.0501545\ttotal: 1.13s\tremaining: 2.44s\n",
      "79:\tlearn: 0.0493506\ttotal: 1.13s\tremaining: 2.41s\n",
      "80:\tlearn: 0.0488117\ttotal: 1.15s\tremaining: 2.41s\n",
      "81:\tlearn: 0.0479909\ttotal: 1.16s\tremaining: 2.38s\n",
      "82:\tlearn: 0.0471908\ttotal: 1.17s\tremaining: 2.35s\n",
      "83:\tlearn: 0.0465287\ttotal: 1.17s\tremaining: 2.32s\n",
      "84:\tlearn: 0.0453502\ttotal: 1.18s\tremaining: 2.29s\n",
      "85:\tlearn: 0.0451538\ttotal: 1.18s\tremaining: 2.26s\n",
      "86:\tlearn: 0.0447002\ttotal: 1.19s\tremaining: 2.23s\n",
      "87:\tlearn: 0.0439049\ttotal: 1.21s\tremaining: 2.22s\n",
      "88:\tlearn: 0.0430751\ttotal: 1.21s\tremaining: 2.19s\n",
      "89:\tlearn: 0.0420766\ttotal: 1.24s\tremaining: 2.2s\n",
      "90:\tlearn: 0.0417580\ttotal: 1.25s\tremaining: 2.19s\n",
      "91:\tlearn: 0.0409740\ttotal: 1.27s\tremaining: 2.18s\n",
      "92:\tlearn: 0.0403189\ttotal: 1.3s\tremaining: 2.2s\n",
      "93:\tlearn: 0.0395893\ttotal: 1.32s\tremaining: 2.19s\n",
      "94:\tlearn: 0.0392294\ttotal: 1.33s\tremaining: 2.17s\n",
      "95:\tlearn: 0.0382265\ttotal: 1.35s\tremaining: 2.16s\n",
      "96:\tlearn: 0.0376721\ttotal: 1.35s\tremaining: 2.14s\n",
      "97:\tlearn: 0.0374484\ttotal: 1.36s\tremaining: 2.11s\n",
      "98:\tlearn: 0.0368217\ttotal: 1.36s\tremaining: 2.08s\n",
      "99:\tlearn: 0.0362439\ttotal: 1.37s\tremaining: 2.05s\n",
      "100:\tlearn: 0.0354283\ttotal: 1.38s\tremaining: 2.04s\n",
      "101:\tlearn: 0.0350295\ttotal: 1.38s\tremaining: 2.01s\n",
      "102:\tlearn: 0.0344586\ttotal: 1.39s\tremaining: 1.98s\n",
      "103:\tlearn: 0.0344277\ttotal: 1.39s\tremaining: 1.96s\n",
      "104:\tlearn: 0.0338108\ttotal: 1.4s\tremaining: 1.93s\n",
      "105:\tlearn: 0.0333018\ttotal: 1.4s\tremaining: 1.91s\n",
      "106:\tlearn: 0.0332757\ttotal: 1.45s\tremaining: 1.94s\n",
      "107:\tlearn: 0.0329797\ttotal: 1.47s\tremaining: 1.93s\n",
      "108:\tlearn: 0.0323805\ttotal: 1.49s\tremaining: 1.92s\n",
      "109:\tlearn: 0.0320458\ttotal: 1.54s\tremaining: 1.96s\n",
      "110:\tlearn: 0.0315593\ttotal: 1.55s\tremaining: 1.94s\n",
      "111:\tlearn: 0.0311823\ttotal: 1.56s\tremaining: 1.92s\n",
      "112:\tlearn: 0.0306893\ttotal: 1.58s\tremaining: 1.92s\n",
      "113:\tlearn: 0.0303435\ttotal: 1.58s\tremaining: 1.89s\n",
      "114:\tlearn: 0.0298974\ttotal: 1.59s\tremaining: 1.87s\n",
      "115:\tlearn: 0.0295966\ttotal: 1.59s\tremaining: 1.84s\n",
      "116:\tlearn: 0.0292359\ttotal: 1.6s\tremaining: 1.82s\n",
      "117:\tlearn: 0.0287661\ttotal: 1.6s\tremaining: 1.79s\n",
      "118:\tlearn: 0.0285976\ttotal: 1.63s\tremaining: 1.79s\n",
      "119:\tlearn: 0.0282835\ttotal: 1.64s\tremaining: 1.78s\n",
      "120:\tlearn: 0.0278339\ttotal: 1.67s\tremaining: 1.78s\n",
      "121:\tlearn: 0.0275126\ttotal: 1.71s\tremaining: 1.79s\n",
      "122:\tlearn: 0.0271000\ttotal: 1.71s\tremaining: 1.77s\n",
      "123:\tlearn: 0.0268558\ttotal: 1.72s\tremaining: 1.74s\n",
      "124:\tlearn: 0.0266354\ttotal: 1.72s\tremaining: 1.72s\n",
      "125:\tlearn: 0.0262998\ttotal: 1.74s\tremaining: 1.71s\n",
      "126:\tlearn: 0.0261835\ttotal: 1.77s\tremaining: 1.71s\n",
      "127:\tlearn: 0.0259584\ttotal: 1.79s\tremaining: 1.71s\n",
      "128:\tlearn: 0.0256235\ttotal: 1.8s\tremaining: 1.69s\n",
      "129:\tlearn: 0.0254485\ttotal: 1.81s\tremaining: 1.67s\n",
      "130:\tlearn: 0.0252323\ttotal: 1.81s\tremaining: 1.65s\n",
      "131:\tlearn: 0.0249188\ttotal: 1.83s\tremaining: 1.64s\n",
      "132:\tlearn: 0.0248678\ttotal: 1.83s\tremaining: 1.61s\n",
      "133:\tlearn: 0.0245443\ttotal: 1.84s\tremaining: 1.6s\n",
      "134:\tlearn: 0.0240120\ttotal: 1.88s\tremaining: 1.6s\n",
      "135:\tlearn: 0.0239768\ttotal: 1.88s\tremaining: 1.58s\n",
      "136:\tlearn: 0.0236814\ttotal: 1.89s\tremaining: 1.56s\n",
      "137:\tlearn: 0.0235602\ttotal: 1.89s\tremaining: 1.53s\n",
      "138:\tlearn: 0.0232575\ttotal: 1.9s\tremaining: 1.51s\n",
      "139:\tlearn: 0.0231074\ttotal: 1.9s\tremaining: 1.49s\n",
      "140:\tlearn: 0.0229315\ttotal: 1.91s\tremaining: 1.47s\n",
      "141:\tlearn: 0.0228141\ttotal: 1.91s\tremaining: 1.45s\n",
      "142:\tlearn: 0.0227792\ttotal: 1.92s\tremaining: 1.43s\n",
      "143:\tlearn: 0.0226626\ttotal: 1.93s\tremaining: 1.42s\n",
      "144:\tlearn: 0.0224361\ttotal: 1.93s\tremaining: 1.4s\n",
      "145:\tlearn: 0.0221516\ttotal: 1.94s\tremaining: 1.38s\n",
      "146:\tlearn: 0.0219756\ttotal: 1.94s\tremaining: 1.36s\n",
      "147:\tlearn: 0.0218153\ttotal: 1.96s\tremaining: 1.35s\n",
      "148:\tlearn: 0.0216257\ttotal: 1.96s\tremaining: 1.33s\n",
      "149:\tlearn: 0.0212572\ttotal: 1.97s\tremaining: 1.31s\n",
      "150:\tlearn: 0.0210345\ttotal: 1.97s\tremaining: 1.29s\n",
      "151:\tlearn: 0.0209200\ttotal: 1.99s\tremaining: 1.28s\n",
      "152:\tlearn: 0.0207091\ttotal: 1.99s\tremaining: 1.26s\n",
      "153:\tlearn: 0.0205300\ttotal: 2s\tremaining: 1.24s\n",
      "154:\tlearn: 0.0203110\ttotal: 2s\tremaining: 1.23s\n",
      "155:\tlearn: 0.0201810\ttotal: 2.01s\tremaining: 1.21s\n",
      "156:\tlearn: 0.0199417\ttotal: 2.02s\tremaining: 1.2s\n",
      "157:\tlearn: 0.0197786\ttotal: 2.02s\tremaining: 1.18s\n",
      "158:\tlearn: 0.0195327\ttotal: 2.03s\tremaining: 1.16s\n",
      "159:\tlearn: 0.0194291\ttotal: 2.03s\tremaining: 1.14s\n",
      "160:\tlearn: 0.0190733\ttotal: 2.04s\tremaining: 1.13s\n",
      "161:\tlearn: 0.0189616\ttotal: 2.04s\tremaining: 1.11s\n",
      "162:\tlearn: 0.0187919\ttotal: 2.05s\tremaining: 1.09s\n",
      "163:\tlearn: 0.0186224\ttotal: 2.06s\tremaining: 1.08s\n",
      "164:\tlearn: 0.0184228\ttotal: 2.07s\tremaining: 1.07s\n",
      "165:\tlearn: 0.0181939\ttotal: 2.07s\tremaining: 1.05s\n",
      "166:\tlearn: 0.0181163\ttotal: 2.08s\tremaining: 1.03s\n",
      "167:\tlearn: 0.0180047\ttotal: 2.08s\tremaining: 1.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168:\tlearn: 0.0178466\ttotal: 2.09s\tremaining: 1s\n",
      "169:\tlearn: 0.0175330\ttotal: 2.1s\tremaining: 986ms\n",
      "170:\tlearn: 0.0174037\ttotal: 2.1s\tremaining: 970ms\n",
      "171:\tlearn: 0.0173452\ttotal: 2.11s\tremaining: 957ms\n",
      "172:\tlearn: 0.0172333\ttotal: 2.12s\tremaining: 942ms\n",
      "173:\tlearn: 0.0170213\ttotal: 2.13s\tremaining: 929ms\n",
      "174:\tlearn: 0.0168953\ttotal: 2.13s\tremaining: 915ms\n",
      "175:\tlearn: 0.0166478\ttotal: 2.14s\tremaining: 900ms\n",
      "176:\tlearn: 0.0164770\ttotal: 2.17s\tremaining: 897ms\n",
      "177:\tlearn: 0.0163689\ttotal: 2.21s\tremaining: 892ms\n",
      "178:\tlearn: 0.0162986\ttotal: 2.24s\tremaining: 888ms\n",
      "179:\tlearn: 0.0161618\ttotal: 2.25s\tremaining: 876ms\n",
      "180:\tlearn: 0.0161172\ttotal: 2.29s\tremaining: 871ms\n",
      "181:\tlearn: 0.0160141\ttotal: 2.3s\tremaining: 860ms\n",
      "182:\tlearn: 0.0159315\ttotal: 2.33s\tremaining: 854ms\n",
      "183:\tlearn: 0.0158876\ttotal: 2.34s\tremaining: 839ms\n",
      "184:\tlearn: 0.0156774\ttotal: 2.34s\tremaining: 823ms\n",
      "185:\tlearn: 0.0156774\ttotal: 2.35s\tremaining: 808ms\n",
      "186:\tlearn: 0.0156000\ttotal: 2.36s\tremaining: 797ms\n",
      "187:\tlearn: 0.0153947\ttotal: 2.37s\tremaining: 781ms\n",
      "188:\tlearn: 0.0152840\ttotal: 2.38s\tremaining: 768ms\n",
      "189:\tlearn: 0.0152522\ttotal: 2.4s\tremaining: 756ms\n",
      "190:\tlearn: 0.0152102\ttotal: 2.43s\tremaining: 750ms\n",
      "191:\tlearn: 0.0151025\ttotal: 2.43s\tremaining: 735ms\n",
      "192:\tlearn: 0.0149958\ttotal: 2.44s\tremaining: 720ms\n",
      "193:\tlearn: 0.0148906\ttotal: 2.44s\tremaining: 705ms\n",
      "194:\tlearn: 0.0147262\ttotal: 2.45s\tremaining: 690ms\n",
      "195:\tlearn: 0.0145864\ttotal: 2.47s\tremaining: 680ms\n",
      "196:\tlearn: 0.0145028\ttotal: 2.49s\tremaining: 669ms\n",
      "197:\tlearn: 0.0143798\ttotal: 2.53s\tremaining: 666ms\n",
      "198:\tlearn: 0.0143568\ttotal: 2.56s\tremaining: 657ms\n",
      "199:\tlearn: 0.0142770\ttotal: 2.57s\tremaining: 643ms\n",
      "200:\tlearn: 0.0142199\ttotal: 2.58s\tremaining: 629ms\n",
      "201:\tlearn: 0.0141076\ttotal: 2.58s\tremaining: 614ms\n",
      "202:\tlearn: 0.0139819\ttotal: 2.59s\tremaining: 600ms\n",
      "203:\tlearn: 0.0138776\ttotal: 2.6s\tremaining: 585ms\n",
      "204:\tlearn: 0.0138328\ttotal: 2.6s\tremaining: 571ms\n",
      "205:\tlearn: 0.0137313\ttotal: 2.61s\tremaining: 558ms\n",
      "206:\tlearn: 0.0135729\ttotal: 2.62s\tremaining: 544ms\n",
      "207:\tlearn: 0.0135167\ttotal: 2.62s\tremaining: 529ms\n",
      "208:\tlearn: 0.0134738\ttotal: 2.63s\tremaining: 515ms\n",
      "209:\tlearn: 0.0133562\ttotal: 2.63s\tremaining: 501ms\n",
      "210:\tlearn: 0.0133168\ttotal: 2.63s\tremaining: 487ms\n",
      "211:\tlearn: 0.0132234\ttotal: 2.64s\tremaining: 473ms\n",
      "212:\tlearn: 0.0131360\ttotal: 2.65s\tremaining: 459ms\n",
      "213:\tlearn: 0.0131097\ttotal: 2.66s\tremaining: 447ms\n",
      "214:\tlearn: 0.0128924\ttotal: 2.66s\tremaining: 434ms\n",
      "215:\tlearn: 0.0128577\ttotal: 2.67s\tremaining: 420ms\n",
      "216:\tlearn: 0.0127800\ttotal: 2.67s\tremaining: 407ms\n",
      "217:\tlearn: 0.0127800\ttotal: 2.69s\tremaining: 394ms\n",
      "218:\tlearn: 0.0126340\ttotal: 2.69s\tremaining: 381ms\n",
      "219:\tlearn: 0.0125968\ttotal: 2.72s\tremaining: 371ms\n",
      "220:\tlearn: 0.0125766\ttotal: 2.73s\tremaining: 359ms\n",
      "221:\tlearn: 0.0125317\ttotal: 2.75s\tremaining: 347ms\n",
      "222:\tlearn: 0.0124413\ttotal: 2.78s\tremaining: 337ms\n",
      "223:\tlearn: 0.0124413\ttotal: 2.81s\tremaining: 327ms\n",
      "224:\tlearn: 0.0123715\ttotal: 2.85s\tremaining: 316ms\n",
      "225:\tlearn: 0.0122088\ttotal: 2.9s\tremaining: 307ms\n",
      "226:\tlearn: 0.0121074\ttotal: 2.9s\tremaining: 294ms\n",
      "227:\tlearn: 0.0120673\ttotal: 2.9s\tremaining: 280ms\n",
      "228:\tlearn: 0.0120090\ttotal: 2.94s\tremaining: 270ms\n",
      "229:\tlearn: 0.0119772\ttotal: 2.94s\tremaining: 256ms\n",
      "230:\tlearn: 0.0118614\ttotal: 2.97s\tremaining: 244ms\n",
      "231:\tlearn: 0.0117848\ttotal: 2.98s\tremaining: 231ms\n",
      "232:\tlearn: 0.0116371\ttotal: 2.99s\tremaining: 218ms\n",
      "233:\tlearn: 0.0115279\ttotal: 3s\tremaining: 205ms\n",
      "234:\tlearn: 0.0114788\ttotal: 3s\tremaining: 192ms\n",
      "235:\tlearn: 0.0113829\ttotal: 3.03s\tremaining: 180ms\n",
      "236:\tlearn: 0.0113436\ttotal: 3.05s\tremaining: 167ms\n",
      "237:\tlearn: 0.0112797\ttotal: 3.08s\tremaining: 155ms\n",
      "238:\tlearn: 0.0112456\ttotal: 3.09s\tremaining: 142ms\n",
      "239:\tlearn: 0.0111761\ttotal: 3.12s\tremaining: 130ms\n",
      "240:\tlearn: 0.0111149\ttotal: 3.13s\tremaining: 117ms\n",
      "241:\tlearn: 0.0110813\ttotal: 3.13s\tremaining: 104ms\n",
      "242:\tlearn: 0.0110439\ttotal: 3.14s\tremaining: 90.5ms\n",
      "243:\tlearn: 0.0110039\ttotal: 3.17s\tremaining: 78ms\n",
      "244:\tlearn: 0.0109845\ttotal: 3.17s\tremaining: 64.8ms\n",
      "245:\tlearn: 0.0109213\ttotal: 3.18s\tremaining: 51.7ms\n",
      "246:\tlearn: 0.0109213\ttotal: 3.19s\tremaining: 38.7ms\n",
      "247:\tlearn: 0.0108952\ttotal: 3.19s\tremaining: 25.7ms\n",
      "248:\tlearn: 0.0108897\ttotal: 3.19s\tremaining: 12.8ms\n",
      "249:\tlearn: 0.0108897\ttotal: 3.2s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7320f30a11ed4c7aa6631ea2f37a62a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.47853713615856464, Recall = 0.9894160583941606, Aging Rate = 0.9230152365677626, Precision = 0.5888357949609035, f1 = 0.7382897603485838\n",
      "Epoch 2: Train Loss = 0.31140661048430296, Recall = 0.9635036496350365, Aging Rate = 0.6904570970328789, Precision = 0.7665505226480837, f1 = 0.8538163001293662\n",
      "Epoch 3: Train Loss = 0.24965355213680743, Recall = 0.9667883211678832, Aging Rate = 0.6533680834001604, Precision = 0.8128260202516109, f1 = 0.883147191198533\n",
      "Epoch 4: Train Loss = 0.20117889288529073, Recall = 0.9715328467153285, Aging Rate = 0.628307939053729, Precision = 0.8493937460114869, f1 = 0.9063670411985019\n",
      "Epoch 5: Train Loss = 0.16928770346529692, Recall = 0.9722627737226277, Aging Rate = 0.6088612670408982, Precision = 0.8771814290418176, f1 = 0.9222779989613986\n",
      "Test Loss = 0.13531345744417875, Recall = 0.9821167883211679, Aging Rate = 0.5878107457898958, precision = 0.9178035470668485\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.12695350601623895, Recall = 0.9835766423357665, Aging Rate = 0.588813151563753, Precision = 0.9176029962546817, f1 = 0.9494451294697904\n",
      "Epoch 7: Train Loss = 0.10266805813612705, Recall = 0.9868613138686131, Aging Rate = 0.5781876503608661, Precision = 0.9375866851595007, f1 = 0.9615931721194878\n",
      "Epoch 8: Train Loss = 0.08276953280820498, Recall = 0.9912408759124087, Aging Rate = 0.5721732157177225, Precision = 0.9516468114926419, f1 = 0.9710404004290312\n",
      "Epoch 9: Train Loss = 0.06795774595012834, Recall = 0.9927007299270073, Aging Rate = 0.5635525260625501, Precision = 0.967627178939879, f1 = 0.9800036029544227\n",
      "Epoch 10: Train Loss = 0.05722765078724817, Recall = 0.9937956204379562, Aging Rate = 0.5617481956696071, Precision = 0.9718058529621699, f1 = 0.9826777336701552\n",
      "Test Loss = 0.048754172777546435, Recall = 0.997080291970803, Aging Rate = 0.5609462710505213, precision = 0.9764117226590422\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04873374368586154, Recall = 0.9948905109489051, Aging Rate = 0.5567361668003208, Precision = 0.9816348577601729, f1 = 0.9882182345477615\n",
      "Epoch 12: Train Loss = 0.04101039218001588, Recall = 0.9974452554744525, Aging Rate = 0.5569366479550922, Precision = 0.9838012958963283, f1 = 0.990576295759333\n",
      "Epoch 13: Train Loss = 0.03601872565489152, Recall = 0.9967153284671533, Aging Rate = 0.5547313552526063, Precision = 0.9869895193350199, f1 = 0.9918285818049755\n",
      "Epoch 14: Train Loss = 0.031017739101706646, Recall = 0.9981751824817519, Aging Rate = 0.5545308740978349, Precision = 0.9887924801156905, f1 = 0.9934616781692699\n",
      "Epoch 15: Train Loss = 0.027397212142956765, Recall = 0.9985401459854014, Aging Rate = 0.5543303929430633, Precision = 0.989511754068716, f1 = 0.9940054495912806\n",
      "Test Loss = 0.023197733302717936, Recall = 0.9985401459854014, Aging Rate = 0.5517241379310345, precision = 0.9941860465116279\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.023968987945846634, Recall = 0.9992700729927008, Aging Rate = 0.5531275060144346, Precision = 0.992388546574846, f1 = 0.9958174213493363\n",
      "Epoch 17: Train Loss = 0.021656780683681213, Recall = 0.9992700729927008, Aging Rate = 0.5535284683239775, Precision = 0.9916696848967765, f1 = 0.9954553717505907\n",
      "Epoch 18: Train Loss = 0.019480848882448918, Recall = 0.9989051094890511, Aging Rate = 0.5519246190858059, Precision = 0.9941881583726844, f1 = 0.996541052248316\n",
      "Epoch 19: Train Loss = 0.01709325317855037, Recall = 0.9996350364963503, Aging Rate = 0.551523656776263, Precision = 0.9956379498364231, f1 = 0.997632489528319\n",
      "Epoch 20: Train Loss = 0.015160387288554966, Recall = 0.9992700729927008, Aging Rate = 0.5509222133119487, Precision = 0.9963609898107715, f1 = 0.9978134110787172\n",
      "Test Loss = 0.013304024565824529, Recall = 1.0, Aging Rate = 0.5505212510024058, precision = 0.9978150036416606\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.013695694034158849, Recall = 0.9996350364963503, Aging Rate = 0.5509222133119487, Precision = 0.9967248908296943, f1 = 0.9981778425655976\n",
      "Epoch 22: Train Loss = 0.012690605270081074, Recall = 1.0, Aging Rate = 0.5509222133119487, Precision = 0.9970887918486172, f1 = 0.9985422740524782\n",
      "Epoch 23: Train Loss = 0.01160057399828217, Recall = 1.0, Aging Rate = 0.5507217321571772, Precision = 0.9974517655624318, f1 = 0.9987242573355203\n",
      "Epoch 24: Train Loss = 0.010396657652746655, Recall = 1.0, Aging Rate = 0.5505212510024058, Precision = 0.9978150036416606, f1 = 0.9989063069631791\n",
      "Epoch 25: Train Loss = 0.009405182150611542, Recall = 1.0, Aging Rate = 0.5503207698476343, Precision = 0.9981785063752276, f1 = 0.999088422971741\n",
      "Test Loss = 0.007569773230214738, Recall = 1.0, Aging Rate = 0.5499198075380914, precision = 0.998906306963179\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.008695430186171601, Recall = 0.9996350364963503, Aging Rate = 0.5499198075380914, Precision = 0.998541742617572, f1 = 0.9990880904614262\n",
      "Epoch 27: Train Loss = 0.00786106302025754, Recall = 1.0, Aging Rate = 0.5501202886928629, Precision = 0.9985422740524781, f1 = 0.99927060539752\n",
      "Epoch 28: Train Loss = 0.007375201783960785, Recall = 1.0, Aging Rate = 0.5501202886928629, Precision = 0.9985422740524781, f1 = 0.99927060539752\n",
      "Epoch 29: Train Loss = 0.00683645997345806, Recall = 1.0, Aging Rate = 0.5499198075380914, Precision = 0.998906306963179, f1 = 0.9994528542768557\n",
      "Epoch 30: Train Loss = 0.006252264574285128, Recall = 1.0, Aging Rate = 0.5499198075380914, Precision = 0.998906306963179, f1 = 0.9994528542768557\n",
      "Test Loss = 0.0051606728638572364, Recall = 1.0, Aging Rate = 0.54971932638332, precision = 0.9992706053975201\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.006013615293250551, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 32: Train Loss = 0.005927722586868533, Recall = 1.0, Aging Rate = 0.5499198075380914, Precision = 0.998906306963179, f1 = 0.9994528542768557\n",
      "Epoch 33: Train Loss = 0.005196874729920038, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 34: Train Loss = 0.004989942765041122, Recall = 1.0, Aging Rate = 0.54971932638332, Precision = 0.9992706053975201, f1 = 0.9996351696461145\n",
      "Epoch 35: Train Loss = 0.004786163547958315, Recall = 1.0, Aging Rate = 0.5499198075380914, Precision = 0.998906306963179, f1 = 0.9994528542768557\n",
      "Test Loss = 0.004561642238720146, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.004422590034209497, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 37: Train Loss = 0.00414027529003124, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.004050194219713725, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.00392140272294142, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 40: Train Loss = 0.003733633132016625, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0031570144448206786, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.0038652468280218474, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 42: Train Loss = 0.0035059992923216826, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0033123187542278333, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 44: Train Loss = 0.003177716065147397, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0031768328018423764, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0025325000255108, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.003719605776573193, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 47: Train Loss = 0.0029102318566187964, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.0027585067205039937, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.0027068266532579532, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.002696362568839261, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026084096131752206, Recall = 1.0, Aging Rate = 0.5495188452285485, precision = 0.9996351696461145\n",
      "\n",
      "Epoch 51: Train Loss = 0.0030825976398119997, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 52: Train Loss = 0.0028600233849533853, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0026097731175963148, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0028069379344379413, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 55: Train Loss = 0.002937364464146985, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022632770241053706, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.002561316314230299, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002323616559504447, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0027276341971039842, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.003063900423301167, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.002547811352892061, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020515012393131217, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.002315551312356506, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0023722962135842643, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0020490637086981603, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0022951073622676068, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.002504740690636212, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033501488702377693, Recall = 1.0, Aging Rate = 0.5499198075380914, precision = 0.998906306963179\n",
      "\n",
      "Epoch 66: Train Loss = 0.002421699590165446, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 67: Train Loss = 0.0022590596504425803, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.002286425238684695, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.002675984315454673, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.002120350817918222, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018042044155075609, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0021593841814093215, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.002275739803648111, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.002636534068416042, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.003318115094946974, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 75: Train Loss = 0.001934173166469552, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016097616586899916, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0021668827668491096, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0022185465789406763, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.003495500592362515, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 79: Train Loss = 0.002361592412373587, Recall = 1.0, Aging Rate = 0.5495188452285485, Precision = 0.9996351696461145, f1 = 0.9998175515416895\n",
      "Epoch 80: Train Loss = 0.0018693358116837722, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017617000187089952, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.001984699456351298, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0020783688628832485, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0019874553684945838, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0021520496610068114, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.002053863755470245, Recall = 1.0, Aging Rate = 0.5493183640737771, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017775053654311176, Recall = 1.0, Aging Rate = 0.5493183640737771, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6082341\ttotal: 23.9ms\tremaining: 5.96s\n",
      "1:\tlearn: 0.5342307\ttotal: 54.5ms\tremaining: 6.76s\n",
      "2:\tlearn: 0.4672491\ttotal: 70.1ms\tremaining: 5.77s\n",
      "3:\tlearn: 0.4233481\ttotal: 84.8ms\tremaining: 5.21s\n",
      "4:\tlearn: 0.4025339\ttotal: 132ms\tremaining: 6.45s\n",
      "5:\tlearn: 0.3793818\ttotal: 137ms\tremaining: 5.57s\n",
      "6:\tlearn: 0.3499704\ttotal: 148ms\tremaining: 5.14s\n",
      "7:\tlearn: 0.3290233\ttotal: 164ms\tremaining: 4.95s\n",
      "8:\tlearn: 0.3164501\ttotal: 169ms\tremaining: 4.53s\n",
      "9:\tlearn: 0.3051416\ttotal: 179ms\tremaining: 4.3s\n",
      "10:\tlearn: 0.2854167\ttotal: 211ms\tremaining: 4.58s\n",
      "11:\tlearn: 0.2729753\ttotal: 243ms\tremaining: 4.81s\n",
      "12:\tlearn: 0.2619297\ttotal: 275ms\tremaining: 5.01s\n",
      "13:\tlearn: 0.2591773\ttotal: 294ms\tremaining: 4.96s\n",
      "14:\tlearn: 0.2488574\ttotal: 322ms\tremaining: 5.04s\n",
      "15:\tlearn: 0.2416108\ttotal: 370ms\tremaining: 5.41s\n",
      "16:\tlearn: 0.2347766\ttotal: 416ms\tremaining: 5.7s\n",
      "17:\tlearn: 0.2288689\ttotal: 441ms\tremaining: 5.68s\n",
      "18:\tlearn: 0.2218572\ttotal: 447ms\tremaining: 5.43s\n",
      "19:\tlearn: 0.2117630\ttotal: 473ms\tremaining: 5.44s\n",
      "20:\tlearn: 0.2060220\ttotal: 489ms\tremaining: 5.33s\n",
      "21:\tlearn: 0.1993269\ttotal: 494ms\tremaining: 5.12s\n",
      "22:\tlearn: 0.1979894\ttotal: 496ms\tremaining: 4.9s\n",
      "23:\tlearn: 0.1915391\ttotal: 501ms\tremaining: 4.72s\n",
      "24:\tlearn: 0.1869931\ttotal: 506ms\tremaining: 4.55s\n",
      "25:\tlearn: 0.1834209\ttotal: 552ms\tremaining: 4.75s\n",
      "26:\tlearn: 0.1766807\ttotal: 566ms\tremaining: 4.68s\n",
      "27:\tlearn: 0.1708795\ttotal: 571ms\tremaining: 4.53s\n",
      "28:\tlearn: 0.1648253\ttotal: 576ms\tremaining: 4.39s\n",
      "29:\tlearn: 0.1606654\ttotal: 580ms\tremaining: 4.25s\n",
      "30:\tlearn: 0.1564780\ttotal: 598ms\tremaining: 4.22s\n",
      "31:\tlearn: 0.1489071\ttotal: 603ms\tremaining: 4.11s\n",
      "32:\tlearn: 0.1440968\ttotal: 629ms\tremaining: 4.14s\n",
      "33:\tlearn: 0.1400468\ttotal: 661ms\tremaining: 4.2s\n",
      "34:\tlearn: 0.1375406\ttotal: 692ms\tremaining: 4.25s\n",
      "35:\tlearn: 0.1349666\ttotal: 697ms\tremaining: 4.14s\n",
      "36:\tlearn: 0.1329810\ttotal: 701ms\tremaining: 4.04s\n",
      "37:\tlearn: 0.1292511\ttotal: 706ms\tremaining: 3.94s\n",
      "38:\tlearn: 0.1237061\ttotal: 738ms\tremaining: 3.99s\n",
      "39:\tlearn: 0.1195058\ttotal: 743ms\tremaining: 3.9s\n",
      "40:\tlearn: 0.1158553\ttotal: 748ms\tremaining: 3.81s\n",
      "41:\tlearn: 0.1143906\ttotal: 753ms\tremaining: 3.73s\n",
      "42:\tlearn: 0.1124367\ttotal: 758ms\tremaining: 3.65s\n",
      "43:\tlearn: 0.1108949\ttotal: 762ms\tremaining: 3.57s\n",
      "44:\tlearn: 0.1083199\ttotal: 799ms\tremaining: 3.64s\n",
      "45:\tlearn: 0.1051056\ttotal: 810ms\tremaining: 3.59s\n",
      "46:\tlearn: 0.1020288\ttotal: 817ms\tremaining: 3.53s\n",
      "47:\tlearn: 0.0989677\ttotal: 822ms\tremaining: 3.46s\n",
      "48:\tlearn: 0.0962518\ttotal: 828ms\tremaining: 3.4s\n",
      "49:\tlearn: 0.0946398\ttotal: 844ms\tremaining: 3.38s\n",
      "50:\tlearn: 0.0932192\ttotal: 876ms\tremaining: 3.42s\n",
      "51:\tlearn: 0.0907048\ttotal: 881ms\tremaining: 3.35s\n",
      "52:\tlearn: 0.0878690\ttotal: 886ms\tremaining: 3.29s\n",
      "53:\tlearn: 0.0857621\ttotal: 891ms\tremaining: 3.23s\n",
      "54:\tlearn: 0.0838009\ttotal: 896ms\tremaining: 3.17s\n",
      "55:\tlearn: 0.0824036\ttotal: 900ms\tremaining: 3.12s\n",
      "56:\tlearn: 0.0811879\ttotal: 905ms\tremaining: 3.06s\n",
      "57:\tlearn: 0.0789633\ttotal: 910ms\tremaining: 3.01s\n",
      "58:\tlearn: 0.0763923\ttotal: 922ms\tremaining: 2.98s\n",
      "59:\tlearn: 0.0741213\ttotal: 953ms\tremaining: 3.02s\n",
      "60:\tlearn: 0.0726658\ttotal: 958ms\tremaining: 2.97s\n",
      "61:\tlearn: 0.0706089\ttotal: 963ms\tremaining: 2.92s\n",
      "62:\tlearn: 0.0695129\ttotal: 968ms\tremaining: 2.87s\n",
      "63:\tlearn: 0.0685617\ttotal: 983ms\tremaining: 2.85s\n",
      "64:\tlearn: 0.0678827\ttotal: 987ms\tremaining: 2.81s\n",
      "65:\tlearn: 0.0672160\ttotal: 992ms\tremaining: 2.77s\n",
      "66:\tlearn: 0.0672097\ttotal: 995ms\tremaining: 2.72s\n",
      "67:\tlearn: 0.0663768\ttotal: 1.03s\tremaining: 2.75s\n",
      "68:\tlearn: 0.0657959\ttotal: 1.04s\tremaining: 2.74s\n",
      "69:\tlearn: 0.0641602\ttotal: 1.09s\tremaining: 2.81s\n",
      "70:\tlearn: 0.0631288\ttotal: 1.1s\tremaining: 2.77s\n",
      "71:\tlearn: 0.0623944\ttotal: 1.11s\tremaining: 2.74s\n",
      "72:\tlearn: 0.0614729\ttotal: 1.11s\tremaining: 2.7s\n",
      "73:\tlearn: 0.0611534\ttotal: 1.12s\tremaining: 2.67s\n",
      "74:\tlearn: 0.0602959\ttotal: 1.15s\tremaining: 2.69s\n",
      "75:\tlearn: 0.0578373\ttotal: 1.16s\tremaining: 2.65s\n",
      "76:\tlearn: 0.0562993\ttotal: 1.16s\tremaining: 2.62s\n",
      "77:\tlearn: 0.0554182\ttotal: 1.17s\tremaining: 2.58s\n",
      "78:\tlearn: 0.0550186\ttotal: 1.17s\tremaining: 2.54s\n",
      "79:\tlearn: 0.0532472\ttotal: 1.2s\tremaining: 2.54s\n",
      "80:\tlearn: 0.0520738\ttotal: 1.21s\tremaining: 2.52s\n",
      "81:\tlearn: 0.0512100\ttotal: 1.22s\tremaining: 2.49s\n",
      "82:\tlearn: 0.0498350\ttotal: 1.22s\tremaining: 2.46s\n",
      "83:\tlearn: 0.0498350\ttotal: 1.22s\tremaining: 2.42s\n",
      "84:\tlearn: 0.0490622\ttotal: 1.23s\tremaining: 2.39s\n",
      "85:\tlearn: 0.0483508\ttotal: 1.26s\tremaining: 2.41s\n",
      "86:\tlearn: 0.0479271\ttotal: 1.27s\tremaining: 2.37s\n",
      "87:\tlearn: 0.0473404\ttotal: 1.27s\tremaining: 2.34s\n",
      "88:\tlearn: 0.0465709\ttotal: 1.28s\tremaining: 2.31s\n",
      "89:\tlearn: 0.0454726\ttotal: 1.28s\tremaining: 2.28s\n",
      "90:\tlearn: 0.0446186\ttotal: 1.28s\tremaining: 2.25s\n",
      "91:\tlearn: 0.0436481\ttotal: 1.29s\tremaining: 2.21s\n",
      "92:\tlearn: 0.0430823\ttotal: 1.29s\tremaining: 2.19s\n",
      "93:\tlearn: 0.0427439\ttotal: 1.3s\tremaining: 2.16s\n",
      "94:\tlearn: 0.0422336\ttotal: 1.3s\tremaining: 2.13s\n",
      "95:\tlearn: 0.0414447\ttotal: 1.33s\tremaining: 2.14s\n",
      "96:\tlearn: 0.0410998\ttotal: 1.34s\tremaining: 2.12s\n",
      "97:\tlearn: 0.0402902\ttotal: 1.35s\tremaining: 2.1s\n",
      "98:\tlearn: 0.0396794\ttotal: 1.36s\tremaining: 2.08s\n",
      "99:\tlearn: 0.0396526\ttotal: 1.36s\tremaining: 2.05s\n",
      "100:\tlearn: 0.0393386\ttotal: 1.37s\tremaining: 2.02s\n",
      "101:\tlearn: 0.0389339\ttotal: 1.38s\tremaining: 2s\n",
      "102:\tlearn: 0.0383634\ttotal: 1.38s\tremaining: 1.98s\n",
      "103:\tlearn: 0.0379683\ttotal: 1.39s\tremaining: 1.95s\n",
      "104:\tlearn: 0.0374565\ttotal: 1.39s\tremaining: 1.93s\n",
      "105:\tlearn: 0.0366658\ttotal: 1.4s\tremaining: 1.9s\n",
      "106:\tlearn: 0.0359499\ttotal: 1.4s\tremaining: 1.88s\n",
      "107:\tlearn: 0.0355533\ttotal: 1.41s\tremaining: 1.85s\n",
      "108:\tlearn: 0.0349109\ttotal: 1.41s\tremaining: 1.83s\n",
      "109:\tlearn: 0.0341929\ttotal: 1.42s\tremaining: 1.8s\n",
      "110:\tlearn: 0.0337366\ttotal: 1.42s\tremaining: 1.78s\n",
      "111:\tlearn: 0.0333053\ttotal: 1.43s\tremaining: 1.76s\n",
      "112:\tlearn: 0.0329783\ttotal: 1.44s\tremaining: 1.74s\n",
      "113:\tlearn: 0.0325277\ttotal: 1.44s\tremaining: 1.72s\n",
      "114:\tlearn: 0.0321539\ttotal: 1.46s\tremaining: 1.72s\n",
      "115:\tlearn: 0.0319663\ttotal: 1.47s\tremaining: 1.69s\n",
      "116:\tlearn: 0.0316586\ttotal: 1.47s\tremaining: 1.67s\n",
      "117:\tlearn: 0.0309884\ttotal: 1.48s\tremaining: 1.65s\n",
      "118:\tlearn: 0.0305932\ttotal: 1.49s\tremaining: 1.64s\n",
      "119:\tlearn: 0.0300671\ttotal: 1.5s\tremaining: 1.62s\n",
      "120:\tlearn: 0.0296672\ttotal: 1.5s\tremaining: 1.6s\n",
      "121:\tlearn: 0.0293616\ttotal: 1.51s\tremaining: 1.58s\n",
      "122:\tlearn: 0.0290003\ttotal: 1.52s\tremaining: 1.57s\n",
      "123:\tlearn: 0.0286512\ttotal: 1.53s\tremaining: 1.55s\n",
      "124:\tlearn: 0.0281404\ttotal: 1.53s\tremaining: 1.53s\n",
      "125:\tlearn: 0.0277520\ttotal: 1.54s\tremaining: 1.51s\n",
      "126:\tlearn: 0.0273960\ttotal: 1.54s\tremaining: 1.49s\n",
      "127:\tlearn: 0.0268821\ttotal: 1.55s\tremaining: 1.48s\n",
      "128:\tlearn: 0.0264806\ttotal: 1.59s\tremaining: 1.49s\n",
      "129:\tlearn: 0.0261186\ttotal: 1.62s\tremaining: 1.49s\n",
      "130:\tlearn: 0.0258550\ttotal: 1.65s\tremaining: 1.5s\n",
      "131:\tlearn: 0.0256058\ttotal: 1.67s\tremaining: 1.49s\n",
      "132:\tlearn: 0.0252334\ttotal: 1.7s\tremaining: 1.49s\n",
      "133:\tlearn: 0.0250712\ttotal: 1.75s\tremaining: 1.51s\n",
      "134:\tlearn: 0.0248463\ttotal: 1.77s\tremaining: 1.51s\n",
      "135:\tlearn: 0.0245119\ttotal: 1.78s\tremaining: 1.49s\n",
      "136:\tlearn: 0.0240922\ttotal: 1.79s\tremaining: 1.47s\n",
      "137:\tlearn: 0.0239636\ttotal: 1.81s\tremaining: 1.47s\n",
      "138:\tlearn: 0.0238246\ttotal: 1.81s\tremaining: 1.45s\n",
      "139:\tlearn: 0.0234939\ttotal: 1.82s\tremaining: 1.43s\n",
      "140:\tlearn: 0.0233305\ttotal: 1.82s\tremaining: 1.41s\n",
      "141:\tlearn: 0.0230227\ttotal: 1.85s\tremaining: 1.41s\n",
      "142:\tlearn: 0.0228121\ttotal: 1.87s\tremaining: 1.4s\n",
      "143:\tlearn: 0.0225073\ttotal: 1.88s\tremaining: 1.39s\n",
      "144:\tlearn: 0.0223872\ttotal: 1.9s\tremaining: 1.37s\n",
      "145:\tlearn: 0.0220375\ttotal: 1.91s\tremaining: 1.36s\n",
      "146:\tlearn: 0.0217815\ttotal: 1.94s\tremaining: 1.36s\n",
      "147:\tlearn: 0.0216884\ttotal: 1.96s\tremaining: 1.35s\n",
      "148:\tlearn: 0.0214974\ttotal: 1.97s\tremaining: 1.33s\n",
      "149:\tlearn: 0.0213190\ttotal: 1.97s\tremaining: 1.31s\n",
      "150:\tlearn: 0.0211824\ttotal: 1.98s\tremaining: 1.29s\n",
      "151:\tlearn: 0.0210048\ttotal: 1.98s\tremaining: 1.28s\n",
      "152:\tlearn: 0.0207675\ttotal: 1.99s\tremaining: 1.26s\n",
      "153:\tlearn: 0.0205696\ttotal: 1.99s\tremaining: 1.24s\n",
      "154:\tlearn: 0.0203127\ttotal: 1.99s\tremaining: 1.22s\n",
      "155:\tlearn: 0.0201743\ttotal: 2.02s\tremaining: 1.22s\n",
      "156:\tlearn: 0.0201011\ttotal: 2.04s\tremaining: 1.21s\n",
      "157:\tlearn: 0.0200060\ttotal: 2.04s\tremaining: 1.19s\n",
      "158:\tlearn: 0.0197948\ttotal: 2.05s\tremaining: 1.17s\n",
      "159:\tlearn: 0.0196268\ttotal: 2.05s\tremaining: 1.15s\n",
      "160:\tlearn: 0.0194587\ttotal: 2.06s\tremaining: 1.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.0191384\ttotal: 2.06s\tremaining: 1.12s\n",
      "162:\tlearn: 0.0189205\ttotal: 2.07s\tremaining: 1.1s\n",
      "163:\tlearn: 0.0187567\ttotal: 2.07s\tremaining: 1.09s\n",
      "164:\tlearn: 0.0185694\ttotal: 2.08s\tremaining: 1.07s\n",
      "165:\tlearn: 0.0185091\ttotal: 2.08s\tremaining: 1.05s\n",
      "166:\tlearn: 0.0182747\ttotal: 2.09s\tremaining: 1.04s\n",
      "167:\tlearn: 0.0180621\ttotal: 2.09s\tremaining: 1.02s\n",
      "168:\tlearn: 0.0180298\ttotal: 2.1s\tremaining: 1.01s\n",
      "169:\tlearn: 0.0179447\ttotal: 2.13s\tremaining: 1s\n",
      "170:\tlearn: 0.0177642\ttotal: 2.15s\tremaining: 992ms\n",
      "171:\tlearn: 0.0176393\ttotal: 2.18s\tremaining: 988ms\n",
      "172:\tlearn: 0.0174237\ttotal: 2.19s\tremaining: 977ms\n",
      "173:\tlearn: 0.0173452\ttotal: 2.23s\tremaining: 973ms\n",
      "174:\tlearn: 0.0171919\ttotal: 2.24s\tremaining: 961ms\n",
      "175:\tlearn: 0.0171011\ttotal: 2.27s\tremaining: 956ms\n",
      "176:\tlearn: 0.0170469\ttotal: 2.29s\tremaining: 945ms\n",
      "177:\tlearn: 0.0168352\ttotal: 2.29s\tremaining: 928ms\n",
      "178:\tlearn: 0.0166965\ttotal: 2.3s\tremaining: 912ms\n",
      "179:\tlearn: 0.0165817\ttotal: 2.3s\tremaining: 896ms\n",
      "180:\tlearn: 0.0164782\ttotal: 2.31s\tremaining: 880ms\n",
      "181:\tlearn: 0.0163501\ttotal: 2.31s\tremaining: 865ms\n",
      "182:\tlearn: 0.0162488\ttotal: 2.32s\tremaining: 849ms\n",
      "183:\tlearn: 0.0160436\ttotal: 2.32s\tremaining: 834ms\n",
      "184:\tlearn: 0.0159764\ttotal: 2.33s\tremaining: 821ms\n",
      "185:\tlearn: 0.0158886\ttotal: 2.35s\tremaining: 809ms\n",
      "186:\tlearn: 0.0157945\ttotal: 2.36s\tremaining: 794ms\n",
      "187:\tlearn: 0.0157599\ttotal: 2.36s\tremaining: 779ms\n",
      "188:\tlearn: 0.0156692\ttotal: 2.37s\tremaining: 764ms\n",
      "189:\tlearn: 0.0156026\ttotal: 2.41s\tremaining: 762ms\n",
      "190:\tlearn: 0.0154550\ttotal: 2.43s\tremaining: 751ms\n",
      "191:\tlearn: 0.0153338\ttotal: 2.44s\tremaining: 737ms\n",
      "192:\tlearn: 0.0151222\ttotal: 2.44s\tremaining: 722ms\n",
      "193:\tlearn: 0.0149935\ttotal: 2.47s\tremaining: 714ms\n",
      "194:\tlearn: 0.0148937\ttotal: 2.51s\tremaining: 707ms\n",
      "195:\tlearn: 0.0148834\ttotal: 2.51s\tremaining: 692ms\n",
      "196:\tlearn: 0.0148164\ttotal: 2.52s\tremaining: 677ms\n",
      "197:\tlearn: 0.0147982\ttotal: 2.52s\tremaining: 662ms\n",
      "198:\tlearn: 0.0146860\ttotal: 2.52s\tremaining: 647ms\n",
      "199:\tlearn: 0.0146293\ttotal: 2.53s\tremaining: 632ms\n",
      "200:\tlearn: 0.0145503\ttotal: 2.53s\tremaining: 618ms\n",
      "201:\tlearn: 0.0143817\ttotal: 2.56s\tremaining: 609ms\n",
      "202:\tlearn: 0.0143542\ttotal: 2.57s\tremaining: 595ms\n",
      "203:\tlearn: 0.0142104\ttotal: 2.58s\tremaining: 581ms\n",
      "204:\tlearn: 0.0141169\ttotal: 2.58s\tremaining: 567ms\n",
      "205:\tlearn: 0.0140753\ttotal: 2.58s\tremaining: 552ms\n",
      "206:\tlearn: 0.0140145\ttotal: 2.59s\tremaining: 538ms\n",
      "207:\tlearn: 0.0138625\ttotal: 2.6s\tremaining: 525ms\n",
      "208:\tlearn: 0.0137392\ttotal: 2.6s\tremaining: 511ms\n",
      "209:\tlearn: 0.0137118\ttotal: 2.61s\tremaining: 497ms\n",
      "210:\tlearn: 0.0136975\ttotal: 2.61s\tremaining: 483ms\n",
      "211:\tlearn: 0.0136152\ttotal: 2.63s\tremaining: 472ms\n",
      "212:\tlearn: 0.0135450\ttotal: 2.63s\tremaining: 458ms\n",
      "213:\tlearn: 0.0135450\ttotal: 2.64s\tremaining: 444ms\n",
      "214:\tlearn: 0.0135326\ttotal: 2.64s\tremaining: 430ms\n",
      "215:\tlearn: 0.0134989\ttotal: 2.65s\tremaining: 417ms\n",
      "216:\tlearn: 0.0133676\ttotal: 2.66s\tremaining: 405ms\n",
      "217:\tlearn: 0.0132883\ttotal: 2.68s\tremaining: 393ms\n",
      "218:\tlearn: 0.0131054\ttotal: 2.69s\tremaining: 381ms\n",
      "219:\tlearn: 0.0130740\ttotal: 2.73s\tremaining: 372ms\n",
      "220:\tlearn: 0.0129278\ttotal: 2.76s\tremaining: 362ms\n",
      "221:\tlearn: 0.0128395\ttotal: 2.77s\tremaining: 350ms\n",
      "222:\tlearn: 0.0127159\ttotal: 2.79s\tremaining: 338ms\n",
      "223:\tlearn: 0.0126003\ttotal: 2.82s\tremaining: 327ms\n",
      "224:\tlearn: 0.0125602\ttotal: 2.87s\tremaining: 319ms\n",
      "225:\tlearn: 0.0125121\ttotal: 2.88s\tremaining: 306ms\n",
      "226:\tlearn: 0.0124121\ttotal: 2.88s\tremaining: 292ms\n",
      "227:\tlearn: 0.0123231\ttotal: 2.89s\tremaining: 279ms\n",
      "228:\tlearn: 0.0122944\ttotal: 2.9s\tremaining: 266ms\n",
      "229:\tlearn: 0.0122043\ttotal: 2.93s\tremaining: 255ms\n",
      "230:\tlearn: 0.0121575\ttotal: 2.93s\tremaining: 241ms\n",
      "231:\tlearn: 0.0120350\ttotal: 2.94s\tremaining: 228ms\n",
      "232:\tlearn: 0.0119349\ttotal: 2.94s\tremaining: 215ms\n",
      "233:\tlearn: 0.0117960\ttotal: 2.95s\tremaining: 202ms\n",
      "234:\tlearn: 0.0117499\ttotal: 2.95s\tremaining: 189ms\n",
      "235:\tlearn: 0.0115916\ttotal: 2.96s\tremaining: 176ms\n",
      "236:\tlearn: 0.0115124\ttotal: 2.96s\tremaining: 163ms\n",
      "237:\tlearn: 0.0115123\ttotal: 2.98s\tremaining: 150ms\n",
      "238:\tlearn: 0.0114673\ttotal: 2.98s\tremaining: 137ms\n",
      "239:\tlearn: 0.0114050\ttotal: 2.99s\tremaining: 124ms\n",
      "240:\tlearn: 0.0114050\ttotal: 2.99s\tremaining: 112ms\n",
      "241:\tlearn: 0.0112936\ttotal: 3s\tremaining: 99.4ms\n",
      "242:\tlearn: 0.0112768\ttotal: 3.04s\tremaining: 87.5ms\n",
      "243:\tlearn: 0.0112272\ttotal: 3.07s\tremaining: 75.5ms\n",
      "244:\tlearn: 0.0111554\ttotal: 3.07s\tremaining: 62.7ms\n",
      "245:\tlearn: 0.0111183\ttotal: 3.08s\tremaining: 50.1ms\n",
      "246:\tlearn: 0.0110709\ttotal: 3.08s\tremaining: 37.5ms\n",
      "247:\tlearn: 0.0110489\ttotal: 3.09s\tremaining: 24.9ms\n",
      "248:\tlearn: 0.0110212\ttotal: 3.09s\tremaining: 12.4ms\n",
      "249:\tlearn: 0.0110211\ttotal: 3.1s\tremaining: 0us\n",
      "Dataset 7:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf9e082028243ac98606e23c0f360b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf0bb3ce647407aa760919ae6c10dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5409947199965711, Recall = 0.9368327402135231, Aging Rate = 0.7588967971530249, Precision = 0.6172332942555686, f1 = 0.7441696113074205\n",
      "Epoch 2: Train Loss = 0.35756091375792154, Recall = 0.8821174377224199, Aging Rate = 0.5360320284697508, Precision = 0.8228215767634854, f1 = 0.8514383855732074\n",
      "Epoch 3: Train Loss = 0.27951070231475017, Recall = 0.905693950177936, Aging Rate = 0.5171263345195729, Precision = 0.8756989247311828, f1 = 0.8904439099059699\n",
      "Epoch 4: Train Loss = 0.2329825386139846, Recall = 0.9301601423487544, Aging Rate = 0.5126779359430605, Precision = 0.9071583514099784, f1 = 0.9185152646606634\n",
      "Epoch 5: Train Loss = 0.1934192873457997, Recall = 0.9435053380782918, Aging Rate = 0.5093416370106761, Precision = 0.9262008733624454, f1 = 0.9347730277655355\n",
      "Test Loss = 0.1518478770591186, Recall = 0.978202846975089, Aging Rate = 0.5160142348754448, precision = 0.9478448275862069\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.13761170934952027, Recall = 0.9688612099644128, Aging Rate = 0.5020017793594306, Precision = 0.9649977846699158, f1 = 0.9669256381798003\n",
      "Epoch 7: Train Loss = 0.10968908646352775, Recall = 0.9786476868327402, Aging Rate = 0.5011120996441281, Precision = 0.9764758100310696, f1 = 0.9775605421017551\n",
      "Epoch 8: Train Loss = 0.08925931326700275, Recall = 0.9839857651245552, Aging Rate = 0.5004448398576512, Precision = 0.9831111111111112, f1 = 0.9835482436638506\n",
      "Epoch 9: Train Loss = 0.07429069468877493, Recall = 0.9884341637010676, Aging Rate = 0.5006672597864769, Precision = 0.9871168369613506, f1 = 0.9877750611246944\n",
      "Epoch 10: Train Loss = 0.062154584945414836, Recall = 0.9928825622775801, Aging Rate = 0.5013345195729537, Precision = 0.9902395740905058, f1 = 0.9915593069746779\n",
      "Test Loss = 0.052615634739080784, Recall = 0.9973309608540926, Aging Rate = 0.5008896797153025, precision = 0.9955595026642984\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.053347005803814136, Recall = 0.9968861209964412, Aging Rate = 0.5015569395017794, Precision = 0.9937915742793791, f1 = 0.9953364423717522\n",
      "Epoch 12: Train Loss = 0.04635882415407705, Recall = 0.9968861209964412, Aging Rate = 0.5008896797153025, Precision = 0.9951154529307282, f1 = 0.9959999999999999\n",
      "Epoch 13: Train Loss = 0.0407092996352614, Recall = 0.9973309608540926, Aging Rate = 0.49933274021352314, Precision = 0.9986636971046771, f1 = 0.9979968840418428\n",
      "Epoch 14: Train Loss = 0.036450203368937416, Recall = 0.998220640569395, Aging Rate = 0.5006672597864769, Precision = 0.9968902709906708, f1 = 0.9975550122249388\n",
      "Epoch 15: Train Loss = 0.03257560645796862, Recall = 0.9986654804270463, Aging Rate = 0.5002224199288257, Precision = 0.9982214317474433, f1 = 0.9984434067155882\n",
      "Test Loss = 0.029539102874033392, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, precision = 0.9991107158737217\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.02999532386474974, Recall = 0.9991103202846975, Aging Rate = 0.5004448398576512, Precision = 0.9982222222222222, f1 = 0.9986660738105825\n",
      "Epoch 17: Train Loss = 0.027656444537310837, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 18: Train Loss = 0.02628494356344603, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 19: Train Loss = 0.02450568659158152, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 20: Train Loss = 0.023035650791142972, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.020928463083277605, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.022171284224559617, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 22: Train Loss = 0.020798171654979953, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.02062999552904605, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.019856326232309655, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.01937831573692081, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01758315939177822, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.019338118240528897, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.018684456801022074, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.018458381792351444, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.01871550559096065, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.017832108965521395, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015650806895513552, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.017707686125569284, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.017536778831556174, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.017598693965009522, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.017231966105365988, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.01722731960612578, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01481963771319156, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.016755994210463827, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.016690747550559427, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.017347825379558306, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.016406310563144736, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01685821435669457, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0146736287846631, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.016315209231873, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.016732299186660513, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.016913777416127856, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.016348833098901547, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016450365034207118, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014255870753415327, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.01642134003142339, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 47: Train Loss = 0.01661483011476934, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016405498174771507, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016128948902930016, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01600666881416298, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013782455042349274, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.016160817187133632, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.016171396325518016, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.015810248554680908, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 54: Train Loss = 0.01590469925999111, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.016486873737539685, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.013959796999803218, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016147420675331377, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 57: Train Loss = 0.015387166059473231, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.01571832526660793, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.01611959272365871, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015455434682159238, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014141520263222825, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 61: Train Loss = 0.015821583917052934, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 62: Train Loss = 0.01558363789301432, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01613834012445606, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015422931975052026, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.01578054356875952, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013898093304745986, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015762099644891732, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01572451603622093, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.01572274808768594, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015323526938561868, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015550181259702745, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.01376303629717687, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.01603551489363998, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.015299525844377344, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.015412152446541285, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015273832182443015, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015855435129533458, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013848908401336322, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.015477435636827954, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.01622067599348538, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.015090286304173308, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.01557666974082536, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.01610976061203726, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01785579703589139, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 80.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3187490\ttotal: 8.89ms\tremaining: 1.77s\n",
      "1:\tlearn: 0.2134222\ttotal: 18.5ms\tremaining: 1.84s\n",
      "2:\tlearn: 0.1598986\ttotal: 27.7ms\tremaining: 1.82s\n",
      "3:\tlearn: 0.1353480\ttotal: 37.5ms\tremaining: 1.84s\n",
      "4:\tlearn: 0.1194818\ttotal: 47.1ms\tremaining: 1.84s\n",
      "5:\tlearn: 0.1037631\ttotal: 56.8ms\tremaining: 1.83s\n",
      "6:\tlearn: 0.0928881\ttotal: 65.9ms\tremaining: 1.82s\n",
      "7:\tlearn: 0.0844161\ttotal: 75ms\tremaining: 1.8s\n",
      "8:\tlearn: 0.0759571\ttotal: 83.9ms\tremaining: 1.78s\n",
      "9:\tlearn: 0.0680421\ttotal: 92.1ms\tremaining: 1.75s\n",
      "10:\tlearn: 0.0607073\ttotal: 100ms\tremaining: 1.72s\n",
      "11:\tlearn: 0.0558762\ttotal: 108ms\tremaining: 1.7s\n",
      "12:\tlearn: 0.0503507\ttotal: 116ms\tremaining: 1.68s\n",
      "13:\tlearn: 0.0457837\ttotal: 125ms\tremaining: 1.66s\n",
      "14:\tlearn: 0.0415449\ttotal: 134ms\tremaining: 1.66s\n",
      "15:\tlearn: 0.0384506\ttotal: 144ms\tremaining: 1.65s\n",
      "16:\tlearn: 0.0352634\ttotal: 153ms\tremaining: 1.64s\n",
      "17:\tlearn: 0.0320374\ttotal: 161ms\tremaining: 1.62s\n",
      "18:\tlearn: 0.0304308\ttotal: 168ms\tremaining: 1.6s\n",
      "19:\tlearn: 0.0287751\ttotal: 177ms\tremaining: 1.59s\n",
      "20:\tlearn: 0.0270818\ttotal: 185ms\tremaining: 1.58s\n",
      "21:\tlearn: 0.0256829\ttotal: 193ms\tremaining: 1.56s\n",
      "22:\tlearn: 0.0244532\ttotal: 200ms\tremaining: 1.54s\n",
      "23:\tlearn: 0.0225009\ttotal: 208ms\tremaining: 1.53s\n",
      "24:\tlearn: 0.0208253\ttotal: 216ms\tremaining: 1.51s\n",
      "25:\tlearn: 0.0193190\ttotal: 223ms\tremaining: 1.49s\n",
      "26:\tlearn: 0.0176512\ttotal: 231ms\tremaining: 1.48s\n",
      "27:\tlearn: 0.0156443\ttotal: 239ms\tremaining: 1.47s\n",
      "28:\tlearn: 0.0143993\ttotal: 246ms\tremaining: 1.45s\n",
      "29:\tlearn: 0.0133626\ttotal: 253ms\tremaining: 1.43s\n",
      "30:\tlearn: 0.0122089\ttotal: 260ms\tremaining: 1.42s\n",
      "31:\tlearn: 0.0113355\ttotal: 267ms\tremaining: 1.4s\n",
      "32:\tlearn: 0.0104587\ttotal: 274ms\tremaining: 1.39s\n",
      "33:\tlearn: 0.0097038\ttotal: 281ms\tremaining: 1.37s\n",
      "34:\tlearn: 0.0089249\ttotal: 288ms\tremaining: 1.36s\n",
      "35:\tlearn: 0.0084022\ttotal: 295ms\tremaining: 1.34s\n",
      "36:\tlearn: 0.0080379\ttotal: 302ms\tremaining: 1.33s\n",
      "37:\tlearn: 0.0073373\ttotal: 310ms\tremaining: 1.32s\n",
      "38:\tlearn: 0.0066683\ttotal: 318ms\tremaining: 1.31s\n",
      "39:\tlearn: 0.0060855\ttotal: 325ms\tremaining: 1.3s\n",
      "40:\tlearn: 0.0056240\ttotal: 333ms\tremaining: 1.29s\n",
      "41:\tlearn: 0.0052555\ttotal: 342ms\tremaining: 1.28s\n",
      "42:\tlearn: 0.0048980\ttotal: 349ms\tremaining: 1.28s\n",
      "43:\tlearn: 0.0046100\ttotal: 358ms\tremaining: 1.27s\n",
      "44:\tlearn: 0.0042387\ttotal: 365ms\tremaining: 1.26s\n",
      "45:\tlearn: 0.0040640\ttotal: 373ms\tremaining: 1.25s\n",
      "46:\tlearn: 0.0037240\ttotal: 380ms\tremaining: 1.24s\n",
      "47:\tlearn: 0.0035102\ttotal: 387ms\tremaining: 1.23s\n",
      "48:\tlearn: 0.0033275\ttotal: 395ms\tremaining: 1.22s\n",
      "49:\tlearn: 0.0031641\ttotal: 401ms\tremaining: 1.2s\n",
      "50:\tlearn: 0.0029691\ttotal: 408ms\tremaining: 1.19s\n",
      "51:\tlearn: 0.0028012\ttotal: 415ms\tremaining: 1.18s\n",
      "52:\tlearn: 0.0026786\ttotal: 423ms\tremaining: 1.17s\n",
      "53:\tlearn: 0.0025380\ttotal: 431ms\tremaining: 1.17s\n",
      "54:\tlearn: 0.0024146\ttotal: 439ms\tremaining: 1.16s\n",
      "55:\tlearn: 0.0023109\ttotal: 446ms\tremaining: 1.15s\n",
      "56:\tlearn: 0.0022019\ttotal: 453ms\tremaining: 1.14s\n",
      "57:\tlearn: 0.0020459\ttotal: 461ms\tremaining: 1.13s\n",
      "58:\tlearn: 0.0019431\ttotal: 468ms\tremaining: 1.12s\n",
      "59:\tlearn: 0.0018716\ttotal: 476ms\tremaining: 1.11s\n",
      "60:\tlearn: 0.0017826\ttotal: 483ms\tremaining: 1.1s\n",
      "61:\tlearn: 0.0017050\ttotal: 490ms\tremaining: 1.09s\n",
      "62:\tlearn: 0.0016245\ttotal: 498ms\tremaining: 1.08s\n",
      "63:\tlearn: 0.0015418\ttotal: 506ms\tremaining: 1.07s\n",
      "64:\tlearn: 0.0014679\ttotal: 513ms\tremaining: 1.07s\n",
      "65:\tlearn: 0.0014177\ttotal: 521ms\tremaining: 1.06s\n",
      "66:\tlearn: 0.0013728\ttotal: 529ms\tremaining: 1.05s\n",
      "67:\tlearn: 0.0013045\ttotal: 536ms\tremaining: 1.04s\n",
      "68:\tlearn: 0.0012589\ttotal: 544ms\tremaining: 1.03s\n",
      "69:\tlearn: 0.0012059\ttotal: 552ms\tremaining: 1.02s\n",
      "70:\tlearn: 0.0011675\ttotal: 560ms\tremaining: 1.02s\n",
      "71:\tlearn: 0.0011675\ttotal: 569ms\tremaining: 1.01s\n",
      "72:\tlearn: 0.0011674\ttotal: 576ms\tremaining: 1s\n",
      "73:\tlearn: 0.0011674\ttotal: 584ms\tremaining: 995ms\n",
      "74:\tlearn: 0.0011673\ttotal: 592ms\tremaining: 987ms\n",
      "75:\tlearn: 0.0011672\ttotal: 600ms\tremaining: 978ms\n",
      "76:\tlearn: 0.0011100\ttotal: 608ms\tremaining: 971ms\n",
      "77:\tlearn: 0.0010606\ttotal: 616ms\tremaining: 964ms\n",
      "78:\tlearn: 0.0010106\ttotal: 625ms\tremaining: 957ms\n",
      "79:\tlearn: 0.0010105\ttotal: 633ms\tremaining: 950ms\n",
      "80:\tlearn: 0.0009720\ttotal: 642ms\tremaining: 943ms\n",
      "81:\tlearn: 0.0009720\ttotal: 650ms\tremaining: 935ms\n",
      "82:\tlearn: 0.0009719\ttotal: 658ms\tremaining: 927ms\n",
      "83:\tlearn: 0.0009719\ttotal: 666ms\tremaining: 920ms\n",
      "84:\tlearn: 0.0009719\ttotal: 674ms\tremaining: 912ms\n",
      "85:\tlearn: 0.0009719\ttotal: 681ms\tremaining: 903ms\n",
      "86:\tlearn: 0.0009275\ttotal: 690ms\tremaining: 896ms\n",
      "87:\tlearn: 0.0009275\ttotal: 698ms\tremaining: 889ms\n",
      "88:\tlearn: 0.0009274\ttotal: 706ms\tremaining: 881ms\n",
      "89:\tlearn: 0.0009274\ttotal: 713ms\tremaining: 872ms\n",
      "90:\tlearn: 0.0009274\ttotal: 721ms\tremaining: 864ms\n",
      "91:\tlearn: 0.0009274\ttotal: 728ms\tremaining: 855ms\n",
      "92:\tlearn: 0.0009273\ttotal: 735ms\tremaining: 845ms\n",
      "93:\tlearn: 0.0009273\ttotal: 742ms\tremaining: 837ms\n",
      "94:\tlearn: 0.0009273\ttotal: 749ms\tremaining: 828ms\n",
      "95:\tlearn: 0.0009272\ttotal: 756ms\tremaining: 820ms\n",
      "96:\tlearn: 0.0009272\ttotal: 763ms\tremaining: 810ms\n",
      "97:\tlearn: 0.0009271\ttotal: 770ms\tremaining: 801ms\n",
      "98:\tlearn: 0.0009271\ttotal: 777ms\tremaining: 792ms\n",
      "99:\tlearn: 0.0009271\ttotal: 783ms\tremaining: 783ms\n",
      "100:\tlearn: 0.0009270\ttotal: 791ms\tremaining: 775ms\n",
      "101:\tlearn: 0.0009270\ttotal: 798ms\tremaining: 767ms\n",
      "102:\tlearn: 0.0009269\ttotal: 805ms\tremaining: 758ms\n",
      "103:\tlearn: 0.0009269\ttotal: 811ms\tremaining: 749ms\n",
      "104:\tlearn: 0.0009269\ttotal: 818ms\tremaining: 740ms\n",
      "105:\tlearn: 0.0009268\ttotal: 824ms\tremaining: 731ms\n",
      "106:\tlearn: 0.0009268\ttotal: 831ms\tremaining: 722ms\n",
      "107:\tlearn: 0.0009268\ttotal: 838ms\tremaining: 714ms\n",
      "108:\tlearn: 0.0009267\ttotal: 845ms\tremaining: 705ms\n",
      "109:\tlearn: 0.0009266\ttotal: 852ms\tremaining: 697ms\n",
      "110:\tlearn: 0.0009266\ttotal: 859ms\tremaining: 688ms\n",
      "111:\tlearn: 0.0009265\ttotal: 865ms\tremaining: 680ms\n",
      "112:\tlearn: 0.0009265\ttotal: 872ms\tremaining: 671ms\n",
      "113:\tlearn: 0.0009264\ttotal: 879ms\tremaining: 663ms\n",
      "114:\tlearn: 0.0009264\ttotal: 887ms\tremaining: 656ms\n",
      "115:\tlearn: 0.0009263\ttotal: 895ms\tremaining: 648ms\n",
      "116:\tlearn: 0.0009263\ttotal: 902ms\tremaining: 640ms\n",
      "117:\tlearn: 0.0009263\ttotal: 910ms\tremaining: 632ms\n",
      "118:\tlearn: 0.0009263\ttotal: 917ms\tremaining: 624ms\n",
      "119:\tlearn: 0.0009263\ttotal: 925ms\tremaining: 617ms\n",
      "120:\tlearn: 0.0009262\ttotal: 932ms\tremaining: 609ms\n",
      "121:\tlearn: 0.0009262\ttotal: 940ms\tremaining: 601ms\n",
      "122:\tlearn: 0.0009262\ttotal: 946ms\tremaining: 592ms\n",
      "123:\tlearn: 0.0009262\ttotal: 953ms\tremaining: 584ms\n",
      "124:\tlearn: 0.0009262\ttotal: 960ms\tremaining: 576ms\n",
      "125:\tlearn: 0.0008897\ttotal: 966ms\tremaining: 568ms\n",
      "126:\tlearn: 0.0008897\ttotal: 973ms\tremaining: 560ms\n",
      "127:\tlearn: 0.0008897\ttotal: 980ms\tremaining: 552ms\n",
      "128:\tlearn: 0.0008896\ttotal: 988ms\tremaining: 544ms\n",
      "129:\tlearn: 0.0008896\ttotal: 996ms\tremaining: 536ms\n",
      "130:\tlearn: 0.0008895\ttotal: 1s\tremaining: 529ms\n",
      "131:\tlearn: 0.0008895\ttotal: 1.01s\tremaining: 521ms\n",
      "132:\tlearn: 0.0008895\ttotal: 1.02s\tremaining: 513ms\n",
      "133:\tlearn: 0.0008895\ttotal: 1.02s\tremaining: 504ms\n",
      "134:\tlearn: 0.0008895\ttotal: 1.03s\tremaining: 496ms\n",
      "135:\tlearn: 0.0008894\ttotal: 1.04s\tremaining: 488ms\n",
      "136:\tlearn: 0.0008894\ttotal: 1.04s\tremaining: 481ms\n",
      "137:\tlearn: 0.0008893\ttotal: 1.05s\tremaining: 473ms\n",
      "138:\tlearn: 0.0008893\ttotal: 1.06s\tremaining: 465ms\n",
      "139:\tlearn: 0.0008893\ttotal: 1.07s\tremaining: 457ms\n",
      "140:\tlearn: 0.0008892\ttotal: 1.07s\tremaining: 449ms\n",
      "141:\tlearn: 0.0008892\ttotal: 1.08s\tremaining: 441ms\n",
      "142:\tlearn: 0.0008892\ttotal: 1.09s\tremaining: 433ms\n",
      "143:\tlearn: 0.0008892\ttotal: 1.09s\tremaining: 425ms\n",
      "144:\tlearn: 0.0008891\ttotal: 1.1s\tremaining: 417ms\n",
      "145:\tlearn: 0.0008891\ttotal: 1.11s\tremaining: 410ms\n",
      "146:\tlearn: 0.0008889\ttotal: 1.11s\tremaining: 402ms\n",
      "147:\tlearn: 0.0008889\ttotal: 1.12s\tremaining: 395ms\n",
      "148:\tlearn: 0.0008889\ttotal: 1.13s\tremaining: 387ms\n",
      "149:\tlearn: 0.0008889\ttotal: 1.14s\tremaining: 380ms\n",
      "150:\tlearn: 0.0008889\ttotal: 1.15s\tremaining: 372ms\n",
      "151:\tlearn: 0.0008889\ttotal: 1.16s\tremaining: 365ms\n",
      "152:\tlearn: 0.0008889\ttotal: 1.16s\tremaining: 357ms\n",
      "153:\tlearn: 0.0008889\ttotal: 1.17s\tremaining: 350ms\n",
      "154:\tlearn: 0.0008888\ttotal: 1.18s\tremaining: 342ms\n",
      "155:\tlearn: 0.0008888\ttotal: 1.19s\tremaining: 334ms\n",
      "156:\tlearn: 0.0008888\ttotal: 1.19s\tremaining: 327ms\n",
      "157:\tlearn: 0.0008888\ttotal: 1.2s\tremaining: 319ms\n",
      "158:\tlearn: 0.0008887\ttotal: 1.21s\tremaining: 312ms\n",
      "159:\tlearn: 0.0008886\ttotal: 1.22s\tremaining: 304ms\n",
      "160:\tlearn: 0.0008886\ttotal: 1.23s\tremaining: 297ms\n",
      "161:\tlearn: 0.0008886\ttotal: 1.23s\tremaining: 289ms\n",
      "162:\tlearn: 0.0008886\ttotal: 1.24s\tremaining: 282ms\n",
      "163:\tlearn: 0.0008885\ttotal: 1.25s\tremaining: 274ms\n",
      "164:\tlearn: 0.0008885\ttotal: 1.25s\tremaining: 266ms\n",
      "165:\tlearn: 0.0008885\ttotal: 1.26s\tremaining: 259ms\n",
      "166:\tlearn: 0.0008884\ttotal: 1.27s\tremaining: 251ms\n",
      "167:\tlearn: 0.0008884\ttotal: 1.28s\tremaining: 244ms\n",
      "168:\tlearn: 0.0008884\ttotal: 1.29s\tremaining: 236ms\n",
      "169:\tlearn: 0.0008884\ttotal: 1.29s\tremaining: 228ms\n",
      "170:\tlearn: 0.0008883\ttotal: 1.3s\tremaining: 221ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171:\tlearn: 0.0008883\ttotal: 1.31s\tremaining: 213ms\n",
      "172:\tlearn: 0.0008883\ttotal: 1.32s\tremaining: 205ms\n",
      "173:\tlearn: 0.0008882\ttotal: 1.32s\tremaining: 198ms\n",
      "174:\tlearn: 0.0008882\ttotal: 1.33s\tremaining: 190ms\n",
      "175:\tlearn: 0.0008882\ttotal: 1.34s\tremaining: 182ms\n",
      "176:\tlearn: 0.0008881\ttotal: 1.34s\tremaining: 175ms\n",
      "177:\tlearn: 0.0008881\ttotal: 1.35s\tremaining: 167ms\n",
      "178:\tlearn: 0.0008881\ttotal: 1.36s\tremaining: 159ms\n",
      "179:\tlearn: 0.0008881\ttotal: 1.36s\tremaining: 152ms\n",
      "180:\tlearn: 0.0008880\ttotal: 1.37s\tremaining: 144ms\n",
      "181:\tlearn: 0.0008880\ttotal: 1.38s\tremaining: 136ms\n",
      "182:\tlearn: 0.0008879\ttotal: 1.38s\tremaining: 129ms\n",
      "183:\tlearn: 0.0008879\ttotal: 1.39s\tremaining: 121ms\n",
      "184:\tlearn: 0.0008879\ttotal: 1.4s\tremaining: 113ms\n",
      "185:\tlearn: 0.0008879\ttotal: 1.4s\tremaining: 106ms\n",
      "186:\tlearn: 0.0008879\ttotal: 1.41s\tremaining: 98.1ms\n",
      "187:\tlearn: 0.0008879\ttotal: 1.42s\tremaining: 90.5ms\n",
      "188:\tlearn: 0.0008877\ttotal: 1.42s\tremaining: 82.9ms\n",
      "189:\tlearn: 0.0008877\ttotal: 1.43s\tremaining: 75.3ms\n",
      "190:\tlearn: 0.0008877\ttotal: 1.44s\tremaining: 67.7ms\n",
      "191:\tlearn: 0.0008877\ttotal: 1.44s\tremaining: 60.2ms\n",
      "192:\tlearn: 0.0008876\ttotal: 1.45s\tremaining: 52.7ms\n",
      "193:\tlearn: 0.0008876\ttotal: 1.46s\tremaining: 45.1ms\n",
      "194:\tlearn: 0.0008875\ttotal: 1.47s\tremaining: 37.6ms\n",
      "195:\tlearn: 0.0008875\ttotal: 1.47s\tremaining: 30.1ms\n",
      "196:\tlearn: 0.0008875\ttotal: 1.48s\tremaining: 22.6ms\n",
      "197:\tlearn: 0.0008874\ttotal: 1.49s\tremaining: 15ms\n",
      "198:\tlearn: 0.0008874\ttotal: 1.5s\tremaining: 7.53ms\n",
      "199:\tlearn: 0.0008874\ttotal: 1.5s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fae59b21ed04e5aaf358c78f3f936f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5532618271074261, Recall = 0.8816725978647687, Aging Rate = 0.7110765124555161, Precision = 0.6199562089458868, f1 = 0.7280073461891645\n",
      "Epoch 2: Train Loss = 0.3751503740341214, Recall = 0.880338078291815, Aging Rate = 0.542482206405694, Precision = 0.8113981139811398, f1 = 0.8444634094303393\n",
      "Epoch 3: Train Loss = 0.2957768763193456, Recall = 0.9016903914590747, Aging Rate = 0.5195729537366548, Precision = 0.867722602739726, f1 = 0.8843804537521816\n",
      "Epoch 4: Train Loss = 0.24033562308740786, Recall = 0.927491103202847, Aging Rate = 0.5124555160142349, Precision = 0.9049479166666666, f1 = 0.9160808435852372\n",
      "Epoch 5: Train Loss = 0.20677574031929952, Recall = 0.9350533807829181, Aging Rate = 0.5088967971530249, Precision = 0.9187062937062938, f1 = 0.9268077601410936\n",
      "Test Loss = 0.16585407042842742, Recall = 0.9741992882562278, Aging Rate = 0.5264679715302492, precision = 0.9252217997465145\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.15227086229574638, Recall = 0.9590747330960854, Aging Rate = 0.5024466192170819, Precision = 0.9544046038069942, f1 = 0.956733969380963\n",
      "Epoch 7: Train Loss = 0.12242890862595568, Recall = 0.9741992882562278, Aging Rate = 0.5044483985765125, Precision = 0.9656084656084656, f1 = 0.9698848538529672\n",
      "Epoch 8: Train Loss = 0.09926106947808927, Recall = 0.9826512455516014, Aging Rate = 0.5022241992882562, Precision = 0.9782993799822852, f1 = 0.9804704837993786\n",
      "Epoch 9: Train Loss = 0.0812393098645363, Recall = 0.9879893238434164, Aging Rate = 0.5015569395017794, Precision = 0.9849223946784922, f1 = 0.986453475460804\n",
      "Epoch 10: Train Loss = 0.06792454379096999, Recall = 0.9897686832740213, Aging Rate = 0.49933274021352314, Precision = 0.9910913140311804, f1 = 0.9904295570888049\n",
      "Test Loss = 0.057213616076843596, Recall = 0.9933274021352313, Aging Rate = 0.4997775800711744, precision = 0.9937694704049844\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05766819467854245, Recall = 0.9942170818505338, Aging Rate = 0.5008896797153025, Precision = 0.9924511545293073, f1 = 0.9933333333333334\n",
      "Epoch 12: Train Loss = 0.049043728893761955, Recall = 0.9951067615658363, Aging Rate = 0.5008896797153025, Precision = 0.9933392539964476, f1 = 0.9942222222222221\n",
      "Epoch 13: Train Loss = 0.04290822530132171, Recall = 0.9959964412811388, Aging Rate = 0.5, Precision = 0.9959964412811388, f1 = 0.9959964412811388\n",
      "Epoch 14: Train Loss = 0.03749053648199051, Recall = 0.998220640569395, Aging Rate = 0.5008896797153025, Precision = 0.9964476021314387, f1 = 0.9973333333333333\n",
      "Epoch 15: Train Loss = 0.033963767652568866, Recall = 0.9991103202846975, Aging Rate = 0.5008896797153025, Precision = 0.9973357015985791, f1 = 0.9982222222222222\n",
      "Test Loss = 0.02940007638570677, Recall = 0.9986654804270463, Aging Rate = 0.5, precision = 0.9986654804270463\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03079986102295728, Recall = 0.9986654804270463, Aging Rate = 0.5, Precision = 0.9986654804270463, f1 = 0.9986654804270463\n",
      "Epoch 17: Train Loss = 0.02828883024403209, Recall = 0.9995551601423488, Aging Rate = 0.5004448398576512, Precision = 0.9986666666666667, f1 = 0.9991107158737217\n",
      "Epoch 18: Train Loss = 0.026535392510190978, Recall = 0.9991103202846975, Aging Rate = 0.5, Precision = 0.9991103202846975, f1 = 0.9991103202846975\n",
      "Epoch 19: Train Loss = 0.024750253238658888, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 20: Train Loss = 0.023366290333538292, Recall = 1.0, Aging Rate = 0.5006672597864769, Precision = 0.9986672589960017, f1 = 0.999333185152256\n",
      "Test Loss = 0.020634489114068157, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.022775560252872227, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 22: Train Loss = 0.021360781753031386, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 23: Train Loss = 0.020623919623655357, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 24: Train Loss = 0.019859062106716462, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 25: Train Loss = 0.019912282140839354, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.01940032634618974, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.019253209823232104, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 27: Train Loss = 0.019020812877609636, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 28: Train Loss = 0.01818708508040133, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 29: Train Loss = 0.01824610107628052, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.01779847698173489, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015835013528921214, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 31: Train Loss = 0.01790512930535549, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 32: Train Loss = 0.017550694143061537, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 33: Train Loss = 0.017535612865407484, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.01688189132079429, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 35: Train Loss = 0.016988385223462063, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015206417619015398, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 36: Train Loss = 0.016752160316684493, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 37: Train Loss = 0.01728143510193269, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 38: Train Loss = 0.01693422969735602, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 39: Train Loss = 0.01658112692623482, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01696818774015878, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01671555565398359, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.016751509587350052, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.017014599209684494, Recall = 1.0, Aging Rate = 0.5004448398576512, Precision = 0.9991111111111111, f1 = 0.9995553579368608\n",
      "Epoch 43: Train Loss = 0.01625126906176797, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.016679734057909228, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016194845576276772, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014419946335653817, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 46: Train Loss = 0.016066654830561203, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 47: Train Loss = 0.0160963651674445, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016314055966180203, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 49: Train Loss = 0.016178031601144324, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 50: Train Loss = 0.01634253441518524, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01482129045281546, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.016636914255435568, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.016051347748611746, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016747476768371686, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 54: Train Loss = 0.015818350988165128, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015574653437394264, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.01396041504004672, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 56: Train Loss = 0.015735479830714518, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 57: Train Loss = 0.015667424527298513, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 58: Train Loss = 0.015884492549634276, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.01639336451318022, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.01571040273083911, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014040505076059242, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.016263579833735563, Recall = 1.0, Aging Rate = 0.5004448398576512, Precision = 0.9991111111111111, f1 = 0.9995553579368608\n",
      "Epoch 62: Train Loss = 0.01581216144153444, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01621013375005794, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.016207644836877696, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 65: Train Loss = 0.015757546753513218, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.014874207313268634, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 66: Train Loss = 0.015520920045673847, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01570642630824839, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 68: Train Loss = 0.015763472649683096, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 69: Train Loss = 0.015296222439890867, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015851502392931552, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013478320168192065, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.015753122928197697, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.01580088202811751, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.014974325429648161, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 74: Train Loss = 0.01576278003071869, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.01604016841094265, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01421036712454095, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.015689799035120477, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.01526255807524474, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.015761615970301245, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 79: Train Loss = 0.015507954151247002, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.015706875928250073, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.013831021008993171, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.016143941284021447, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 82: Train Loss = 0.015379341557834072, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.015372700833473554, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.015618533278845277, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 85: Train Loss = 0.015238728848189859, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014086516123278285, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.015750127877655614, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.015695854275755823, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 88: Train Loss = 0.015158861723075559, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 89: Train Loss = 0.015490366822088741, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0172205587113243, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.0144041357363087, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3239085\ttotal: 8.73ms\tremaining: 1.74s\n",
      "1:\tlearn: 0.1962577\ttotal: 17ms\tremaining: 1.68s\n",
      "2:\tlearn: 0.1644832\ttotal: 25ms\tremaining: 1.64s\n",
      "3:\tlearn: 0.1359258\ttotal: 33.6ms\tremaining: 1.65s\n",
      "4:\tlearn: 0.1161920\ttotal: 42.7ms\tremaining: 1.67s\n",
      "5:\tlearn: 0.1029289\ttotal: 51.9ms\tremaining: 1.68s\n",
      "6:\tlearn: 0.0918378\ttotal: 59.9ms\tremaining: 1.65s\n",
      "7:\tlearn: 0.0833756\ttotal: 68.8ms\tremaining: 1.65s\n",
      "8:\tlearn: 0.0761621\ttotal: 77ms\tremaining: 1.63s\n",
      "9:\tlearn: 0.0683666\ttotal: 84.3ms\tremaining: 1.6s\n",
      "10:\tlearn: 0.0624690\ttotal: 91.7ms\tremaining: 1.58s\n",
      "11:\tlearn: 0.0560032\ttotal: 99.6ms\tremaining: 1.56s\n",
      "12:\tlearn: 0.0510893\ttotal: 107ms\tremaining: 1.54s\n",
      "13:\tlearn: 0.0464116\ttotal: 115ms\tremaining: 1.53s\n",
      "14:\tlearn: 0.0415271\ttotal: 122ms\tremaining: 1.51s\n",
      "15:\tlearn: 0.0376092\ttotal: 130ms\tremaining: 1.5s\n",
      "16:\tlearn: 0.0332680\ttotal: 138ms\tremaining: 1.49s\n",
      "17:\tlearn: 0.0306368\ttotal: 145ms\tremaining: 1.47s\n",
      "18:\tlearn: 0.0278502\ttotal: 154ms\tremaining: 1.46s\n",
      "19:\tlearn: 0.0256408\ttotal: 163ms\tremaining: 1.46s\n",
      "20:\tlearn: 0.0234793\ttotal: 171ms\tremaining: 1.46s\n",
      "21:\tlearn: 0.0215448\ttotal: 180ms\tremaining: 1.45s\n",
      "22:\tlearn: 0.0204080\ttotal: 188ms\tremaining: 1.45s\n",
      "23:\tlearn: 0.0189859\ttotal: 196ms\tremaining: 1.44s\n",
      "24:\tlearn: 0.0172636\ttotal: 204ms\tremaining: 1.43s\n",
      "25:\tlearn: 0.0155467\ttotal: 211ms\tremaining: 1.41s\n",
      "26:\tlearn: 0.0144845\ttotal: 218ms\tremaining: 1.4s\n",
      "27:\tlearn: 0.0132949\ttotal: 225ms\tremaining: 1.38s\n",
      "28:\tlearn: 0.0121444\ttotal: 232ms\tremaining: 1.37s\n",
      "29:\tlearn: 0.0111202\ttotal: 240ms\tremaining: 1.36s\n",
      "30:\tlearn: 0.0101871\ttotal: 247ms\tremaining: 1.34s\n",
      "31:\tlearn: 0.0093092\ttotal: 254ms\tremaining: 1.33s\n",
      "32:\tlearn: 0.0085878\ttotal: 262ms\tremaining: 1.32s\n",
      "33:\tlearn: 0.0075793\ttotal: 269ms\tremaining: 1.31s\n",
      "34:\tlearn: 0.0071036\ttotal: 277ms\tremaining: 1.3s\n",
      "35:\tlearn: 0.0066051\ttotal: 284ms\tremaining: 1.29s\n",
      "36:\tlearn: 0.0060346\ttotal: 292ms\tremaining: 1.29s\n",
      "37:\tlearn: 0.0056881\ttotal: 299ms\tremaining: 1.27s\n",
      "38:\tlearn: 0.0052934\ttotal: 308ms\tremaining: 1.27s\n",
      "39:\tlearn: 0.0050003\ttotal: 315ms\tremaining: 1.26s\n",
      "40:\tlearn: 0.0046168\ttotal: 323ms\tremaining: 1.25s\n",
      "41:\tlearn: 0.0043363\ttotal: 330ms\tremaining: 1.24s\n",
      "42:\tlearn: 0.0040421\ttotal: 338ms\tremaining: 1.23s\n",
      "43:\tlearn: 0.0036842\ttotal: 347ms\tremaining: 1.23s\n",
      "44:\tlearn: 0.0034589\ttotal: 354ms\tremaining: 1.22s\n",
      "45:\tlearn: 0.0032994\ttotal: 361ms\tremaining: 1.21s\n",
      "46:\tlearn: 0.0031153\ttotal: 368ms\tremaining: 1.2s\n",
      "47:\tlearn: 0.0029326\ttotal: 377ms\tremaining: 1.19s\n",
      "48:\tlearn: 0.0027443\ttotal: 385ms\tremaining: 1.19s\n",
      "49:\tlearn: 0.0026184\ttotal: 393ms\tremaining: 1.18s\n",
      "50:\tlearn: 0.0024647\ttotal: 401ms\tremaining: 1.17s\n",
      "51:\tlearn: 0.0023555\ttotal: 410ms\tremaining: 1.17s\n",
      "52:\tlearn: 0.0022592\ttotal: 418ms\tremaining: 1.16s\n",
      "53:\tlearn: 0.0021527\ttotal: 426ms\tremaining: 1.15s\n",
      "54:\tlearn: 0.0020456\ttotal: 433ms\tremaining: 1.14s\n",
      "55:\tlearn: 0.0019460\ttotal: 442ms\tremaining: 1.14s\n",
      "56:\tlearn: 0.0018500\ttotal: 451ms\tremaining: 1.13s\n",
      "57:\tlearn: 0.0017643\ttotal: 459ms\tremaining: 1.12s\n",
      "58:\tlearn: 0.0016768\ttotal: 467ms\tremaining: 1.11s\n",
      "59:\tlearn: 0.0015886\ttotal: 475ms\tremaining: 1.11s\n",
      "60:\tlearn: 0.0015264\ttotal: 483ms\tremaining: 1.1s\n",
      "61:\tlearn: 0.0014709\ttotal: 492ms\tremaining: 1.09s\n",
      "62:\tlearn: 0.0013999\ttotal: 500ms\tremaining: 1.09s\n",
      "63:\tlearn: 0.0013543\ttotal: 508ms\tremaining: 1.08s\n",
      "64:\tlearn: 0.0012889\ttotal: 517ms\tremaining: 1.07s\n",
      "65:\tlearn: 0.0012340\ttotal: 525ms\tremaining: 1.06s\n",
      "66:\tlearn: 0.0012338\ttotal: 533ms\tremaining: 1.06s\n",
      "67:\tlearn: 0.0012338\ttotal: 541ms\tremaining: 1.05s\n",
      "68:\tlearn: 0.0011814\ttotal: 549ms\tremaining: 1.04s\n",
      "69:\tlearn: 0.0011313\ttotal: 557ms\tremaining: 1.03s\n",
      "70:\tlearn: 0.0011313\ttotal: 565ms\tremaining: 1.03s\n",
      "71:\tlearn: 0.0010749\ttotal: 572ms\tremaining: 1.02s\n",
      "72:\tlearn: 0.0010183\ttotal: 579ms\tremaining: 1.01s\n",
      "73:\tlearn: 0.0010183\ttotal: 586ms\tremaining: 997ms\n",
      "74:\tlearn: 0.0009615\ttotal: 593ms\tremaining: 988ms\n",
      "75:\tlearn: 0.0009615\ttotal: 601ms\tremaining: 980ms\n",
      "76:\tlearn: 0.0009614\ttotal: 607ms\tremaining: 970ms\n",
      "77:\tlearn: 0.0009614\ttotal: 614ms\tremaining: 961ms\n",
      "78:\tlearn: 0.0009099\ttotal: 622ms\tremaining: 953ms\n",
      "79:\tlearn: 0.0009098\ttotal: 630ms\tremaining: 944ms\n",
      "80:\tlearn: 0.0009098\ttotal: 636ms\tremaining: 935ms\n",
      "81:\tlearn: 0.0009097\ttotal: 643ms\tremaining: 925ms\n",
      "82:\tlearn: 0.0009097\ttotal: 650ms\tremaining: 916ms\n",
      "83:\tlearn: 0.0008667\ttotal: 655ms\tremaining: 905ms\n",
      "84:\tlearn: 0.0008667\ttotal: 663ms\tremaining: 897ms\n",
      "85:\tlearn: 0.0008666\ttotal: 669ms\tremaining: 887ms\n",
      "86:\tlearn: 0.0008665\ttotal: 676ms\tremaining: 878ms\n",
      "87:\tlearn: 0.0008665\ttotal: 683ms\tremaining: 869ms\n",
      "88:\tlearn: 0.0008665\ttotal: 689ms\tremaining: 860ms\n",
      "89:\tlearn: 0.0008664\ttotal: 696ms\tremaining: 850ms\n",
      "90:\tlearn: 0.0008664\ttotal: 703ms\tremaining: 842ms\n",
      "91:\tlearn: 0.0008663\ttotal: 710ms\tremaining: 834ms\n",
      "92:\tlearn: 0.0008662\ttotal: 718ms\tremaining: 826ms\n",
      "93:\tlearn: 0.0008662\ttotal: 725ms\tremaining: 818ms\n",
      "94:\tlearn: 0.0008662\ttotal: 733ms\tremaining: 810ms\n",
      "95:\tlearn: 0.0008662\ttotal: 740ms\tremaining: 802ms\n",
      "96:\tlearn: 0.0008661\ttotal: 748ms\tremaining: 794ms\n",
      "97:\tlearn: 0.0008660\ttotal: 755ms\tremaining: 786ms\n",
      "98:\tlearn: 0.0008660\ttotal: 762ms\tremaining: 777ms\n",
      "99:\tlearn: 0.0008660\ttotal: 768ms\tremaining: 768ms\n",
      "100:\tlearn: 0.0008660\ttotal: 775ms\tremaining: 759ms\n",
      "101:\tlearn: 0.0008660\ttotal: 781ms\tremaining: 750ms\n",
      "102:\tlearn: 0.0008659\ttotal: 787ms\tremaining: 741ms\n",
      "103:\tlearn: 0.0008659\ttotal: 794ms\tremaining: 733ms\n",
      "104:\tlearn: 0.0008658\ttotal: 800ms\tremaining: 724ms\n",
      "105:\tlearn: 0.0008658\ttotal: 807ms\tremaining: 715ms\n",
      "106:\tlearn: 0.0008658\ttotal: 813ms\tremaining: 707ms\n",
      "107:\tlearn: 0.0008657\ttotal: 820ms\tremaining: 699ms\n",
      "108:\tlearn: 0.0008657\ttotal: 827ms\tremaining: 690ms\n",
      "109:\tlearn: 0.0008657\ttotal: 834ms\tremaining: 682ms\n",
      "110:\tlearn: 0.0008657\ttotal: 841ms\tremaining: 674ms\n",
      "111:\tlearn: 0.0008656\ttotal: 848ms\tremaining: 666ms\n",
      "112:\tlearn: 0.0008656\ttotal: 855ms\tremaining: 658ms\n",
      "113:\tlearn: 0.0008361\ttotal: 862ms\tremaining: 651ms\n",
      "114:\tlearn: 0.0008011\ttotal: 870ms\tremaining: 643ms\n",
      "115:\tlearn: 0.0008011\ttotal: 878ms\tremaining: 636ms\n",
      "116:\tlearn: 0.0008011\ttotal: 885ms\tremaining: 628ms\n",
      "117:\tlearn: 0.0008011\ttotal: 892ms\tremaining: 620ms\n",
      "118:\tlearn: 0.0008010\ttotal: 900ms\tremaining: 612ms\n",
      "119:\tlearn: 0.0008010\ttotal: 907ms\tremaining: 605ms\n",
      "120:\tlearn: 0.0007713\ttotal: 915ms\tremaining: 597ms\n",
      "121:\tlearn: 0.0007713\ttotal: 922ms\tremaining: 590ms\n",
      "122:\tlearn: 0.0007712\ttotal: 930ms\tremaining: 582ms\n",
      "123:\tlearn: 0.0007712\ttotal: 938ms\tremaining: 575ms\n",
      "124:\tlearn: 0.0007412\ttotal: 945ms\tremaining: 567ms\n",
      "125:\tlearn: 0.0007412\ttotal: 952ms\tremaining: 559ms\n",
      "126:\tlearn: 0.0007412\ttotal: 959ms\tremaining: 551ms\n",
      "127:\tlearn: 0.0007411\ttotal: 966ms\tremaining: 544ms\n",
      "128:\tlearn: 0.0007411\ttotal: 973ms\tremaining: 536ms\n",
      "129:\tlearn: 0.0007411\ttotal: 980ms\tremaining: 528ms\n",
      "130:\tlearn: 0.0007411\ttotal: 987ms\tremaining: 520ms\n",
      "131:\tlearn: 0.0007411\ttotal: 994ms\tremaining: 512ms\n",
      "132:\tlearn: 0.0007411\ttotal: 1s\tremaining: 504ms\n",
      "133:\tlearn: 0.0007411\ttotal: 1.01s\tremaining: 497ms\n",
      "134:\tlearn: 0.0007411\ttotal: 1.01s\tremaining: 489ms\n",
      "135:\tlearn: 0.0007410\ttotal: 1.02s\tremaining: 481ms\n",
      "136:\tlearn: 0.0007410\ttotal: 1.03s\tremaining: 473ms\n",
      "137:\tlearn: 0.0007410\ttotal: 1.03s\tremaining: 465ms\n",
      "138:\tlearn: 0.0007410\ttotal: 1.04s\tremaining: 458ms\n",
      "139:\tlearn: 0.0007410\ttotal: 1.05s\tremaining: 450ms\n",
      "140:\tlearn: 0.0007410\ttotal: 1.06s\tremaining: 442ms\n",
      "141:\tlearn: 0.0007409\ttotal: 1.06s\tremaining: 435ms\n",
      "142:\tlearn: 0.0007409\ttotal: 1.07s\tremaining: 427ms\n",
      "143:\tlearn: 0.0007409\ttotal: 1.08s\tremaining: 419ms\n",
      "144:\tlearn: 0.0007409\ttotal: 1.08s\tremaining: 412ms\n",
      "145:\tlearn: 0.0007409\ttotal: 1.09s\tremaining: 404ms\n",
      "146:\tlearn: 0.0007409\ttotal: 1.1s\tremaining: 397ms\n",
      "147:\tlearn: 0.0007409\ttotal: 1.11s\tremaining: 389ms\n",
      "148:\tlearn: 0.0007409\ttotal: 1.11s\tremaining: 381ms\n",
      "149:\tlearn: 0.0007409\ttotal: 1.12s\tremaining: 374ms\n",
      "150:\tlearn: 0.0007408\ttotal: 1.13s\tremaining: 366ms\n",
      "151:\tlearn: 0.0007408\ttotal: 1.14s\tremaining: 359ms\n",
      "152:\tlearn: 0.0007408\ttotal: 1.14s\tremaining: 351ms\n",
      "153:\tlearn: 0.0007408\ttotal: 1.15s\tremaining: 344ms\n",
      "154:\tlearn: 0.0007407\ttotal: 1.16s\tremaining: 336ms\n",
      "155:\tlearn: 0.0007407\ttotal: 1.17s\tremaining: 329ms\n",
      "156:\tlearn: 0.0007407\ttotal: 1.17s\tremaining: 321ms\n",
      "157:\tlearn: 0.0007407\ttotal: 1.18s\tremaining: 314ms\n",
      "158:\tlearn: 0.0007407\ttotal: 1.19s\tremaining: 307ms\n",
      "159:\tlearn: 0.0007406\ttotal: 1.2s\tremaining: 299ms\n",
      "160:\tlearn: 0.0007406\ttotal: 1.2s\tremaining: 292ms\n",
      "161:\tlearn: 0.0007406\ttotal: 1.21s\tremaining: 285ms\n",
      "162:\tlearn: 0.0007406\ttotal: 1.22s\tremaining: 277ms\n",
      "163:\tlearn: 0.0007406\ttotal: 1.23s\tremaining: 270ms\n",
      "164:\tlearn: 0.0007406\ttotal: 1.24s\tremaining: 262ms\n",
      "165:\tlearn: 0.0007406\ttotal: 1.24s\tremaining: 255ms\n",
      "166:\tlearn: 0.0007405\ttotal: 1.25s\tremaining: 247ms\n",
      "167:\tlearn: 0.0007405\ttotal: 1.26s\tremaining: 240ms\n",
      "168:\tlearn: 0.0007405\ttotal: 1.27s\tremaining: 232ms\n",
      "169:\tlearn: 0.0007405\ttotal: 1.27s\tremaining: 225ms\n",
      "170:\tlearn: 0.0007405\ttotal: 1.28s\tremaining: 217ms\n",
      "171:\tlearn: 0.0007405\ttotal: 1.29s\tremaining: 210ms\n",
      "172:\tlearn: 0.0007405\ttotal: 1.3s\tremaining: 202ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173:\tlearn: 0.0007405\ttotal: 1.3s\tremaining: 195ms\n",
      "174:\tlearn: 0.0007404\ttotal: 1.31s\tremaining: 188ms\n",
      "175:\tlearn: 0.0007404\ttotal: 1.32s\tremaining: 180ms\n",
      "176:\tlearn: 0.0007404\ttotal: 1.33s\tremaining: 173ms\n",
      "177:\tlearn: 0.0007404\ttotal: 1.33s\tremaining: 165ms\n",
      "178:\tlearn: 0.0007403\ttotal: 1.34s\tremaining: 158ms\n",
      "179:\tlearn: 0.0007403\ttotal: 1.35s\tremaining: 150ms\n",
      "180:\tlearn: 0.0007403\ttotal: 1.36s\tremaining: 143ms\n",
      "181:\tlearn: 0.0007403\ttotal: 1.37s\tremaining: 136ms\n",
      "182:\tlearn: 0.0007402\ttotal: 1.38s\tremaining: 128ms\n",
      "183:\tlearn: 0.0007402\ttotal: 1.39s\tremaining: 121ms\n",
      "184:\tlearn: 0.0007402\ttotal: 1.4s\tremaining: 113ms\n",
      "185:\tlearn: 0.0007402\ttotal: 1.41s\tremaining: 106ms\n",
      "186:\tlearn: 0.0007402\ttotal: 1.42s\tremaining: 98.4ms\n",
      "187:\tlearn: 0.0007402\ttotal: 1.42s\tremaining: 90.9ms\n",
      "188:\tlearn: 0.0007401\ttotal: 1.43s\tremaining: 83.3ms\n",
      "189:\tlearn: 0.0007401\ttotal: 1.44s\tremaining: 75.8ms\n",
      "190:\tlearn: 0.0007401\ttotal: 1.45s\tremaining: 68.2ms\n",
      "191:\tlearn: 0.0007401\ttotal: 1.46s\tremaining: 60.7ms\n",
      "192:\tlearn: 0.0007401\ttotal: 1.46s\tremaining: 53.1ms\n",
      "193:\tlearn: 0.0007401\ttotal: 1.47s\tremaining: 45.5ms\n",
      "194:\tlearn: 0.0007072\ttotal: 1.48s\tremaining: 37.9ms\n",
      "195:\tlearn: 0.0007072\ttotal: 1.48s\tremaining: 30.3ms\n",
      "196:\tlearn: 0.0007072\ttotal: 1.49s\tremaining: 22.7ms\n",
      "197:\tlearn: 0.0007072\ttotal: 1.5s\tremaining: 15.1ms\n",
      "198:\tlearn: 0.0007072\ttotal: 1.5s\tremaining: 7.56ms\n",
      "199:\tlearn: 0.0007072\ttotal: 1.51s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb77e4582894f4382e6b1105353fdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5554693513069289, Recall = 0.925711743772242, Aging Rate = 0.770685053380783, Precision = 0.6005772005772005, f1 = 0.728513915631017\n",
      "Epoch 2: Train Loss = 0.3844648307434605, Recall = 0.8607651245551602, Aging Rate = 0.5340302491103203, Precision = 0.8059142024156601, f1 = 0.8324370832437082\n",
      "Epoch 3: Train Loss = 0.3142451308269942, Recall = 0.8985765124555161, Aging Rate = 0.5255782918149466, Precision = 0.8548455353364367, f1 = 0.876165690739536\n",
      "Epoch 4: Train Loss = 0.25997526423999, Recall = 0.9159252669039146, Aging Rate = 0.5186832740213523, Precision = 0.8829331046312179, f1 = 0.8991266375545852\n",
      "Epoch 5: Train Loss = 0.2191449311492282, Recall = 0.9297153024911032, Aging Rate = 0.5077846975088968, Precision = 0.9154621112571178, f1 = 0.9225336570293534\n",
      "Test Loss = 0.17496948617633126, Recall = 0.9568505338078291, Aging Rate = 0.5048932384341637, precision = 0.9475770925110132\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.16567522913111488, Recall = 0.9524021352313167, Aging Rate = 0.5013345195729537, Precision = 0.9498669032830523, f1 = 0.9511328298533985\n",
      "Epoch 7: Train Loss = 0.13312243291067483, Recall = 0.9710854092526691, Aging Rate = 0.5044483985765125, Precision = 0.9625220458553791, f1 = 0.966784765279008\n",
      "Epoch 8: Train Loss = 0.10823645193487724, Recall = 0.9795373665480427, Aging Rate = 0.5020017793594306, Precision = 0.9756313690739921, f1 = 0.9775804661487236\n",
      "Epoch 9: Train Loss = 0.08932470295618013, Recall = 0.9866548042704626, Aging Rate = 0.5033362989323843, Precision = 0.9801148917366328, f1 = 0.9833739747284417\n",
      "Epoch 10: Train Loss = 0.07518200109649807, Recall = 0.9893238434163701, Aging Rate = 0.501779359430605, Precision = 0.9858156028368794, f1 = 0.9875666074600357\n",
      "Test Loss = 0.06280482704900338, Recall = 0.9919928825622776, Aging Rate = 0.5, precision = 0.9919928825622776\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06294288486905157, Recall = 0.9924377224199288, Aging Rate = 0.5020017793594306, Precision = 0.9884802835622508, f1 = 0.9904550499445061\n",
      "Epoch 12: Train Loss = 0.05438977909469944, Recall = 0.9951067615658363, Aging Rate = 0.5015569395017794, Precision = 0.9920177383592018, f1 = 0.9935598489895626\n",
      "Epoch 13: Train Loss = 0.04719719497546607, Recall = 0.9973309608540926, Aging Rate = 0.5004448398576512, Precision = 0.9964444444444445, f1 = 0.9968875055580259\n",
      "Epoch 14: Train Loss = 0.04233336790706763, Recall = 0.998220640569395, Aging Rate = 0.5011120996441281, Precision = 0.996005326231691, f1 = 0.9971117529437902\n",
      "Epoch 15: Train Loss = 0.036859633725839155, Recall = 0.998220640569395, Aging Rate = 0.5004448398576512, Precision = 0.9973333333333333, f1 = 0.9977767896843039\n",
      "Test Loss = 0.033421054495717285, Recall = 0.9995551601423488, Aging Rate = 0.5011120996441281, precision = 0.9973368841544608\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.03353150020887101, Recall = 0.9991103202846975, Aging Rate = 0.5004448398576512, Precision = 0.9982222222222222, f1 = 0.9986660738105825\n",
      "Epoch 17: Train Loss = 0.03079501488428311, Recall = 0.9991103202846975, Aging Rate = 0.5004448398576512, Precision = 0.9982222222222222, f1 = 0.9986660738105825\n",
      "Epoch 18: Train Loss = 0.028360146611557738, Recall = 0.9991103202846975, Aging Rate = 0.5004448398576512, Precision = 0.9982222222222222, f1 = 0.9986660738105825\n",
      "Epoch 19: Train Loss = 0.026759228924309233, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 20: Train Loss = 0.025003712604425556, Recall = 0.9986654804270463, Aging Rate = 0.5, Precision = 0.9986654804270463, f1 = 0.9986654804270463\n",
      "Test Loss = 0.022046530082344584, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.023442967460089732, Recall = 0.9995551601423488, Aging Rate = 0.5004448398576512, Precision = 0.9986666666666667, f1 = 0.9991107158737217\n",
      "Epoch 22: Train Loss = 0.02274254533160624, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 23: Train Loss = 0.021377843744999785, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 24: Train Loss = 0.020856708997456206, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.02025829923746849, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.018119905405807112, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.019471447981125094, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 27: Train Loss = 0.01930255656299642, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 28: Train Loss = 0.018478408754137064, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 29: Train Loss = 0.01825153391834786, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.018274064725960912, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.015882081372060174, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.017797070127842266, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 32: Train Loss = 0.01776144354607499, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.01753225621976144, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 34: Train Loss = 0.017193037529353358, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.017992449659204567, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.015802007821451514, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.016816386024987993, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 37: Train Loss = 0.016778639593953763, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 38: Train Loss = 0.016696506739988445, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.016618732962876664, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 40: Train Loss = 0.016635812069968822, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.014422338189113076, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.01611712453986296, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 42: Train Loss = 0.01659650266753821, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.016179490403613585, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.01598071052404485, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 45: Train Loss = 0.016583254869059524, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014740120709021956, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 46: Train Loss = 0.016385575355398484, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 47: Train Loss = 0.016412337496213854, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 48: Train Loss = 0.015626990852804584, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.015988490380200945, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 50: Train Loss = 0.016228321843591662, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.01457980596145542, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 51: Train Loss = 0.016451881953193623, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.015831274434499894, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 53: Train Loss = 0.015942621826992978, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.01540601801365826, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.016888346059042363, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.014888536967204559, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 56: Train Loss = 0.016111987145796577, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 57: Train Loss = 0.015468992935858163, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015859751875953107, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 59: Train Loss = 0.015414765538826744, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 60: Train Loss = 0.015247411034948248, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01717158852085418, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 61: Train Loss = 0.015791078114689883, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 62: Train Loss = 0.01571465103333531, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 63: Train Loss = 0.0155000609123675, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015463067841752567, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015577966244592997, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.014094166106335633, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015988294323541728, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.015541284696407802, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015642283669941366, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 69: Train Loss = 0.015980548391058988, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 70: Train Loss = 0.015004237179124058, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014720815992864426, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 71: Train Loss = 0.015430010997713461, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 72: Train Loss = 0.015662539627707937, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 73: Train Loss = 0.015025318504122228, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015668135630198858, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 75: Train Loss = 0.0153511839842563, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013810997185864059, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3162833\ttotal: 8.52ms\tremaining: 1.7s\n",
      "1:\tlearn: 0.2270990\ttotal: 18ms\tremaining: 1.79s\n",
      "2:\tlearn: 0.1565156\ttotal: 26.7ms\tremaining: 1.75s\n",
      "3:\tlearn: 0.1332602\ttotal: 35.7ms\tremaining: 1.75s\n",
      "4:\tlearn: 0.1159801\ttotal: 45.9ms\tremaining: 1.79s\n",
      "5:\tlearn: 0.1038376\ttotal: 56.1ms\tremaining: 1.81s\n",
      "6:\tlearn: 0.0927010\ttotal: 64.4ms\tremaining: 1.78s\n",
      "7:\tlearn: 0.0838856\ttotal: 72.9ms\tremaining: 1.75s\n",
      "8:\tlearn: 0.0734201\ttotal: 81ms\tremaining: 1.72s\n",
      "9:\tlearn: 0.0648476\ttotal: 89.9ms\tremaining: 1.71s\n",
      "10:\tlearn: 0.0583761\ttotal: 97.7ms\tremaining: 1.68s\n",
      "11:\tlearn: 0.0524957\ttotal: 106ms\tremaining: 1.67s\n",
      "12:\tlearn: 0.0485324\ttotal: 114ms\tremaining: 1.64s\n",
      "13:\tlearn: 0.0446136\ttotal: 123ms\tremaining: 1.63s\n",
      "14:\tlearn: 0.0405143\ttotal: 131ms\tremaining: 1.62s\n",
      "15:\tlearn: 0.0379094\ttotal: 140ms\tremaining: 1.61s\n",
      "16:\tlearn: 0.0349058\ttotal: 149ms\tremaining: 1.6s\n",
      "17:\tlearn: 0.0313343\ttotal: 157ms\tremaining: 1.59s\n",
      "18:\tlearn: 0.0292287\ttotal: 165ms\tremaining: 1.57s\n",
      "19:\tlearn: 0.0273860\ttotal: 172ms\tremaining: 1.55s\n",
      "20:\tlearn: 0.0249478\ttotal: 180ms\tremaining: 1.54s\n",
      "21:\tlearn: 0.0229172\ttotal: 187ms\tremaining: 1.52s\n",
      "22:\tlearn: 0.0214320\ttotal: 195ms\tremaining: 1.5s\n",
      "23:\tlearn: 0.0194150\ttotal: 203ms\tremaining: 1.49s\n",
      "24:\tlearn: 0.0179939\ttotal: 210ms\tremaining: 1.47s\n",
      "25:\tlearn: 0.0171084\ttotal: 217ms\tremaining: 1.45s\n",
      "26:\tlearn: 0.0157833\ttotal: 224ms\tremaining: 1.43s\n",
      "27:\tlearn: 0.0146592\ttotal: 231ms\tremaining: 1.42s\n",
      "28:\tlearn: 0.0133356\ttotal: 238ms\tremaining: 1.41s\n",
      "29:\tlearn: 0.0121946\ttotal: 246ms\tremaining: 1.39s\n",
      "30:\tlearn: 0.0113232\ttotal: 253ms\tremaining: 1.38s\n",
      "31:\tlearn: 0.0106182\ttotal: 260ms\tremaining: 1.36s\n",
      "32:\tlearn: 0.0099310\ttotal: 268ms\tremaining: 1.35s\n",
      "33:\tlearn: 0.0091152\ttotal: 276ms\tremaining: 1.34s\n",
      "34:\tlearn: 0.0083767\ttotal: 283ms\tremaining: 1.33s\n",
      "35:\tlearn: 0.0077766\ttotal: 291ms\tremaining: 1.33s\n",
      "36:\tlearn: 0.0073511\ttotal: 299ms\tremaining: 1.32s\n",
      "37:\tlearn: 0.0068667\ttotal: 307ms\tremaining: 1.31s\n",
      "38:\tlearn: 0.0062955\ttotal: 314ms\tremaining: 1.29s\n",
      "39:\tlearn: 0.0059523\ttotal: 321ms\tremaining: 1.28s\n",
      "40:\tlearn: 0.0055441\ttotal: 327ms\tremaining: 1.27s\n",
      "41:\tlearn: 0.0052546\ttotal: 334ms\tremaining: 1.26s\n",
      "42:\tlearn: 0.0049463\ttotal: 342ms\tremaining: 1.25s\n",
      "43:\tlearn: 0.0045469\ttotal: 349ms\tremaining: 1.24s\n",
      "44:\tlearn: 0.0043362\ttotal: 356ms\tremaining: 1.22s\n",
      "45:\tlearn: 0.0040622\ttotal: 363ms\tremaining: 1.21s\n",
      "46:\tlearn: 0.0037328\ttotal: 370ms\tremaining: 1.2s\n",
      "47:\tlearn: 0.0035172\ttotal: 378ms\tremaining: 1.2s\n",
      "48:\tlearn: 0.0033606\ttotal: 386ms\tremaining: 1.19s\n",
      "49:\tlearn: 0.0032305\ttotal: 393ms\tremaining: 1.18s\n",
      "50:\tlearn: 0.0030227\ttotal: 402ms\tremaining: 1.17s\n",
      "51:\tlearn: 0.0028750\ttotal: 410ms\tremaining: 1.17s\n",
      "52:\tlearn: 0.0026742\ttotal: 418ms\tremaining: 1.16s\n",
      "53:\tlearn: 0.0025394\ttotal: 426ms\tremaining: 1.15s\n",
      "54:\tlearn: 0.0023991\ttotal: 434ms\tremaining: 1.14s\n",
      "55:\tlearn: 0.0022744\ttotal: 443ms\tremaining: 1.14s\n",
      "56:\tlearn: 0.0021511\ttotal: 451ms\tremaining: 1.13s\n",
      "57:\tlearn: 0.0020620\ttotal: 459ms\tremaining: 1.12s\n",
      "58:\tlearn: 0.0019659\ttotal: 466ms\tremaining: 1.11s\n",
      "59:\tlearn: 0.0019078\ttotal: 473ms\tremaining: 1.1s\n",
      "60:\tlearn: 0.0017982\ttotal: 481ms\tremaining: 1.09s\n",
      "61:\tlearn: 0.0017045\ttotal: 488ms\tremaining: 1.09s\n",
      "62:\tlearn: 0.0016392\ttotal: 496ms\tremaining: 1.08s\n",
      "63:\tlearn: 0.0015615\ttotal: 503ms\tremaining: 1.07s\n",
      "64:\tlearn: 0.0015150\ttotal: 510ms\tremaining: 1.06s\n",
      "65:\tlearn: 0.0014478\ttotal: 518ms\tremaining: 1.05s\n",
      "66:\tlearn: 0.0013960\ttotal: 525ms\tremaining: 1.04s\n",
      "67:\tlearn: 0.0013389\ttotal: 533ms\tremaining: 1.03s\n",
      "68:\tlearn: 0.0012886\ttotal: 541ms\tremaining: 1.03s\n",
      "69:\tlearn: 0.0012886\ttotal: 548ms\tremaining: 1.02s\n",
      "70:\tlearn: 0.0012222\ttotal: 555ms\tremaining: 1.01s\n",
      "71:\tlearn: 0.0012222\ttotal: 562ms\tremaining: 998ms\n",
      "72:\tlearn: 0.0012221\ttotal: 568ms\tremaining: 989ms\n",
      "73:\tlearn: 0.0011814\ttotal: 576ms\tremaining: 980ms\n",
      "74:\tlearn: 0.0011814\ttotal: 582ms\tremaining: 970ms\n",
      "75:\tlearn: 0.0011103\ttotal: 590ms\tremaining: 962ms\n",
      "76:\tlearn: 0.0011102\ttotal: 597ms\tremaining: 953ms\n",
      "77:\tlearn: 0.0011101\ttotal: 604ms\tremaining: 945ms\n",
      "78:\tlearn: 0.0011101\ttotal: 611ms\tremaining: 936ms\n",
      "79:\tlearn: 0.0010570\ttotal: 618ms\tremaining: 927ms\n",
      "80:\tlearn: 0.0010152\ttotal: 626ms\tremaining: 919ms\n",
      "81:\tlearn: 0.0010151\ttotal: 633ms\tremaining: 911ms\n",
      "82:\tlearn: 0.0009736\ttotal: 640ms\tremaining: 903ms\n",
      "83:\tlearn: 0.0009735\ttotal: 648ms\tremaining: 896ms\n",
      "84:\tlearn: 0.0009735\ttotal: 656ms\tremaining: 887ms\n",
      "85:\tlearn: 0.0009354\ttotal: 663ms\tremaining: 879ms\n",
      "86:\tlearn: 0.0009354\ttotal: 670ms\tremaining: 871ms\n",
      "87:\tlearn: 0.0009354\ttotal: 678ms\tremaining: 863ms\n",
      "88:\tlearn: 0.0009354\ttotal: 685ms\tremaining: 855ms\n",
      "89:\tlearn: 0.0008933\ttotal: 693ms\tremaining: 847ms\n",
      "90:\tlearn: 0.0008933\ttotal: 700ms\tremaining: 839ms\n",
      "91:\tlearn: 0.0008933\ttotal: 708ms\tremaining: 832ms\n",
      "92:\tlearn: 0.0008743\ttotal: 716ms\tremaining: 824ms\n",
      "93:\tlearn: 0.0008743\ttotal: 725ms\tremaining: 818ms\n",
      "94:\tlearn: 0.0008438\ttotal: 733ms\tremaining: 811ms\n",
      "95:\tlearn: 0.0008438\ttotal: 741ms\tremaining: 803ms\n",
      "96:\tlearn: 0.0008437\ttotal: 750ms\tremaining: 796ms\n",
      "97:\tlearn: 0.0008437\ttotal: 758ms\tremaining: 789ms\n",
      "98:\tlearn: 0.0008437\ttotal: 765ms\tremaining: 781ms\n",
      "99:\tlearn: 0.0008437\ttotal: 773ms\tremaining: 773ms\n",
      "100:\tlearn: 0.0008437\ttotal: 781ms\tremaining: 765ms\n",
      "101:\tlearn: 0.0008437\ttotal: 789ms\tremaining: 758ms\n",
      "102:\tlearn: 0.0008437\ttotal: 796ms\tremaining: 749ms\n",
      "103:\tlearn: 0.0008436\ttotal: 803ms\tremaining: 741ms\n",
      "104:\tlearn: 0.0008436\ttotal: 811ms\tremaining: 733ms\n",
      "105:\tlearn: 0.0008436\ttotal: 818ms\tremaining: 726ms\n",
      "106:\tlearn: 0.0008436\ttotal: 826ms\tremaining: 718ms\n",
      "107:\tlearn: 0.0008436\ttotal: 833ms\tremaining: 710ms\n",
      "108:\tlearn: 0.0008435\ttotal: 840ms\tremaining: 702ms\n",
      "109:\tlearn: 0.0008435\ttotal: 848ms\tremaining: 694ms\n",
      "110:\tlearn: 0.0008435\ttotal: 856ms\tremaining: 686ms\n",
      "111:\tlearn: 0.0008435\ttotal: 863ms\tremaining: 678ms\n",
      "112:\tlearn: 0.0008435\ttotal: 871ms\tremaining: 671ms\n",
      "113:\tlearn: 0.0008434\ttotal: 878ms\tremaining: 663ms\n",
      "114:\tlearn: 0.0008434\ttotal: 885ms\tremaining: 654ms\n",
      "115:\tlearn: 0.0008434\ttotal: 893ms\tremaining: 647ms\n",
      "116:\tlearn: 0.0008433\ttotal: 901ms\tremaining: 639ms\n",
      "117:\tlearn: 0.0008433\ttotal: 908ms\tremaining: 631ms\n",
      "118:\tlearn: 0.0008433\ttotal: 916ms\tremaining: 624ms\n",
      "119:\tlearn: 0.0008433\ttotal: 923ms\tremaining: 616ms\n",
      "120:\tlearn: 0.0008433\ttotal: 931ms\tremaining: 608ms\n",
      "121:\tlearn: 0.0008432\ttotal: 939ms\tremaining: 600ms\n",
      "122:\tlearn: 0.0008432\ttotal: 946ms\tremaining: 592ms\n",
      "123:\tlearn: 0.0008432\ttotal: 954ms\tremaining: 584ms\n",
      "124:\tlearn: 0.0008432\ttotal: 961ms\tremaining: 577ms\n",
      "125:\tlearn: 0.0008432\ttotal: 969ms\tremaining: 569ms\n",
      "126:\tlearn: 0.0008432\ttotal: 977ms\tremaining: 561ms\n",
      "127:\tlearn: 0.0008431\ttotal: 984ms\tremaining: 553ms\n",
      "128:\tlearn: 0.0008431\ttotal: 992ms\tremaining: 546ms\n",
      "129:\tlearn: 0.0008430\ttotal: 1s\tremaining: 539ms\n",
      "130:\tlearn: 0.0008430\ttotal: 1.01s\tremaining: 531ms\n",
      "131:\tlearn: 0.0008430\ttotal: 1.01s\tremaining: 523ms\n",
      "132:\tlearn: 0.0008430\ttotal: 1.02s\tremaining: 514ms\n",
      "133:\tlearn: 0.0008429\ttotal: 1.03s\tremaining: 506ms\n",
      "134:\tlearn: 0.0008429\ttotal: 1.03s\tremaining: 498ms\n",
      "135:\tlearn: 0.0008429\ttotal: 1.04s\tremaining: 490ms\n",
      "136:\tlearn: 0.0008429\ttotal: 1.05s\tremaining: 482ms\n",
      "137:\tlearn: 0.0008429\ttotal: 1.05s\tremaining: 474ms\n",
      "138:\tlearn: 0.0008429\ttotal: 1.06s\tremaining: 465ms\n",
      "139:\tlearn: 0.0008429\ttotal: 1.07s\tremaining: 457ms\n",
      "140:\tlearn: 0.0008429\ttotal: 1.07s\tremaining: 449ms\n",
      "141:\tlearn: 0.0008428\ttotal: 1.08s\tremaining: 441ms\n",
      "142:\tlearn: 0.0008428\ttotal: 1.09s\tremaining: 433ms\n",
      "143:\tlearn: 0.0008428\ttotal: 1.09s\tremaining: 426ms\n",
      "144:\tlearn: 0.0008428\ttotal: 1.1s\tremaining: 417ms\n",
      "145:\tlearn: 0.0008428\ttotal: 1.11s\tremaining: 409ms\n",
      "146:\tlearn: 0.0008428\ttotal: 1.11s\tremaining: 401ms\n",
      "147:\tlearn: 0.0008427\ttotal: 1.12s\tremaining: 394ms\n",
      "148:\tlearn: 0.0008427\ttotal: 1.13s\tremaining: 386ms\n",
      "149:\tlearn: 0.0008427\ttotal: 1.13s\tremaining: 378ms\n",
      "150:\tlearn: 0.0008427\ttotal: 1.14s\tremaining: 370ms\n",
      "151:\tlearn: 0.0008427\ttotal: 1.15s\tremaining: 363ms\n",
      "152:\tlearn: 0.0008426\ttotal: 1.16s\tremaining: 355ms\n",
      "153:\tlearn: 0.0008426\ttotal: 1.16s\tremaining: 347ms\n",
      "154:\tlearn: 0.0008426\ttotal: 1.17s\tremaining: 339ms\n",
      "155:\tlearn: 0.0008426\ttotal: 1.17s\tremaining: 331ms\n",
      "156:\tlearn: 0.0008426\ttotal: 1.18s\tremaining: 324ms\n",
      "157:\tlearn: 0.0008426\ttotal: 1.19s\tremaining: 316ms\n",
      "158:\tlearn: 0.0008425\ttotal: 1.19s\tremaining: 308ms\n",
      "159:\tlearn: 0.0008425\ttotal: 1.2s\tremaining: 300ms\n",
      "160:\tlearn: 0.0008425\ttotal: 1.21s\tremaining: 292ms\n",
      "161:\tlearn: 0.0008425\ttotal: 1.21s\tremaining: 285ms\n",
      "162:\tlearn: 0.0008425\ttotal: 1.22s\tremaining: 277ms\n",
      "163:\tlearn: 0.0008424\ttotal: 1.23s\tremaining: 269ms\n",
      "164:\tlearn: 0.0008424\ttotal: 1.23s\tremaining: 262ms\n",
      "165:\tlearn: 0.0008424\ttotal: 1.24s\tremaining: 254ms\n",
      "166:\tlearn: 0.0008424\ttotal: 1.25s\tremaining: 247ms\n",
      "167:\tlearn: 0.0008423\ttotal: 1.26s\tremaining: 239ms\n",
      "168:\tlearn: 0.0008423\ttotal: 1.26s\tremaining: 232ms\n",
      "169:\tlearn: 0.0008422\ttotal: 1.27s\tremaining: 225ms\n",
      "170:\tlearn: 0.0008422\ttotal: 1.28s\tremaining: 217ms\n",
      "171:\tlearn: 0.0008422\ttotal: 1.29s\tremaining: 210ms\n",
      "172:\tlearn: 0.0008422\ttotal: 1.29s\tremaining: 202ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173:\tlearn: 0.0008422\ttotal: 1.3s\tremaining: 194ms\n",
      "174:\tlearn: 0.0008422\ttotal: 1.31s\tremaining: 187ms\n",
      "175:\tlearn: 0.0008422\ttotal: 1.31s\tremaining: 179ms\n",
      "176:\tlearn: 0.0008422\ttotal: 1.32s\tremaining: 172ms\n",
      "177:\tlearn: 0.0008422\ttotal: 1.33s\tremaining: 164ms\n",
      "178:\tlearn: 0.0008422\ttotal: 1.33s\tremaining: 157ms\n",
      "179:\tlearn: 0.0008421\ttotal: 1.34s\tremaining: 149ms\n",
      "180:\tlearn: 0.0008421\ttotal: 1.35s\tremaining: 142ms\n",
      "181:\tlearn: 0.0008421\ttotal: 1.35s\tremaining: 134ms\n",
      "182:\tlearn: 0.0008421\ttotal: 1.36s\tremaining: 127ms\n",
      "183:\tlearn: 0.0008421\ttotal: 1.37s\tremaining: 119ms\n",
      "184:\tlearn: 0.0008127\ttotal: 1.38s\tremaining: 112ms\n",
      "185:\tlearn: 0.0008127\ttotal: 1.38s\tremaining: 104ms\n",
      "186:\tlearn: 0.0008127\ttotal: 1.39s\tremaining: 96.7ms\n",
      "187:\tlearn: 0.0008126\ttotal: 1.4s\tremaining: 89.3ms\n",
      "188:\tlearn: 0.0008126\ttotal: 1.41s\tremaining: 81.8ms\n",
      "189:\tlearn: 0.0008126\ttotal: 1.41s\tremaining: 74.4ms\n",
      "190:\tlearn: 0.0008126\ttotal: 1.42s\tremaining: 66.9ms\n",
      "191:\tlearn: 0.0008126\ttotal: 1.43s\tremaining: 59.5ms\n",
      "192:\tlearn: 0.0008126\ttotal: 1.43s\tremaining: 52ms\n",
      "193:\tlearn: 0.0008126\ttotal: 1.44s\tremaining: 44.5ms\n",
      "194:\tlearn: 0.0008125\ttotal: 1.45s\tremaining: 37.1ms\n",
      "195:\tlearn: 0.0008125\ttotal: 1.45s\tremaining: 29.7ms\n",
      "196:\tlearn: 0.0008125\ttotal: 1.46s\tremaining: 22.2ms\n",
      "197:\tlearn: 0.0008125\ttotal: 1.47s\tremaining: 14.8ms\n",
      "198:\tlearn: 0.0008125\ttotal: 1.47s\tremaining: 7.41ms\n",
      "199:\tlearn: 0.0008124\ttotal: 1.48s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dce366eb6e94475aaf69470dd2f9c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5441222786903381, Recall = 0.8505338078291815, Aging Rate = 0.6563612099644128, Precision = 0.6479159606912911, f1 = 0.7355260627043663\n",
      "Epoch 2: Train Loss = 0.38203495899977635, Recall = 0.87144128113879, Aging Rate = 0.5400355871886121, Precision = 0.806836902800659, f1 = 0.8378956372968349\n",
      "Epoch 3: Train Loss = 0.3122306168079376, Recall = 0.8972419928825622, Aging Rate = 0.5213523131672598, Precision = 0.860494880546075, f1 = 0.8784843205574911\n",
      "Epoch 4: Train Loss = 0.25594353946290405, Recall = 0.9177046263345195, Aging Rate = 0.5162366548042705, Precision = 0.8888410168031021, f1 = 0.9030422411906326\n",
      "Epoch 5: Train Loss = 0.2110457550800568, Recall = 0.9359430604982206, Aging Rate = 0.5108985765124555, Precision = 0.9159773617762299, f1 = 0.9258525852585259\n",
      "Test Loss = 0.1693500891539974, Recall = 0.9497330960854092, Aging Rate = 0.49644128113879005, precision = 0.9565412186379928\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.15861135344700458, Recall = 0.9564056939501779, Aging Rate = 0.50355871886121, Precision = 0.9496466431095406, f1 = 0.953014184397163\n",
      "Epoch 7: Train Loss = 0.12643063110813127, Recall = 0.9755338078291815, Aging Rate = 0.5028914590747331, Precision = 0.9699248120300752, f1 = 0.9727212242182302\n",
      "Epoch 8: Train Loss = 0.10250844590339372, Recall = 0.983540925266904, Aging Rate = 0.5033362989323843, Precision = 0.9770216526734423, f1 = 0.980270450011084\n",
      "Epoch 9: Train Loss = 0.08384822676243307, Recall = 0.9879893238434164, Aging Rate = 0.5031138790035588, Precision = 0.9818744473916887, f1 = 0.9849223946784922\n",
      "Epoch 10: Train Loss = 0.06952238806732185, Recall = 0.9924377224199288, Aging Rate = 0.5020017793594306, Precision = 0.9884802835622508, f1 = 0.9904550499445061\n",
      "Test Loss = 0.05904663423096158, Recall = 0.99644128113879, Aging Rate = 0.5028914590747331, precision = 0.9907120743034056\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05945938840190287, Recall = 0.9968861209964412, Aging Rate = 0.5024466192170819, Precision = 0.9920318725099602, f1 = 0.9944530729975594\n",
      "Epoch 12: Train Loss = 0.050776228520793845, Recall = 0.9973309608540926, Aging Rate = 0.5015569395017794, Precision = 0.9942350332594235, f1 = 0.9957805907172997\n",
      "Epoch 13: Train Loss = 0.04445226756968532, Recall = 0.9968861209964412, Aging Rate = 0.5004448398576512, Precision = 0.996, f1 = 0.9964428634948866\n",
      "Epoch 14: Train Loss = 0.03895686944661616, Recall = 0.9986654804270463, Aging Rate = 0.501779359430605, Precision = 0.9951241134751773, f1 = 0.9968916518650089\n",
      "Epoch 15: Train Loss = 0.034751694662715195, Recall = 0.9986654804270463, Aging Rate = 0.5004448398576512, Precision = 0.9977777777777778, f1 = 0.9982214317474434\n",
      "Test Loss = 0.03154623617825771, Recall = 0.9991103202846975, Aging Rate = 0.5004448398576512, precision = 0.9982222222222222\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.032433773089346085, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 17: Train Loss = 0.029265500890137464, Recall = 0.9986654804270463, Aging Rate = 0.5, Precision = 0.9986654804270463, f1 = 0.9986654804270463\n",
      "Epoch 18: Train Loss = 0.026892477616641233, Recall = 0.9991103202846975, Aging Rate = 0.5, Precision = 0.9991103202846975, f1 = 0.9991103202846975\n",
      "Epoch 19: Train Loss = 0.02520423181673174, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 20: Train Loss = 0.023835294449111956, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.022060419462647727, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.02286410568699612, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 22: Train Loss = 0.021739531348184335, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.021051321152160175, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 24: Train Loss = 0.020170866087529795, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.019636325385699916, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.017778855766703332, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.018878850785955734, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.018283189768203638, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.01840191471367226, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.01820413817011907, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.017741178410969595, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.015807166377420526, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.017923403090509018, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 32: Train Loss = 0.01751474503533802, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.01705543738972144, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.01731984220193268, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.017173494673019204, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.015014168687058725, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.01710942183087623, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.016544290626043737, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 38: Train Loss = 0.016211709902485917, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 39: Train Loss = 0.01617881153242868, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 40: Train Loss = 0.01674818789778655, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014769550233920273, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 41: Train Loss = 0.016381567052409743, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.015914588626407856, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.015959292819252853, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.016213684790234124, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.0164024783464089, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013852415894724933, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016200337575556334, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.015543941325537887, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016354133772706857, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 49: Train Loss = 0.01640598029494816, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016038864821169087, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.016423747619594118, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss = 0.015558693021058612, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.015389045173454751, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.015442637551005837, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015616618405059776, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015833052203350857, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01375784812448712, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016387126133429197, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 57: Train Loss = 0.01557829286704284, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015160163944036934, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.01606466955574912, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.016068685965552873, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013905107116598463, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015314919658641585, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.01600363853907882, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.014894246239732168, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 64: Train Loss = 0.015041339489568383, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.01520671689173611, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013542761870417943, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015469742093725774, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.01524009467583163, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015659612739372508, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015315257758565008, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015478067404965065, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013312053349651264, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.015192489156307274, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0150458263117303, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.015393515657699829, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015295047912865983, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015081692503572995, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.013245092867293603, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3100808\ttotal: 8.65ms\tremaining: 1.72s\n",
      "1:\tlearn: 0.1912036\ttotal: 19.1ms\tremaining: 1.89s\n",
      "2:\tlearn: 0.1541439\ttotal: 27.9ms\tremaining: 1.83s\n",
      "3:\tlearn: 0.1295865\ttotal: 35.7ms\tremaining: 1.75s\n",
      "4:\tlearn: 0.1101562\ttotal: 44.8ms\tremaining: 1.75s\n",
      "5:\tlearn: 0.0973487\ttotal: 53.9ms\tremaining: 1.74s\n",
      "6:\tlearn: 0.0894781\ttotal: 63.7ms\tremaining: 1.76s\n",
      "7:\tlearn: 0.0781522\ttotal: 73.2ms\tremaining: 1.76s\n",
      "8:\tlearn: 0.0707239\ttotal: 82.1ms\tremaining: 1.74s\n",
      "9:\tlearn: 0.0624090\ttotal: 91.4ms\tremaining: 1.74s\n",
      "10:\tlearn: 0.0558132\ttotal: 100ms\tremaining: 1.72s\n",
      "11:\tlearn: 0.0512710\ttotal: 109ms\tremaining: 1.71s\n",
      "12:\tlearn: 0.0466681\ttotal: 118ms\tremaining: 1.7s\n",
      "13:\tlearn: 0.0418969\ttotal: 126ms\tremaining: 1.68s\n",
      "14:\tlearn: 0.0377535\ttotal: 135ms\tremaining: 1.67s\n",
      "15:\tlearn: 0.0345154\ttotal: 143ms\tremaining: 1.64s\n",
      "16:\tlearn: 0.0322602\ttotal: 151ms\tremaining: 1.63s\n",
      "17:\tlearn: 0.0294265\ttotal: 160ms\tremaining: 1.61s\n",
      "18:\tlearn: 0.0272346\ttotal: 168ms\tremaining: 1.6s\n",
      "19:\tlearn: 0.0249478\ttotal: 177ms\tremaining: 1.59s\n",
      "20:\tlearn: 0.0227064\ttotal: 185ms\tremaining: 1.58s\n",
      "21:\tlearn: 0.0204728\ttotal: 194ms\tremaining: 1.56s\n",
      "22:\tlearn: 0.0190936\ttotal: 203ms\tremaining: 1.56s\n",
      "23:\tlearn: 0.0175868\ttotal: 212ms\tremaining: 1.56s\n",
      "24:\tlearn: 0.0163001\ttotal: 220ms\tremaining: 1.54s\n",
      "25:\tlearn: 0.0146664\ttotal: 228ms\tremaining: 1.53s\n",
      "26:\tlearn: 0.0135944\ttotal: 236ms\tremaining: 1.51s\n",
      "27:\tlearn: 0.0122110\ttotal: 244ms\tremaining: 1.5s\n",
      "28:\tlearn: 0.0112809\ttotal: 253ms\tremaining: 1.49s\n",
      "29:\tlearn: 0.0103957\ttotal: 261ms\tremaining: 1.48s\n",
      "30:\tlearn: 0.0096139\ttotal: 269ms\tremaining: 1.47s\n",
      "31:\tlearn: 0.0090172\ttotal: 277ms\tremaining: 1.46s\n",
      "32:\tlearn: 0.0081887\ttotal: 285ms\tremaining: 1.44s\n",
      "33:\tlearn: 0.0075715\ttotal: 294ms\tremaining: 1.43s\n",
      "34:\tlearn: 0.0071367\ttotal: 303ms\tremaining: 1.43s\n",
      "35:\tlearn: 0.0065922\ttotal: 311ms\tremaining: 1.42s\n",
      "36:\tlearn: 0.0060853\ttotal: 319ms\tremaining: 1.4s\n",
      "37:\tlearn: 0.0056934\ttotal: 327ms\tremaining: 1.39s\n",
      "38:\tlearn: 0.0052833\ttotal: 335ms\tremaining: 1.38s\n",
      "39:\tlearn: 0.0048431\ttotal: 342ms\tremaining: 1.37s\n",
      "40:\tlearn: 0.0045502\ttotal: 350ms\tremaining: 1.36s\n",
      "41:\tlearn: 0.0042706\ttotal: 358ms\tremaining: 1.35s\n",
      "42:\tlearn: 0.0039891\ttotal: 365ms\tremaining: 1.33s\n",
      "43:\tlearn: 0.0037107\ttotal: 373ms\tremaining: 1.32s\n",
      "44:\tlearn: 0.0034559\ttotal: 381ms\tremaining: 1.31s\n",
      "45:\tlearn: 0.0032841\ttotal: 388ms\tremaining: 1.3s\n",
      "46:\tlearn: 0.0030796\ttotal: 396ms\tremaining: 1.29s\n",
      "47:\tlearn: 0.0029255\ttotal: 403ms\tremaining: 1.28s\n",
      "48:\tlearn: 0.0027491\ttotal: 410ms\tremaining: 1.26s\n",
      "49:\tlearn: 0.0026274\ttotal: 418ms\tremaining: 1.25s\n",
      "50:\tlearn: 0.0024779\ttotal: 425ms\tremaining: 1.24s\n",
      "51:\tlearn: 0.0023366\ttotal: 433ms\tremaining: 1.23s\n",
      "52:\tlearn: 0.0022224\ttotal: 441ms\tremaining: 1.22s\n",
      "53:\tlearn: 0.0020708\ttotal: 449ms\tremaining: 1.21s\n",
      "54:\tlearn: 0.0019793\ttotal: 457ms\tremaining: 1.21s\n",
      "55:\tlearn: 0.0019043\ttotal: 466ms\tremaining: 1.2s\n",
      "56:\tlearn: 0.0018311\ttotal: 474ms\tremaining: 1.19s\n",
      "57:\tlearn: 0.0017680\ttotal: 482ms\tremaining: 1.18s\n",
      "58:\tlearn: 0.0016527\ttotal: 490ms\tremaining: 1.17s\n",
      "59:\tlearn: 0.0015433\ttotal: 498ms\tremaining: 1.16s\n",
      "60:\tlearn: 0.0014520\ttotal: 506ms\tremaining: 1.15s\n",
      "61:\tlearn: 0.0013797\ttotal: 514ms\tremaining: 1.14s\n",
      "62:\tlearn: 0.0013244\ttotal: 521ms\tremaining: 1.13s\n",
      "63:\tlearn: 0.0012736\ttotal: 528ms\tremaining: 1.12s\n",
      "64:\tlearn: 0.0012736\ttotal: 535ms\tremaining: 1.11s\n",
      "65:\tlearn: 0.0012735\ttotal: 542ms\tremaining: 1.1s\n",
      "66:\tlearn: 0.0012167\ttotal: 550ms\tremaining: 1.09s\n",
      "67:\tlearn: 0.0011697\ttotal: 558ms\tremaining: 1.08s\n",
      "68:\tlearn: 0.0011175\ttotal: 566ms\tremaining: 1.07s\n",
      "69:\tlearn: 0.0011175\ttotal: 573ms\tremaining: 1.06s\n",
      "70:\tlearn: 0.0010613\ttotal: 581ms\tremaining: 1.05s\n",
      "71:\tlearn: 0.0010120\ttotal: 588ms\tremaining: 1.04s\n",
      "72:\tlearn: 0.0009694\ttotal: 596ms\tremaining: 1.04s\n",
      "73:\tlearn: 0.0009246\ttotal: 604ms\tremaining: 1.03s\n",
      "74:\tlearn: 0.0008848\ttotal: 612ms\tremaining: 1.02s\n",
      "75:\tlearn: 0.0008470\ttotal: 619ms\tremaining: 1.01s\n",
      "76:\tlearn: 0.0008469\ttotal: 626ms\tremaining: 999ms\n",
      "77:\tlearn: 0.0008469\ttotal: 632ms\tremaining: 989ms\n",
      "78:\tlearn: 0.0008469\ttotal: 639ms\tremaining: 979ms\n",
      "79:\tlearn: 0.0008469\ttotal: 645ms\tremaining: 967ms\n",
      "80:\tlearn: 0.0008468\ttotal: 650ms\tremaining: 955ms\n",
      "81:\tlearn: 0.0008468\ttotal: 658ms\tremaining: 946ms\n",
      "82:\tlearn: 0.0008468\ttotal: 665ms\tremaining: 937ms\n",
      "83:\tlearn: 0.0008468\ttotal: 672ms\tremaining: 928ms\n",
      "84:\tlearn: 0.0008468\ttotal: 679ms\tremaining: 918ms\n",
      "85:\tlearn: 0.0008468\ttotal: 683ms\tremaining: 906ms\n",
      "86:\tlearn: 0.0008468\ttotal: 691ms\tremaining: 897ms\n",
      "87:\tlearn: 0.0008467\ttotal: 698ms\tremaining: 888ms\n",
      "88:\tlearn: 0.0008467\ttotal: 705ms\tremaining: 879ms\n",
      "89:\tlearn: 0.0008467\ttotal: 711ms\tremaining: 869ms\n",
      "90:\tlearn: 0.0008467\ttotal: 717ms\tremaining: 858ms\n",
      "91:\tlearn: 0.0008467\ttotal: 723ms\tremaining: 848ms\n",
      "92:\tlearn: 0.0008467\ttotal: 730ms\tremaining: 840ms\n",
      "93:\tlearn: 0.0008466\ttotal: 735ms\tremaining: 829ms\n",
      "94:\tlearn: 0.0008466\ttotal: 743ms\tremaining: 821ms\n",
      "95:\tlearn: 0.0008110\ttotal: 751ms\tremaining: 814ms\n",
      "96:\tlearn: 0.0008110\ttotal: 759ms\tremaining: 806ms\n",
      "97:\tlearn: 0.0008109\ttotal: 767ms\tremaining: 798ms\n",
      "98:\tlearn: 0.0008109\ttotal: 775ms\tremaining: 790ms\n",
      "99:\tlearn: 0.0008108\ttotal: 782ms\tremaining: 782ms\n",
      "100:\tlearn: 0.0008108\ttotal: 790ms\tremaining: 775ms\n",
      "101:\tlearn: 0.0008108\ttotal: 799ms\tremaining: 767ms\n",
      "102:\tlearn: 0.0008107\ttotal: 806ms\tremaining: 759ms\n",
      "103:\tlearn: 0.0008107\ttotal: 814ms\tremaining: 751ms\n",
      "104:\tlearn: 0.0008107\ttotal: 821ms\tremaining: 743ms\n",
      "105:\tlearn: 0.0008106\ttotal: 829ms\tremaining: 735ms\n",
      "106:\tlearn: 0.0008106\ttotal: 836ms\tremaining: 727ms\n",
      "107:\tlearn: 0.0008106\ttotal: 843ms\tremaining: 718ms\n",
      "108:\tlearn: 0.0008106\ttotal: 850ms\tremaining: 710ms\n",
      "109:\tlearn: 0.0008106\ttotal: 858ms\tremaining: 702ms\n",
      "110:\tlearn: 0.0008106\ttotal: 866ms\tremaining: 694ms\n",
      "111:\tlearn: 0.0008105\ttotal: 873ms\tremaining: 686ms\n",
      "112:\tlearn: 0.0008105\ttotal: 881ms\tremaining: 678ms\n",
      "113:\tlearn: 0.0008105\ttotal: 889ms\tremaining: 671ms\n",
      "114:\tlearn: 0.0008105\ttotal: 896ms\tremaining: 663ms\n",
      "115:\tlearn: 0.0008104\ttotal: 905ms\tremaining: 655ms\n",
      "116:\tlearn: 0.0008104\ttotal: 913ms\tremaining: 647ms\n",
      "117:\tlearn: 0.0008104\ttotal: 921ms\tremaining: 640ms\n",
      "118:\tlearn: 0.0008104\ttotal: 929ms\tremaining: 632ms\n",
      "119:\tlearn: 0.0008104\ttotal: 936ms\tremaining: 624ms\n",
      "120:\tlearn: 0.0008104\ttotal: 944ms\tremaining: 616ms\n",
      "121:\tlearn: 0.0008104\ttotal: 952ms\tremaining: 609ms\n",
      "122:\tlearn: 0.0008104\ttotal: 959ms\tremaining: 601ms\n",
      "123:\tlearn: 0.0008103\ttotal: 967ms\tremaining: 593ms\n",
      "124:\tlearn: 0.0008103\ttotal: 975ms\tremaining: 585ms\n",
      "125:\tlearn: 0.0008103\ttotal: 984ms\tremaining: 578ms\n",
      "126:\tlearn: 0.0008103\ttotal: 992ms\tremaining: 570ms\n",
      "127:\tlearn: 0.0008103\ttotal: 1000ms\tremaining: 562ms\n",
      "128:\tlearn: 0.0008102\ttotal: 1.01s\tremaining: 554ms\n",
      "129:\tlearn: 0.0008101\ttotal: 1.01s\tremaining: 546ms\n",
      "130:\tlearn: 0.0008101\ttotal: 1.02s\tremaining: 538ms\n",
      "131:\tlearn: 0.0008101\ttotal: 1.03s\tremaining: 531ms\n",
      "132:\tlearn: 0.0008100\ttotal: 1.04s\tremaining: 522ms\n",
      "133:\tlearn: 0.0008100\ttotal: 1.04s\tremaining: 515ms\n",
      "134:\tlearn: 0.0008100\ttotal: 1.05s\tremaining: 507ms\n",
      "135:\tlearn: 0.0008100\ttotal: 1.06s\tremaining: 499ms\n",
      "136:\tlearn: 0.0008099\ttotal: 1.07s\tremaining: 491ms\n",
      "137:\tlearn: 0.0008099\ttotal: 1.07s\tremaining: 482ms\n",
      "138:\tlearn: 0.0008099\ttotal: 1.08s\tremaining: 474ms\n",
      "139:\tlearn: 0.0008099\ttotal: 1.09s\tremaining: 466ms\n",
      "140:\tlearn: 0.0008099\ttotal: 1.09s\tremaining: 458ms\n",
      "141:\tlearn: 0.0008099\ttotal: 1.1s\tremaining: 450ms\n",
      "142:\tlearn: 0.0008099\ttotal: 1.11s\tremaining: 442ms\n",
      "143:\tlearn: 0.0008098\ttotal: 1.11s\tremaining: 433ms\n",
      "144:\tlearn: 0.0008098\ttotal: 1.12s\tremaining: 425ms\n",
      "145:\tlearn: 0.0008098\ttotal: 1.13s\tremaining: 417ms\n",
      "146:\tlearn: 0.0008098\ttotal: 1.14s\tremaining: 409ms\n",
      "147:\tlearn: 0.0008098\ttotal: 1.14s\tremaining: 401ms\n",
      "148:\tlearn: 0.0008098\ttotal: 1.15s\tremaining: 393ms\n",
      "149:\tlearn: 0.0008097\ttotal: 1.15s\tremaining: 385ms\n",
      "150:\tlearn: 0.0008097\ttotal: 1.16s\tremaining: 377ms\n",
      "151:\tlearn: 0.0008097\ttotal: 1.17s\tremaining: 369ms\n",
      "152:\tlearn: 0.0008096\ttotal: 1.18s\tremaining: 361ms\n",
      "153:\tlearn: 0.0008096\ttotal: 1.18s\tremaining: 353ms\n",
      "154:\tlearn: 0.0008096\ttotal: 1.19s\tremaining: 345ms\n",
      "155:\tlearn: 0.0008096\ttotal: 1.2s\tremaining: 338ms\n",
      "156:\tlearn: 0.0008096\ttotal: 1.2s\tremaining: 330ms\n",
      "157:\tlearn: 0.0008095\ttotal: 1.21s\tremaining: 322ms\n",
      "158:\tlearn: 0.0008095\ttotal: 1.22s\tremaining: 314ms\n",
      "159:\tlearn: 0.0008095\ttotal: 1.22s\tremaining: 306ms\n",
      "160:\tlearn: 0.0008095\ttotal: 1.23s\tremaining: 298ms\n",
      "161:\tlearn: 0.0008094\ttotal: 1.24s\tremaining: 290ms\n",
      "162:\tlearn: 0.0008094\ttotal: 1.24s\tremaining: 282ms\n",
      "163:\tlearn: 0.0008093\ttotal: 1.25s\tremaining: 275ms\n",
      "164:\tlearn: 0.0008093\ttotal: 1.26s\tremaining: 267ms\n",
      "165:\tlearn: 0.0008093\ttotal: 1.26s\tremaining: 259ms\n",
      "166:\tlearn: 0.0008093\ttotal: 1.27s\tremaining: 251ms\n",
      "167:\tlearn: 0.0008093\ttotal: 1.28s\tremaining: 243ms\n",
      "168:\tlearn: 0.0008092\ttotal: 1.29s\tremaining: 236ms\n",
      "169:\tlearn: 0.0008092\ttotal: 1.29s\tremaining: 228ms\n",
      "170:\tlearn: 0.0008091\ttotal: 1.3s\tremaining: 221ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171:\tlearn: 0.0008091\ttotal: 1.31s\tremaining: 213ms\n",
      "172:\tlearn: 0.0008091\ttotal: 1.31s\tremaining: 205ms\n",
      "173:\tlearn: 0.0008091\ttotal: 1.32s\tremaining: 198ms\n",
      "174:\tlearn: 0.0008091\ttotal: 1.33s\tremaining: 190ms\n",
      "175:\tlearn: 0.0008091\ttotal: 1.34s\tremaining: 182ms\n",
      "176:\tlearn: 0.0008091\ttotal: 1.34s\tremaining: 175ms\n",
      "177:\tlearn: 0.0008091\ttotal: 1.35s\tremaining: 167ms\n",
      "178:\tlearn: 0.0008090\ttotal: 1.36s\tremaining: 160ms\n",
      "179:\tlearn: 0.0008090\ttotal: 1.37s\tremaining: 152ms\n",
      "180:\tlearn: 0.0008090\ttotal: 1.37s\tremaining: 144ms\n",
      "181:\tlearn: 0.0008090\ttotal: 1.38s\tremaining: 137ms\n",
      "182:\tlearn: 0.0008090\ttotal: 1.39s\tremaining: 129ms\n",
      "183:\tlearn: 0.0008090\ttotal: 1.39s\tremaining: 121ms\n",
      "184:\tlearn: 0.0008088\ttotal: 1.4s\tremaining: 114ms\n",
      "185:\tlearn: 0.0008088\ttotal: 1.41s\tremaining: 106ms\n",
      "186:\tlearn: 0.0008088\ttotal: 1.42s\tremaining: 98.4ms\n",
      "187:\tlearn: 0.0008087\ttotal: 1.42s\tremaining: 90.8ms\n",
      "188:\tlearn: 0.0008087\ttotal: 1.43s\tremaining: 83.2ms\n",
      "189:\tlearn: 0.0007803\ttotal: 1.44s\tremaining: 75.6ms\n",
      "190:\tlearn: 0.0007803\ttotal: 1.44s\tremaining: 68.1ms\n",
      "191:\tlearn: 0.0007803\ttotal: 1.45s\tremaining: 60.5ms\n",
      "192:\tlearn: 0.0007803\ttotal: 1.46s\tremaining: 52.9ms\n",
      "193:\tlearn: 0.0007802\ttotal: 1.46s\tremaining: 45.3ms\n",
      "194:\tlearn: 0.0007802\ttotal: 1.47s\tremaining: 37.7ms\n",
      "195:\tlearn: 0.0007802\ttotal: 1.48s\tremaining: 30.2ms\n",
      "196:\tlearn: 0.0007802\ttotal: 1.49s\tremaining: 22.6ms\n",
      "197:\tlearn: 0.0007802\ttotal: 1.49s\tremaining: 15.1ms\n",
      "198:\tlearn: 0.0007802\ttotal: 1.5s\tremaining: 7.54ms\n",
      "199:\tlearn: 0.0007801\ttotal: 1.51s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9976a4f6585f4c098a2120bbdcda29c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5506830637565286, Recall = 0.9132562277580071, Aging Rate = 0.7451067615658363, Precision = 0.6128358208955224, f1 = 0.7334762415148266\n",
      "Epoch 2: Train Loss = 0.3742307442575163, Recall = 0.869661921708185, Aging Rate = 0.531806049822064, Precision = 0.8176495190296947, f1 = 0.8428540633757274\n",
      "Epoch 3: Train Loss = 0.3001550469958485, Recall = 0.8927935943060499, Aging Rate = 0.510008896797153, Precision = 0.8752725686873092, f1 = 0.8839462673419952\n",
      "Epoch 4: Train Loss = 0.24817427048903767, Recall = 0.9185943060498221, Aging Rate = 0.5151245551601423, Precision = 0.8916234887737479, f1 = 0.9049079754601228\n",
      "Epoch 5: Train Loss = 0.2071930465961266, Recall = 0.9306049822064056, Aging Rate = 0.5022241992882562, Precision = 0.9264836138175376, f1 = 0.9285397248113627\n",
      "Test Loss = 0.17449703563362678, Recall = 0.974644128113879, Aging Rate = 0.5358096085409253, precision = 0.9095060190950602\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.15671601838488597, Recall = 0.9590747330960854, Aging Rate = 0.5004448398576512, Precision = 0.9582222222222222, f1 = 0.958648288128057\n",
      "Epoch 7: Train Loss = 0.12695965420096794, Recall = 0.9733096085409253, Aging Rate = 0.5028914590747331, Precision = 0.9677134011499336, f1 = 0.9705034375693058\n",
      "Epoch 8: Train Loss = 0.102786260620555, Recall = 0.9813167259786477, Aging Rate = 0.5006672597864769, Precision = 0.9800088849400267, f1 = 0.9806623694154257\n",
      "Epoch 9: Train Loss = 0.08451368227548023, Recall = 0.9870996441281139, Aging Rate = 0.5011120996441281, Precision = 0.9849090102086108, f1 = 0.9860031104199067\n",
      "Epoch 10: Train Loss = 0.0707006324944335, Recall = 0.9906583629893239, Aging Rate = 0.5002224199288257, Precision = 0.9902178746109382, f1 = 0.9904380698243274\n",
      "Test Loss = 0.06111924000270002, Recall = 0.9942170818505338, Aging Rate = 0.4997775800711744, precision = 0.9946595460614153\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.06000764300761698, Recall = 0.9919928825622776, Aging Rate = 0.49955516014234874, Precision = 0.9928762243989314, f1 = 0.9924343569203382\n",
      "Epoch 12: Train Loss = 0.051823182283400215, Recall = 0.994661921708185, Aging Rate = 0.5002224199288257, Precision = 0.9942196531791907, f1 = 0.9944407382699577\n",
      "Epoch 13: Train Loss = 0.04553359217968275, Recall = 0.998220640569395, Aging Rate = 0.501779359430605, Precision = 0.9946808510638298, f1 = 0.9964476021314387\n",
      "Epoch 14: Train Loss = 0.04044209562533691, Recall = 0.998220640569395, Aging Rate = 0.5008896797153025, Precision = 0.9964476021314387, f1 = 0.9973333333333333\n",
      "Epoch 15: Train Loss = 0.035968506834685166, Recall = 0.9995551601423488, Aging Rate = 0.5006672597864769, Precision = 0.998223011994669, f1 = 0.9988886419204268\n",
      "Test Loss = 0.035497347807703916, Recall = 1.0, Aging Rate = 0.5020017793594306, precision = 0.9960124058484714\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.033132298461748186, Recall = 0.9986654804270463, Aging Rate = 0.5002224199288257, Precision = 0.9982214317474433, f1 = 0.9984434067155882\n",
      "Epoch 17: Train Loss = 0.0297767312607307, Recall = 0.9995551601423488, Aging Rate = 0.5004448398576512, Precision = 0.9986666666666667, f1 = 0.9991107158737217\n",
      "Epoch 18: Train Loss = 0.02781573252825338, Recall = 0.9991103202846975, Aging Rate = 0.5006672597864769, Precision = 0.9977787649933363, f1 = 0.9984440986885974\n",
      "Epoch 19: Train Loss = 0.026298594667685627, Recall = 1.0, Aging Rate = 0.5004448398576512, Precision = 0.9991111111111111, f1 = 0.9995553579368608\n",
      "Epoch 20: Train Loss = 0.02479415418732633, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Test Loss = 0.022976200378025977, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.023977397675907695, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 22: Train Loss = 0.022204855099002236, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.021181455548203075, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.021283808127841065, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.020304421075616443, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.018889675152179822, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 26: Train Loss = 0.019931881536076713, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.01903065427925663, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.01885526025753959, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 29: Train Loss = 0.01832338221026485, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.018312384623013357, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.020549568377461722, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.018008765071842594, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.01864791024489547, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.017578277405470716, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.017148012875370496, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.017064299659796882, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015525109906849912, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.017401363922687398, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01700575625543811, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 38: Train Loss = 0.0171769549075872, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.01701786783706145, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.017147797405109924, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01560290904344454, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.016837310453618335, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.01632561997155861, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01678952608204503, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.01647701080941539, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016618563780342346, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014440976530711209, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016544419092215677, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.016617399739810794, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.01600277357240907, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016008933900036847, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016577420136443873, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014213002684569232, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.015998686978666382, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.016005430228053674, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016841690732146284, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.016182303415392642, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.016127605097255672, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013790231693562558, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016278223903498402, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 57: Train Loss = 0.016527756783461654, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Train Loss = 0.015944352761255675, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.016075127376939478, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.016025961456270194, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01611487196552902, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 61: Train Loss = 0.016101652166385243, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.016112942500470795, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015707841222555613, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.016083334728386054, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015993720152518077, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014354587453036334, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015508900138009273, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.016374795793744592, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015421007020776806, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.01596684777426444, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015754229196900785, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01385602004680239, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3236561\ttotal: 8.7ms\tremaining: 1.73s\n",
      "1:\tlearn: 0.2259915\ttotal: 17ms\tremaining: 1.68s\n",
      "2:\tlearn: 0.1613678\ttotal: 25.7ms\tremaining: 1.69s\n",
      "3:\tlearn: 0.1371273\ttotal: 35ms\tremaining: 1.71s\n",
      "4:\tlearn: 0.1224312\ttotal: 43.5ms\tremaining: 1.7s\n",
      "5:\tlearn: 0.1038822\ttotal: 52.3ms\tremaining: 1.69s\n",
      "6:\tlearn: 0.0946321\ttotal: 61.3ms\tremaining: 1.69s\n",
      "7:\tlearn: 0.0846634\ttotal: 69.4ms\tremaining: 1.67s\n",
      "8:\tlearn: 0.0771076\ttotal: 78.1ms\tremaining: 1.66s\n",
      "9:\tlearn: 0.0709624\ttotal: 86.3ms\tremaining: 1.64s\n",
      "10:\tlearn: 0.0637835\ttotal: 95.1ms\tremaining: 1.63s\n",
      "11:\tlearn: 0.0580582\ttotal: 103ms\tremaining: 1.61s\n",
      "12:\tlearn: 0.0532678\ttotal: 111ms\tremaining: 1.6s\n",
      "13:\tlearn: 0.0476865\ttotal: 119ms\tremaining: 1.58s\n",
      "14:\tlearn: 0.0445297\ttotal: 126ms\tremaining: 1.56s\n",
      "15:\tlearn: 0.0406810\ttotal: 135ms\tremaining: 1.55s\n",
      "16:\tlearn: 0.0379437\ttotal: 142ms\tremaining: 1.52s\n",
      "17:\tlearn: 0.0352169\ttotal: 149ms\tremaining: 1.5s\n",
      "18:\tlearn: 0.0318762\ttotal: 156ms\tremaining: 1.49s\n",
      "19:\tlearn: 0.0288081\ttotal: 165ms\tremaining: 1.48s\n",
      "20:\tlearn: 0.0264622\ttotal: 173ms\tremaining: 1.47s\n",
      "21:\tlearn: 0.0234567\ttotal: 181ms\tremaining: 1.47s\n",
      "22:\tlearn: 0.0217030\ttotal: 189ms\tremaining: 1.45s\n",
      "23:\tlearn: 0.0191989\ttotal: 196ms\tremaining: 1.44s\n",
      "24:\tlearn: 0.0175521\ttotal: 205ms\tremaining: 1.44s\n",
      "25:\tlearn: 0.0163364\ttotal: 213ms\tremaining: 1.43s\n",
      "26:\tlearn: 0.0146303\ttotal: 221ms\tremaining: 1.42s\n",
      "27:\tlearn: 0.0131277\ttotal: 228ms\tremaining: 1.4s\n",
      "28:\tlearn: 0.0117219\ttotal: 236ms\tremaining: 1.39s\n",
      "29:\tlearn: 0.0109028\ttotal: 244ms\tremaining: 1.38s\n",
      "30:\tlearn: 0.0100513\ttotal: 252ms\tremaining: 1.37s\n",
      "31:\tlearn: 0.0093552\ttotal: 259ms\tremaining: 1.36s\n",
      "32:\tlearn: 0.0087075\ttotal: 267ms\tremaining: 1.35s\n",
      "33:\tlearn: 0.0079156\ttotal: 275ms\tremaining: 1.34s\n",
      "34:\tlearn: 0.0072985\ttotal: 282ms\tremaining: 1.33s\n",
      "35:\tlearn: 0.0067711\ttotal: 290ms\tremaining: 1.32s\n",
      "36:\tlearn: 0.0063082\ttotal: 297ms\tremaining: 1.31s\n",
      "37:\tlearn: 0.0058099\ttotal: 305ms\tremaining: 1.3s\n",
      "38:\tlearn: 0.0055489\ttotal: 314ms\tremaining: 1.29s\n",
      "39:\tlearn: 0.0050331\ttotal: 322ms\tremaining: 1.29s\n",
      "40:\tlearn: 0.0046545\ttotal: 329ms\tremaining: 1.28s\n",
      "41:\tlearn: 0.0043930\ttotal: 337ms\tremaining: 1.27s\n",
      "42:\tlearn: 0.0040849\ttotal: 345ms\tremaining: 1.26s\n",
      "43:\tlearn: 0.0037873\ttotal: 352ms\tremaining: 1.25s\n",
      "44:\tlearn: 0.0035834\ttotal: 361ms\tremaining: 1.24s\n",
      "45:\tlearn: 0.0034090\ttotal: 368ms\tremaining: 1.23s\n",
      "46:\tlearn: 0.0031909\ttotal: 376ms\tremaining: 1.22s\n",
      "47:\tlearn: 0.0029804\ttotal: 384ms\tremaining: 1.21s\n",
      "48:\tlearn: 0.0027596\ttotal: 392ms\tremaining: 1.21s\n",
      "49:\tlearn: 0.0025676\ttotal: 400ms\tremaining: 1.2s\n",
      "50:\tlearn: 0.0024379\ttotal: 407ms\tremaining: 1.19s\n",
      "51:\tlearn: 0.0022951\ttotal: 414ms\tremaining: 1.18s\n",
      "52:\tlearn: 0.0021889\ttotal: 422ms\tremaining: 1.17s\n",
      "53:\tlearn: 0.0020898\ttotal: 430ms\tremaining: 1.16s\n",
      "54:\tlearn: 0.0019950\ttotal: 437ms\tremaining: 1.15s\n",
      "55:\tlearn: 0.0018747\ttotal: 444ms\tremaining: 1.14s\n",
      "56:\tlearn: 0.0017790\ttotal: 451ms\tremaining: 1.13s\n",
      "57:\tlearn: 0.0017048\ttotal: 458ms\tremaining: 1.12s\n",
      "58:\tlearn: 0.0015910\ttotal: 466ms\tremaining: 1.11s\n",
      "59:\tlearn: 0.0015275\ttotal: 473ms\tremaining: 1.1s\n",
      "60:\tlearn: 0.0014339\ttotal: 481ms\tremaining: 1.09s\n",
      "61:\tlearn: 0.0013544\ttotal: 488ms\tremaining: 1.09s\n",
      "62:\tlearn: 0.0013543\ttotal: 495ms\tremaining: 1.08s\n",
      "63:\tlearn: 0.0012888\ttotal: 503ms\tremaining: 1.07s\n",
      "64:\tlearn: 0.0012216\ttotal: 510ms\tremaining: 1.06s\n",
      "65:\tlearn: 0.0011546\ttotal: 518ms\tremaining: 1.05s\n",
      "66:\tlearn: 0.0010938\ttotal: 525ms\tremaining: 1.04s\n",
      "67:\tlearn: 0.0010417\ttotal: 533ms\tremaining: 1.03s\n",
      "68:\tlearn: 0.0010004\ttotal: 541ms\tremaining: 1.03s\n",
      "69:\tlearn: 0.0010002\ttotal: 548ms\tremaining: 1.02s\n",
      "70:\tlearn: 0.0009600\ttotal: 555ms\tremaining: 1.01s\n",
      "71:\tlearn: 0.0009599\ttotal: 563ms\tremaining: 1s\n",
      "72:\tlearn: 0.0009599\ttotal: 570ms\tremaining: 992ms\n",
      "73:\tlearn: 0.0009099\ttotal: 578ms\tremaining: 984ms\n",
      "74:\tlearn: 0.0009099\ttotal: 586ms\tremaining: 976ms\n",
      "75:\tlearn: 0.0009098\ttotal: 592ms\tremaining: 967ms\n",
      "76:\tlearn: 0.0008617\ttotal: 600ms\tremaining: 958ms\n",
      "77:\tlearn: 0.0008617\ttotal: 607ms\tremaining: 949ms\n",
      "78:\tlearn: 0.0008617\ttotal: 614ms\tremaining: 940ms\n",
      "79:\tlearn: 0.0008616\ttotal: 621ms\tremaining: 932ms\n",
      "80:\tlearn: 0.0008616\ttotal: 628ms\tremaining: 923ms\n",
      "81:\tlearn: 0.0008616\ttotal: 635ms\tremaining: 914ms\n",
      "82:\tlearn: 0.0008616\ttotal: 642ms\tremaining: 905ms\n",
      "83:\tlearn: 0.0008616\ttotal: 648ms\tremaining: 896ms\n",
      "84:\tlearn: 0.0008616\ttotal: 655ms\tremaining: 886ms\n",
      "85:\tlearn: 0.0008615\ttotal: 662ms\tremaining: 878ms\n",
      "86:\tlearn: 0.0008615\ttotal: 669ms\tremaining: 869ms\n",
      "87:\tlearn: 0.0008615\ttotal: 676ms\tremaining: 860ms\n",
      "88:\tlearn: 0.0008614\ttotal: 683ms\tremaining: 851ms\n",
      "89:\tlearn: 0.0008613\ttotal: 690ms\tremaining: 843ms\n",
      "90:\tlearn: 0.0008613\ttotal: 696ms\tremaining: 834ms\n",
      "91:\tlearn: 0.0008613\ttotal: 703ms\tremaining: 825ms\n",
      "92:\tlearn: 0.0008612\ttotal: 710ms\tremaining: 817ms\n",
      "93:\tlearn: 0.0008611\ttotal: 718ms\tremaining: 809ms\n",
      "94:\tlearn: 0.0008611\ttotal: 725ms\tremaining: 801ms\n",
      "95:\tlearn: 0.0008611\ttotal: 732ms\tremaining: 793ms\n",
      "96:\tlearn: 0.0008610\ttotal: 739ms\tremaining: 785ms\n",
      "97:\tlearn: 0.0008610\ttotal: 746ms\tremaining: 776ms\n",
      "98:\tlearn: 0.0008610\ttotal: 753ms\tremaining: 769ms\n",
      "99:\tlearn: 0.0008610\ttotal: 760ms\tremaining: 760ms\n",
      "100:\tlearn: 0.0008166\ttotal: 768ms\tremaining: 752ms\n",
      "101:\tlearn: 0.0008166\ttotal: 775ms\tremaining: 745ms\n",
      "102:\tlearn: 0.0007800\ttotal: 783ms\tremaining: 737ms\n",
      "103:\tlearn: 0.0007800\ttotal: 789ms\tremaining: 729ms\n",
      "104:\tlearn: 0.0007800\ttotal: 797ms\tremaining: 721ms\n",
      "105:\tlearn: 0.0007800\ttotal: 804ms\tremaining: 713ms\n",
      "106:\tlearn: 0.0007800\ttotal: 811ms\tremaining: 705ms\n",
      "107:\tlearn: 0.0007426\ttotal: 819ms\tremaining: 697ms\n",
      "108:\tlearn: 0.0007426\ttotal: 826ms\tremaining: 690ms\n",
      "109:\tlearn: 0.0007426\ttotal: 833ms\tremaining: 682ms\n",
      "110:\tlearn: 0.0007426\ttotal: 840ms\tremaining: 674ms\n",
      "111:\tlearn: 0.0007425\ttotal: 848ms\tremaining: 666ms\n",
      "112:\tlearn: 0.0007425\ttotal: 855ms\tremaining: 658ms\n",
      "113:\tlearn: 0.0007424\ttotal: 863ms\tremaining: 651ms\n",
      "114:\tlearn: 0.0007424\ttotal: 870ms\tremaining: 643ms\n",
      "115:\tlearn: 0.0007423\ttotal: 877ms\tremaining: 635ms\n",
      "116:\tlearn: 0.0007423\ttotal: 885ms\tremaining: 628ms\n",
      "117:\tlearn: 0.0007423\ttotal: 892ms\tremaining: 620ms\n",
      "118:\tlearn: 0.0007423\ttotal: 899ms\tremaining: 612ms\n",
      "119:\tlearn: 0.0007423\ttotal: 906ms\tremaining: 604ms\n",
      "120:\tlearn: 0.0007423\ttotal: 912ms\tremaining: 596ms\n",
      "121:\tlearn: 0.0007422\ttotal: 919ms\tremaining: 588ms\n",
      "122:\tlearn: 0.0007422\ttotal: 927ms\tremaining: 580ms\n",
      "123:\tlearn: 0.0007422\ttotal: 933ms\tremaining: 572ms\n",
      "124:\tlearn: 0.0007048\ttotal: 942ms\tremaining: 565ms\n",
      "125:\tlearn: 0.0007048\ttotal: 949ms\tremaining: 557ms\n",
      "126:\tlearn: 0.0007048\ttotal: 956ms\tremaining: 550ms\n",
      "127:\tlearn: 0.0007048\ttotal: 964ms\tremaining: 542ms\n",
      "128:\tlearn: 0.0007048\ttotal: 970ms\tremaining: 534ms\n",
      "129:\tlearn: 0.0007047\ttotal: 977ms\tremaining: 526ms\n",
      "130:\tlearn: 0.0007047\ttotal: 985ms\tremaining: 519ms\n",
      "131:\tlearn: 0.0007047\ttotal: 993ms\tremaining: 511ms\n",
      "132:\tlearn: 0.0007047\ttotal: 1s\tremaining: 504ms\n",
      "133:\tlearn: 0.0007047\ttotal: 1.01s\tremaining: 496ms\n",
      "134:\tlearn: 0.0007046\ttotal: 1.01s\tremaining: 489ms\n",
      "135:\tlearn: 0.0007046\ttotal: 1.02s\tremaining: 481ms\n",
      "136:\tlearn: 0.0007046\ttotal: 1.03s\tremaining: 474ms\n",
      "137:\tlearn: 0.0007046\ttotal: 1.04s\tremaining: 466ms\n",
      "138:\tlearn: 0.0007046\ttotal: 1.04s\tremaining: 458ms\n",
      "139:\tlearn: 0.0007045\ttotal: 1.05s\tremaining: 451ms\n",
      "140:\tlearn: 0.0007045\ttotal: 1.06s\tremaining: 443ms\n",
      "141:\tlearn: 0.0007045\ttotal: 1.07s\tremaining: 436ms\n",
      "142:\tlearn: 0.0007045\ttotal: 1.07s\tremaining: 428ms\n",
      "143:\tlearn: 0.0007044\ttotal: 1.08s\tremaining: 421ms\n",
      "144:\tlearn: 0.0007044\ttotal: 1.09s\tremaining: 413ms\n",
      "145:\tlearn: 0.0007044\ttotal: 1.09s\tremaining: 405ms\n",
      "146:\tlearn: 0.0007042\ttotal: 1.1s\tremaining: 397ms\n",
      "147:\tlearn: 0.0007042\ttotal: 1.11s\tremaining: 390ms\n",
      "148:\tlearn: 0.0007041\ttotal: 1.12s\tremaining: 382ms\n",
      "149:\tlearn: 0.0007041\ttotal: 1.12s\tremaining: 375ms\n",
      "150:\tlearn: 0.0007040\ttotal: 1.13s\tremaining: 367ms\n",
      "151:\tlearn: 0.0007040\ttotal: 1.14s\tremaining: 360ms\n",
      "152:\tlearn: 0.0007040\ttotal: 1.15s\tremaining: 352ms\n",
      "153:\tlearn: 0.0007039\ttotal: 1.15s\tremaining: 345ms\n",
      "154:\tlearn: 0.0007039\ttotal: 1.16s\tremaining: 337ms\n",
      "155:\tlearn: 0.0007039\ttotal: 1.17s\tremaining: 330ms\n",
      "156:\tlearn: 0.0007039\ttotal: 1.18s\tremaining: 322ms\n",
      "157:\tlearn: 0.0007039\ttotal: 1.18s\tremaining: 315ms\n",
      "158:\tlearn: 0.0007038\ttotal: 1.19s\tremaining: 307ms\n",
      "159:\tlearn: 0.0007038\ttotal: 1.2s\tremaining: 299ms\n",
      "160:\tlearn: 0.0007038\ttotal: 1.2s\tremaining: 292ms\n",
      "161:\tlearn: 0.0007038\ttotal: 1.21s\tremaining: 284ms\n",
      "162:\tlearn: 0.0007038\ttotal: 1.22s\tremaining: 277ms\n",
      "163:\tlearn: 0.0007037\ttotal: 1.23s\tremaining: 269ms\n",
      "164:\tlearn: 0.0007037\ttotal: 1.23s\tremaining: 262ms\n",
      "165:\tlearn: 0.0007037\ttotal: 1.24s\tremaining: 254ms\n",
      "166:\tlearn: 0.0007037\ttotal: 1.25s\tremaining: 247ms\n",
      "167:\tlearn: 0.0007036\ttotal: 1.25s\tremaining: 239ms\n",
      "168:\tlearn: 0.0006700\ttotal: 1.26s\tremaining: 232ms\n",
      "169:\tlearn: 0.0006700\ttotal: 1.27s\tremaining: 224ms\n",
      "170:\tlearn: 0.0006700\ttotal: 1.28s\tremaining: 217ms\n",
      "171:\tlearn: 0.0006699\ttotal: 1.28s\tremaining: 209ms\n",
      "172:\tlearn: 0.0006699\ttotal: 1.29s\tremaining: 201ms\n",
      "173:\tlearn: 0.0006699\ttotal: 1.3s\tremaining: 194ms\n",
      "174:\tlearn: 0.0006699\ttotal: 1.3s\tremaining: 186ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175:\tlearn: 0.0006698\ttotal: 1.31s\tremaining: 179ms\n",
      "176:\tlearn: 0.0006698\ttotal: 1.32s\tremaining: 171ms\n",
      "177:\tlearn: 0.0006698\ttotal: 1.32s\tremaining: 164ms\n",
      "178:\tlearn: 0.0006698\ttotal: 1.33s\tremaining: 156ms\n",
      "179:\tlearn: 0.0006698\ttotal: 1.34s\tremaining: 149ms\n",
      "180:\tlearn: 0.0006698\ttotal: 1.35s\tremaining: 141ms\n",
      "181:\tlearn: 0.0006697\ttotal: 1.35s\tremaining: 134ms\n",
      "182:\tlearn: 0.0006697\ttotal: 1.36s\tremaining: 126ms\n",
      "183:\tlearn: 0.0006697\ttotal: 1.37s\tremaining: 119ms\n",
      "184:\tlearn: 0.0006697\ttotal: 1.37s\tremaining: 111ms\n",
      "185:\tlearn: 0.0006696\ttotal: 1.38s\tremaining: 104ms\n",
      "186:\tlearn: 0.0006696\ttotal: 1.39s\tremaining: 96.5ms\n",
      "187:\tlearn: 0.0006696\ttotal: 1.4s\tremaining: 89.1ms\n",
      "188:\tlearn: 0.0006696\ttotal: 1.4s\tremaining: 81.6ms\n",
      "189:\tlearn: 0.0006695\ttotal: 1.41s\tremaining: 74.2ms\n",
      "190:\tlearn: 0.0006695\ttotal: 1.42s\tremaining: 66.7ms\n",
      "191:\tlearn: 0.0006694\ttotal: 1.42s\tremaining: 59.3ms\n",
      "192:\tlearn: 0.0006694\ttotal: 1.43s\tremaining: 51.9ms\n",
      "193:\tlearn: 0.0006694\ttotal: 1.44s\tremaining: 44.4ms\n",
      "194:\tlearn: 0.0006694\ttotal: 1.44s\tremaining: 37ms\n",
      "195:\tlearn: 0.0006693\ttotal: 1.45s\tremaining: 29.6ms\n",
      "196:\tlearn: 0.0006693\ttotal: 1.46s\tremaining: 22.2ms\n",
      "197:\tlearn: 0.0006693\ttotal: 1.46s\tremaining: 14.8ms\n",
      "198:\tlearn: 0.0006693\ttotal: 1.47s\tremaining: 7.39ms\n",
      "199:\tlearn: 0.0006693\ttotal: 1.48s\tremaining: 0us\n",
      "Dataset 8:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b37fd5c7654a95b81472810110a463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3362dd53dd064370a19624128230d1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5995367803607548, Recall = 0.5622775800711743, Aging Rate = 0.35120106761565834, Precision = 0.8005066497783407, f1 = 0.6605696367912203\n",
      "Epoch 2: Train Loss = 0.38808487074655146, Recall = 0.8327402135231317, Aging Rate = 0.48020462633451955, Precision = 0.8670680870773506, f1 = 0.8495575221238939\n",
      "Epoch 3: Train Loss = 0.2866789920792461, Recall = 0.9101423487544484, Aging Rate = 0.5080071174377224, Precision = 0.8957968476357268, f1 = 0.9029126213592232\n",
      "Epoch 4: Train Loss = 0.23639086005526505, Recall = 0.9266014234875445, Aging Rate = 0.5051156583629893, Precision = 0.9172170849845883, f1 = 0.921885372870104\n",
      "Epoch 5: Train Loss = 0.2013871779539407, Recall = 0.9363879003558719, Aging Rate = 0.498220640569395, Precision = 0.9397321428571429, f1 = 0.9380570409982176\n",
      "Test Loss = 0.17522105065529034, Recall = 0.9408362989323843, Aging Rate = 0.4893238434163701, precision = 0.9613636363636363\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.16675134713751566, Recall = 0.943950177935943, Aging Rate = 0.4915480427046263, Precision = 0.9601809954751132, f1 = 0.9519964109466129\n",
      "Epoch 7: Train Loss = 0.1430630225727991, Recall = 0.9488434163701067, Aging Rate = 0.4891014234875445, Precision = 0.9699863574351978, f1 = 0.9592984034180347\n",
      "Epoch 8: Train Loss = 0.12292543198131158, Recall = 0.9541814946619217, Aging Rate = 0.48776690391459077, Precision = 0.9781121751025992, f1 = 0.9659986489529385\n",
      "Epoch 9: Train Loss = 0.10745777958652727, Recall = 0.9608540925266904, Aging Rate = 0.4893238434163701, Precision = 0.9818181818181818, f1 = 0.9712230215827339\n",
      "Epoch 10: Train Loss = 0.09330866170311314, Recall = 0.9653024911032029, Aging Rate = 0.48976868327402134, Precision = 0.9854677565849228, f1 = 0.9752808988764046\n",
      "Test Loss = 0.0827852370369901, Recall = 0.9710854092526691, Aging Rate = 0.4915480427046263, precision = 0.9877828054298643\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.08161969801072973, Recall = 0.9710854092526691, Aging Rate = 0.49088078291814946, Precision = 0.9891255097417309, f1 = 0.9800224466891133\n",
      "Epoch 12: Train Loss = 0.07246726008920908, Recall = 0.972864768683274, Aging Rate = 0.4904359430604982, Precision = 0.9918367346938776, f1 = 0.9822591511340669\n",
      "Epoch 13: Train Loss = 0.06536913265431055, Recall = 0.9768683274021353, Aging Rate = 0.4933274021352313, Precision = 0.9900811541929666, f1 = 0.9834303627407075\n",
      "Epoch 14: Train Loss = 0.05764754061517554, Recall = 0.9813167259786477, Aging Rate = 0.49377224199288255, Precision = 0.9936936936936936, f1 = 0.9874664279319606\n",
      "Epoch 15: Train Loss = 0.05147834236193382, Recall = 0.9830960854092526, Aging Rate = 0.4939946619217082, Precision = 0.995047276001801, f1 = 0.9890355784291787\n",
      "Test Loss = 0.0468267373992561, Recall = 0.9884341637010676, Aging Rate = 0.4962188612099644, precision = 0.9959659345584939\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04779714531533659, Recall = 0.9857651245551602, Aging Rate = 0.4944395017793594, Precision = 0.99685110211426, f1 = 0.9912771192127041\n",
      "Epoch 17: Train Loss = 0.043343254507542506, Recall = 0.9888790035587188, Aging Rate = 0.49644128113879005, Precision = 0.9959677419354839, f1 = 0.9924107142857143\n",
      "Epoch 18: Train Loss = 0.04069673447634402, Recall = 0.9879893238434164, Aging Rate = 0.4957740213523132, Precision = 0.9964109466128309, f1 = 0.9921822649095376\n",
      "Epoch 19: Train Loss = 0.03807392305432689, Recall = 0.9911032028469751, Aging Rate = 0.49666370106761565, Precision = 0.9977608598298253, f1 = 0.9944208881945994\n",
      "Epoch 20: Train Loss = 0.0342373213999212, Recall = 0.9915480427046264, Aging Rate = 0.49644128113879005, Precision = 0.9986559139784946, f1 = 0.9950892857142859\n",
      "Test Loss = 0.030782988818620873, Recall = 0.9933274021352313, Aging Rate = 0.49666370106761565, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.032313680123816184, Recall = 0.9933274021352313, Aging Rate = 0.49755338078291816, Precision = 0.9982118909253465, f1 = 0.9957636566332219\n",
      "Epoch 22: Train Loss = 0.030725039843324028, Recall = 0.9942170818505338, Aging Rate = 0.49777580071174377, Precision = 0.9986595174262735, f1 = 0.9964333481943826\n",
      "Epoch 23: Train Loss = 0.028215497520046304, Recall = 0.9951067615658363, Aging Rate = 0.4979982206405694, Precision = 0.9991067440821796, f1 = 0.9971027412525072\n",
      "Epoch 24: Train Loss = 0.027580384188248805, Recall = 0.994661921708185, Aging Rate = 0.49777580071174377, Precision = 0.9991063449508489, f1 = 0.9968791796700847\n",
      "Epoch 25: Train Loss = 0.025954418981388158, Recall = 0.9968861209964412, Aging Rate = 0.4988879003558719, Precision = 0.9991083370485956, f1 = 0.9979959919839678\n",
      "Test Loss = 0.023520673993060175, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.02476689649010044, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 27: Train Loss = 0.023873303894258478, Recall = 0.9973309608540926, Aging Rate = 0.4986654804270463, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.022922772363994894, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.021994671759483973, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 30: Train Loss = 0.022432904809819423, Recall = 0.9977758007117438, Aging Rate = 0.49933274021352314, Precision = 0.9991091314031181, f1 = 0.9984420209214333\n",
      "Test Loss = 0.01975189762196613, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.020978838029678818, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.020471124618158434, Recall = 0.998220640569395, Aging Rate = 0.4997775800711744, Precision = 0.9986648865153538, f1 = 0.9984427141268075\n",
      "Epoch 33: Train Loss = 0.019471682056294112, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.019210997014389344, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 35: Train Loss = 0.019233487374099547, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.017653018832737018, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.01955375024299817, Recall = 0.9968861209964412, Aging Rate = 0.4984430604982206, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01838242352088363, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.017642974853515625, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 39: Train Loss = 0.017226103587188756, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.017548113918840036, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Test Loss = 0.0160180601801763, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.017098541944900864, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0169011540478924, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 43: Train Loss = 0.01728127739024332, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 44: Train Loss = 0.01629997564817769, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 45: Train Loss = 0.01627863189570102, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.01613073841771409, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016186373082927537, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 47: Train Loss = 0.01611755971939326, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.015815882728140124, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.015806943832754133, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01605155641126887, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014414473953805997, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.015816825047612615, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0155394506206557, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.015473147847580316, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015557273602597017, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 55: Train Loss = 0.015274561373218629, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01441538909920806, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015197456724862187, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.01580508085783054, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 58: Train Loss = 0.015515351823773036, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.015075670674549303, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015343229528317672, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013954533582215206, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.014858587078246357, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 62: Train Loss = 0.014845533276579982, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.014947934593223168, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015350589833596847, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015166324884813027, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013295594897797311, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.015653007073838304, Recall = 0.9991103202846975, Aging Rate = 0.5, Precision = 0.9991103202846975, f1 = 0.9991103202846975\n",
      "Epoch 67: Train Loss = 0.014772386416076977, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.014597053689305469, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.01449431254266632, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015340180169406522, Recall = 0.9991103202846975, Aging Rate = 0.5, Precision = 0.9991103202846975, f1 = 0.9991103202846975\n",
      "Test Loss = 0.013415739324699517, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.015047066112536128, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.014437665241468844, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 73: Train Loss = 0.014488504049484204, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.014446932036973191, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 75: Train Loss = 0.014369713679433294, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013521296638345804, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.01503146172988351, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014048123261576445, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.014892691908755548, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 79: Train Loss = 0.014477309581966162, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.015458545790077105, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015348889825209391, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 81: Train Loss = 0.014626412722033537, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014027012522030768, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.014006368421495385, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.01427493796905907, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.014304410295024036, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.013603706337060793, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.014302176241083502, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.014477256194976725, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 88: Train Loss = 0.014034545062328574, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.017171504864223912, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 90: Train Loss = 0.01449575964560288, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014389137707624147, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014610824098949754, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.013981440651406173, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.01415194797385948, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 94: Train Loss = 0.01423821172733324, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014721406696636057, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.01318616100199176, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.013987106209773292, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.01459252374293117, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.014279749562625782, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.014058154460798378, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.01425110970978423, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013327258996983652, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5410074\ttotal: 5.82ms\tremaining: 1.16s\n",
      "1:\tlearn: 0.4155695\ttotal: 12.2ms\tremaining: 1.21s\n",
      "2:\tlearn: 0.3295578\ttotal: 18.4ms\tremaining: 1.21s\n",
      "3:\tlearn: 0.2764538\ttotal: 24.6ms\tremaining: 1.2s\n",
      "4:\tlearn: 0.2277032\ttotal: 30.6ms\tremaining: 1.19s\n",
      "5:\tlearn: 0.1929187\ttotal: 37.1ms\tremaining: 1.2s\n",
      "6:\tlearn: 0.1689342\ttotal: 43.2ms\tremaining: 1.19s\n",
      "7:\tlearn: 0.1453687\ttotal: 50ms\tremaining: 1.2s\n",
      "8:\tlearn: 0.1280922\ttotal: 55.9ms\tremaining: 1.19s\n",
      "9:\tlearn: 0.1153587\ttotal: 62ms\tremaining: 1.18s\n",
      "10:\tlearn: 0.0997860\ttotal: 69.1ms\tremaining: 1.19s\n",
      "11:\tlearn: 0.0889124\ttotal: 76.2ms\tremaining: 1.19s\n",
      "12:\tlearn: 0.0782703\ttotal: 82.1ms\tremaining: 1.18s\n",
      "13:\tlearn: 0.0702764\ttotal: 87.2ms\tremaining: 1.16s\n",
      "14:\tlearn: 0.0646176\ttotal: 92.2ms\tremaining: 1.14s\n",
      "15:\tlearn: 0.0561863\ttotal: 97.6ms\tremaining: 1.12s\n",
      "16:\tlearn: 0.0520133\ttotal: 102ms\tremaining: 1.1s\n",
      "17:\tlearn: 0.0474189\ttotal: 107ms\tremaining: 1.08s\n",
      "18:\tlearn: 0.0436519\ttotal: 112ms\tremaining: 1.07s\n",
      "19:\tlearn: 0.0394172\ttotal: 117ms\tremaining: 1.06s\n",
      "20:\tlearn: 0.0356505\ttotal: 122ms\tremaining: 1.04s\n",
      "21:\tlearn: 0.0327981\ttotal: 127ms\tremaining: 1.03s\n",
      "22:\tlearn: 0.0297435\ttotal: 132ms\tremaining: 1.02s\n",
      "23:\tlearn: 0.0276491\ttotal: 137ms\tremaining: 1s\n",
      "24:\tlearn: 0.0256723\ttotal: 142ms\tremaining: 993ms\n",
      "25:\tlearn: 0.0244544\ttotal: 146ms\tremaining: 977ms\n",
      "26:\tlearn: 0.0223207\ttotal: 151ms\tremaining: 966ms\n",
      "27:\tlearn: 0.0213173\ttotal: 155ms\tremaining: 949ms\n",
      "28:\tlearn: 0.0194408\ttotal: 159ms\tremaining: 939ms\n",
      "29:\tlearn: 0.0177405\ttotal: 164ms\tremaining: 930ms\n",
      "30:\tlearn: 0.0166534\ttotal: 169ms\tremaining: 922ms\n",
      "31:\tlearn: 0.0156412\ttotal: 174ms\tremaining: 914ms\n",
      "32:\tlearn: 0.0145714\ttotal: 179ms\tremaining: 907ms\n",
      "33:\tlearn: 0.0134091\ttotal: 184ms\tremaining: 899ms\n",
      "34:\tlearn: 0.0122584\ttotal: 189ms\tremaining: 890ms\n",
      "35:\tlearn: 0.0116291\ttotal: 194ms\tremaining: 883ms\n",
      "36:\tlearn: 0.0109992\ttotal: 199ms\tremaining: 875ms\n",
      "37:\tlearn: 0.0105575\ttotal: 203ms\tremaining: 864ms\n",
      "38:\tlearn: 0.0098683\ttotal: 208ms\tremaining: 858ms\n",
      "39:\tlearn: 0.0090592\ttotal: 213ms\tremaining: 852ms\n",
      "40:\tlearn: 0.0083442\ttotal: 218ms\tremaining: 845ms\n",
      "41:\tlearn: 0.0078357\ttotal: 223ms\tremaining: 838ms\n",
      "42:\tlearn: 0.0072718\ttotal: 228ms\tremaining: 831ms\n",
      "43:\tlearn: 0.0068376\ttotal: 233ms\tremaining: 825ms\n",
      "44:\tlearn: 0.0064115\ttotal: 237ms\tremaining: 816ms\n",
      "45:\tlearn: 0.0060296\ttotal: 242ms\tremaining: 809ms\n",
      "46:\tlearn: 0.0055845\ttotal: 247ms\tremaining: 802ms\n",
      "47:\tlearn: 0.0052045\ttotal: 251ms\tremaining: 796ms\n",
      "48:\tlearn: 0.0048113\ttotal: 256ms\tremaining: 790ms\n",
      "49:\tlearn: 0.0044179\ttotal: 262ms\tremaining: 785ms\n",
      "50:\tlearn: 0.0041303\ttotal: 266ms\tremaining: 778ms\n",
      "51:\tlearn: 0.0038356\ttotal: 271ms\tremaining: 772ms\n",
      "52:\tlearn: 0.0035763\ttotal: 276ms\tremaining: 766ms\n",
      "53:\tlearn: 0.0032869\ttotal: 281ms\tremaining: 759ms\n",
      "54:\tlearn: 0.0030092\ttotal: 285ms\tremaining: 752ms\n",
      "55:\tlearn: 0.0027834\ttotal: 290ms\tremaining: 746ms\n",
      "56:\tlearn: 0.0026414\ttotal: 295ms\tremaining: 739ms\n",
      "57:\tlearn: 0.0025114\ttotal: 299ms\tremaining: 733ms\n",
      "58:\tlearn: 0.0023805\ttotal: 303ms\tremaining: 725ms\n",
      "59:\tlearn: 0.0022619\ttotal: 308ms\tremaining: 719ms\n",
      "60:\tlearn: 0.0021816\ttotal: 312ms\tremaining: 711ms\n",
      "61:\tlearn: 0.0020909\ttotal: 317ms\tremaining: 706ms\n",
      "62:\tlearn: 0.0019775\ttotal: 322ms\tremaining: 700ms\n",
      "63:\tlearn: 0.0018638\ttotal: 327ms\tremaining: 694ms\n",
      "64:\tlearn: 0.0017207\ttotal: 332ms\tremaining: 689ms\n",
      "65:\tlearn: 0.0016475\ttotal: 336ms\tremaining: 682ms\n",
      "66:\tlearn: 0.0015692\ttotal: 340ms\tremaining: 676ms\n",
      "67:\tlearn: 0.0015073\ttotal: 344ms\tremaining: 668ms\n",
      "68:\tlearn: 0.0014392\ttotal: 349ms\tremaining: 662ms\n",
      "69:\tlearn: 0.0014026\ttotal: 352ms\tremaining: 654ms\n",
      "70:\tlearn: 0.0013662\ttotal: 356ms\tremaining: 647ms\n",
      "71:\tlearn: 0.0012889\ttotal: 360ms\tremaining: 640ms\n",
      "72:\tlearn: 0.0012573\ttotal: 364ms\tremaining: 634ms\n",
      "73:\tlearn: 0.0012262\ttotal: 368ms\tremaining: 626ms\n",
      "74:\tlearn: 0.0011897\ttotal: 371ms\tremaining: 619ms\n",
      "75:\tlearn: 0.0011429\ttotal: 376ms\tremaining: 613ms\n",
      "76:\tlearn: 0.0011155\ttotal: 379ms\tremaining: 606ms\n",
      "77:\tlearn: 0.0010862\ttotal: 384ms\tremaining: 600ms\n",
      "78:\tlearn: 0.0010353\ttotal: 389ms\tremaining: 595ms\n",
      "79:\tlearn: 0.0009851\ttotal: 393ms\tremaining: 589ms\n",
      "80:\tlearn: 0.0009851\ttotal: 396ms\tremaining: 581ms\n",
      "81:\tlearn: 0.0009851\ttotal: 398ms\tremaining: 573ms\n",
      "82:\tlearn: 0.0009851\ttotal: 401ms\tremaining: 565ms\n",
      "83:\tlearn: 0.0009851\ttotal: 403ms\tremaining: 557ms\n",
      "84:\tlearn: 0.0009850\ttotal: 406ms\tremaining: 549ms\n",
      "85:\tlearn: 0.0009850\ttotal: 408ms\tremaining: 541ms\n",
      "86:\tlearn: 0.0009850\ttotal: 411ms\tremaining: 534ms\n",
      "87:\tlearn: 0.0009850\ttotal: 414ms\tremaining: 526ms\n",
      "88:\tlearn: 0.0009850\ttotal: 416ms\tremaining: 519ms\n",
      "89:\tlearn: 0.0009850\ttotal: 419ms\tremaining: 512ms\n",
      "90:\tlearn: 0.0009850\ttotal: 422ms\tremaining: 505ms\n",
      "91:\tlearn: 0.0009850\ttotal: 424ms\tremaining: 498ms\n",
      "92:\tlearn: 0.0009850\ttotal: 427ms\tremaining: 491ms\n",
      "93:\tlearn: 0.0009850\ttotal: 429ms\tremaining: 484ms\n",
      "94:\tlearn: 0.0009850\ttotal: 432ms\tremaining: 477ms\n",
      "95:\tlearn: 0.0009850\ttotal: 434ms\tremaining: 471ms\n",
      "96:\tlearn: 0.0009850\ttotal: 437ms\tremaining: 464ms\n",
      "97:\tlearn: 0.0009850\ttotal: 439ms\tremaining: 457ms\n",
      "98:\tlearn: 0.0009850\ttotal: 442ms\tremaining: 451ms\n",
      "99:\tlearn: 0.0009850\ttotal: 444ms\tremaining: 444ms\n",
      "100:\tlearn: 0.0009850\ttotal: 447ms\tremaining: 438ms\n",
      "101:\tlearn: 0.0009850\ttotal: 450ms\tremaining: 432ms\n",
      "102:\tlearn: 0.0009850\ttotal: 452ms\tremaining: 426ms\n",
      "103:\tlearn: 0.0009850\ttotal: 455ms\tremaining: 420ms\n",
      "104:\tlearn: 0.0009850\ttotal: 457ms\tremaining: 414ms\n",
      "105:\tlearn: 0.0009850\ttotal: 460ms\tremaining: 408ms\n",
      "106:\tlearn: 0.0009850\ttotal: 462ms\tremaining: 402ms\n",
      "107:\tlearn: 0.0009850\ttotal: 465ms\tremaining: 396ms\n",
      "108:\tlearn: 0.0009850\ttotal: 468ms\tremaining: 390ms\n",
      "109:\tlearn: 0.0009850\ttotal: 470ms\tremaining: 385ms\n",
      "110:\tlearn: 0.0009849\ttotal: 473ms\tremaining: 379ms\n",
      "111:\tlearn: 0.0009849\ttotal: 475ms\tremaining: 374ms\n",
      "112:\tlearn: 0.0009849\ttotal: 478ms\tremaining: 368ms\n",
      "113:\tlearn: 0.0009849\ttotal: 481ms\tremaining: 363ms\n",
      "114:\tlearn: 0.0009849\ttotal: 483ms\tremaining: 357ms\n",
      "115:\tlearn: 0.0009849\ttotal: 486ms\tremaining: 352ms\n",
      "116:\tlearn: 0.0009849\ttotal: 488ms\tremaining: 346ms\n",
      "117:\tlearn: 0.0009849\ttotal: 491ms\tremaining: 341ms\n",
      "118:\tlearn: 0.0009849\ttotal: 493ms\tremaining: 336ms\n",
      "119:\tlearn: 0.0009849\ttotal: 496ms\tremaining: 331ms\n",
      "120:\tlearn: 0.0009849\ttotal: 498ms\tremaining: 325ms\n",
      "121:\tlearn: 0.0009849\ttotal: 501ms\tremaining: 320ms\n",
      "122:\tlearn: 0.0009849\ttotal: 503ms\tremaining: 315ms\n",
      "123:\tlearn: 0.0009849\ttotal: 506ms\tremaining: 310ms\n",
      "124:\tlearn: 0.0009849\ttotal: 508ms\tremaining: 305ms\n",
      "125:\tlearn: 0.0009849\ttotal: 511ms\tremaining: 300ms\n",
      "126:\tlearn: 0.0009849\ttotal: 513ms\tremaining: 295ms\n",
      "127:\tlearn: 0.0009849\ttotal: 516ms\tremaining: 290ms\n",
      "128:\tlearn: 0.0009848\ttotal: 519ms\tremaining: 285ms\n",
      "129:\tlearn: 0.0009848\ttotal: 521ms\tremaining: 281ms\n",
      "130:\tlearn: 0.0009848\ttotal: 524ms\tremaining: 276ms\n",
      "131:\tlearn: 0.0009848\ttotal: 527ms\tremaining: 271ms\n",
      "132:\tlearn: 0.0009848\ttotal: 529ms\tremaining: 267ms\n",
      "133:\tlearn: 0.0009848\ttotal: 532ms\tremaining: 262ms\n",
      "134:\tlearn: 0.0009848\ttotal: 534ms\tremaining: 257ms\n",
      "135:\tlearn: 0.0009848\ttotal: 537ms\tremaining: 253ms\n",
      "136:\tlearn: 0.0009848\ttotal: 539ms\tremaining: 248ms\n",
      "137:\tlearn: 0.0009848\ttotal: 542ms\tremaining: 243ms\n",
      "138:\tlearn: 0.0009848\ttotal: 544ms\tremaining: 239ms\n",
      "139:\tlearn: 0.0009848\ttotal: 547ms\tremaining: 234ms\n",
      "140:\tlearn: 0.0009848\ttotal: 549ms\tremaining: 230ms\n",
      "141:\tlearn: 0.0009848\ttotal: 552ms\tremaining: 225ms\n",
      "142:\tlearn: 0.0009848\ttotal: 555ms\tremaining: 221ms\n",
      "143:\tlearn: 0.0009848\ttotal: 557ms\tremaining: 217ms\n",
      "144:\tlearn: 0.0009848\ttotal: 560ms\tremaining: 212ms\n",
      "145:\tlearn: 0.0009848\ttotal: 562ms\tremaining: 208ms\n",
      "146:\tlearn: 0.0009848\ttotal: 565ms\tremaining: 204ms\n",
      "147:\tlearn: 0.0009848\ttotal: 568ms\tremaining: 199ms\n",
      "148:\tlearn: 0.0009848\ttotal: 570ms\tremaining: 195ms\n",
      "149:\tlearn: 0.0009848\ttotal: 573ms\tremaining: 191ms\n",
      "150:\tlearn: 0.0009848\ttotal: 575ms\tremaining: 187ms\n",
      "151:\tlearn: 0.0009848\ttotal: 578ms\tremaining: 182ms\n",
      "152:\tlearn: 0.0009848\ttotal: 580ms\tremaining: 178ms\n",
      "153:\tlearn: 0.0009848\ttotal: 583ms\tremaining: 174ms\n",
      "154:\tlearn: 0.0009848\ttotal: 586ms\tremaining: 170ms\n",
      "155:\tlearn: 0.0009848\ttotal: 588ms\tremaining: 166ms\n",
      "156:\tlearn: 0.0009848\ttotal: 591ms\tremaining: 162ms\n",
      "157:\tlearn: 0.0009848\ttotal: 593ms\tremaining: 158ms\n",
      "158:\tlearn: 0.0009848\ttotal: 596ms\tremaining: 154ms\n",
      "159:\tlearn: 0.0009848\ttotal: 598ms\tremaining: 150ms\n",
      "160:\tlearn: 0.0009848\ttotal: 601ms\tremaining: 146ms\n",
      "161:\tlearn: 0.0009848\ttotal: 604ms\tremaining: 142ms\n",
      "162:\tlearn: 0.0009848\ttotal: 606ms\tremaining: 138ms\n",
      "163:\tlearn: 0.0009848\ttotal: 609ms\tremaining: 134ms\n",
      "164:\tlearn: 0.0009848\ttotal: 611ms\tremaining: 130ms\n",
      "165:\tlearn: 0.0009848\ttotal: 614ms\tremaining: 126ms\n",
      "166:\tlearn: 0.0009848\ttotal: 616ms\tremaining: 122ms\n",
      "167:\tlearn: 0.0009848\ttotal: 619ms\tremaining: 118ms\n",
      "168:\tlearn: 0.0009848\ttotal: 622ms\tremaining: 114ms\n",
      "169:\tlearn: 0.0009848\ttotal: 624ms\tremaining: 110ms\n",
      "170:\tlearn: 0.0009848\ttotal: 627ms\tremaining: 106ms\n",
      "171:\tlearn: 0.0009848\ttotal: 629ms\tremaining: 102ms\n",
      "172:\tlearn: 0.0009848\ttotal: 632ms\tremaining: 98.6ms\n",
      "173:\tlearn: 0.0009848\ttotal: 634ms\tremaining: 94.8ms\n",
      "174:\tlearn: 0.0009848\ttotal: 637ms\tremaining: 91ms\n",
      "175:\tlearn: 0.0009848\ttotal: 639ms\tremaining: 87.2ms\n",
      "176:\tlearn: 0.0009848\ttotal: 642ms\tremaining: 83.4ms\n",
      "177:\tlearn: 0.0009848\ttotal: 644ms\tremaining: 79.6ms\n",
      "178:\tlearn: 0.0009848\ttotal: 647ms\tremaining: 75.9ms\n",
      "179:\tlearn: 0.0009848\ttotal: 649ms\tremaining: 72.2ms\n",
      "180:\tlearn: 0.0009848\ttotal: 652ms\tremaining: 68.4ms\n",
      "181:\tlearn: 0.0009848\ttotal: 654ms\tremaining: 64.7ms\n",
      "182:\tlearn: 0.0009848\ttotal: 657ms\tremaining: 61ms\n",
      "183:\tlearn: 0.0009848\ttotal: 659ms\tremaining: 57.3ms\n",
      "184:\tlearn: 0.0009848\ttotal: 662ms\tremaining: 53.7ms\n",
      "185:\tlearn: 0.0009848\ttotal: 665ms\tremaining: 50ms\n",
      "186:\tlearn: 0.0009848\ttotal: 667ms\tremaining: 46.4ms\n",
      "187:\tlearn: 0.0009848\ttotal: 670ms\tremaining: 42.7ms\n",
      "188:\tlearn: 0.0009848\ttotal: 672ms\tremaining: 39.1ms\n",
      "189:\tlearn: 0.0009848\ttotal: 675ms\tremaining: 35.5ms\n",
      "190:\tlearn: 0.0009848\ttotal: 677ms\tremaining: 31.9ms\n",
      "191:\tlearn: 0.0009848\ttotal: 680ms\tremaining: 28.3ms\n",
      "192:\tlearn: 0.0009848\ttotal: 682ms\tremaining: 24.7ms\n",
      "193:\tlearn: 0.0009848\ttotal: 685ms\tremaining: 21.2ms\n",
      "194:\tlearn: 0.0009848\ttotal: 687ms\tremaining: 17.6ms\n",
      "195:\tlearn: 0.0009848\ttotal: 690ms\tremaining: 14.1ms\n",
      "196:\tlearn: 0.0009848\ttotal: 692ms\tremaining: 10.5ms\n",
      "197:\tlearn: 0.0009848\ttotal: 695ms\tremaining: 7.02ms\n",
      "198:\tlearn: 0.0009848\ttotal: 697ms\tremaining: 3.5ms\n",
      "199:\tlearn: 0.0009848\ttotal: 700ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03776741d55b4e07bf44d4c76b758552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5994809677168144, Recall = 0.5467081850533808, Aging Rate = 0.33830071174377224, Precision = 0.8080210387902695, f1 = 0.6521623772884054\n",
      "Epoch 2: Train Loss = 0.3820210790719002, Recall = 0.8540925266903915, Aging Rate = 0.4991103202846975, Precision = 0.8556149732620321, f1 = 0.8548530721282279\n",
      "Epoch 3: Train Loss = 0.2766612456362443, Recall = 0.9092526690391459, Aging Rate = 0.5044483985765125, Precision = 0.9012345679012346, f1 = 0.9052258635961028\n",
      "Epoch 4: Train Loss = 0.22577089378842255, Recall = 0.9252669039145908, Aging Rate = 0.4991103202846975, Precision = 0.9269162210338681, f1 = 0.9260908281389136\n",
      "Epoch 5: Train Loss = 0.19329774528211546, Recall = 0.931049822064057, Aging Rate = 0.49377224199288255, Precision = 0.9427927927927928, f1 = 0.9368845120859446\n",
      "Test Loss = 0.16476843503446342, Recall = 0.9443950177935944, Aging Rate = 0.49065836298932386, precision = 0.9623753399818676\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.15757534257882005, Recall = 0.943950177935943, Aging Rate = 0.4915480427046263, Precision = 0.9601809954751132, f1 = 0.9519964109466129\n",
      "Epoch 7: Train Loss = 0.1368233230349432, Recall = 0.9483985765124555, Aging Rate = 0.4875444839857651, Precision = 0.9726277372262774, f1 = 0.9603603603603604\n",
      "Epoch 8: Train Loss = 0.11800283478619365, Recall = 0.9559608540925267, Aging Rate = 0.4904359430604982, Precision = 0.9746031746031746, f1 = 0.965192005389625\n",
      "Epoch 9: Train Loss = 0.1022063870499991, Recall = 0.9626334519572953, Aging Rate = 0.48976868327402134, Precision = 0.9827429609445958, f1 = 0.9725842696629213\n",
      "Epoch 10: Train Loss = 0.08892500767769339, Recall = 0.9675266903914591, Aging Rate = 0.48976868327402134, Precision = 0.9877384196185286, f1 = 0.9775280898876405\n",
      "Test Loss = 0.07986022659255941, Recall = 0.9737544483985765, Aging Rate = 0.4922153024911032, precision = 0.9891549932218707\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07815506904521871, Recall = 0.9706405693950177, Aging Rate = 0.49088078291814946, Precision = 0.9886724059809696, f1 = 0.9795735129068462\n",
      "Epoch 12: Train Loss = 0.06961923404812176, Recall = 0.972864768683274, Aging Rate = 0.48976868327402134, Precision = 0.9931880108991825, f1 = 0.9829213483146068\n",
      "Epoch 13: Train Loss = 0.06212259426023612, Recall = 0.9768683274021353, Aging Rate = 0.4913256227758007, Precision = 0.9941149841557265, f1 = 0.9854161992371551\n",
      "Epoch 14: Train Loss = 0.05570424238080656, Recall = 0.983540925266904, Aging Rate = 0.4939946619217082, Precision = 0.9954975236380009, f1 = 0.9894831058402327\n",
      "Epoch 15: Train Loss = 0.05012351054751258, Recall = 0.9866548042704626, Aging Rate = 0.4957740213523132, Precision = 0.9950650515926425, f1 = 0.9908420817511726\n",
      "Test Loss = 0.04712713104125868, Recall = 0.9817615658362989, Aging Rate = 0.4915480427046263, precision = 0.9986425339366516\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04621344173055315, Recall = 0.9879893238434164, Aging Rate = 0.4957740213523132, Precision = 0.9964109466128309, f1 = 0.9921822649095376\n",
      "Epoch 17: Train Loss = 0.04287815067479619, Recall = 0.9888790035587188, Aging Rate = 0.4957740213523132, Precision = 0.9973082099596231, f1 = 0.9930757203484476\n",
      "Epoch 18: Train Loss = 0.039281835169713696, Recall = 0.9924377224199288, Aging Rate = 0.498220640569395, Precision = 0.9959821428571428, f1 = 0.9942067736185382\n",
      "Epoch 19: Train Loss = 0.03606721459228373, Recall = 0.9933274021352313, Aging Rate = 0.4971085409252669, Precision = 0.9991051454138703, f1 = 0.9962078964978809\n",
      "Epoch 20: Train Loss = 0.034088990912901976, Recall = 0.9937722419928826, Aging Rate = 0.49755338078291816, Precision = 0.9986589181940099, f1 = 0.9962095875139354\n",
      "Test Loss = 0.030308185065715337, Recall = 0.9959964412811388, Aging Rate = 0.498220640569395, precision = 0.9995535714285714\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03185081375875507, Recall = 0.994661921708185, Aging Rate = 0.4979982206405694, Precision = 0.9986601161232693, f1 = 0.9966570091375082\n",
      "Epoch 22: Train Loss = 0.030008167035055756, Recall = 0.9937722419928826, Aging Rate = 0.4971085409252669, Precision = 0.9995525727069351, f1 = 0.9966540263216596\n",
      "Epoch 23: Train Loss = 0.028398760461854976, Recall = 0.994661921708185, Aging Rate = 0.49755338078291816, Precision = 0.9995529727313366, f1 = 0.9971014492753624\n",
      "Epoch 24: Train Loss = 0.02753772678722054, Recall = 0.9951067615658363, Aging Rate = 0.498220640569395, Precision = 0.9986607142857142, f1 = 0.996880570409982\n",
      "Epoch 25: Train Loss = 0.025593891540986364, Recall = 0.9951067615658363, Aging Rate = 0.49777580071174377, Precision = 0.9995531724754245, f1 = 0.997325011145787\n",
      "Test Loss = 0.023004565202362597, Recall = 0.9968861209964412, Aging Rate = 0.4984430604982206, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.024725595495772957, Recall = 0.9959964412811388, Aging Rate = 0.4984430604982206, Precision = 0.9991075412762159, f1 = 0.9975495656048117\n",
      "Epoch 27: Train Loss = 0.023655849190766912, Recall = 0.994661921708185, Aging Rate = 0.4973309608540925, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.02263178769167632, Recall = 0.99644128113879, Aging Rate = 0.4984430604982206, Precision = 0.999553770638108, f1 = 0.9979950991312097\n",
      "Epoch 29: Train Loss = 0.02204109600403767, Recall = 0.9973309608540926, Aging Rate = 0.4988879003558719, Precision = 0.9995541685242978, f1 = 0.9984413270986416\n",
      "Epoch 30: Train Loss = 0.021283862959871936, Recall = 0.9968861209964412, Aging Rate = 0.4984430604982206, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.020322948439026853, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, precision = 0.9995547640249333\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.020442658432119683, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 32: Train Loss = 0.02037757255793201, Recall = 0.9968861209964412, Aging Rate = 0.4984430604982206, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.019961212077294063, Recall = 0.9959964412811388, Aging Rate = 0.4984430604982206, Precision = 0.9991075412762159, f1 = 0.9975495656048117\n",
      "Epoch 34: Train Loss = 0.01898949265851245, Recall = 0.9973309608540926, Aging Rate = 0.4986654804270463, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.018978132538205787, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Test Loss = 0.0183839121521898, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.01914055720037943, Recall = 0.9973309608540926, Aging Rate = 0.4986654804270463, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01805086081244343, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Epoch 38: Train Loss = 0.017740345495385102, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 39: Train Loss = 0.017228659298734943, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.017332528820559648, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015722682202413837, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.017067305425520044, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 42: Train Loss = 0.016915377912201067, Recall = 0.9973309608540926, Aging Rate = 0.4986654804270463, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.016651818800545247, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 44: Train Loss = 0.016674746905065728, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016460236811553033, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Test Loss = 0.014815888079247864, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.01638819582866392, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.016155587747905178, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.015713867910365404, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 49: Train Loss = 0.016050404587303193, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.015322757109310279, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013934584665924205, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.015612461116973616, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.015598335502225096, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Epoch 53: Train Loss = 0.0153523772305335, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015047107296216954, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015605658690488211, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014223077036175227, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015137049995919563, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.015157217219524961, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015373170113993072, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 59: Train Loss = 0.014847589832369331, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.014934602634338297, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Test Loss = 0.01421800367799306, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.016248584085299875, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.014791253685712602, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.014791304785824543, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 64: Train Loss = 0.014456972393648055, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015634110392969486, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Test Loss = 0.013419769909829432, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.014864183723714428, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.014677110391592746, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.014276495246461914, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.014322857017831021, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.014827704617216705, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013181254672039022, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.014231373850932325, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.01463665260379849, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 73: Train Loss = 0.014541895075757521, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.01428492357324663, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014639292787296492, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Test Loss = 0.01320148829463538, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.014038644270354955, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014637878991570655, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.014862065657549058, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.014341576634723945, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 80: Train Loss = 0.014776251888651653, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014713839167112558, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.01409325629698106, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014247186001376961, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.014148851306412993, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.014361546331771328, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.014304746015269137, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012767604727868084, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.014290049241265793, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.014303347911168673, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.013816720333772304, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.014444459229707718, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 90: Train Loss = 0.014204176540529601, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014698067282283433, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014239733647621398, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.01388720524401215, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.015792629499240277, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 94: Train Loss = 0.014072409250982292, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014579435704388653, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Test Loss = 0.014107314301356408, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 96: Train Loss = 0.013866848219517499, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 97: Train Loss = 0.014026953494076838, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.014123908326931271, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss = 0.014075651748298114, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.013948088037877532, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.012910397981833733, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.01369028137599075, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.013747819934663398, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.013791860072969647, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 104: Train Loss = 0.013753465936913609, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.014380699805782783, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Test Loss = 0.013241711082566675, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.013803601324611288, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.014398577931300602, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.01393273649770382, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.013803853371654542, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.013752754031948984, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012338078265936773, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.014074294998565709, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 112: Train Loss = 0.01365362486911414, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.013830799201761912, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.013418742810173816, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.01383878420574385, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012817970185384844, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.013645964207333178, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.013637793614956619, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.01371229074570579, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.014009675132839865, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 120: Train Loss = 0.013794344329537021, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012513288783963464, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 120.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5237306\ttotal: 7.36ms\tremaining: 1.46s\n",
      "1:\tlearn: 0.4312865\ttotal: 13.4ms\tremaining: 1.33s\n",
      "2:\tlearn: 0.3364103\ttotal: 20ms\tremaining: 1.31s\n",
      "3:\tlearn: 0.2719769\ttotal: 26ms\tremaining: 1.27s\n",
      "4:\tlearn: 0.2360532\ttotal: 32.1ms\tremaining: 1.25s\n",
      "5:\tlearn: 0.2011289\ttotal: 38.3ms\tremaining: 1.24s\n",
      "6:\tlearn: 0.1693043\ttotal: 44.4ms\tremaining: 1.22s\n",
      "7:\tlearn: 0.1495580\ttotal: 50.3ms\tremaining: 1.21s\n",
      "8:\tlearn: 0.1297733\ttotal: 57.6ms\tremaining: 1.22s\n",
      "9:\tlearn: 0.1108461\ttotal: 63.4ms\tremaining: 1.2s\n",
      "10:\tlearn: 0.0973115\ttotal: 69.5ms\tremaining: 1.19s\n",
      "11:\tlearn: 0.0863590\ttotal: 75.2ms\tremaining: 1.18s\n",
      "12:\tlearn: 0.0756124\ttotal: 81.3ms\tremaining: 1.17s\n",
      "13:\tlearn: 0.0682337\ttotal: 87.3ms\tremaining: 1.16s\n",
      "14:\tlearn: 0.0606658\ttotal: 92.5ms\tremaining: 1.14s\n",
      "15:\tlearn: 0.0542155\ttotal: 97.4ms\tremaining: 1.12s\n",
      "16:\tlearn: 0.0487117\ttotal: 102ms\tremaining: 1.1s\n",
      "17:\tlearn: 0.0438254\ttotal: 107ms\tremaining: 1.08s\n",
      "18:\tlearn: 0.0404511\ttotal: 112ms\tremaining: 1.07s\n",
      "19:\tlearn: 0.0370771\ttotal: 117ms\tremaining: 1.05s\n",
      "20:\tlearn: 0.0344132\ttotal: 122ms\tremaining: 1.04s\n",
      "21:\tlearn: 0.0321425\ttotal: 126ms\tremaining: 1.02s\n",
      "22:\tlearn: 0.0296957\ttotal: 131ms\tremaining: 1.01s\n",
      "23:\tlearn: 0.0266619\ttotal: 136ms\tremaining: 998ms\n",
      "24:\tlearn: 0.0244105\ttotal: 141ms\tremaining: 987ms\n",
      "25:\tlearn: 0.0231125\ttotal: 145ms\tremaining: 970ms\n",
      "26:\tlearn: 0.0212077\ttotal: 150ms\tremaining: 959ms\n",
      "27:\tlearn: 0.0193954\ttotal: 155ms\tremaining: 951ms\n",
      "28:\tlearn: 0.0178857\ttotal: 160ms\tremaining: 942ms\n",
      "29:\tlearn: 0.0165557\ttotal: 164ms\tremaining: 932ms\n",
      "30:\tlearn: 0.0152758\ttotal: 169ms\tremaining: 922ms\n",
      "31:\tlearn: 0.0143605\ttotal: 174ms\tremaining: 913ms\n",
      "32:\tlearn: 0.0134484\ttotal: 179ms\tremaining: 904ms\n",
      "33:\tlearn: 0.0121161\ttotal: 184ms\tremaining: 897ms\n",
      "34:\tlearn: 0.0111870\ttotal: 189ms\tremaining: 890ms\n",
      "35:\tlearn: 0.0102952\ttotal: 194ms\tremaining: 885ms\n",
      "36:\tlearn: 0.0096824\ttotal: 199ms\tremaining: 876ms\n",
      "37:\tlearn: 0.0089268\ttotal: 204ms\tremaining: 869ms\n",
      "38:\tlearn: 0.0081882\ttotal: 209ms\tremaining: 861ms\n",
      "39:\tlearn: 0.0078597\ttotal: 213ms\tremaining: 853ms\n",
      "40:\tlearn: 0.0073628\ttotal: 218ms\tremaining: 847ms\n",
      "41:\tlearn: 0.0067712\ttotal: 223ms\tremaining: 839ms\n",
      "42:\tlearn: 0.0064626\ttotal: 227ms\tremaining: 829ms\n",
      "43:\tlearn: 0.0060239\ttotal: 232ms\tremaining: 824ms\n",
      "44:\tlearn: 0.0055627\ttotal: 238ms\tremaining: 819ms\n",
      "45:\tlearn: 0.0051737\ttotal: 243ms\tremaining: 813ms\n",
      "46:\tlearn: 0.0048595\ttotal: 248ms\tremaining: 806ms\n",
      "47:\tlearn: 0.0045398\ttotal: 252ms\tremaining: 800ms\n",
      "48:\tlearn: 0.0041934\ttotal: 257ms\tremaining: 793ms\n",
      "49:\tlearn: 0.0037858\ttotal: 262ms\tremaining: 787ms\n",
      "50:\tlearn: 0.0035043\ttotal: 267ms\tremaining: 781ms\n",
      "51:\tlearn: 0.0033265\ttotal: 272ms\tremaining: 773ms\n",
      "52:\tlearn: 0.0030989\ttotal: 276ms\tremaining: 765ms\n",
      "53:\tlearn: 0.0029495\ttotal: 281ms\tremaining: 759ms\n",
      "54:\tlearn: 0.0027091\ttotal: 286ms\tremaining: 754ms\n",
      "55:\tlearn: 0.0026339\ttotal: 289ms\tremaining: 744ms\n",
      "56:\tlearn: 0.0025250\ttotal: 294ms\tremaining: 736ms\n",
      "57:\tlearn: 0.0023296\ttotal: 298ms\tremaining: 731ms\n",
      "58:\tlearn: 0.0021912\ttotal: 303ms\tremaining: 724ms\n",
      "59:\tlearn: 0.0020527\ttotal: 308ms\tremaining: 718ms\n",
      "60:\tlearn: 0.0018996\ttotal: 313ms\tremaining: 713ms\n",
      "61:\tlearn: 0.0018316\ttotal: 316ms\tremaining: 704ms\n",
      "62:\tlearn: 0.0018028\ttotal: 319ms\tremaining: 694ms\n",
      "63:\tlearn: 0.0017442\ttotal: 323ms\tremaining: 686ms\n",
      "64:\tlearn: 0.0017007\ttotal: 327ms\tremaining: 678ms\n",
      "65:\tlearn: 0.0016711\ttotal: 330ms\tremaining: 669ms\n",
      "66:\tlearn: 0.0015726\ttotal: 334ms\tremaining: 663ms\n",
      "67:\tlearn: 0.0015186\ttotal: 338ms\tremaining: 657ms\n",
      "68:\tlearn: 0.0014754\ttotal: 342ms\tremaining: 649ms\n",
      "69:\tlearn: 0.0013659\ttotal: 346ms\tremaining: 643ms\n",
      "70:\tlearn: 0.0012784\ttotal: 351ms\tremaining: 638ms\n",
      "71:\tlearn: 0.0012323\ttotal: 355ms\tremaining: 631ms\n",
      "72:\tlearn: 0.0012023\ttotal: 359ms\tremaining: 624ms\n",
      "73:\tlearn: 0.0011389\ttotal: 363ms\tremaining: 619ms\n",
      "74:\tlearn: 0.0010808\ttotal: 368ms\tremaining: 614ms\n",
      "75:\tlearn: 0.0010139\ttotal: 373ms\tremaining: 609ms\n",
      "76:\tlearn: 0.0009562\ttotal: 378ms\tremaining: 603ms\n",
      "77:\tlearn: 0.0009250\ttotal: 382ms\tremaining: 598ms\n",
      "78:\tlearn: 0.0009109\ttotal: 385ms\tremaining: 590ms\n",
      "79:\tlearn: 0.0009109\ttotal: 388ms\tremaining: 582ms\n",
      "80:\tlearn: 0.0008833\ttotal: 392ms\tremaining: 576ms\n",
      "81:\tlearn: 0.0008833\ttotal: 395ms\tremaining: 568ms\n",
      "82:\tlearn: 0.0008628\ttotal: 398ms\tremaining: 560ms\n",
      "83:\tlearn: 0.0008284\ttotal: 402ms\tremaining: 555ms\n",
      "84:\tlearn: 0.0008075\ttotal: 406ms\tremaining: 549ms\n",
      "85:\tlearn: 0.0007873\ttotal: 410ms\tremaining: 543ms\n",
      "86:\tlearn: 0.0007610\ttotal: 414ms\tremaining: 537ms\n",
      "87:\tlearn: 0.0007610\ttotal: 417ms\tremaining: 530ms\n",
      "88:\tlearn: 0.0007404\ttotal: 420ms\tremaining: 523ms\n",
      "89:\tlearn: 0.0007404\ttotal: 422ms\tremaining: 516ms\n",
      "90:\tlearn: 0.0007404\ttotal: 425ms\tremaining: 509ms\n",
      "91:\tlearn: 0.0007404\ttotal: 427ms\tremaining: 502ms\n",
      "92:\tlearn: 0.0007341\ttotal: 430ms\tremaining: 495ms\n",
      "93:\tlearn: 0.0007099\ttotal: 434ms\tremaining: 489ms\n",
      "94:\tlearn: 0.0007098\ttotal: 437ms\tremaining: 483ms\n",
      "95:\tlearn: 0.0007098\ttotal: 439ms\tremaining: 476ms\n",
      "96:\tlearn: 0.0007098\ttotal: 442ms\tremaining: 469ms\n",
      "97:\tlearn: 0.0007098\ttotal: 445ms\tremaining: 463ms\n",
      "98:\tlearn: 0.0007098\ttotal: 447ms\tremaining: 456ms\n",
      "99:\tlearn: 0.0007098\ttotal: 450ms\tremaining: 450ms\n",
      "100:\tlearn: 0.0007097\ttotal: 453ms\tremaining: 444ms\n",
      "101:\tlearn: 0.0007097\ttotal: 455ms\tremaining: 438ms\n",
      "102:\tlearn: 0.0007097\ttotal: 458ms\tremaining: 431ms\n",
      "103:\tlearn: 0.0007030\ttotal: 461ms\tremaining: 425ms\n",
      "104:\tlearn: 0.0007030\ttotal: 464ms\tremaining: 420ms\n",
      "105:\tlearn: 0.0007030\ttotal: 466ms\tremaining: 413ms\n",
      "106:\tlearn: 0.0007029\ttotal: 469ms\tremaining: 408ms\n",
      "107:\tlearn: 0.0007029\ttotal: 472ms\tremaining: 402ms\n",
      "108:\tlearn: 0.0007029\ttotal: 474ms\tremaining: 396ms\n",
      "109:\tlearn: 0.0006864\ttotal: 478ms\tremaining: 391ms\n",
      "110:\tlearn: 0.0006864\ttotal: 480ms\tremaining: 385ms\n",
      "111:\tlearn: 0.0006864\ttotal: 483ms\tremaining: 379ms\n",
      "112:\tlearn: 0.0006864\ttotal: 486ms\tremaining: 374ms\n",
      "113:\tlearn: 0.0006864\ttotal: 488ms\tremaining: 368ms\n",
      "114:\tlearn: 0.0006864\ttotal: 491ms\tremaining: 363ms\n",
      "115:\tlearn: 0.0006864\ttotal: 493ms\tremaining: 357ms\n",
      "116:\tlearn: 0.0006864\ttotal: 496ms\tremaining: 352ms\n",
      "117:\tlearn: 0.0006863\ttotal: 498ms\tremaining: 346ms\n",
      "118:\tlearn: 0.0006863\ttotal: 501ms\tremaining: 341ms\n",
      "119:\tlearn: 0.0006863\ttotal: 504ms\tremaining: 336ms\n",
      "120:\tlearn: 0.0006863\ttotal: 506ms\tremaining: 331ms\n",
      "121:\tlearn: 0.0006863\ttotal: 509ms\tremaining: 325ms\n",
      "122:\tlearn: 0.0006863\ttotal: 512ms\tremaining: 320ms\n",
      "123:\tlearn: 0.0006863\ttotal: 514ms\tremaining: 315ms\n",
      "124:\tlearn: 0.0006863\ttotal: 517ms\tremaining: 310ms\n",
      "125:\tlearn: 0.0006863\ttotal: 520ms\tremaining: 305ms\n",
      "126:\tlearn: 0.0006863\ttotal: 522ms\tremaining: 300ms\n",
      "127:\tlearn: 0.0006863\ttotal: 525ms\tremaining: 295ms\n",
      "128:\tlearn: 0.0006863\ttotal: 527ms\tremaining: 290ms\n",
      "129:\tlearn: 0.0006863\ttotal: 529ms\tremaining: 285ms\n",
      "130:\tlearn: 0.0006863\ttotal: 532ms\tremaining: 280ms\n",
      "131:\tlearn: 0.0006863\ttotal: 535ms\tremaining: 275ms\n",
      "132:\tlearn: 0.0006863\ttotal: 537ms\tremaining: 271ms\n",
      "133:\tlearn: 0.0006863\ttotal: 540ms\tremaining: 266ms\n",
      "134:\tlearn: 0.0006863\ttotal: 542ms\tremaining: 261ms\n",
      "135:\tlearn: 0.0006863\ttotal: 545ms\tremaining: 256ms\n",
      "136:\tlearn: 0.0006863\ttotal: 548ms\tremaining: 252ms\n",
      "137:\tlearn: 0.0006863\ttotal: 550ms\tremaining: 247ms\n",
      "138:\tlearn: 0.0006863\ttotal: 553ms\tremaining: 243ms\n",
      "139:\tlearn: 0.0006863\ttotal: 555ms\tremaining: 238ms\n",
      "140:\tlearn: 0.0006863\ttotal: 558ms\tremaining: 233ms\n",
      "141:\tlearn: 0.0006863\ttotal: 561ms\tremaining: 229ms\n",
      "142:\tlearn: 0.0006863\ttotal: 563ms\tremaining: 225ms\n",
      "143:\tlearn: 0.0006863\ttotal: 566ms\tremaining: 220ms\n",
      "144:\tlearn: 0.0006863\ttotal: 569ms\tremaining: 216ms\n",
      "145:\tlearn: 0.0006863\ttotal: 571ms\tremaining: 211ms\n",
      "146:\tlearn: 0.0006863\ttotal: 574ms\tremaining: 207ms\n",
      "147:\tlearn: 0.0006863\ttotal: 576ms\tremaining: 202ms\n",
      "148:\tlearn: 0.0006863\ttotal: 579ms\tremaining: 198ms\n",
      "149:\tlearn: 0.0006863\ttotal: 581ms\tremaining: 194ms\n",
      "150:\tlearn: 0.0006863\ttotal: 584ms\tremaining: 189ms\n",
      "151:\tlearn: 0.0006863\ttotal: 586ms\tremaining: 185ms\n",
      "152:\tlearn: 0.0006863\ttotal: 589ms\tremaining: 181ms\n",
      "153:\tlearn: 0.0006863\ttotal: 591ms\tremaining: 177ms\n",
      "154:\tlearn: 0.0006863\ttotal: 594ms\tremaining: 172ms\n",
      "155:\tlearn: 0.0006863\ttotal: 596ms\tremaining: 168ms\n",
      "156:\tlearn: 0.0006863\ttotal: 599ms\tremaining: 164ms\n",
      "157:\tlearn: 0.0006863\ttotal: 602ms\tremaining: 160ms\n",
      "158:\tlearn: 0.0006863\ttotal: 604ms\tremaining: 156ms\n",
      "159:\tlearn: 0.0006863\ttotal: 607ms\tremaining: 152ms\n",
      "160:\tlearn: 0.0006863\ttotal: 610ms\tremaining: 148ms\n",
      "161:\tlearn: 0.0006863\ttotal: 612ms\tremaining: 144ms\n",
      "162:\tlearn: 0.0006863\ttotal: 615ms\tremaining: 140ms\n",
      "163:\tlearn: 0.0006863\ttotal: 617ms\tremaining: 136ms\n",
      "164:\tlearn: 0.0006863\ttotal: 620ms\tremaining: 131ms\n",
      "165:\tlearn: 0.0006863\ttotal: 622ms\tremaining: 127ms\n",
      "166:\tlearn: 0.0006863\ttotal: 625ms\tremaining: 123ms\n",
      "167:\tlearn: 0.0006863\ttotal: 628ms\tremaining: 120ms\n",
      "168:\tlearn: 0.0006863\ttotal: 630ms\tremaining: 116ms\n",
      "169:\tlearn: 0.0006863\ttotal: 633ms\tremaining: 112ms\n",
      "170:\tlearn: 0.0006863\ttotal: 635ms\tremaining: 108ms\n",
      "171:\tlearn: 0.0006863\ttotal: 638ms\tremaining: 104ms\n",
      "172:\tlearn: 0.0006863\ttotal: 641ms\tremaining: 100ms\n",
      "173:\tlearn: 0.0006863\ttotal: 643ms\tremaining: 96.1ms\n",
      "174:\tlearn: 0.0006863\ttotal: 646ms\tremaining: 92.2ms\n",
      "175:\tlearn: 0.0006863\ttotal: 648ms\tremaining: 88.4ms\n",
      "176:\tlearn: 0.0006862\ttotal: 651ms\tremaining: 84.5ms\n",
      "177:\tlearn: 0.0006862\ttotal: 653ms\tremaining: 80.7ms\n",
      "178:\tlearn: 0.0006862\ttotal: 656ms\tremaining: 76.9ms\n",
      "179:\tlearn: 0.0006862\ttotal: 658ms\tremaining: 73.2ms\n",
      "180:\tlearn: 0.0006862\ttotal: 661ms\tremaining: 69.4ms\n",
      "181:\tlearn: 0.0006862\ttotal: 664ms\tremaining: 65.6ms\n",
      "182:\tlearn: 0.0006861\ttotal: 666ms\tremaining: 61.9ms\n",
      "183:\tlearn: 0.0006861\ttotal: 669ms\tremaining: 58.2ms\n",
      "184:\tlearn: 0.0006861\ttotal: 672ms\tremaining: 54.5ms\n",
      "185:\tlearn: 0.0006861\ttotal: 674ms\tremaining: 50.7ms\n",
      "186:\tlearn: 0.0006861\ttotal: 677ms\tremaining: 47ms\n",
      "187:\tlearn: 0.0006861\ttotal: 679ms\tremaining: 43.4ms\n",
      "188:\tlearn: 0.0006861\ttotal: 682ms\tremaining: 39.7ms\n",
      "189:\tlearn: 0.0006861\ttotal: 685ms\tremaining: 36ms\n",
      "190:\tlearn: 0.0006861\ttotal: 687ms\tremaining: 32.4ms\n",
      "191:\tlearn: 0.0006861\ttotal: 690ms\tremaining: 28.7ms\n",
      "192:\tlearn: 0.0006861\ttotal: 692ms\tremaining: 25.1ms\n",
      "193:\tlearn: 0.0006861\ttotal: 695ms\tremaining: 21.5ms\n",
      "194:\tlearn: 0.0006861\ttotal: 698ms\tremaining: 17.9ms\n",
      "195:\tlearn: 0.0006861\ttotal: 700ms\tremaining: 14.3ms\n",
      "196:\tlearn: 0.0006861\ttotal: 703ms\tremaining: 10.7ms\n",
      "197:\tlearn: 0.0006861\ttotal: 705ms\tremaining: 7.13ms\n",
      "198:\tlearn: 0.0006861\ttotal: 708ms\tremaining: 3.56ms\n",
      "199:\tlearn: 0.0006861\ttotal: 710ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30ca9c47a134ebdb81e46998a932b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5950231617880037, Recall = 0.6236654804270463, Aging Rate = 0.40102313167259784, Precision = 0.7775929007210205, f1 = 0.6921747716613182\n",
      "Epoch 2: Train Loss = 0.3735572226425083, Recall = 0.8661032028469751, Aging Rate = 0.5022241992882562, Precision = 0.8622674933569531, f1 = 0.8641810918774967\n",
      "Epoch 3: Train Loss = 0.2694085768533347, Recall = 0.9177046263345195, Aging Rate = 0.5077846975088968, Precision = 0.9036355672360928, f1 = 0.9106157581107923\n",
      "Epoch 4: Train Loss = 0.22309025740284089, Recall = 0.9292704626334519, Aging Rate = 0.4991103202846975, Precision = 0.9309269162210339, f1 = 0.9300979519145146\n",
      "Epoch 5: Train Loss = 0.19490224907830941, Recall = 0.9350533807829181, Aging Rate = 0.4986654804270463, Precision = 0.9375557537912578, f1 = 0.9363028953229398\n",
      "Test Loss = 0.16414836110063294, Recall = 0.9457295373665481, Aging Rate = 0.4984430604982206, precision = 0.9486836233824185\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.1563367091145804, Recall = 0.9421708185053381, Aging Rate = 0.4904359430604982, Precision = 0.9605442176870749, f1 = 0.951268807545475\n",
      "Epoch 7: Train Loss = 0.13612702091181406, Recall = 0.9497330960854092, Aging Rate = 0.491770462633452, Precision = 0.9656264133876075, f1 = 0.957613814756672\n",
      "Epoch 8: Train Loss = 0.11693774200843322, Recall = 0.9506227758007118, Aging Rate = 0.48576512455516013, Precision = 0.9784798534798534, f1 = 0.9643501805054151\n",
      "Epoch 9: Train Loss = 0.10115873442425846, Recall = 0.9612989323843416, Aging Rate = 0.4886565836298932, Precision = 0.9836140191169777, f1 = 0.9723284589426321\n",
      "Epoch 10: Train Loss = 0.08913342302061909, Recall = 0.9679715302491103, Aging Rate = 0.4922153024911032, Precision = 0.9832806145503841, f1 = 0.9755660165882089\n",
      "Test Loss = 0.08203649105921759, Recall = 0.9684163701067615, Aging Rate = 0.48821174377224197, precision = 0.9917995444191344\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07919311645404299, Recall = 0.9737544483985765, Aging Rate = 0.49243772241992884, Precision = 0.9887082204155375, f1 = 0.9811743612729718\n",
      "Epoch 12: Train Loss = 0.07182849116384771, Recall = 0.9764234875444839, Aging Rate = 0.49377224199288255, Precision = 0.9887387387387387, f1 = 0.9825425246195165\n",
      "Epoch 13: Train Loss = 0.06301149349407793, Recall = 0.9817615658362989, Aging Rate = 0.4951067615658363, Precision = 0.9914645103324349, f1 = 0.9865891819400983\n",
      "Epoch 14: Train Loss = 0.057151400215685155, Recall = 0.983540925266904, Aging Rate = 0.49488434163701067, Precision = 0.9937078651685394, f1 = 0.9885982562038901\n",
      "Epoch 15: Train Loss = 0.05255352029596783, Recall = 0.9848754448398577, Aging Rate = 0.4951067615658363, Precision = 0.9946091644204852, f1 = 0.989718372820742\n",
      "Test Loss = 0.04776869490212393, Recall = 0.9830960854092526, Aging Rate = 0.49266014234875444, precision = 0.9977426636568849\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04851052898157958, Recall = 0.9857651245551602, Aging Rate = 0.4959964412811388, Precision = 0.9937219730941704, f1 = 0.9897275569450649\n",
      "Epoch 17: Train Loss = 0.04420065735980497, Recall = 0.9866548042704626, Aging Rate = 0.49555160142348753, Precision = 0.9955116696588869, f1 = 0.9910634495084898\n",
      "Epoch 18: Train Loss = 0.04213725615398846, Recall = 0.9879893238434164, Aging Rate = 0.4957740213523132, Precision = 0.9964109466128309, f1 = 0.9921822649095376\n",
      "Epoch 19: Train Loss = 0.038227893491656756, Recall = 0.9911032028469751, Aging Rate = 0.4973309608540925, Precision = 0.9964221824686941, f1 = 0.9937555753791257\n",
      "Epoch 20: Train Loss = 0.03585290466020964, Recall = 0.9906583629893239, Aging Rate = 0.49666370106761565, Precision = 0.9973130317957905, f1 = 0.9939745592501674\n",
      "Test Loss = 0.03215571388178024, Recall = 0.9937722419928826, Aging Rate = 0.4973309608540925, precision = 0.9991055456171736\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03338425864342905, Recall = 0.9928825622775801, Aging Rate = 0.49755338078291816, Precision = 0.997764863656683, f1 = 0.9953177257525084\n",
      "Epoch 22: Train Loss = 0.03131233285648543, Recall = 0.9951067615658363, Aging Rate = 0.498220640569395, Precision = 0.9986607142857142, f1 = 0.996880570409982\n",
      "Epoch 23: Train Loss = 0.03039529069171045, Recall = 0.9937722419928826, Aging Rate = 0.4979982206405694, Precision = 0.9977668602054489, f1 = 0.9957655449075107\n",
      "Epoch 24: Train Loss = 0.02835840592657968, Recall = 0.99644128113879, Aging Rate = 0.4988879003558719, Precision = 0.9986625055728935, f1 = 0.9975506568692941\n",
      "Epoch 25: Train Loss = 0.02708102120384839, Recall = 0.9968861209964412, Aging Rate = 0.49933274021352314, Precision = 0.998218262806236, f1 = 0.9975517471622524\n",
      "Test Loss = 0.024384104484340897, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, precision = 0.999554565701559\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.02564576488929499, Recall = 0.99644128113879, Aging Rate = 0.4986654804270463, Precision = 0.9991079393398751, f1 = 0.997772828507795\n",
      "Epoch 27: Train Loss = 0.0249132169112298, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Epoch 28: Train Loss = 0.024282586326643665, Recall = 0.99644128113879, Aging Rate = 0.4986654804270463, Precision = 0.9991079393398751, f1 = 0.997772828507795\n",
      "Epoch 29: Train Loss = 0.023099448419179356, Recall = 0.998220640569395, Aging Rate = 0.49955516014234874, Precision = 0.9991095280498664, f1 = 0.9986648865153537\n",
      "Epoch 30: Train Loss = 0.022784054912361597, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Test Loss = 0.0204842767655001, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.021852856064447305, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.02113215652463487, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.02039345904018107, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.020238489091555418, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.020024637933621626, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.017817045074764944, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.019045427786285766, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 37: Train Loss = 0.01891581266057873, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 38: Train Loss = 0.018398129119408514, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.018744036545347276, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.017882106169474932, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Test Loss = 0.016467928558296367, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.017636996687280537, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 42: Train Loss = 0.017523668938419147, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01742105824322675, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.017537446845044446, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 45: Train Loss = 0.017219951880094845, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01547116661954711, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.01665457347871941, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.01668272525390272, Recall = 0.9986654804270463, Aging Rate = 0.49955516014234874, Precision = 0.9995547640249333, f1 = 0.9991099243435693\n",
      "Epoch 48: Train Loss = 0.01650681023177727, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.01624598540365696, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.015839620947413598, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01469772728657171, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.01597207428032393, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.015909225597076145, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.01583269288186713, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.01585181107830747, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 55: Train Loss = 0.015217811525451331, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014373876734427922, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015638446030985843, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.01627505480500727, Recall = 0.9995551601423488, Aging Rate = 0.5002224199288257, Precision = 0.9991107158737217, f1 = 0.9993328885923949\n",
      "Epoch 58: Train Loss = 0.015440354872746824, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.014963518735352784, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015059504227122803, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Test Loss = 0.013894470547760085, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015289831691126594, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.015074277926382221, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015619221637257477, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.014812928442163824, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.015007182624310361, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.014432917441841334, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.014996283869938494, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.014877638801123642, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 68: Train Loss = 0.014697077395759019, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.015082388973771677, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015307977477426632, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.014037084338185625, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.01489895227567369, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 72: Train Loss = 0.014686130152955598, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.014590916661313633, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.014545787526858023, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014636208127614018, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013631794677908099, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 75 is saved.\n",
      "\n",
      "Epoch 76: Train Loss = 0.014358779335944679, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.01455379518087012, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.015517570356775433, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.014831948276357294, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.014750808788947363, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013177455953087034, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.014097910286573647, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014449168891510081, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.014714675339011747, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 84: Train Loss = 0.014209905095743115, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.014313612378523867, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013140575542674794, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.01453363479138269, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 87: Train Loss = 0.014331263388934614, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.01484276507102828, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.014179151167171706, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.01483343013320317, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012942467249426128, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.01442028493438168, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.01483436946316334, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.014318135258778134, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.014639011627308414, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.01383579002686667, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012660104085383041, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.014257418628105914, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.014282662550723213, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.01389329600084931, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.014404565606306033, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.01391432601030505, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01284896104772214, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: Train Loss = 0.014333481028729063, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.01393939994832375, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.013924045687097247, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.014245879286204157, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.014597775293613563, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013743083745082077, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 106: Train Loss = 0.01449200937199635, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 107: Train Loss = 0.014278128617227714, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.014314965968056717, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.01436855962161916, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.014312531142843575, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012686758504325385, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.014706552222543445, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.01377442348218155, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.013953991487847634, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.013990465883046282, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.01403759562102183, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.012791045232148994, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.014066120679241694, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.013649144533067092, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.013917763750615705, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.014077169015287716, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.014331791974153383, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012866653064497848, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.013635827954261964, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.014120099648462071, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.014224895847848619, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.014514521002981586, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.013830357691889556, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012913696815050953, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5276403\ttotal: 5.62ms\tremaining: 1.12s\n",
      "1:\tlearn: 0.4229303\ttotal: 11.9ms\tremaining: 1.18s\n",
      "2:\tlearn: 0.3594827\ttotal: 17.8ms\tremaining: 1.17s\n",
      "3:\tlearn: 0.2890789\ttotal: 23.7ms\tremaining: 1.16s\n",
      "4:\tlearn: 0.2440786\ttotal: 29.6ms\tremaining: 1.15s\n",
      "5:\tlearn: 0.2130919\ttotal: 35.4ms\tremaining: 1.15s\n",
      "6:\tlearn: 0.1885503\ttotal: 41.5ms\tremaining: 1.14s\n",
      "7:\tlearn: 0.1570275\ttotal: 47.8ms\tremaining: 1.15s\n",
      "8:\tlearn: 0.1382568\ttotal: 53.5ms\tremaining: 1.14s\n",
      "9:\tlearn: 0.1156440\ttotal: 59.7ms\tremaining: 1.13s\n",
      "10:\tlearn: 0.0992250\ttotal: 65.7ms\tremaining: 1.13s\n",
      "11:\tlearn: 0.0867685\ttotal: 71.4ms\tremaining: 1.12s\n",
      "12:\tlearn: 0.0757502\ttotal: 76.4ms\tremaining: 1.1s\n",
      "13:\tlearn: 0.0683303\ttotal: 81.2ms\tremaining: 1.08s\n",
      "14:\tlearn: 0.0628269\ttotal: 86ms\tremaining: 1.06s\n",
      "15:\tlearn: 0.0556037\ttotal: 91.1ms\tremaining: 1.05s\n",
      "16:\tlearn: 0.0513753\ttotal: 95.6ms\tremaining: 1.03s\n",
      "17:\tlearn: 0.0446124\ttotal: 101ms\tremaining: 1.02s\n",
      "18:\tlearn: 0.0400175\ttotal: 106ms\tremaining: 1.01s\n",
      "19:\tlearn: 0.0373105\ttotal: 110ms\tremaining: 992ms\n",
      "20:\tlearn: 0.0341874\ttotal: 115ms\tremaining: 980ms\n",
      "21:\tlearn: 0.0312182\ttotal: 120ms\tremaining: 970ms\n",
      "22:\tlearn: 0.0284486\ttotal: 125ms\tremaining: 961ms\n",
      "23:\tlearn: 0.0260033\ttotal: 130ms\tremaining: 951ms\n",
      "24:\tlearn: 0.0241547\ttotal: 134ms\tremaining: 940ms\n",
      "25:\tlearn: 0.0221162\ttotal: 139ms\tremaining: 931ms\n",
      "26:\tlearn: 0.0199039\ttotal: 144ms\tremaining: 923ms\n",
      "27:\tlearn: 0.0180907\ttotal: 149ms\tremaining: 915ms\n",
      "28:\tlearn: 0.0166705\ttotal: 154ms\tremaining: 907ms\n",
      "29:\tlearn: 0.0151922\ttotal: 158ms\tremaining: 897ms\n",
      "30:\tlearn: 0.0138874\ttotal: 163ms\tremaining: 890ms\n",
      "31:\tlearn: 0.0130836\ttotal: 168ms\tremaining: 882ms\n",
      "32:\tlearn: 0.0122382\ttotal: 173ms\tremaining: 875ms\n",
      "33:\tlearn: 0.0116027\ttotal: 177ms\tremaining: 867ms\n",
      "34:\tlearn: 0.0108243\ttotal: 182ms\tremaining: 860ms\n",
      "35:\tlearn: 0.0099690\ttotal: 187ms\tremaining: 851ms\n",
      "36:\tlearn: 0.0091350\ttotal: 192ms\tremaining: 846ms\n",
      "37:\tlearn: 0.0085961\ttotal: 197ms\tremaining: 839ms\n",
      "38:\tlearn: 0.0081058\ttotal: 201ms\tremaining: 832ms\n",
      "39:\tlearn: 0.0076180\ttotal: 206ms\tremaining: 824ms\n",
      "40:\tlearn: 0.0070175\ttotal: 211ms\tremaining: 818ms\n",
      "41:\tlearn: 0.0064756\ttotal: 216ms\tremaining: 812ms\n",
      "42:\tlearn: 0.0060497\ttotal: 221ms\tremaining: 806ms\n",
      "43:\tlearn: 0.0056482\ttotal: 225ms\tremaining: 799ms\n",
      "44:\tlearn: 0.0054206\ttotal: 229ms\tremaining: 788ms\n",
      "45:\tlearn: 0.0052427\ttotal: 232ms\tremaining: 777ms\n",
      "46:\tlearn: 0.0048186\ttotal: 237ms\tremaining: 772ms\n",
      "47:\tlearn: 0.0044149\ttotal: 242ms\tremaining: 766ms\n",
      "48:\tlearn: 0.0041245\ttotal: 246ms\tremaining: 759ms\n",
      "49:\tlearn: 0.0038058\ttotal: 251ms\tremaining: 754ms\n",
      "50:\tlearn: 0.0035203\ttotal: 256ms\tremaining: 748ms\n",
      "51:\tlearn: 0.0032447\ttotal: 261ms\tremaining: 743ms\n",
      "52:\tlearn: 0.0030677\ttotal: 265ms\tremaining: 734ms\n",
      "53:\tlearn: 0.0028776\ttotal: 269ms\tremaining: 728ms\n",
      "54:\tlearn: 0.0027229\ttotal: 274ms\tremaining: 723ms\n",
      "55:\tlearn: 0.0024916\ttotal: 279ms\tremaining: 717ms\n",
      "56:\tlearn: 0.0022764\ttotal: 283ms\tremaining: 711ms\n",
      "57:\tlearn: 0.0021495\ttotal: 288ms\tremaining: 705ms\n",
      "58:\tlearn: 0.0020305\ttotal: 292ms\tremaining: 699ms\n",
      "59:\tlearn: 0.0019581\ttotal: 296ms\tremaining: 691ms\n",
      "60:\tlearn: 0.0018446\ttotal: 301ms\tremaining: 686ms\n",
      "61:\tlearn: 0.0017369\ttotal: 306ms\tremaining: 681ms\n",
      "62:\tlearn: 0.0016071\ttotal: 311ms\tremaining: 676ms\n",
      "63:\tlearn: 0.0015035\ttotal: 316ms\tremaining: 671ms\n",
      "64:\tlearn: 0.0014749\ttotal: 319ms\tremaining: 662ms\n",
      "65:\tlearn: 0.0014043\ttotal: 323ms\tremaining: 656ms\n",
      "66:\tlearn: 0.0013204\ttotal: 327ms\tremaining: 650ms\n",
      "67:\tlearn: 0.0012529\ttotal: 332ms\tremaining: 645ms\n",
      "68:\tlearn: 0.0012237\ttotal: 336ms\tremaining: 637ms\n",
      "69:\tlearn: 0.0011815\ttotal: 340ms\tremaining: 631ms\n",
      "70:\tlearn: 0.0011473\ttotal: 343ms\tremaining: 624ms\n",
      "71:\tlearn: 0.0010881\ttotal: 348ms\tremaining: 618ms\n",
      "72:\tlearn: 0.0010442\ttotal: 352ms\tremaining: 612ms\n",
      "73:\tlearn: 0.0009841\ttotal: 356ms\tremaining: 607ms\n",
      "74:\tlearn: 0.0009454\ttotal: 360ms\tremaining: 601ms\n",
      "75:\tlearn: 0.0009216\ttotal: 365ms\tremaining: 595ms\n",
      "76:\tlearn: 0.0008890\ttotal: 369ms\tremaining: 589ms\n",
      "77:\tlearn: 0.0008563\ttotal: 373ms\tremaining: 583ms\n",
      "78:\tlearn: 0.0008389\ttotal: 377ms\tremaining: 577ms\n",
      "79:\tlearn: 0.0008058\ttotal: 381ms\tremaining: 572ms\n",
      "80:\tlearn: 0.0007879\ttotal: 386ms\tremaining: 567ms\n",
      "81:\tlearn: 0.0007574\ttotal: 390ms\tremaining: 561ms\n",
      "82:\tlearn: 0.0007232\ttotal: 395ms\tremaining: 556ms\n",
      "83:\tlearn: 0.0007102\ttotal: 399ms\tremaining: 551ms\n",
      "84:\tlearn: 0.0006813\ttotal: 403ms\tremaining: 545ms\n",
      "85:\tlearn: 0.0006813\ttotal: 406ms\tremaining: 538ms\n",
      "86:\tlearn: 0.0006813\ttotal: 408ms\tremaining: 530ms\n",
      "87:\tlearn: 0.0006813\ttotal: 411ms\tremaining: 523ms\n",
      "88:\tlearn: 0.0006813\ttotal: 413ms\tremaining: 516ms\n",
      "89:\tlearn: 0.0006813\ttotal: 416ms\tremaining: 509ms\n",
      "90:\tlearn: 0.0006813\ttotal: 419ms\tremaining: 502ms\n",
      "91:\tlearn: 0.0006813\ttotal: 422ms\tremaining: 495ms\n",
      "92:\tlearn: 0.0006813\ttotal: 425ms\tremaining: 489ms\n",
      "93:\tlearn: 0.0006813\ttotal: 427ms\tremaining: 482ms\n",
      "94:\tlearn: 0.0006813\ttotal: 430ms\tremaining: 476ms\n",
      "95:\tlearn: 0.0006813\ttotal: 433ms\tremaining: 469ms\n",
      "96:\tlearn: 0.0006813\ttotal: 436ms\tremaining: 463ms\n",
      "97:\tlearn: 0.0006812\ttotal: 439ms\tremaining: 457ms\n",
      "98:\tlearn: 0.0006812\ttotal: 442ms\tremaining: 450ms\n",
      "99:\tlearn: 0.0006812\ttotal: 444ms\tremaining: 444ms\n",
      "100:\tlearn: 0.0006812\ttotal: 447ms\tremaining: 438ms\n",
      "101:\tlearn: 0.0006812\ttotal: 450ms\tremaining: 432ms\n",
      "102:\tlearn: 0.0006812\ttotal: 453ms\tremaining: 426ms\n",
      "103:\tlearn: 0.0006812\ttotal: 455ms\tremaining: 420ms\n",
      "104:\tlearn: 0.0006812\ttotal: 458ms\tremaining: 415ms\n",
      "105:\tlearn: 0.0006812\ttotal: 461ms\tremaining: 409ms\n",
      "106:\tlearn: 0.0006812\ttotal: 464ms\tremaining: 403ms\n",
      "107:\tlearn: 0.0006812\ttotal: 467ms\tremaining: 398ms\n",
      "108:\tlearn: 0.0006812\ttotal: 470ms\tremaining: 392ms\n",
      "109:\tlearn: 0.0006812\ttotal: 473ms\tremaining: 387ms\n",
      "110:\tlearn: 0.0006812\ttotal: 475ms\tremaining: 381ms\n",
      "111:\tlearn: 0.0006812\ttotal: 478ms\tremaining: 376ms\n",
      "112:\tlearn: 0.0006633\ttotal: 481ms\tremaining: 370ms\n",
      "113:\tlearn: 0.0006416\ttotal: 485ms\tremaining: 366ms\n",
      "114:\tlearn: 0.0006416\ttotal: 488ms\tremaining: 361ms\n",
      "115:\tlearn: 0.0006416\ttotal: 491ms\tremaining: 355ms\n",
      "116:\tlearn: 0.0006416\ttotal: 494ms\tremaining: 350ms\n",
      "117:\tlearn: 0.0006416\ttotal: 496ms\tremaining: 345ms\n",
      "118:\tlearn: 0.0006416\ttotal: 499ms\tremaining: 340ms\n",
      "119:\tlearn: 0.0006416\ttotal: 502ms\tremaining: 334ms\n",
      "120:\tlearn: 0.0006416\ttotal: 504ms\tremaining: 329ms\n",
      "121:\tlearn: 0.0006416\ttotal: 507ms\tremaining: 324ms\n",
      "122:\tlearn: 0.0006416\ttotal: 510ms\tremaining: 320ms\n",
      "123:\tlearn: 0.0006416\ttotal: 513ms\tremaining: 314ms\n",
      "124:\tlearn: 0.0006416\ttotal: 516ms\tremaining: 310ms\n",
      "125:\tlearn: 0.0006416\ttotal: 519ms\tremaining: 305ms\n",
      "126:\tlearn: 0.0006416\ttotal: 522ms\tremaining: 300ms\n",
      "127:\tlearn: 0.0006416\ttotal: 525ms\tremaining: 295ms\n",
      "128:\tlearn: 0.0006416\ttotal: 528ms\tremaining: 290ms\n",
      "129:\tlearn: 0.0006416\ttotal: 531ms\tremaining: 286ms\n",
      "130:\tlearn: 0.0006416\ttotal: 534ms\tremaining: 281ms\n",
      "131:\tlearn: 0.0006416\ttotal: 537ms\tremaining: 276ms\n",
      "132:\tlearn: 0.0006416\ttotal: 540ms\tremaining: 272ms\n",
      "133:\tlearn: 0.0006416\ttotal: 543ms\tremaining: 267ms\n",
      "134:\tlearn: 0.0006415\ttotal: 546ms\tremaining: 263ms\n",
      "135:\tlearn: 0.0006415\ttotal: 549ms\tremaining: 258ms\n",
      "136:\tlearn: 0.0006415\ttotal: 552ms\tremaining: 254ms\n",
      "137:\tlearn: 0.0006415\ttotal: 555ms\tremaining: 249ms\n",
      "138:\tlearn: 0.0006415\ttotal: 558ms\tremaining: 245ms\n",
      "139:\tlearn: 0.0006415\ttotal: 561ms\tremaining: 240ms\n",
      "140:\tlearn: 0.0006415\ttotal: 564ms\tremaining: 236ms\n",
      "141:\tlearn: 0.0006415\ttotal: 567ms\tremaining: 232ms\n",
      "142:\tlearn: 0.0006415\ttotal: 570ms\tremaining: 227ms\n",
      "143:\tlearn: 0.0006415\ttotal: 573ms\tremaining: 223ms\n",
      "144:\tlearn: 0.0006415\ttotal: 576ms\tremaining: 218ms\n",
      "145:\tlearn: 0.0006415\ttotal: 579ms\tremaining: 214ms\n",
      "146:\tlearn: 0.0006415\ttotal: 582ms\tremaining: 210ms\n",
      "147:\tlearn: 0.0006415\ttotal: 585ms\tremaining: 206ms\n",
      "148:\tlearn: 0.0006415\ttotal: 588ms\tremaining: 201ms\n",
      "149:\tlearn: 0.0006415\ttotal: 591ms\tremaining: 197ms\n",
      "150:\tlearn: 0.0006415\ttotal: 594ms\tremaining: 193ms\n",
      "151:\tlearn: 0.0006415\ttotal: 597ms\tremaining: 189ms\n",
      "152:\tlearn: 0.0006415\ttotal: 600ms\tremaining: 184ms\n",
      "153:\tlearn: 0.0006415\ttotal: 603ms\tremaining: 180ms\n",
      "154:\tlearn: 0.0006415\ttotal: 606ms\tremaining: 176ms\n",
      "155:\tlearn: 0.0006415\ttotal: 609ms\tremaining: 172ms\n",
      "156:\tlearn: 0.0006415\ttotal: 612ms\tremaining: 168ms\n",
      "157:\tlearn: 0.0006415\ttotal: 615ms\tremaining: 163ms\n",
      "158:\tlearn: 0.0006415\ttotal: 618ms\tremaining: 159ms\n",
      "159:\tlearn: 0.0006415\ttotal: 621ms\tremaining: 155ms\n",
      "160:\tlearn: 0.0006415\ttotal: 624ms\tremaining: 151ms\n",
      "161:\tlearn: 0.0006415\ttotal: 627ms\tremaining: 147ms\n",
      "162:\tlearn: 0.0006415\ttotal: 629ms\tremaining: 143ms\n",
      "163:\tlearn: 0.0006415\ttotal: 632ms\tremaining: 139ms\n",
      "164:\tlearn: 0.0006415\ttotal: 635ms\tremaining: 135ms\n",
      "165:\tlearn: 0.0006415\ttotal: 638ms\tremaining: 131ms\n",
      "166:\tlearn: 0.0006415\ttotal: 641ms\tremaining: 127ms\n",
      "167:\tlearn: 0.0006415\ttotal: 644ms\tremaining: 123ms\n",
      "168:\tlearn: 0.0006415\ttotal: 647ms\tremaining: 119ms\n",
      "169:\tlearn: 0.0006415\ttotal: 650ms\tremaining: 115ms\n",
      "170:\tlearn: 0.0006415\ttotal: 653ms\tremaining: 111ms\n",
      "171:\tlearn: 0.0006415\ttotal: 656ms\tremaining: 107ms\n",
      "172:\tlearn: 0.0006415\ttotal: 659ms\tremaining: 103ms\n",
      "173:\tlearn: 0.0006415\ttotal: 662ms\tremaining: 99ms\n",
      "174:\tlearn: 0.0006415\ttotal: 665ms\tremaining: 95ms\n",
      "175:\tlearn: 0.0006415\ttotal: 668ms\tremaining: 91.1ms\n",
      "176:\tlearn: 0.0006415\ttotal: 671ms\tremaining: 87.2ms\n",
      "177:\tlearn: 0.0006415\ttotal: 674ms\tremaining: 83.3ms\n",
      "178:\tlearn: 0.0006415\ttotal: 677ms\tremaining: 79.5ms\n",
      "179:\tlearn: 0.0006415\ttotal: 680ms\tremaining: 75.6ms\n",
      "180:\tlearn: 0.0006415\ttotal: 683ms\tremaining: 71.7ms\n",
      "181:\tlearn: 0.0006415\ttotal: 686ms\tremaining: 67.8ms\n",
      "182:\tlearn: 0.0006415\ttotal: 689ms\tremaining: 64ms\n",
      "183:\tlearn: 0.0006415\ttotal: 692ms\tremaining: 60.2ms\n",
      "184:\tlearn: 0.0006415\ttotal: 695ms\tremaining: 56.4ms\n",
      "185:\tlearn: 0.0006415\ttotal: 698ms\tremaining: 52.6ms\n",
      "186:\tlearn: 0.0006415\ttotal: 701ms\tremaining: 48.8ms\n",
      "187:\tlearn: 0.0006415\ttotal: 704ms\tremaining: 45ms\n",
      "188:\tlearn: 0.0006415\ttotal: 707ms\tremaining: 41.2ms\n",
      "189:\tlearn: 0.0006415\ttotal: 710ms\tremaining: 37.4ms\n",
      "190:\tlearn: 0.0006415\ttotal: 713ms\tremaining: 33.6ms\n",
      "191:\tlearn: 0.0006415\ttotal: 716ms\tremaining: 29.8ms\n",
      "192:\tlearn: 0.0006415\ttotal: 719ms\tremaining: 26.1ms\n",
      "193:\tlearn: 0.0006415\ttotal: 722ms\tremaining: 22.3ms\n",
      "194:\tlearn: 0.0006415\ttotal: 726ms\tremaining: 18.6ms\n",
      "195:\tlearn: 0.0006415\ttotal: 729ms\tremaining: 14.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196:\tlearn: 0.0006415\ttotal: 732ms\tremaining: 11.1ms\n",
      "197:\tlearn: 0.0006415\ttotal: 735ms\tremaining: 7.42ms\n",
      "198:\tlearn: 0.0006415\ttotal: 738ms\tremaining: 3.71ms\n",
      "199:\tlearn: 0.0006415\ttotal: 741ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb78def5e6a4309899a0e128d60500c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5990640947403009, Recall = 0.5774021352313167, Aging Rate = 0.36076512455516013, Precision = 0.8002466091245376, f1 = 0.6708010335917313\n",
      "Epoch 2: Train Loss = 0.3844071723176067, Recall = 0.8362989323843416, Aging Rate = 0.48576512455516013, Precision = 0.8608058608058609, f1 = 0.8483754512635381\n",
      "Epoch 3: Train Loss = 0.2835706485123821, Recall = 0.9119217081850534, Aging Rate = 0.5142348754448398, Precision = 0.8866782006920415, f1 = 0.8991228070175439\n",
      "Epoch 4: Train Loss = 0.23060235959142977, Recall = 0.9252669039145908, Aging Rate = 0.4997775800711744, Precision = 0.9256786826880284, f1 = 0.9254727474972192\n",
      "Epoch 5: Train Loss = 0.1941214288681003, Recall = 0.9395017793594306, Aging Rate = 0.49933274021352314, Precision = 0.9407572383073497, f1 = 0.9401290896950814\n",
      "Test Loss = 0.17477573926338522, Recall = 0.9306049822064056, Aging Rate = 0.47820284697508897, precision = 0.9730232558139534\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.16450345648777442, Recall = 0.9435053380782918, Aging Rate = 0.49243772241992884, Precision = 0.9579945799457995, f1 = 0.9506947557149261\n",
      "Epoch 7: Train Loss = 0.14130969764499055, Recall = 0.9466192170818505, Aging Rate = 0.4902135231316726, Precision = 0.9655172413793104, f1 = 0.9559748427672956\n",
      "Epoch 8: Train Loss = 0.12277232137014857, Recall = 0.9524021352313167, Aging Rate = 0.4875444839857651, Precision = 0.9767335766423357, f1 = 0.9644144144144143\n",
      "Epoch 9: Train Loss = 0.10744072597646204, Recall = 0.9608540925266904, Aging Rate = 0.4904359430604982, Precision = 0.9795918367346939, f1 = 0.9701324949472266\n",
      "Epoch 10: Train Loss = 0.09411970772565048, Recall = 0.969306049822064, Aging Rate = 0.4931049822064057, Precision = 0.9828597203428056, f1 = 0.9760358342665173\n",
      "Test Loss = 0.08493766528007399, Recall = 0.969306049822064, Aging Rate = 0.49088078291814946, precision = 0.987313094698686\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.08622525552837874, Recall = 0.9710854092526691, Aging Rate = 0.49354982206405695, Precision = 0.9837764758900406, f1 = 0.9773897470338035\n",
      "Epoch 12: Train Loss = 0.07496150826962393, Recall = 0.9755338078291815, Aging Rate = 0.49354982206405695, Precision = 0.9882830103650293, f1 = 0.9818670248488919\n",
      "Epoch 13: Train Loss = 0.06708433595734559, Recall = 0.9741992882562278, Aging Rate = 0.491770462633452, Precision = 0.9905020352781547, f1 = 0.9822830230993496\n",
      "Epoch 14: Train Loss = 0.06146463877736886, Recall = 0.978202846975089, Aging Rate = 0.4928825622775801, Precision = 0.9923285198555957, f1 = 0.9852150537634409\n",
      "Epoch 15: Train Loss = 0.055469112099542736, Recall = 0.978202846975089, Aging Rate = 0.4931049822064057, Precision = 0.9918809201623816, f1 = 0.9849944008958565\n",
      "Test Loss = 0.05112126307873539, Recall = 0.9862099644128114, Aging Rate = 0.4957740213523132, precision = 0.9946164199192463\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05109940135182011, Recall = 0.9817615658362989, Aging Rate = 0.4942170818505338, Precision = 0.9932493249324933, f1 = 0.9874720357941835\n",
      "Epoch 17: Train Loss = 0.04734492911774917, Recall = 0.9853202846975089, Aging Rate = 0.4944395017793594, Precision = 0.9964012595591543, f1 = 0.9908297919928428\n",
      "Epoch 18: Train Loss = 0.04358849388106438, Recall = 0.9862099644128114, Aging Rate = 0.49466192170818507, Precision = 0.9968525179856115, f1 = 0.9915026833631485\n",
      "Epoch 19: Train Loss = 0.04020402132404232, Recall = 0.9884341637010676, Aging Rate = 0.4951067615658363, Precision = 0.9982030548068284, f1 = 0.9932945909700491\n",
      "Epoch 20: Train Loss = 0.037066494754359816, Recall = 0.9924377224199288, Aging Rate = 0.4968861209964413, Precision = 0.9986571172784243, f1 = 0.99553770638108\n",
      "Test Loss = 0.033905238549480236, Recall = 0.9955516014234875, Aging Rate = 0.498220640569395, precision = 0.9991071428571429\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.035544031773597745, Recall = 0.9911032028469751, Aging Rate = 0.4962188612099644, Precision = 0.9986553115194979, f1 = 0.9948649252065193\n",
      "Epoch 22: Train Loss = 0.033300315358036355, Recall = 0.9924377224199288, Aging Rate = 0.4962188612099644, Precision = 0, f1 = 0.0\n",
      "Epoch 23: Train Loss = 0.031169919383271308, Recall = 0.994661921708185, Aging Rate = 0.49777580071174377, Precision = 0.9991063449508489, f1 = 0.9968791796700847\n",
      "Epoch 24: Train Loss = 0.030070268923542677, Recall = 0.99644128113879, Aging Rate = 0.4986654804270463, Precision = 0.9991079393398751, f1 = 0.997772828507795\n",
      "Epoch 25: Train Loss = 0.028546986136969935, Recall = 0.9951067615658363, Aging Rate = 0.4979982206405694, Precision = 0.9991067440821796, f1 = 0.9971027412525072\n",
      "Test Loss = 0.02646640587425741, Recall = 0.9955516014234875, Aging Rate = 0.49777580071174377, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.02730707459655521, Recall = 0.9951067615658363, Aging Rate = 0.49777580071174377, Precision = 0.9995531724754245, f1 = 0.997325011145787\n",
      "Epoch 27: Train Loss = 0.02611019606851174, Recall = 0.9951067615658363, Aging Rate = 0.49777580071174377, Precision = 0.9995531724754245, f1 = 0.997325011145787\n",
      "Epoch 28: Train Loss = 0.025313703310129058, Recall = 0.9968861209964412, Aging Rate = 0.4986654804270463, Precision = 0.9995539696699376, f1 = 0.9982182628062362\n",
      "Epoch 29: Train Loss = 0.02343034145697368, Recall = 0.9959964412811388, Aging Rate = 0.4979982206405694, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.023284584124741606, Recall = 0.9968861209964412, Aging Rate = 0.4986654804270463, Precision = 0.9995539696699376, f1 = 0.9982182628062362\n",
      "Test Loss = 0.021090857412838428, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.022201528991456557, Recall = 0.9973309608540926, Aging Rate = 0.4986654804270463, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.021407311497209758, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.02074934973371517, Recall = 0.9973309608540926, Aging Rate = 0.4986654804270463, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.02052217955929741, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.02012871077211302, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01798313575733811, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.019327343106800128, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.01913459364198864, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.018790854230602654, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.018275035432543194, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.018158750005358056, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01610281000251872, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.01767322082350899, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.017506446990969978, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.017195203311714837, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.017790871231528362, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.017315953426407748, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015413114423217298, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016854153873127126, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.01634388857975178, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016937891813492225, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.01599562272522267, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016059241652011447, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014550583662458587, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.01593956662494411, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.015929338664770976, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.015809507603057764, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.015821357237778312, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015399809031883168, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01444404527173772, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015523772724834519, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.015602474939409524, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.016559943642957778, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.015246816695372829, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015399735255226546, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014338300990941686, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.01593191893207857, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.014757850452647726, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.01512852885173733, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015089146554257946, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.014926417362859665, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013525943072956865, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.014699354671684236, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.014951935525174879, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.015470943258730595, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.01459474396954864, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.014926623758525187, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013731699940678911, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.01446306533461894, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.01482090669819999, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.014759198679618564, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015463999306842738, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014583740950134514, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013369549364116693, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.015246992108819747, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.014536089220770314, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.014496606267537935, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.014481013597675278, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.014676640592382896, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013742480715372173, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.014820909505424975, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.015014546164361183, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.014200325389110957, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.014535734642071656, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.014233453113095191, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013420917203126216, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.014133666181320398, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.015165196168253006, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.014431044419571173, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.01446706164972956, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.014127833188720233, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012850250876458939, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5288970\ttotal: 6.5ms\tremaining: 1.29s\n",
      "1:\tlearn: 0.4175062\ttotal: 12.6ms\tremaining: 1.25s\n",
      "2:\tlearn: 0.3335063\ttotal: 18.8ms\tremaining: 1.24s\n",
      "3:\tlearn: 0.2720952\ttotal: 24.7ms\tremaining: 1.21s\n",
      "4:\tlearn: 0.2252287\ttotal: 31.3ms\tremaining: 1.22s\n",
      "5:\tlearn: 0.1931033\ttotal: 37.1ms\tremaining: 1.2s\n",
      "6:\tlearn: 0.1603628\ttotal: 43.6ms\tremaining: 1.2s\n",
      "7:\tlearn: 0.1338237\ttotal: 49.2ms\tremaining: 1.18s\n",
      "8:\tlearn: 0.1168558\ttotal: 54.7ms\tremaining: 1.16s\n",
      "9:\tlearn: 0.1065041\ttotal: 59.3ms\tremaining: 1.13s\n",
      "10:\tlearn: 0.0939330\ttotal: 65.4ms\tremaining: 1.12s\n",
      "11:\tlearn: 0.0832093\ttotal: 71.1ms\tremaining: 1.11s\n",
      "12:\tlearn: 0.0736220\ttotal: 76.5ms\tremaining: 1.1s\n",
      "13:\tlearn: 0.0675034\ttotal: 81.6ms\tremaining: 1.08s\n",
      "14:\tlearn: 0.0606957\ttotal: 86.2ms\tremaining: 1.06s\n",
      "15:\tlearn: 0.0551154\ttotal: 90.9ms\tremaining: 1.04s\n",
      "16:\tlearn: 0.0502789\ttotal: 95.6ms\tremaining: 1.03s\n",
      "17:\tlearn: 0.0447465\ttotal: 101ms\tremaining: 1.02s\n",
      "18:\tlearn: 0.0413746\ttotal: 105ms\tremaining: 1s\n",
      "19:\tlearn: 0.0369117\ttotal: 110ms\tremaining: 994ms\n",
      "20:\tlearn: 0.0339446\ttotal: 115ms\tremaining: 982ms\n",
      "21:\tlearn: 0.0312386\ttotal: 120ms\tremaining: 970ms\n",
      "22:\tlearn: 0.0287053\ttotal: 125ms\tremaining: 961ms\n",
      "23:\tlearn: 0.0267312\ttotal: 130ms\tremaining: 951ms\n",
      "24:\tlearn: 0.0248599\ttotal: 134ms\tremaining: 940ms\n",
      "25:\tlearn: 0.0236080\ttotal: 138ms\tremaining: 926ms\n",
      "26:\tlearn: 0.0216895\ttotal: 143ms\tremaining: 918ms\n",
      "27:\tlearn: 0.0202670\ttotal: 148ms\tremaining: 907ms\n",
      "28:\tlearn: 0.0186503\ttotal: 152ms\tremaining: 897ms\n",
      "29:\tlearn: 0.0172048\ttotal: 157ms\tremaining: 888ms\n",
      "30:\tlearn: 0.0159778\ttotal: 162ms\tremaining: 881ms\n",
      "31:\tlearn: 0.0148531\ttotal: 166ms\tremaining: 872ms\n",
      "32:\tlearn: 0.0131944\ttotal: 171ms\tremaining: 864ms\n",
      "33:\tlearn: 0.0122268\ttotal: 176ms\tremaining: 857ms\n",
      "34:\tlearn: 0.0113369\ttotal: 180ms\tremaining: 850ms\n",
      "35:\tlearn: 0.0106517\ttotal: 185ms\tremaining: 843ms\n",
      "36:\tlearn: 0.0097768\ttotal: 190ms\tremaining: 838ms\n",
      "37:\tlearn: 0.0093027\ttotal: 194ms\tremaining: 829ms\n",
      "38:\tlearn: 0.0086863\ttotal: 199ms\tremaining: 822ms\n",
      "39:\tlearn: 0.0078949\ttotal: 204ms\tremaining: 815ms\n",
      "40:\tlearn: 0.0074005\ttotal: 208ms\tremaining: 807ms\n",
      "41:\tlearn: 0.0069817\ttotal: 213ms\tremaining: 801ms\n",
      "42:\tlearn: 0.0063388\ttotal: 218ms\tremaining: 794ms\n",
      "43:\tlearn: 0.0059716\ttotal: 222ms\tremaining: 788ms\n",
      "44:\tlearn: 0.0056824\ttotal: 227ms\tremaining: 782ms\n",
      "45:\tlearn: 0.0052890\ttotal: 232ms\tremaining: 775ms\n",
      "46:\tlearn: 0.0048826\ttotal: 236ms\tremaining: 769ms\n",
      "47:\tlearn: 0.0044940\ttotal: 241ms\tremaining: 763ms\n",
      "48:\tlearn: 0.0042904\ttotal: 245ms\tremaining: 754ms\n",
      "49:\tlearn: 0.0040340\ttotal: 250ms\tremaining: 749ms\n",
      "50:\tlearn: 0.0037085\ttotal: 254ms\tremaining: 743ms\n",
      "51:\tlearn: 0.0035663\ttotal: 258ms\tremaining: 736ms\n",
      "52:\tlearn: 0.0033361\ttotal: 263ms\tremaining: 730ms\n",
      "53:\tlearn: 0.0031243\ttotal: 268ms\tremaining: 724ms\n",
      "54:\tlearn: 0.0029556\ttotal: 272ms\tremaining: 718ms\n",
      "55:\tlearn: 0.0028138\ttotal: 276ms\tremaining: 711ms\n",
      "56:\tlearn: 0.0026497\ttotal: 281ms\tremaining: 705ms\n",
      "57:\tlearn: 0.0025525\ttotal: 285ms\tremaining: 697ms\n",
      "58:\tlearn: 0.0023899\ttotal: 289ms\tremaining: 691ms\n",
      "59:\tlearn: 0.0022850\ttotal: 294ms\tremaining: 685ms\n",
      "60:\tlearn: 0.0021683\ttotal: 298ms\tremaining: 679ms\n",
      "61:\tlearn: 0.0020639\ttotal: 303ms\tremaining: 673ms\n",
      "62:\tlearn: 0.0019198\ttotal: 307ms\tremaining: 669ms\n",
      "63:\tlearn: 0.0018268\ttotal: 312ms\tremaining: 663ms\n",
      "64:\tlearn: 0.0017234\ttotal: 316ms\tremaining: 657ms\n",
      "65:\tlearn: 0.0016368\ttotal: 321ms\tremaining: 652ms\n",
      "66:\tlearn: 0.0015536\ttotal: 326ms\tremaining: 646ms\n",
      "67:\tlearn: 0.0014591\ttotal: 330ms\tremaining: 641ms\n",
      "68:\tlearn: 0.0014030\ttotal: 335ms\tremaining: 635ms\n",
      "69:\tlearn: 0.0013113\ttotal: 339ms\tremaining: 630ms\n",
      "70:\tlearn: 0.0012441\ttotal: 344ms\tremaining: 624ms\n",
      "71:\tlearn: 0.0012111\ttotal: 348ms\tremaining: 618ms\n",
      "72:\tlearn: 0.0011819\ttotal: 351ms\tremaining: 611ms\n",
      "73:\tlearn: 0.0011352\ttotal: 355ms\tremaining: 605ms\n",
      "74:\tlearn: 0.0011116\ttotal: 359ms\tremaining: 598ms\n",
      "75:\tlearn: 0.0010472\ttotal: 363ms\tremaining: 593ms\n",
      "76:\tlearn: 0.0009959\ttotal: 368ms\tremaining: 588ms\n",
      "77:\tlearn: 0.0009507\ttotal: 373ms\tremaining: 583ms\n",
      "78:\tlearn: 0.0009285\ttotal: 377ms\tremaining: 577ms\n",
      "79:\tlearn: 0.0008918\ttotal: 381ms\tremaining: 571ms\n",
      "80:\tlearn: 0.0008700\ttotal: 384ms\tremaining: 565ms\n",
      "81:\tlearn: 0.0008445\ttotal: 388ms\tremaining: 558ms\n",
      "82:\tlearn: 0.0008046\ttotal: 393ms\tremaining: 553ms\n",
      "83:\tlearn: 0.0008046\ttotal: 395ms\tremaining: 546ms\n",
      "84:\tlearn: 0.0007780\ttotal: 399ms\tremaining: 539ms\n",
      "85:\tlearn: 0.0007657\ttotal: 402ms\tremaining: 532ms\n",
      "86:\tlearn: 0.0007657\ttotal: 404ms\tremaining: 525ms\n",
      "87:\tlearn: 0.0007416\ttotal: 408ms\tremaining: 519ms\n",
      "88:\tlearn: 0.0007416\ttotal: 410ms\tremaining: 512ms\n",
      "89:\tlearn: 0.0007416\ttotal: 413ms\tremaining: 505ms\n",
      "90:\tlearn: 0.0007416\ttotal: 416ms\tremaining: 498ms\n",
      "91:\tlearn: 0.0007270\ttotal: 419ms\tremaining: 492ms\n",
      "92:\tlearn: 0.0007022\ttotal: 422ms\tremaining: 486ms\n",
      "93:\tlearn: 0.0006819\ttotal: 426ms\tremaining: 481ms\n",
      "94:\tlearn: 0.0006819\ttotal: 430ms\tremaining: 475ms\n",
      "95:\tlearn: 0.0006631\ttotal: 434ms\tremaining: 470ms\n",
      "96:\tlearn: 0.0006631\ttotal: 437ms\tremaining: 464ms\n",
      "97:\tlearn: 0.0006519\ttotal: 441ms\tremaining: 459ms\n",
      "98:\tlearn: 0.0006519\ttotal: 444ms\tremaining: 453ms\n",
      "99:\tlearn: 0.0006519\ttotal: 447ms\tremaining: 447ms\n",
      "100:\tlearn: 0.0006519\ttotal: 450ms\tremaining: 441ms\n",
      "101:\tlearn: 0.0006518\ttotal: 453ms\tremaining: 436ms\n",
      "102:\tlearn: 0.0006518\ttotal: 456ms\tremaining: 430ms\n",
      "103:\tlearn: 0.0006518\ttotal: 459ms\tremaining: 424ms\n",
      "104:\tlearn: 0.0006518\ttotal: 462ms\tremaining: 418ms\n",
      "105:\tlearn: 0.0006518\ttotal: 465ms\tremaining: 413ms\n",
      "106:\tlearn: 0.0006518\ttotal: 469ms\tremaining: 407ms\n",
      "107:\tlearn: 0.0006518\ttotal: 472ms\tremaining: 402ms\n",
      "108:\tlearn: 0.0006518\ttotal: 475ms\tremaining: 396ms\n",
      "109:\tlearn: 0.0006518\ttotal: 478ms\tremaining: 391ms\n",
      "110:\tlearn: 0.0006518\ttotal: 481ms\tremaining: 386ms\n",
      "111:\tlearn: 0.0006518\ttotal: 484ms\tremaining: 380ms\n",
      "112:\tlearn: 0.0006518\ttotal: 487ms\tremaining: 375ms\n",
      "113:\tlearn: 0.0006518\ttotal: 490ms\tremaining: 370ms\n",
      "114:\tlearn: 0.0006518\ttotal: 493ms\tremaining: 365ms\n",
      "115:\tlearn: 0.0006518\ttotal: 496ms\tremaining: 359ms\n",
      "116:\tlearn: 0.0006518\ttotal: 499ms\tremaining: 354ms\n",
      "117:\tlearn: 0.0006517\ttotal: 503ms\tremaining: 349ms\n",
      "118:\tlearn: 0.0006517\ttotal: 506ms\tremaining: 344ms\n",
      "119:\tlearn: 0.0006517\ttotal: 509ms\tremaining: 339ms\n",
      "120:\tlearn: 0.0006517\ttotal: 512ms\tremaining: 334ms\n",
      "121:\tlearn: 0.0006517\ttotal: 515ms\tremaining: 329ms\n",
      "122:\tlearn: 0.0006517\ttotal: 518ms\tremaining: 324ms\n",
      "123:\tlearn: 0.0006517\ttotal: 521ms\tremaining: 319ms\n",
      "124:\tlearn: 0.0006517\ttotal: 524ms\tremaining: 314ms\n",
      "125:\tlearn: 0.0006517\ttotal: 527ms\tremaining: 309ms\n",
      "126:\tlearn: 0.0006517\ttotal: 530ms\tremaining: 305ms\n",
      "127:\tlearn: 0.0006517\ttotal: 533ms\tremaining: 300ms\n",
      "128:\tlearn: 0.0006517\ttotal: 536ms\tremaining: 295ms\n",
      "129:\tlearn: 0.0006517\ttotal: 539ms\tremaining: 290ms\n",
      "130:\tlearn: 0.0006516\ttotal: 542ms\tremaining: 285ms\n",
      "131:\tlearn: 0.0006516\ttotal: 545ms\tremaining: 281ms\n",
      "132:\tlearn: 0.0006516\ttotal: 548ms\tremaining: 276ms\n",
      "133:\tlearn: 0.0006516\ttotal: 551ms\tremaining: 272ms\n",
      "134:\tlearn: 0.0006516\ttotal: 554ms\tremaining: 267ms\n",
      "135:\tlearn: 0.0006516\ttotal: 557ms\tremaining: 262ms\n",
      "136:\tlearn: 0.0006516\ttotal: 560ms\tremaining: 258ms\n",
      "137:\tlearn: 0.0006516\ttotal: 564ms\tremaining: 253ms\n",
      "138:\tlearn: 0.0006516\ttotal: 567ms\tremaining: 249ms\n",
      "139:\tlearn: 0.0006516\ttotal: 570ms\tremaining: 244ms\n",
      "140:\tlearn: 0.0006516\ttotal: 573ms\tremaining: 240ms\n",
      "141:\tlearn: 0.0006516\ttotal: 576ms\tremaining: 235ms\n",
      "142:\tlearn: 0.0006516\ttotal: 579ms\tremaining: 231ms\n",
      "143:\tlearn: 0.0006516\ttotal: 582ms\tremaining: 226ms\n",
      "144:\tlearn: 0.0006516\ttotal: 585ms\tremaining: 222ms\n",
      "145:\tlearn: 0.0006516\ttotal: 588ms\tremaining: 217ms\n",
      "146:\tlearn: 0.0006515\ttotal: 591ms\tremaining: 213ms\n",
      "147:\tlearn: 0.0006515\ttotal: 594ms\tremaining: 209ms\n",
      "148:\tlearn: 0.0006515\ttotal: 597ms\tremaining: 204ms\n",
      "149:\tlearn: 0.0006515\ttotal: 600ms\tremaining: 200ms\n",
      "150:\tlearn: 0.0006515\ttotal: 603ms\tremaining: 196ms\n",
      "151:\tlearn: 0.0006515\ttotal: 606ms\tremaining: 191ms\n",
      "152:\tlearn: 0.0006515\ttotal: 609ms\tremaining: 187ms\n",
      "153:\tlearn: 0.0006515\ttotal: 612ms\tremaining: 183ms\n",
      "154:\tlearn: 0.0006515\ttotal: 615ms\tremaining: 179ms\n",
      "155:\tlearn: 0.0006515\ttotal: 618ms\tremaining: 174ms\n",
      "156:\tlearn: 0.0006515\ttotal: 621ms\tremaining: 170ms\n",
      "157:\tlearn: 0.0006515\ttotal: 625ms\tremaining: 166ms\n",
      "158:\tlearn: 0.0006515\ttotal: 628ms\tremaining: 162ms\n",
      "159:\tlearn: 0.0006515\ttotal: 631ms\tremaining: 158ms\n",
      "160:\tlearn: 0.0006514\ttotal: 634ms\tremaining: 153ms\n",
      "161:\tlearn: 0.0006514\ttotal: 637ms\tremaining: 149ms\n",
      "162:\tlearn: 0.0006514\ttotal: 640ms\tremaining: 145ms\n",
      "163:\tlearn: 0.0006514\ttotal: 642ms\tremaining: 141ms\n",
      "164:\tlearn: 0.0006514\ttotal: 646ms\tremaining: 137ms\n",
      "165:\tlearn: 0.0006514\ttotal: 649ms\tremaining: 133ms\n",
      "166:\tlearn: 0.0006514\ttotal: 652ms\tremaining: 129ms\n",
      "167:\tlearn: 0.0006514\ttotal: 655ms\tremaining: 125ms\n",
      "168:\tlearn: 0.0006514\ttotal: 658ms\tremaining: 121ms\n",
      "169:\tlearn: 0.0006514\ttotal: 661ms\tremaining: 117ms\n",
      "170:\tlearn: 0.0006514\ttotal: 664ms\tremaining: 113ms\n",
      "171:\tlearn: 0.0006514\ttotal: 667ms\tremaining: 109ms\n",
      "172:\tlearn: 0.0006514\ttotal: 670ms\tremaining: 105ms\n",
      "173:\tlearn: 0.0006514\ttotal: 673ms\tremaining: 101ms\n",
      "174:\tlearn: 0.0006514\ttotal: 676ms\tremaining: 96.6ms\n",
      "175:\tlearn: 0.0006514\ttotal: 679ms\tremaining: 92.6ms\n",
      "176:\tlearn: 0.0006514\ttotal: 682ms\tremaining: 88.6ms\n",
      "177:\tlearn: 0.0006514\ttotal: 685ms\tremaining: 84.7ms\n",
      "178:\tlearn: 0.0006514\ttotal: 688ms\tremaining: 80.7ms\n",
      "179:\tlearn: 0.0006514\ttotal: 691ms\tremaining: 76.8ms\n",
      "180:\tlearn: 0.0006514\ttotal: 694ms\tremaining: 72.9ms\n",
      "181:\tlearn: 0.0006514\ttotal: 697ms\tremaining: 68.9ms\n",
      "182:\tlearn: 0.0006514\ttotal: 700ms\tremaining: 65ms\n",
      "183:\tlearn: 0.0006514\ttotal: 703ms\tremaining: 61.1ms\n",
      "184:\tlearn: 0.0006513\ttotal: 706ms\tremaining: 57.3ms\n",
      "185:\tlearn: 0.0006513\ttotal: 709ms\tremaining: 53.4ms\n",
      "186:\tlearn: 0.0006513\ttotal: 712ms\tremaining: 49.5ms\n",
      "187:\tlearn: 0.0006513\ttotal: 715ms\tremaining: 45.6ms\n",
      "188:\tlearn: 0.0006513\ttotal: 718ms\tremaining: 41.8ms\n",
      "189:\tlearn: 0.0006513\ttotal: 721ms\tremaining: 38ms\n",
      "190:\tlearn: 0.0006513\ttotal: 724ms\tremaining: 34.1ms\n",
      "191:\tlearn: 0.0006513\ttotal: 727ms\tremaining: 30.3ms\n",
      "192:\tlearn: 0.0006513\ttotal: 730ms\tremaining: 26.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193:\tlearn: 0.0006513\ttotal: 733ms\tremaining: 22.7ms\n",
      "194:\tlearn: 0.0006513\ttotal: 736ms\tremaining: 18.9ms\n",
      "195:\tlearn: 0.0006513\ttotal: 739ms\tremaining: 15.1ms\n",
      "196:\tlearn: 0.0006513\ttotal: 742ms\tremaining: 11.3ms\n",
      "197:\tlearn: 0.0006513\ttotal: 745ms\tremaining: 7.53ms\n",
      "198:\tlearn: 0.0006513\ttotal: 748ms\tremaining: 3.76ms\n",
      "199:\tlearn: 0.0006513\ttotal: 751ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c6dfdedca0447c9bec42ca92650c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6065596995404606, Recall = 0.5827402135231317, Aging Rate = 0.36610320284697506, Precision = 0.795868772782503, f1 = 0.672829994863893\n",
      "Epoch 2: Train Loss = 0.39244725515409723, Recall = 0.8336298932384342, Aging Rate = 0.48821174377224197, Precision = 0.85375854214123, f1 = 0.8435741616025207\n",
      "Epoch 3: Train Loss = 0.2866333067629261, Recall = 0.902135231316726, Aging Rate = 0.5031138790035588, Precision = 0.896551724137931, f1 = 0.8993348115299334\n",
      "Epoch 4: Train Loss = 0.23554242765564087, Recall = 0.9270462633451957, Aging Rate = 0.5031138790035588, Precision = 0.9213085764809903, f1 = 0.9241685144124168\n",
      "Epoch 5: Train Loss = 0.20026725307902407, Recall = 0.9350533807829181, Aging Rate = 0.4962188612099644, Precision = 0.9421783953384133, f1 = 0.9386023665996875\n",
      "Test Loss = 0.17210853882001387, Recall = 0.9408362989323843, Aging Rate = 0.4891014234875445, precision = 0.9618008185538881\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.16360596368533437, Recall = 0.9430604982206405, Aging Rate = 0.49088078291814946, Precision = 0.9605799728137744, f1 = 0.9517396184062852\n",
      "Epoch 7: Train Loss = 0.1380817415557298, Recall = 0.9457295373665481, Aging Rate = 0.48665480427046265, Precision = 0.9716636197440585, f1 = 0.958521190261497\n",
      "Epoch 8: Train Loss = 0.11859218505777923, Recall = 0.9546263345195729, Aging Rate = 0.4888790035587189, Precision = 0.9763421292083713, f1 = 0.9653621232568602\n",
      "Epoch 9: Train Loss = 0.1032750993648882, Recall = 0.9621886120996441, Aging Rate = 0.4884341637010676, Precision = 0.9849726775956285, f1 = 0.9734473447344735\n",
      "Epoch 10: Train Loss = 0.08931165501422306, Recall = 0.9653024911032029, Aging Rate = 0.4884341637010676, Precision = 0.98816029143898, f1 = 0.9765976597659766\n",
      "Test Loss = 0.07968692444397461, Recall = 0.974644128113879, Aging Rate = 0.49266014234875444, precision = 0.9891647855530474\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07869533267124056, Recall = 0.969306049822064, Aging Rate = 0.489991103202847, Precision = 0.9891057648660917, f1 = 0.9791058189170972\n",
      "Epoch 12: Train Loss = 0.06982622597883605, Recall = 0.9741992882562278, Aging Rate = 0.49110320284697506, Precision = 0.9918478260869565, f1 = 0.9829443447037702\n",
      "Epoch 13: Train Loss = 0.06351665884680595, Recall = 0.9790925266903915, Aging Rate = 0.49354982206405695, Precision = 0.9918882379450202, f1 = 0.9854488471009626\n",
      "Epoch 14: Train Loss = 0.05612274943403502, Recall = 0.9795373665480427, Aging Rate = 0.49243772241992884, Precision = 0.994579945799458, f1 = 0.9870013446884806\n",
      "Epoch 15: Train Loss = 0.0523457691550679, Recall = 0.9822064056939501, Aging Rate = 0.4939946619217082, Precision = 0.9941467807294012, f1 = 0.9881405236070708\n",
      "Test Loss = 0.04634499415967388, Recall = 0.9902135231316725, Aging Rate = 0.49755338078291816, precision = 0.9950827000447027\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04742753532221309, Recall = 0.9857651245551602, Aging Rate = 0.4951067615658363, Precision = 0.995507637017071, f1 = 0.9906124273580689\n",
      "Epoch 17: Train Loss = 0.04282522942618967, Recall = 0.9884341637010676, Aging Rate = 0.4957740213523132, Precision = 0.996859578286227, f1 = 0.9926289926289926\n",
      "Epoch 18: Train Loss = 0.039683792302616976, Recall = 0.9902135231316725, Aging Rate = 0.49666370106761565, Precision = 0.9968652037617555, f1 = 0.9935282303057352\n",
      "Epoch 19: Train Loss = 0.03691379382570019, Recall = 0.9919928825622776, Aging Rate = 0.4979982206405694, Precision = 0.995980348369808, f1 = 0.9939826164475151\n",
      "Epoch 20: Train Loss = 0.03474767191262644, Recall = 0.9919928825622776, Aging Rate = 0.49666370106761565, Precision = 0.9986565158978952, f1 = 0.9953135460834635\n",
      "Test Loss = 0.03114919173547806, Recall = 0.9959964412811388, Aging Rate = 0.4986654804270463, precision = 0.9986619090098127\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.0322729958837779, Recall = 0.9937722419928826, Aging Rate = 0.4984430604982206, Precision = 0.9968763944667559, f1 = 0.9953218979728226\n",
      "Epoch 22: Train Loss = 0.030558813399771144, Recall = 0.994661921708185, Aging Rate = 0.49777580071174377, Precision = 0.9991063449508489, f1 = 0.9968791796700847\n",
      "Epoch 23: Train Loss = 0.029217473095369085, Recall = 0.9951067615658363, Aging Rate = 0.4984430604982206, Precision = 0.998215082552432, f1 = 0.996658498552016\n",
      "Epoch 24: Train Loss = 0.02756635027231642, Recall = 0.9955516014234875, Aging Rate = 0.4979982206405694, Precision = 0.9995533720410897, f1 = 0.9975484733675061\n",
      "Epoch 25: Train Loss = 0.0263819788645595, Recall = 0.9955516014234875, Aging Rate = 0.498220640569395, Precision = 0.9991071428571429, f1 = 0.9973262032085563\n",
      "Test Loss = 0.02399036760882763, Recall = 0.9977758007117438, Aging Rate = 0.49933274021352314, precision = 0.9991091314031181\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.02490300256957787, Recall = 0.9968861209964412, Aging Rate = 0.4991103202846975, Precision = 0.9986631016042781, f1 = 0.9977738201246661\n",
      "Epoch 27: Train Loss = 0.024651832288641522, Recall = 0.9973309608540926, Aging Rate = 0.4988879003558719, Precision = 0.9995541685242978, f1 = 0.9984413270986416\n",
      "Epoch 28: Train Loss = 0.023024320138412862, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Epoch 29: Train Loss = 0.022393810228043605, Recall = 0.9973309608540926, Aging Rate = 0.4991103202846975, Precision = 0.9991087344028521, f1 = 0.9982190560997329\n",
      "Epoch 30: Train Loss = 0.021714084307759257, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Test Loss = 0.019796112411426797, Recall = 0.9977758007117438, Aging Rate = 0.4988879003558719, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.021662069060359137, Recall = 0.9968861209964412, Aging Rate = 0.4986654804270463, Precision = 0.9995539696699376, f1 = 0.9982182628062362\n",
      "Epoch 32: Train Loss = 0.020803229863427287, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Epoch 33: Train Loss = 0.020172152323548905, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.019464209503444487, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 35: Train Loss = 0.019054276580355557, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.017387216778147262, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.01867084318831616, Recall = 0.9977758007117438, Aging Rate = 0.4991103202846975, Precision = 0.999554367201426, f1 = 0.9986642920747996\n",
      "Epoch 37: Train Loss = 0.018409709986365562, Recall = 0.998220640569395, Aging Rate = 0.49933274021352314, Precision = 0.999554565701559, f1 = 0.9988871578010238\n",
      "Epoch 38: Train Loss = 0.017762835641321976, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 39: Train Loss = 0.017994486571047654, Recall = 0.998220640569395, Aging Rate = 0.49955516014234874, Precision = 0.9991095280498664, f1 = 0.9986648865153537\n",
      "Epoch 40: Train Loss = 0.01764236598729663, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01631282230182899, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.017682929142142954, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.016938829244349776, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01733218075117607, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.01737653709338229, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 45: Train Loss = 0.016529547173224947, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014732207652194644, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.01653025325825206, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.016003355702047247, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016055199576150692, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016129320262006593, Recall = 0.9986654804270463, Aging Rate = 0.49933274021352314, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.015986398014892888, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.014464989258142128, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.015509287491971063, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 52: Train Loss = 0.015434894288204, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.015490666323497201, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.01553681164025412, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015355772473037349, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Test Loss = 0.014321191779924457, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 56: Train Loss = 0.015493298516181229, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.015133011085229837, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.015508630511838133, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.01498398332263333, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.015113057836300273, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01387545707705182, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.014883315833650025, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.015061537917451502, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.014867739268044983, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 64: Train Loss = 0.014986470686371217, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.01484255437503877, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01369941784930664, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.014900659757841949, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.015147944736655709, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.014983408749633836, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0148736046912195, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.01445932685666025, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013602582472658243, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 71: Train Loss = 0.014998397269548045, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 72: Train Loss = 0.014422097936060505, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.01452803156368461, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.01477906242795156, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014565846687215927, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01299651662961603, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.014410764244103346, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.015157070977023382, Recall = 0.9991103202846975, Aging Rate = 0.4997775800711744, Precision = 0.9995549621717846, f1 = 0.9993325917686319\n",
      "Epoch 78: Train Loss = 0.014290223428416082, Recall = 0.9991103202846975, Aging Rate = 0.49955516014234874, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.014524113274182713, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.014116115079284564, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013144913156023017, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0144039262517014, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014596653664716621, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 83: Train Loss = 0.014797888278589978, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.014213630021466903, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.014003183480577537, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012931728821738335, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Epoch 86: Train Loss = 0.014335657499757102, Recall = 0.9995551601423488, Aging Rate = 0.5, Precision = 0.9995551601423488, f1 = 0.9995551601423488\n",
      "Epoch 87: Train Loss = 0.014627977085442305, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.015455289856527412, Recall = 1.0, Aging Rate = 0.5002224199288257, Precision = 0.9995553579368608, f1 = 0.9997776295307983\n",
      "Epoch 89: Train Loss = 0.014501695398546198, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.013995404324570531, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013044093672544083, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.013869099295780965, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.013747721350357414, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.01398855510078291, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0147090955402079, Recall = 0.9995551601423488, Aging Rate = 0.4997775800711744, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.01410110714411184, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012937736419092506, Recall = 1.0, Aging Rate = 0.5002224199288257, precision = 0.9995553579368608\n",
      "\n",
      "Training Finished at epoch 95.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5428444\ttotal: 6.8ms\tremaining: 1.35s\n",
      "1:\tlearn: 0.4223242\ttotal: 13.4ms\tremaining: 1.33s\n",
      "2:\tlearn: 0.3521023\ttotal: 19.5ms\tremaining: 1.28s\n",
      "3:\tlearn: 0.2827732\ttotal: 25.4ms\tremaining: 1.25s\n",
      "4:\tlearn: 0.2315665\ttotal: 31ms\tremaining: 1.21s\n",
      "5:\tlearn: 0.1928738\ttotal: 37.2ms\tremaining: 1.2s\n",
      "6:\tlearn: 0.1671783\ttotal: 43.3ms\tremaining: 1.19s\n",
      "7:\tlearn: 0.1453775\ttotal: 49.1ms\tremaining: 1.18s\n",
      "8:\tlearn: 0.1261194\ttotal: 54.9ms\tremaining: 1.17s\n",
      "9:\tlearn: 0.1124995\ttotal: 60.9ms\tremaining: 1.16s\n",
      "10:\tlearn: 0.0956577\ttotal: 66.9ms\tremaining: 1.15s\n",
      "11:\tlearn: 0.0835932\ttotal: 72.3ms\tremaining: 1.13s\n",
      "12:\tlearn: 0.0751392\ttotal: 78.1ms\tremaining: 1.12s\n",
      "13:\tlearn: 0.0688574\ttotal: 83.5ms\tremaining: 1.11s\n",
      "14:\tlearn: 0.0624231\ttotal: 87.8ms\tremaining: 1.08s\n",
      "15:\tlearn: 0.0580551\ttotal: 92.6ms\tremaining: 1.06s\n",
      "16:\tlearn: 0.0523757\ttotal: 97.5ms\tremaining: 1.05s\n",
      "17:\tlearn: 0.0480792\ttotal: 102ms\tremaining: 1.03s\n",
      "18:\tlearn: 0.0439785\ttotal: 107ms\tremaining: 1.02s\n",
      "19:\tlearn: 0.0407553\ttotal: 112ms\tremaining: 1s\n",
      "20:\tlearn: 0.0384014\ttotal: 116ms\tremaining: 988ms\n",
      "21:\tlearn: 0.0373711\ttotal: 119ms\tremaining: 963ms\n",
      "22:\tlearn: 0.0335891\ttotal: 124ms\tremaining: 953ms\n",
      "23:\tlearn: 0.0294657\ttotal: 129ms\tremaining: 944ms\n",
      "24:\tlearn: 0.0272398\ttotal: 133ms\tremaining: 934ms\n",
      "25:\tlearn: 0.0256802\ttotal: 138ms\tremaining: 923ms\n",
      "26:\tlearn: 0.0236868\ttotal: 143ms\tremaining: 914ms\n",
      "27:\tlearn: 0.0218615\ttotal: 147ms\tremaining: 906ms\n",
      "28:\tlearn: 0.0201151\ttotal: 152ms\tremaining: 897ms\n",
      "29:\tlearn: 0.0190546\ttotal: 156ms\tremaining: 885ms\n",
      "30:\tlearn: 0.0173962\ttotal: 161ms\tremaining: 880ms\n",
      "31:\tlearn: 0.0163010\ttotal: 166ms\tremaining: 872ms\n",
      "32:\tlearn: 0.0151249\ttotal: 171ms\tremaining: 867ms\n",
      "33:\tlearn: 0.0142695\ttotal: 176ms\tremaining: 860ms\n",
      "34:\tlearn: 0.0132937\ttotal: 181ms\tremaining: 852ms\n",
      "35:\tlearn: 0.0123893\ttotal: 185ms\tremaining: 844ms\n",
      "36:\tlearn: 0.0114423\ttotal: 190ms\tremaining: 837ms\n",
      "37:\tlearn: 0.0105199\ttotal: 195ms\tremaining: 830ms\n",
      "38:\tlearn: 0.0097808\ttotal: 199ms\tremaining: 824ms\n",
      "39:\tlearn: 0.0090777\ttotal: 204ms\tremaining: 817ms\n",
      "40:\tlearn: 0.0083673\ttotal: 209ms\tremaining: 811ms\n",
      "41:\tlearn: 0.0077696\ttotal: 214ms\tremaining: 804ms\n",
      "42:\tlearn: 0.0072367\ttotal: 218ms\tremaining: 798ms\n",
      "43:\tlearn: 0.0066719\ttotal: 223ms\tremaining: 792ms\n",
      "44:\tlearn: 0.0061295\ttotal: 228ms\tremaining: 786ms\n",
      "45:\tlearn: 0.0057293\ttotal: 233ms\tremaining: 780ms\n",
      "46:\tlearn: 0.0053517\ttotal: 237ms\tremaining: 773ms\n",
      "47:\tlearn: 0.0049785\ttotal: 242ms\tremaining: 767ms\n",
      "48:\tlearn: 0.0046822\ttotal: 247ms\tremaining: 762ms\n",
      "49:\tlearn: 0.0044423\ttotal: 252ms\tremaining: 755ms\n",
      "50:\tlearn: 0.0042137\ttotal: 256ms\tremaining: 749ms\n",
      "51:\tlearn: 0.0038973\ttotal: 261ms\tremaining: 743ms\n",
      "52:\tlearn: 0.0036580\ttotal: 266ms\tremaining: 737ms\n",
      "53:\tlearn: 0.0033555\ttotal: 271ms\tremaining: 732ms\n",
      "54:\tlearn: 0.0030914\ttotal: 275ms\tremaining: 726ms\n",
      "55:\tlearn: 0.0028980\ttotal: 280ms\tremaining: 719ms\n",
      "56:\tlearn: 0.0027213\ttotal: 284ms\tremaining: 713ms\n",
      "57:\tlearn: 0.0025500\ttotal: 289ms\tremaining: 707ms\n",
      "58:\tlearn: 0.0023526\ttotal: 294ms\tremaining: 702ms\n",
      "59:\tlearn: 0.0021611\ttotal: 299ms\tremaining: 697ms\n",
      "60:\tlearn: 0.0020195\ttotal: 303ms\tremaining: 691ms\n",
      "61:\tlearn: 0.0018929\ttotal: 308ms\tremaining: 686ms\n",
      "62:\tlearn: 0.0017699\ttotal: 313ms\tremaining: 680ms\n",
      "63:\tlearn: 0.0016657\ttotal: 317ms\tremaining: 674ms\n",
      "64:\tlearn: 0.0016038\ttotal: 321ms\tremaining: 667ms\n",
      "65:\tlearn: 0.0015357\ttotal: 325ms\tremaining: 659ms\n",
      "66:\tlearn: 0.0014209\ttotal: 329ms\tremaining: 654ms\n",
      "67:\tlearn: 0.0013687\ttotal: 333ms\tremaining: 646ms\n",
      "68:\tlearn: 0.0013104\ttotal: 337ms\tremaining: 640ms\n",
      "69:\tlearn: 0.0012267\ttotal: 342ms\tremaining: 635ms\n",
      "70:\tlearn: 0.0011565\ttotal: 346ms\tremaining: 628ms\n",
      "71:\tlearn: 0.0011116\ttotal: 349ms\tremaining: 621ms\n",
      "72:\tlearn: 0.0010412\ttotal: 354ms\tremaining: 616ms\n",
      "73:\tlearn: 0.0009786\ttotal: 359ms\tremaining: 611ms\n",
      "74:\tlearn: 0.0009523\ttotal: 363ms\tremaining: 605ms\n",
      "75:\tlearn: 0.0009325\ttotal: 366ms\tremaining: 598ms\n",
      "76:\tlearn: 0.0009052\ttotal: 370ms\tremaining: 591ms\n",
      "77:\tlearn: 0.0008756\ttotal: 374ms\tremaining: 584ms\n",
      "78:\tlearn: 0.0008400\ttotal: 378ms\tremaining: 579ms\n",
      "79:\tlearn: 0.0008399\ttotal: 380ms\tremaining: 571ms\n",
      "80:\tlearn: 0.0008256\ttotal: 383ms\tremaining: 563ms\n",
      "81:\tlearn: 0.0008256\ttotal: 386ms\tremaining: 556ms\n",
      "82:\tlearn: 0.0007989\ttotal: 389ms\tremaining: 549ms\n",
      "83:\tlearn: 0.0007803\ttotal: 394ms\tremaining: 544ms\n",
      "84:\tlearn: 0.0007525\ttotal: 398ms\tremaining: 538ms\n",
      "85:\tlearn: 0.0007524\ttotal: 401ms\tremaining: 532ms\n",
      "86:\tlearn: 0.0007524\ttotal: 404ms\tremaining: 524ms\n",
      "87:\tlearn: 0.0007524\ttotal: 406ms\tremaining: 517ms\n",
      "88:\tlearn: 0.0007524\ttotal: 409ms\tremaining: 510ms\n",
      "89:\tlearn: 0.0007524\ttotal: 412ms\tremaining: 503ms\n",
      "90:\tlearn: 0.0007524\ttotal: 414ms\tremaining: 496ms\n",
      "91:\tlearn: 0.0007524\ttotal: 417ms\tremaining: 489ms\n",
      "92:\tlearn: 0.0007523\ttotal: 420ms\tremaining: 483ms\n",
      "93:\tlearn: 0.0007523\ttotal: 422ms\tremaining: 476ms\n",
      "94:\tlearn: 0.0007523\ttotal: 425ms\tremaining: 470ms\n",
      "95:\tlearn: 0.0007523\ttotal: 427ms\tremaining: 463ms\n",
      "96:\tlearn: 0.0007523\ttotal: 430ms\tremaining: 457ms\n",
      "97:\tlearn: 0.0007522\ttotal: 433ms\tremaining: 450ms\n",
      "98:\tlearn: 0.0007522\ttotal: 435ms\tremaining: 444ms\n",
      "99:\tlearn: 0.0007522\ttotal: 438ms\tremaining: 438ms\n",
      "100:\tlearn: 0.0007522\ttotal: 440ms\tremaining: 432ms\n",
      "101:\tlearn: 0.0007522\ttotal: 443ms\tremaining: 425ms\n",
      "102:\tlearn: 0.0007522\ttotal: 445ms\tremaining: 419ms\n",
      "103:\tlearn: 0.0007522\ttotal: 448ms\tremaining: 414ms\n",
      "104:\tlearn: 0.0007522\ttotal: 451ms\tremaining: 408ms\n",
      "105:\tlearn: 0.0007522\ttotal: 453ms\tremaining: 402ms\n",
      "106:\tlearn: 0.0007521\ttotal: 456ms\tremaining: 396ms\n",
      "107:\tlearn: 0.0007521\ttotal: 458ms\tremaining: 391ms\n",
      "108:\tlearn: 0.0007521\ttotal: 461ms\tremaining: 385ms\n",
      "109:\tlearn: 0.0007521\ttotal: 464ms\tremaining: 380ms\n",
      "110:\tlearn: 0.0007521\ttotal: 466ms\tremaining: 374ms\n",
      "111:\tlearn: 0.0007521\ttotal: 469ms\tremaining: 368ms\n",
      "112:\tlearn: 0.0007521\ttotal: 471ms\tremaining: 363ms\n",
      "113:\tlearn: 0.0007521\ttotal: 474ms\tremaining: 357ms\n",
      "114:\tlearn: 0.0007521\ttotal: 476ms\tremaining: 352ms\n",
      "115:\tlearn: 0.0007521\ttotal: 479ms\tremaining: 347ms\n",
      "116:\tlearn: 0.0007520\ttotal: 481ms\tremaining: 341ms\n",
      "117:\tlearn: 0.0007520\ttotal: 484ms\tremaining: 336ms\n",
      "118:\tlearn: 0.0007520\ttotal: 487ms\tremaining: 331ms\n",
      "119:\tlearn: 0.0007520\ttotal: 489ms\tremaining: 326ms\n",
      "120:\tlearn: 0.0007520\ttotal: 491ms\tremaining: 321ms\n",
      "121:\tlearn: 0.0007520\ttotal: 494ms\tremaining: 316ms\n",
      "122:\tlearn: 0.0007520\ttotal: 496ms\tremaining: 311ms\n",
      "123:\tlearn: 0.0007520\ttotal: 499ms\tremaining: 306ms\n",
      "124:\tlearn: 0.0007520\ttotal: 501ms\tremaining: 301ms\n",
      "125:\tlearn: 0.0007520\ttotal: 504ms\tremaining: 296ms\n",
      "126:\tlearn: 0.0007520\ttotal: 507ms\tremaining: 291ms\n",
      "127:\tlearn: 0.0007520\ttotal: 509ms\tremaining: 287ms\n",
      "128:\tlearn: 0.0007520\ttotal: 512ms\tremaining: 282ms\n",
      "129:\tlearn: 0.0007519\ttotal: 515ms\tremaining: 277ms\n",
      "130:\tlearn: 0.0007519\ttotal: 517ms\tremaining: 272ms\n",
      "131:\tlearn: 0.0007519\ttotal: 520ms\tremaining: 268ms\n",
      "132:\tlearn: 0.0007519\ttotal: 522ms\tremaining: 263ms\n",
      "133:\tlearn: 0.0007519\ttotal: 525ms\tremaining: 258ms\n",
      "134:\tlearn: 0.0007519\ttotal: 527ms\tremaining: 254ms\n",
      "135:\tlearn: 0.0007518\ttotal: 530ms\tremaining: 249ms\n",
      "136:\tlearn: 0.0007518\ttotal: 533ms\tremaining: 245ms\n",
      "137:\tlearn: 0.0007518\ttotal: 535ms\tremaining: 241ms\n",
      "138:\tlearn: 0.0007518\ttotal: 538ms\tremaining: 236ms\n",
      "139:\tlearn: 0.0007517\ttotal: 541ms\tremaining: 232ms\n",
      "140:\tlearn: 0.0007517\ttotal: 543ms\tremaining: 227ms\n",
      "141:\tlearn: 0.0007517\ttotal: 546ms\tremaining: 223ms\n",
      "142:\tlearn: 0.0007517\ttotal: 549ms\tremaining: 219ms\n",
      "143:\tlearn: 0.0007517\ttotal: 551ms\tremaining: 214ms\n",
      "144:\tlearn: 0.0007517\ttotal: 554ms\tremaining: 210ms\n",
      "145:\tlearn: 0.0007517\ttotal: 556ms\tremaining: 206ms\n",
      "146:\tlearn: 0.0007517\ttotal: 559ms\tremaining: 201ms\n",
      "147:\tlearn: 0.0007517\ttotal: 561ms\tremaining: 197ms\n",
      "148:\tlearn: 0.0007516\ttotal: 564ms\tremaining: 193ms\n",
      "149:\tlearn: 0.0007516\ttotal: 566ms\tremaining: 189ms\n",
      "150:\tlearn: 0.0007516\ttotal: 569ms\tremaining: 185ms\n",
      "151:\tlearn: 0.0007516\ttotal: 572ms\tremaining: 181ms\n",
      "152:\tlearn: 0.0007516\ttotal: 574ms\tremaining: 176ms\n",
      "153:\tlearn: 0.0007516\ttotal: 577ms\tremaining: 172ms\n",
      "154:\tlearn: 0.0007516\ttotal: 579ms\tremaining: 168ms\n",
      "155:\tlearn: 0.0007516\ttotal: 582ms\tremaining: 164ms\n",
      "156:\tlearn: 0.0007516\ttotal: 585ms\tremaining: 160ms\n",
      "157:\tlearn: 0.0007516\ttotal: 587ms\tremaining: 156ms\n",
      "158:\tlearn: 0.0007516\ttotal: 590ms\tremaining: 152ms\n",
      "159:\tlearn: 0.0007516\ttotal: 593ms\tremaining: 148ms\n",
      "160:\tlearn: 0.0007516\ttotal: 596ms\tremaining: 144ms\n",
      "161:\tlearn: 0.0007516\ttotal: 598ms\tremaining: 140ms\n",
      "162:\tlearn: 0.0007515\ttotal: 601ms\tremaining: 136ms\n",
      "163:\tlearn: 0.0007515\ttotal: 603ms\tremaining: 132ms\n",
      "164:\tlearn: 0.0007515\ttotal: 606ms\tremaining: 129ms\n",
      "165:\tlearn: 0.0007515\ttotal: 608ms\tremaining: 125ms\n",
      "166:\tlearn: 0.0007515\ttotal: 611ms\tremaining: 121ms\n",
      "167:\tlearn: 0.0007515\ttotal: 613ms\tremaining: 117ms\n",
      "168:\tlearn: 0.0007515\ttotal: 616ms\tremaining: 113ms\n",
      "169:\tlearn: 0.0007515\ttotal: 618ms\tremaining: 109ms\n",
      "170:\tlearn: 0.0007515\ttotal: 621ms\tremaining: 105ms\n",
      "171:\tlearn: 0.0007515\ttotal: 623ms\tremaining: 101ms\n",
      "172:\tlearn: 0.0007515\ttotal: 626ms\tremaining: 97.6ms\n",
      "173:\tlearn: 0.0007515\ttotal: 628ms\tremaining: 93.8ms\n",
      "174:\tlearn: 0.0007515\ttotal: 630ms\tremaining: 90.1ms\n",
      "175:\tlearn: 0.0007514\ttotal: 633ms\tremaining: 86.3ms\n",
      "176:\tlearn: 0.0007514\ttotal: 636ms\tremaining: 82.6ms\n",
      "177:\tlearn: 0.0007514\ttotal: 638ms\tremaining: 78.9ms\n",
      "178:\tlearn: 0.0007514\ttotal: 641ms\tremaining: 75.2ms\n",
      "179:\tlearn: 0.0007514\ttotal: 643ms\tremaining: 71.5ms\n",
      "180:\tlearn: 0.0007514\ttotal: 646ms\tremaining: 67.8ms\n",
      "181:\tlearn: 0.0007514\ttotal: 648ms\tremaining: 64.1ms\n",
      "182:\tlearn: 0.0007514\ttotal: 651ms\tremaining: 60.5ms\n",
      "183:\tlearn: 0.0007514\ttotal: 653ms\tremaining: 56.8ms\n",
      "184:\tlearn: 0.0007514\ttotal: 656ms\tremaining: 53.2ms\n",
      "185:\tlearn: 0.0007513\ttotal: 658ms\tremaining: 49.6ms\n",
      "186:\tlearn: 0.0007513\ttotal: 661ms\tremaining: 46ms\n",
      "187:\tlearn: 0.0007513\ttotal: 664ms\tremaining: 42.4ms\n",
      "188:\tlearn: 0.0007513\ttotal: 666ms\tremaining: 38.8ms\n",
      "189:\tlearn: 0.0007513\ttotal: 669ms\tremaining: 35.2ms\n",
      "190:\tlearn: 0.0007513\ttotal: 671ms\tremaining: 31.6ms\n",
      "191:\tlearn: 0.0007513\ttotal: 674ms\tremaining: 28.1ms\n",
      "192:\tlearn: 0.0007513\ttotal: 676ms\tremaining: 24.5ms\n",
      "193:\tlearn: 0.0007513\ttotal: 679ms\tremaining: 21ms\n",
      "194:\tlearn: 0.0007513\ttotal: 681ms\tremaining: 17.5ms\n",
      "195:\tlearn: 0.0007513\ttotal: 684ms\tremaining: 14ms\n",
      "196:\tlearn: 0.0007513\ttotal: 686ms\tremaining: 10.5ms\n",
      "197:\tlearn: 0.0007513\ttotal: 689ms\tremaining: 6.96ms\n",
      "198:\tlearn: 0.0007513\ttotal: 691ms\tremaining: 3.47ms\n",
      "199:\tlearn: 0.0007513\ttotal: 694ms\tremaining: 0us\n",
      "Dataset 9:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d58b97937d4141bb8e014756ba0e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16e5b30edad4c99bdeaf917595084d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5684398950182664, Recall = 0.07555555555555556, Aging Rate = 0.08491710473109583, Precision = 0.08095238095238096, f1 = 0.0781609195402299\n",
      "Epoch 2: Train Loss = 0.4612008869238243, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.42651129839056856, Recall = 0.0044444444444444444, Aging Rate = 0.0004043671653861706, Precision = 0, f1 = 0.0\n",
      "Epoch 4: Train Loss = 0.38677481008453307, Recall = 0.04888888888888889, Aging Rate = 0.004448038819247877, Precision = 0, f1 = 0.0\n",
      "Epoch 5: Train Loss = 0.34833189190711467, Recall = 0.21777777777777776, Aging Rate = 0.023048928427011728, Precision = 0.8596491228070176, f1 = 0.34751773049645385\n",
      "Test Loss = 0.3187098237506976, Recall = 0.3466666666666667, Aging Rate = 0.032753740396279825, precision = 0.9629629629629629\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3071730349693422, Recall = 0.3511111111111111, Aging Rate = 0.039223615042458554, Precision = 0.8144329896907216, f1 = 0.4906832298136646\n",
      "Epoch 7: Train Loss = 0.2723407156846806, Recall = 0.5155555555555555, Aging Rate = 0.055802668823291546, Precision = 0.8405797101449275, f1 = 0.6391184573002755\n",
      "Epoch 8: Train Loss = 0.2511070579037317, Recall = 0.5511111111111111, Aging Rate = 0.059441973311767086, Precision = 0.8435374149659864, f1 = 0.6666666666666666\n",
      "Epoch 9: Train Loss = 0.23010472348355301, Recall = 0.6, Aging Rate = 0.06429437929640114, Precision = 0.8490566037735849, f1 = 0.7031249999999999\n",
      "Epoch 10: Train Loss = 0.2133647437808825, Recall = 0.6622222222222223, Aging Rate = 0.06955115244642135, Precision = 0.8662790697674418, f1 = 0.7506297229219143\n",
      "Test Loss = 0.20009468494640686, Recall = 0.7288888888888889, Aging Rate = 0.0744035584310554, precision = 0.8913043478260869\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.1979739581024989, Recall = 0.6844444444444444, Aging Rate = 0.068742418115649, Precision = 0.9058823529411765, f1 = 0.779746835443038\n",
      "Epoch 12: Train Loss = 0.18599826414431825, Recall = 0.76, Aging Rate = 0.07804286291953093, Precision = 0.8860103626943006, f1 = 0.8181818181818182\n",
      "Epoch 13: Train Loss = 0.17277707187236682, Recall = 0.7644444444444445, Aging Rate = 0.07682976142337242, Precision = 0.9052631578947369, f1 = 0.8289156626506023\n",
      "Epoch 14: Train Loss = 0.1615355185655221, Recall = 0.8, Aging Rate = 0.0841083704003235, Precision = 0.8653846153846154, f1 = 0.8314087759815243\n",
      "Epoch 15: Train Loss = 0.14967315798580913, Recall = 0.8266666666666667, Aging Rate = 0.08249090173877881, Precision = 0.9117647058823529, f1 = 0.8671328671328671\n",
      "Test Loss = 0.14431548897091118, Recall = 0.7688888888888888, Aging Rate = 0.07278608976951072, precision = 0.9611111111111111\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.14022835113900517, Recall = 0.8444444444444444, Aging Rate = 0.08329963606955115, Precision = 0.9223300970873787, f1 = 0.8816705336426914\n",
      "Epoch 17: Train Loss = 0.13153867201493233, Recall = 0.8577777777777778, Aging Rate = 0.08653457339264052, Precision = 0.9018691588785047, f1 = 0.8792710706150343\n",
      "Epoch 18: Train Loss = 0.1215656997731616, Recall = 0.8488888888888889, Aging Rate = 0.0812778002426203, Precision = 0.9502487562189055, f1 = 0.8967136150234742\n",
      "Epoch 19: Train Loss = 0.11447296345518322, Recall = 0.8711111111111111, Aging Rate = 0.08532147189648201, Precision = 0.9289099526066351, f1 = 0.8990825688073394\n",
      "Epoch 20: Train Loss = 0.10676111286341107, Recall = 0.8844444444444445, Aging Rate = 0.08451273756570966, Precision = 0.9521531100478469, f1 = 0.9170506912442397\n",
      "Test Loss = 0.09950114526321567, Recall = 0.8977777777777778, Aging Rate = 0.08532147189648201, precision = 0.957345971563981\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.0999626954057127, Recall = 0.8977777777777778, Aging Rate = 0.08572583906186818, Precision = 0.9528301886792453, f1 = 0.9244851258581236\n",
      "Epoch 22: Train Loss = 0.09329094795010806, Recall = 0.9066666666666666, Aging Rate = 0.08613020622725434, Precision = 0.9577464788732394, f1 = 0.9315068493150686\n",
      "Epoch 23: Train Loss = 0.08821089567260322, Recall = 0.9066666666666666, Aging Rate = 0.08532147189648201, Precision = 0.966824644549763, f1 = 0.9357798165137615\n",
      "Epoch 24: Train Loss = 0.0838804803171044, Recall = 0.92, Aging Rate = 0.08572583906186818, Precision = 0.9764150943396226, f1 = 0.9473684210526316\n",
      "Epoch 25: Train Loss = 0.07787206778027873, Recall = 0.9288888888888889, Aging Rate = 0.08693894055802669, Precision = 0.9720930232558139, f1 = 0.9500000000000001\n",
      "Test Loss = 0.07220051341207703, Recall = 0.9288888888888889, Aging Rate = 0.08653457339264052, precision = 0.9766355140186916\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.07349315767123349, Recall = 0.9244444444444444, Aging Rate = 0.08613020622725434, Precision = 0.9765258215962441, f1 = 0.9497716894977168\n",
      "Epoch 27: Train Loss = 0.06816055074189531, Recall = 0.9333333333333333, Aging Rate = 0.08774767488879903, Precision = 0.967741935483871, f1 = 0.9502262443438915\n",
      "Epoch 28: Train Loss = 0.06427846447964983, Recall = 0.9333333333333333, Aging Rate = 0.08653457339264052, Precision = 0.9813084112149533, f1 = 0.9567198177676537\n",
      "Epoch 29: Train Loss = 0.06028269187483622, Recall = 0.9377777777777778, Aging Rate = 0.08734330772341285, Precision = 0.9768518518518519, f1 = 0.9569160997732427\n",
      "Epoch 30: Train Loss = 0.05774934900479427, Recall = 0.9466666666666667, Aging Rate = 0.08653457339264052, Precision = 0.9953271028037384, f1 = 0.9703872437357631\n",
      "Test Loss = 0.055538314978000464, Recall = 0.9644444444444444, Aging Rate = 0.0909826122118884, precision = 0.9644444444444444\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.054660720754455466, Recall = 0.9422222222222222, Aging Rate = 0.08734330772341285, Precision = 0.9814814814814815, f1 = 0.9614512471655329\n",
      "Epoch 32: Train Loss = 0.05107173269853339, Recall = 0.9555555555555556, Aging Rate = 0.08774767488879903, Precision = 0.9907834101382489, f1 = 0.9728506787330318\n",
      "Epoch 33: Train Loss = 0.047575766584480816, Recall = 0.9555555555555556, Aging Rate = 0.0881520420541852, Precision = 0.9862385321100917, f1 = 0.9706546275395033\n",
      "Epoch 34: Train Loss = 0.045617119499150434, Recall = 0.96, Aging Rate = 0.08734330772341285, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.04344232171906472, Recall = 0.9644444444444444, Aging Rate = 0.08855640921957138, Precision = 0.9908675799086758, f1 = 0.9774774774774775\n",
      "Test Loss = 0.03977484679624613, Recall = 0.9733333333333334, Aging Rate = 0.08896077638495754, precision = 0.9954545454545455\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.040562088835910656, Recall = 0.9733333333333334, Aging Rate = 0.08936514355034371, Precision = 0.9909502262443439, f1 = 0.9820627802690584\n",
      "Epoch 37: Train Loss = 0.038554048597258306, Recall = 0.9733333333333334, Aging Rate = 0.08896077638495754, Precision = 0.9954545454545455, f1 = 0.9842696629213483\n",
      "Epoch 38: Train Loss = 0.036574418684019526, Recall = 0.9688888888888889, Aging Rate = 0.0881520420541852, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.03409390597466004, Recall = 0.9777777777777777, Aging Rate = 0.08936514355034371, Precision = 0.995475113122172, f1 = 0.9865470852017937\n",
      "Epoch 40: Train Loss = 0.03234487552808886, Recall = 0.9822222222222222, Aging Rate = 0.08936514355034371, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.03005660325665369, Recall = 0.9822222222222222, Aging Rate = 0.08976951071572989, precision = 0.9954954954954955\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.030512914924704974, Recall = 0.9822222222222222, Aging Rate = 0.08976951071572989, Precision = 0.9954954954954955, f1 = 0.988814317673378\n",
      "Epoch 42: Train Loss = 0.029372124344086387, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Epoch 43: Train Loss = 0.02775131528040946, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Epoch 44: Train Loss = 0.02689674454321275, Recall = 0.9822222222222222, Aging Rate = 0.08976951071572989, Precision = 0.9954954954954955, f1 = 0.988814317673378\n",
      "Epoch 45: Train Loss = 0.02486453190207891, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Test Loss = 0.023297773432355635, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, precision = 0.9955156950672646\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.023634413401101024, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Epoch 47: Train Loss = 0.022620420608845768, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Epoch 48: Train Loss = 0.021725805320397012, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.02043486018753635, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Epoch 50: Train Loss = 0.02018798736488583, Recall = 0.9911111111111112, Aging Rate = 0.09057824504650222, Precision = 0.9955357142857143, f1 = 0.9933184855233853\n",
      "Test Loss = 0.0184087193560863, Recall = 0.9911111111111112, Aging Rate = 0.09057824504650222, precision = 0.9955357142857143\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.019127647018491668, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 52: Train Loss = 0.018175801260753098, Recall = 0.9911111111111112, Aging Rate = 0.09057824504650222, Precision = 0.9955357142857143, f1 = 0.9933184855233853\n",
      "Epoch 53: Train Loss = 0.017092600731827615, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 54: Train Loss = 0.016623931825070597, Recall = 0.9911111111111112, Aging Rate = 0.09057824504650222, Precision = 0.9955357142857143, f1 = 0.9933184855233853\n",
      "Epoch 55: Train Loss = 0.015977330647448128, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.014924416297069818, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.015227064382611765, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 57: Train Loss = 0.014464156870875964, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 58: Train Loss = 0.014344262632549266, Recall = 0.9911111111111112, Aging Rate = 0.09057824504650222, Precision = 0.9955357142857143, f1 = 0.9933184855233853\n",
      "Epoch 59: Train Loss = 0.013658505311221475, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 60: Train Loss = 0.013142115154423744, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.012278016369294239, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 61: Train Loss = 0.012578014819139033, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 62: Train Loss = 0.01215536514951652, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 63: Train Loss = 0.011622764194271992, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 64: Train Loss = 0.011366773328943095, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 65: Train Loss = 0.011115648610509097, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.011228143842344794, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 66: Train Loss = 0.010832011979487542, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 67: Train Loss = 0.010416720270563716, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 68: Train Loss = 0.01006669983956202, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 69: Train Loss = 0.009581814430054507, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 70: Train Loss = 0.009235930241745311, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.008684708442338627, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 71: Train Loss = 0.00881395745929067, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 72: Train Loss = 0.008589683567560543, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 73: Train Loss = 0.008586034300751017, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 74: Train Loss = 0.008489985874595306, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 75: Train Loss = 0.007813686034214723, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.0074529959356997525, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 76: Train Loss = 0.007733104017469392, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 77: Train Loss = 0.007388309589927972, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.00724872320308962, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 79: Train Loss = 0.007140504337509803, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.007251791407275467, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.00651527837775964, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "Model in epoch 80 is saved.\n",
      "\n",
      "Epoch 81: Train Loss = 0.006621681466725761, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 82: Train Loss = 0.006524183293499484, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.006437572677349736, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 84: Train Loss = 0.00618548049176096, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 85: Train Loss = 0.0059820373851039174, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.005658455734023297, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.005852898294719978, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 87: Train Loss = 0.0056446334110814705, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.005800508720227101, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.005683694595943276, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.005374888037065902, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00527448948827492, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 91: Train Loss = 0.005221286349377402, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 92: Train Loss = 0.005048928302748515, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.004956492298541404, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.004801362363890482, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.004700482992422976, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00454458999906972, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.004649536538789304, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.004495887263831289, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.004434953997867446, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.004437565993991395, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss = 0.00444344172270775, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004087024760947348, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.0042208333327813435, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.004131834785025785, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.004066186942562334, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.004053132955189772, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.004013193571129252, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003789390707512797, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.0038059760492145894, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.003811788196712009, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.0037251204285006917, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.0037129045554669857, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.0035582864239022552, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003394574087167372, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0034711388145184413, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.0034225256985533203, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.003373078192880598, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.003352174560249069, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.003352350412818629, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0031463522373653837, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.003329516211663946, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.0031625051158708185, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.003144880709284672, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.003054711817821365, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.0030574163964663713, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029348833906342638, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.0030229776152727837, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.0029693832087153145, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.0030127177232431284, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.0028612097354136553, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.0028575074523227917, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002738601226972328, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.002781548189433134, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 127: Train Loss = 0.0027515708048102246, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 128: Train Loss = 0.0026951798285311333, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.0027131508145928746, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.0026616731461241473, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026163543105932096, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 130.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5174888\ttotal: 2.77ms\tremaining: 413ms\n",
      "1:\tlearn: 0.3954658\ttotal: 5.53ms\tremaining: 409ms\n",
      "2:\tlearn: 0.3280764\ttotal: 7.72ms\tremaining: 378ms\n",
      "3:\tlearn: 0.2818021\ttotal: 9.84ms\tremaining: 359ms\n",
      "4:\tlearn: 0.2424950\ttotal: 12.3ms\tremaining: 357ms\n",
      "5:\tlearn: 0.2234512\ttotal: 14.4ms\tremaining: 345ms\n",
      "6:\tlearn: 0.1959235\ttotal: 16.8ms\tremaining: 344ms\n",
      "7:\tlearn: 0.1778619\ttotal: 19.2ms\tremaining: 341ms\n",
      "8:\tlearn: 0.1632559\ttotal: 21.9ms\tremaining: 343ms\n",
      "9:\tlearn: 0.1529899\ttotal: 24.1ms\tremaining: 337ms\n",
      "10:\tlearn: 0.1427442\ttotal: 26.4ms\tremaining: 334ms\n",
      "11:\tlearn: 0.1316962\ttotal: 28.9ms\tremaining: 333ms\n",
      "12:\tlearn: 0.1234892\ttotal: 31.3ms\tremaining: 329ms\n",
      "13:\tlearn: 0.1178894\ttotal: 33.4ms\tremaining: 324ms\n",
      "14:\tlearn: 0.1114972\ttotal: 35.7ms\tremaining: 321ms\n",
      "15:\tlearn: 0.1049266\ttotal: 38.1ms\tremaining: 319ms\n",
      "16:\tlearn: 0.0973783\ttotal: 40.5ms\tremaining: 317ms\n",
      "17:\tlearn: 0.0893973\ttotal: 43.1ms\tremaining: 316ms\n",
      "18:\tlearn: 0.0855447\ttotal: 45.3ms\tremaining: 313ms\n",
      "19:\tlearn: 0.0812145\ttotal: 47.6ms\tremaining: 310ms\n",
      "20:\tlearn: 0.0764365\ttotal: 50.1ms\tremaining: 307ms\n",
      "21:\tlearn: 0.0715022\ttotal: 52.8ms\tremaining: 307ms\n",
      "22:\tlearn: 0.0682623\ttotal: 55.1ms\tremaining: 304ms\n",
      "23:\tlearn: 0.0646561\ttotal: 57.4ms\tremaining: 301ms\n",
      "24:\tlearn: 0.0592380\ttotal: 60.2ms\tremaining: 301ms\n",
      "25:\tlearn: 0.0539539\ttotal: 62.9ms\tremaining: 300ms\n",
      "26:\tlearn: 0.0520859\ttotal: 65.1ms\tremaining: 296ms\n",
      "27:\tlearn: 0.0492580\ttotal: 67.5ms\tremaining: 294ms\n",
      "28:\tlearn: 0.0462307\ttotal: 70ms\tremaining: 292ms\n",
      "29:\tlearn: 0.0438687\ttotal: 72.3ms\tremaining: 289ms\n",
      "30:\tlearn: 0.0406942\ttotal: 74.6ms\tremaining: 287ms\n",
      "31:\tlearn: 0.0394903\ttotal: 76.8ms\tremaining: 283ms\n",
      "32:\tlearn: 0.0362890\ttotal: 79.5ms\tremaining: 282ms\n",
      "33:\tlearn: 0.0344908\ttotal: 81.6ms\tremaining: 278ms\n",
      "34:\tlearn: 0.0332705\ttotal: 83.4ms\tremaining: 274ms\n",
      "35:\tlearn: 0.0314611\ttotal: 85.5ms\tremaining: 271ms\n",
      "36:\tlearn: 0.0298206\ttotal: 87.6ms\tremaining: 267ms\n",
      "37:\tlearn: 0.0285937\ttotal: 89.5ms\tremaining: 264ms\n",
      "38:\tlearn: 0.0272115\ttotal: 91.4ms\tremaining: 260ms\n",
      "39:\tlearn: 0.0259401\ttotal: 93.4ms\tremaining: 257ms\n",
      "40:\tlearn: 0.0251928\ttotal: 95.4ms\tremaining: 254ms\n",
      "41:\tlearn: 0.0236966\ttotal: 97.5ms\tremaining: 251ms\n",
      "42:\tlearn: 0.0222104\ttotal: 99.6ms\tremaining: 248ms\n",
      "43:\tlearn: 0.0215280\ttotal: 101ms\tremaining: 245ms\n",
      "44:\tlearn: 0.0204789\ttotal: 104ms\tremaining: 242ms\n",
      "45:\tlearn: 0.0197898\ttotal: 105ms\tremaining: 238ms\n",
      "46:\tlearn: 0.0189524\ttotal: 107ms\tremaining: 236ms\n",
      "47:\tlearn: 0.0178035\ttotal: 110ms\tremaining: 233ms\n",
      "48:\tlearn: 0.0169788\ttotal: 111ms\tremaining: 230ms\n",
      "49:\tlearn: 0.0165082\ttotal: 113ms\tremaining: 226ms\n",
      "50:\tlearn: 0.0159573\ttotal: 115ms\tremaining: 223ms\n",
      "51:\tlearn: 0.0157041\ttotal: 117ms\tremaining: 220ms\n",
      "52:\tlearn: 0.0151912\ttotal: 119ms\tremaining: 217ms\n",
      "53:\tlearn: 0.0149249\ttotal: 120ms\tremaining: 214ms\n",
      "54:\tlearn: 0.0142146\ttotal: 122ms\tremaining: 211ms\n",
      "55:\tlearn: 0.0136532\ttotal: 124ms\tremaining: 209ms\n",
      "56:\tlearn: 0.0131524\ttotal: 126ms\tremaining: 206ms\n",
      "57:\tlearn: 0.0122108\ttotal: 129ms\tremaining: 204ms\n",
      "58:\tlearn: 0.0119217\ttotal: 130ms\tremaining: 201ms\n",
      "59:\tlearn: 0.0114311\ttotal: 132ms\tremaining: 199ms\n",
      "60:\tlearn: 0.0112809\ttotal: 134ms\tremaining: 196ms\n",
      "61:\tlearn: 0.0106588\ttotal: 136ms\tremaining: 193ms\n",
      "62:\tlearn: 0.0101917\ttotal: 138ms\tremaining: 191ms\n",
      "63:\tlearn: 0.0097600\ttotal: 140ms\tremaining: 189ms\n",
      "64:\tlearn: 0.0093921\ttotal: 142ms\tremaining: 186ms\n",
      "65:\tlearn: 0.0092059\ttotal: 144ms\tremaining: 183ms\n",
      "66:\tlearn: 0.0090002\ttotal: 146ms\tremaining: 181ms\n",
      "67:\tlearn: 0.0087557\ttotal: 148ms\tremaining: 178ms\n",
      "68:\tlearn: 0.0085368\ttotal: 150ms\tremaining: 176ms\n",
      "69:\tlearn: 0.0082765\ttotal: 152ms\tremaining: 173ms\n",
      "70:\tlearn: 0.0078390\ttotal: 154ms\tremaining: 171ms\n",
      "71:\tlearn: 0.0075909\ttotal: 156ms\tremaining: 169ms\n",
      "72:\tlearn: 0.0074215\ttotal: 158ms\tremaining: 166ms\n",
      "73:\tlearn: 0.0070875\ttotal: 160ms\tremaining: 164ms\n",
      "74:\tlearn: 0.0068033\ttotal: 162ms\tremaining: 162ms\n",
      "75:\tlearn: 0.0065416\ttotal: 164ms\tremaining: 159ms\n",
      "76:\tlearn: 0.0063106\ttotal: 166ms\tremaining: 157ms\n",
      "77:\tlearn: 0.0060845\ttotal: 168ms\tremaining: 155ms\n",
      "78:\tlearn: 0.0059035\ttotal: 169ms\tremaining: 152ms\n",
      "79:\tlearn: 0.0056373\ttotal: 171ms\tremaining: 150ms\n",
      "80:\tlearn: 0.0055610\ttotal: 173ms\tremaining: 148ms\n",
      "81:\tlearn: 0.0054881\ttotal: 175ms\tremaining: 145ms\n",
      "82:\tlearn: 0.0051720\ttotal: 177ms\tremaining: 143ms\n",
      "83:\tlearn: 0.0050023\ttotal: 179ms\tremaining: 141ms\n",
      "84:\tlearn: 0.0048059\ttotal: 181ms\tremaining: 139ms\n",
      "85:\tlearn: 0.0046508\ttotal: 183ms\tremaining: 136ms\n",
      "86:\tlearn: 0.0045362\ttotal: 185ms\tremaining: 134ms\n",
      "87:\tlearn: 0.0044377\ttotal: 187ms\tremaining: 132ms\n",
      "88:\tlearn: 0.0043268\ttotal: 189ms\tremaining: 130ms\n",
      "89:\tlearn: 0.0041655\ttotal: 191ms\tremaining: 127ms\n",
      "90:\tlearn: 0.0040266\ttotal: 193ms\tremaining: 125ms\n",
      "91:\tlearn: 0.0039670\ttotal: 195ms\tremaining: 123ms\n",
      "92:\tlearn: 0.0038162\ttotal: 197ms\tremaining: 121ms\n",
      "93:\tlearn: 0.0037114\ttotal: 199ms\tremaining: 118ms\n",
      "94:\tlearn: 0.0036072\ttotal: 201ms\tremaining: 116ms\n",
      "95:\tlearn: 0.0035637\ttotal: 203ms\tremaining: 114ms\n",
      "96:\tlearn: 0.0034963\ttotal: 204ms\tremaining: 112ms\n",
      "97:\tlearn: 0.0034019\ttotal: 206ms\tremaining: 109ms\n",
      "98:\tlearn: 0.0032682\ttotal: 209ms\tremaining: 107ms\n",
      "99:\tlearn: 0.0031891\ttotal: 210ms\tremaining: 105ms\n",
      "100:\tlearn: 0.0031055\ttotal: 212ms\tremaining: 103ms\n",
      "101:\tlearn: 0.0030365\ttotal: 214ms\tremaining: 101ms\n",
      "102:\tlearn: 0.0029754\ttotal: 216ms\tremaining: 98.6ms\n",
      "103:\tlearn: 0.0029404\ttotal: 218ms\tremaining: 96.3ms\n",
      "104:\tlearn: 0.0028222\ttotal: 220ms\tremaining: 94.3ms\n",
      "105:\tlearn: 0.0027692\ttotal: 222ms\tremaining: 92ms\n",
      "106:\tlearn: 0.0027040\ttotal: 224ms\tremaining: 89.8ms\n",
      "107:\tlearn: 0.0026425\ttotal: 226ms\tremaining: 87.8ms\n",
      "108:\tlearn: 0.0025963\ttotal: 228ms\tremaining: 85.6ms\n",
      "109:\tlearn: 0.0025575\ttotal: 229ms\tremaining: 83.4ms\n",
      "110:\tlearn: 0.0024748\ttotal: 232ms\tremaining: 81.4ms\n",
      "111:\tlearn: 0.0024083\ttotal: 234ms\tremaining: 79.3ms\n",
      "112:\tlearn: 0.0023566\ttotal: 235ms\tremaining: 77.1ms\n",
      "113:\tlearn: 0.0022900\ttotal: 237ms\tremaining: 75ms\n",
      "114:\tlearn: 0.0022018\ttotal: 239ms\tremaining: 72.9ms\n",
      "115:\tlearn: 0.0021533\ttotal: 241ms\tremaining: 70.7ms\n",
      "116:\tlearn: 0.0021386\ttotal: 243ms\tremaining: 68.5ms\n",
      "117:\tlearn: 0.0020828\ttotal: 245ms\tremaining: 66.5ms\n",
      "118:\tlearn: 0.0020525\ttotal: 247ms\tremaining: 64.3ms\n",
      "119:\tlearn: 0.0020145\ttotal: 249ms\tremaining: 62.2ms\n",
      "120:\tlearn: 0.0019839\ttotal: 251ms\tremaining: 60.1ms\n",
      "121:\tlearn: 0.0019453\ttotal: 252ms\tremaining: 57.9ms\n",
      "122:\tlearn: 0.0018892\ttotal: 254ms\tremaining: 55.8ms\n",
      "123:\tlearn: 0.0018031\ttotal: 256ms\tremaining: 53.8ms\n",
      "124:\tlearn: 0.0017613\ttotal: 258ms\tremaining: 51.7ms\n",
      "125:\tlearn: 0.0017170\ttotal: 260ms\tremaining: 49.6ms\n",
      "126:\tlearn: 0.0016837\ttotal: 262ms\tremaining: 47.5ms\n",
      "127:\tlearn: 0.0016613\ttotal: 264ms\tremaining: 45.4ms\n",
      "128:\tlearn: 0.0016469\ttotal: 266ms\tremaining: 43.3ms\n",
      "129:\tlearn: 0.0016371\ttotal: 267ms\tremaining: 41.1ms\n",
      "130:\tlearn: 0.0016103\ttotal: 269ms\tremaining: 39.1ms\n",
      "131:\tlearn: 0.0015646\ttotal: 271ms\tremaining: 37ms\n",
      "132:\tlearn: 0.0015355\ttotal: 273ms\tremaining: 34.9ms\n",
      "133:\tlearn: 0.0015107\ttotal: 275ms\tremaining: 32.8ms\n",
      "134:\tlearn: 0.0014799\ttotal: 277ms\tremaining: 30.8ms\n",
      "135:\tlearn: 0.0014623\ttotal: 279ms\tremaining: 28.7ms\n",
      "136:\tlearn: 0.0014552\ttotal: 280ms\tremaining: 26.6ms\n",
      "137:\tlearn: 0.0014266\ttotal: 282ms\tremaining: 24.6ms\n",
      "138:\tlearn: 0.0014060\ttotal: 284ms\tremaining: 22.5ms\n",
      "139:\tlearn: 0.0013719\ttotal: 286ms\tremaining: 20.4ms\n",
      "140:\tlearn: 0.0013663\ttotal: 288ms\tremaining: 18.4ms\n",
      "141:\tlearn: 0.0013434\ttotal: 290ms\tremaining: 16.3ms\n",
      "142:\tlearn: 0.0013168\ttotal: 292ms\tremaining: 14.3ms\n",
      "143:\tlearn: 0.0012989\ttotal: 294ms\tremaining: 12.2ms\n",
      "144:\tlearn: 0.0012762\ttotal: 295ms\tremaining: 10.2ms\n",
      "145:\tlearn: 0.0012537\ttotal: 297ms\tremaining: 8.15ms\n",
      "146:\tlearn: 0.0012290\ttotal: 299ms\tremaining: 6.11ms\n",
      "147:\tlearn: 0.0012164\ttotal: 301ms\tremaining: 4.07ms\n",
      "148:\tlearn: 0.0011942\ttotal: 303ms\tremaining: 2.03ms\n",
      "149:\tlearn: 0.0011712\ttotal: 305ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d547bc2302ec4762ba8b9d1847124522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5585864493589482, Recall = 0.05333333333333334, Aging Rate = 0.053780832996360696, Precision = 0.09022556390977443, f1 = 0.0670391061452514\n",
      "Epoch 2: Train Loss = 0.463221131176588, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.42784298594259224, Recall = 0.0044444444444444444, Aging Rate = 0.0004043671653861706, Precision = 0, f1 = 0.0\n",
      "Epoch 4: Train Loss = 0.39000399442660005, Recall = 0.044444444444444446, Aging Rate = 0.0040436716538617065, Precision = 0, f1 = 0.0\n",
      "Epoch 5: Train Loss = 0.3527091619604954, Recall = 0.18666666666666668, Aging Rate = 0.01981399110392236, Precision = 0.8571428571428571, f1 = 0.30656934306569344\n",
      "Test Loss = 0.32790476965894594, Recall = 0.14666666666666667, Aging Rate = 0.013748483623129802, precision = 0.9705882352941176\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3089368715731663, Recall = 0.29333333333333333, Aging Rate = 0.029518803073190457, Precision = 0.9041095890410958, f1 = 0.4429530201342282\n",
      "Epoch 7: Train Loss = 0.284566463160698, Recall = 0.48444444444444446, Aging Rate = 0.055802668823291546, Precision = 0.7898550724637681, f1 = 0.6005509641873278\n",
      "Epoch 8: Train Loss = 0.2548833339502587, Recall = 0.5777777777777777, Aging Rate = 0.06186817630408411, Precision = 0.8496732026143791, f1 = 0.6878306878306878\n",
      "Epoch 9: Train Loss = 0.2322863596463271, Recall = 0.6088888888888889, Aging Rate = 0.06105944197331177, Precision = 0.9072847682119205, f1 = 0.7287234042553191\n",
      "Epoch 10: Train Loss = 0.21306435587287575, Recall = 0.68, Aging Rate = 0.07116862110796603, Precision = 0.8693181818181818, f1 = 0.7630922693266834\n",
      "Test Loss = 0.20081174597174253, Recall = 0.7644444444444445, Aging Rate = 0.08572583906186818, precision = 0.8113207547169812\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.19676206628176254, Recall = 0.7377777777777778, Aging Rate = 0.07521229276182774, Precision = 0.8924731182795699, f1 = 0.8077858880778589\n",
      "Epoch 12: Train Loss = 0.18058002553654257, Recall = 0.76, Aging Rate = 0.07602102709260009, Precision = 0.9095744680851063, f1 = 0.8280871670702179\n",
      "Epoch 13: Train Loss = 0.1645634649611416, Recall = 0.7866666666666666, Aging Rate = 0.07602102709260009, Precision = 0.9414893617021277, f1 = 0.8571428571428572\n",
      "Epoch 14: Train Loss = 0.1528591841242355, Recall = 0.8088888888888889, Aging Rate = 0.08006469874646178, Precision = 0.9191919191919192, f1 = 0.8605200945626478\n",
      "Epoch 15: Train Loss = 0.13890437555250446, Recall = 0.8177777777777778, Aging Rate = 0.07925596441568944, Precision = 0.9387755102040817, f1 = 0.8741092636579573\n",
      "Test Loss = 0.1295292497500687, Recall = 0.8133333333333334, Aging Rate = 0.07763849575414476, precision = 0.953125\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.12838171900005607, Recall = 0.8355555555555556, Aging Rate = 0.08006469874646178, Precision = 0.9494949494949495, f1 = 0.8888888888888891\n",
      "Epoch 17: Train Loss = 0.11933961436694757, Recall = 0.8622222222222222, Aging Rate = 0.08370400323493732, Precision = 0.9371980676328503, f1 = 0.8981481481481483\n",
      "Epoch 18: Train Loss = 0.11228406342522991, Recall = 0.8533333333333334, Aging Rate = 0.0812778002426203, Precision = 0.9552238805970149, f1 = 0.9014084507042254\n",
      "Epoch 19: Train Loss = 0.10082019696797719, Recall = 0.8933333333333333, Aging Rate = 0.08370400323493732, Precision = 0.9710144927536232, f1 = 0.9305555555555556\n",
      "Epoch 20: Train Loss = 0.09435634964868408, Recall = 0.9066666666666666, Aging Rate = 0.08572583906186818, Precision = 0.9622641509433962, f1 = 0.9336384439359268\n",
      "Test Loss = 0.08579432503521756, Recall = 0.9244444444444444, Aging Rate = 0.08734330772341285, precision = 0.9629629629629629\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.08562142811971689, Recall = 0.9155555555555556, Aging Rate = 0.08653457339264052, Precision = 0.9626168224299065, f1 = 0.9384965831435079\n",
      "Epoch 22: Train Loss = 0.07917167534061255, Recall = 0.92, Aging Rate = 0.08532147189648201, Precision = 0.981042654028436, f1 = 0.9495412844036697\n",
      "Epoch 23: Train Loss = 0.07262549561994837, Recall = 0.9333333333333333, Aging Rate = 0.08693894055802669, Precision = 0.9767441860465116, f1 = 0.9545454545454545\n",
      "Epoch 24: Train Loss = 0.06866047303299648, Recall = 0.9377777777777778, Aging Rate = 0.08774767488879903, Precision = 0.9723502304147466, f1 = 0.9547511312217195\n",
      "Epoch 25: Train Loss = 0.06224314799457728, Recall = 0.9511111111111111, Aging Rate = 0.08734330772341285, Precision = 0.9907407407407407, f1 = 0.9705215419501133\n",
      "Test Loss = 0.05696541086001961, Recall = 0.9511111111111111, Aging Rate = 0.08734330772341285, precision = 0.9907407407407407\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.05783719134099369, Recall = 0.9466666666666667, Aging Rate = 0.08774767488879903, Precision = 0.9815668202764977, f1 = 0.9638009049773756\n",
      "Epoch 27: Train Loss = 0.05332687470777238, Recall = 0.9511111111111111, Aging Rate = 0.08734330772341285, Precision = 0.9907407407407407, f1 = 0.9705215419501133\n",
      "Epoch 28: Train Loss = 0.04885630575006807, Recall = 0.9511111111111111, Aging Rate = 0.08693894055802669, Precision = 0.9953488372093023, f1 = 0.9727272727272728\n",
      "Epoch 29: Train Loss = 0.04745995295804568, Recall = 0.9733333333333334, Aging Rate = 0.08976951071572989, Precision = 0.9864864864864865, f1 = 0.9798657718120806\n",
      "Epoch 30: Train Loss = 0.04247638346379554, Recall = 0.9733333333333334, Aging Rate = 0.08936514355034371, Precision = 0.9909502262443439, f1 = 0.9820627802690584\n",
      "Test Loss = 0.03889011513491512, Recall = 0.9777777777777777, Aging Rate = 0.08976951071572989, precision = 0.990990990990991\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.039314341985571974, Recall = 0.9777777777777777, Aging Rate = 0.08976951071572989, Precision = 0.990990990990991, f1 = 0.9843400447427293\n",
      "Epoch 32: Train Loss = 0.03745683759083355, Recall = 0.9866666666666667, Aging Rate = 0.09057824504650222, Precision = 0.9910714285714286, f1 = 0.9888641425389756\n",
      "Epoch 33: Train Loss = 0.03598882922254205, Recall = 0.9733333333333334, Aging Rate = 0.08936514355034371, Precision = 0.9909502262443439, f1 = 0.9820627802690584\n",
      "Epoch 34: Train Loss = 0.032038853364799655, Recall = 0.9911111111111112, Aging Rate = 0.0909826122118884, Precision = 0.9911111111111112, f1 = 0.9911111111111112\n",
      "Epoch 35: Train Loss = 0.030280262715376392, Recall = 0.9911111111111112, Aging Rate = 0.0909826122118884, Precision = 0.9911111111111112, f1 = 0.9911111111111112\n",
      "Test Loss = 0.02799489908372276, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, precision = 0.9955156950672646\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.02799129006300337, Recall = 0.9911111111111112, Aging Rate = 0.0909826122118884, Precision = 0.9911111111111112, f1 = 0.9911111111111112\n",
      "Epoch 37: Train Loss = 0.02629979910367397, Recall = 0.9955555555555555, Aging Rate = 0.09138697937727457, Precision = 0.9911504424778761, f1 = 0.9933481152993349\n",
      "Epoch 38: Train Loss = 0.0251073129550436, Recall = 0.9955555555555555, Aging Rate = 0.09138697937727457, Precision = 0.9911504424778761, f1 = 0.9933481152993349\n",
      "Epoch 39: Train Loss = 0.02317170871461614, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 40: Train Loss = 0.022004960155287013, Recall = 1.0, Aging Rate = 0.09179134654266073, Precision = 0.9911894273127754, f1 = 0.9955752212389382\n",
      "Test Loss = 0.020648294908785045, Recall = 1.0, Aging Rate = 0.09179134654266073, precision = 0.9911894273127754\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.020664188990522338, Recall = 0.9955555555555555, Aging Rate = 0.09138697937727457, Precision = 0.9911504424778761, f1 = 0.9933481152993349\n",
      "Epoch 42: Train Loss = 0.01936557635497731, Recall = 1.0, Aging Rate = 0.09179134654266073, Precision = 0.9911894273127754, f1 = 0.9955752212389382\n",
      "Epoch 43: Train Loss = 0.018645765029761625, Recall = 1.0, Aging Rate = 0.09179134654266073, Precision = 0.9911894273127754, f1 = 0.9955752212389382\n",
      "Epoch 44: Train Loss = 0.017314873451989912, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 45: Train Loss = 0.01672426323164247, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.015545592051311225, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.015890095542605665, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 47: Train Loss = 0.015119311563577239, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 48: Train Loss = 0.014328099096181105, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 49: Train Loss = 0.013534090653426412, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss = 0.012857755074248146, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.012203724418977314, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.012493264356765895, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.01205171599722058, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 53: Train Loss = 0.011479035042846945, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 54: Train Loss = 0.010824232520381184, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.010539067494112424, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00990879611182601, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.010075103075029249, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.009612736600239149, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.009149948402213401, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.008935371685352854, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.008682527767853594, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.008239295265271663, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.008395962975241374, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.007892325369500337, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.007705877781527405, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0073926510960405966, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.007279118320016282, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0068666366037407655, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.006950436401931479, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.006714207454468924, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.006641530435384705, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.006340682004935048, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.006238244427236236, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005882226699794497, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.005986936538402267, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.005744280130894282, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0056665432121600166, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.005517257176591192, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.005258929599385318, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0050743941239880085, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.005140795726615301, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.005029805448980309, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.004899230289502311, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.004719502331927762, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.00470267472320959, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004464482505392664, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.004518553493352395, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.004461399415862666, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.004309673663688197, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.004240257278185683, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.004106703760411954, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003950339249228652, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.004008924867807192, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0039361551077490365, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0038476875006664486, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0037382296525661973, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.003673712114942457, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003544857671318272, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.003608055973897759, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.0035102835076258904, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.003459317067839895, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.003419141894077609, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.003324551800445921, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0032018140687377883, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0032575114645603163, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0032355925622797248, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.003273270175567449, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0031055757973427493, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0030519823582140218, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0029224401741528494, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5062368\ttotal: 2.53ms\tremaining: 377ms\n",
      "1:\tlearn: 0.3989237\ttotal: 5.34ms\tremaining: 395ms\n",
      "2:\tlearn: 0.3378015\ttotal: 7.58ms\tremaining: 372ms\n",
      "3:\tlearn: 0.2906212\ttotal: 9.9ms\tremaining: 361ms\n",
      "4:\tlearn: 0.2459184\ttotal: 12.5ms\tremaining: 362ms\n",
      "5:\tlearn: 0.2074933\ttotal: 15.5ms\tremaining: 371ms\n",
      "6:\tlearn: 0.1804718\ttotal: 17.9ms\tremaining: 366ms\n",
      "7:\tlearn: 0.1691638\ttotal: 20.2ms\tremaining: 358ms\n",
      "8:\tlearn: 0.1529885\ttotal: 22.6ms\tremaining: 354ms\n",
      "9:\tlearn: 0.1421676\ttotal: 25ms\tremaining: 349ms\n",
      "10:\tlearn: 0.1297506\ttotal: 27.4ms\tremaining: 346ms\n",
      "11:\tlearn: 0.1243065\ttotal: 29.3ms\tremaining: 337ms\n",
      "12:\tlearn: 0.1142532\ttotal: 31.8ms\tremaining: 335ms\n",
      "13:\tlearn: 0.1082013\ttotal: 34ms\tremaining: 330ms\n",
      "14:\tlearn: 0.1021789\ttotal: 36ms\tremaining: 324ms\n",
      "15:\tlearn: 0.1000295\ttotal: 37.9ms\tremaining: 317ms\n",
      "16:\tlearn: 0.0938646\ttotal: 40ms\tremaining: 313ms\n",
      "17:\tlearn: 0.0904275\ttotal: 42.1ms\tremaining: 309ms\n",
      "18:\tlearn: 0.0850961\ttotal: 44.4ms\tremaining: 306ms\n",
      "19:\tlearn: 0.0810462\ttotal: 46.6ms\tremaining: 303ms\n",
      "20:\tlearn: 0.0759190\ttotal: 49.1ms\tremaining: 302ms\n",
      "21:\tlearn: 0.0722634\ttotal: 51.5ms\tremaining: 299ms\n",
      "22:\tlearn: 0.0671641\ttotal: 53.9ms\tremaining: 297ms\n",
      "23:\tlearn: 0.0623330\ttotal: 56.3ms\tremaining: 296ms\n",
      "24:\tlearn: 0.0578657\ttotal: 58.7ms\tremaining: 294ms\n",
      "25:\tlearn: 0.0552570\ttotal: 60.9ms\tremaining: 291ms\n",
      "26:\tlearn: 0.0509667\ttotal: 63.4ms\tremaining: 289ms\n",
      "27:\tlearn: 0.0474403\ttotal: 66ms\tremaining: 287ms\n",
      "28:\tlearn: 0.0442337\ttotal: 68.4ms\tremaining: 285ms\n",
      "29:\tlearn: 0.0418186\ttotal: 70.6ms\tremaining: 283ms\n",
      "30:\tlearn: 0.0393305\ttotal: 73.2ms\tremaining: 281ms\n",
      "31:\tlearn: 0.0372868\ttotal: 75.5ms\tremaining: 278ms\n",
      "32:\tlearn: 0.0348133\ttotal: 78ms\tremaining: 277ms\n",
      "33:\tlearn: 0.0334964\ttotal: 80.2ms\tremaining: 274ms\n",
      "34:\tlearn: 0.0324458\ttotal: 82.6ms\tremaining: 271ms\n",
      "35:\tlearn: 0.0301796\ttotal: 84.8ms\tremaining: 268ms\n",
      "36:\tlearn: 0.0282624\ttotal: 86.9ms\tremaining: 265ms\n",
      "37:\tlearn: 0.0265869\ttotal: 88.9ms\tremaining: 262ms\n",
      "38:\tlearn: 0.0256206\ttotal: 90.7ms\tremaining: 258ms\n",
      "39:\tlearn: 0.0239767\ttotal: 92.8ms\tremaining: 255ms\n",
      "40:\tlearn: 0.0228544\ttotal: 94.8ms\tremaining: 252ms\n",
      "41:\tlearn: 0.0219975\ttotal: 96.7ms\tremaining: 249ms\n",
      "42:\tlearn: 0.0204311\ttotal: 98.8ms\tremaining: 246ms\n",
      "43:\tlearn: 0.0189183\ttotal: 101ms\tremaining: 243ms\n",
      "44:\tlearn: 0.0179886\ttotal: 103ms\tremaining: 240ms\n",
      "45:\tlearn: 0.0167338\ttotal: 105ms\tremaining: 237ms\n",
      "46:\tlearn: 0.0163393\ttotal: 107ms\tremaining: 234ms\n",
      "47:\tlearn: 0.0160637\ttotal: 108ms\tremaining: 230ms\n",
      "48:\tlearn: 0.0153366\ttotal: 110ms\tremaining: 227ms\n",
      "49:\tlearn: 0.0145836\ttotal: 112ms\tremaining: 224ms\n",
      "50:\tlearn: 0.0142366\ttotal: 114ms\tremaining: 221ms\n",
      "51:\tlearn: 0.0137409\ttotal: 116ms\tremaining: 218ms\n",
      "52:\tlearn: 0.0127781\ttotal: 118ms\tremaining: 215ms\n",
      "53:\tlearn: 0.0125088\ttotal: 119ms\tremaining: 212ms\n",
      "54:\tlearn: 0.0119384\ttotal: 121ms\tremaining: 210ms\n",
      "55:\tlearn: 0.0115292\ttotal: 123ms\tremaining: 207ms\n",
      "56:\tlearn: 0.0111143\ttotal: 125ms\tremaining: 204ms\n",
      "57:\tlearn: 0.0106762\ttotal: 127ms\tremaining: 202ms\n",
      "58:\tlearn: 0.0101187\ttotal: 129ms\tremaining: 199ms\n",
      "59:\tlearn: 0.0098905\ttotal: 131ms\tremaining: 196ms\n",
      "60:\tlearn: 0.0096162\ttotal: 133ms\tremaining: 194ms\n",
      "61:\tlearn: 0.0092828\ttotal: 135ms\tremaining: 191ms\n",
      "62:\tlearn: 0.0089451\ttotal: 136ms\tremaining: 188ms\n",
      "63:\tlearn: 0.0087133\ttotal: 138ms\tremaining: 186ms\n",
      "64:\tlearn: 0.0084296\ttotal: 140ms\tremaining: 183ms\n",
      "65:\tlearn: 0.0082301\ttotal: 142ms\tremaining: 180ms\n",
      "66:\tlearn: 0.0077686\ttotal: 144ms\tremaining: 178ms\n",
      "67:\tlearn: 0.0073712\ttotal: 146ms\tremaining: 176ms\n",
      "68:\tlearn: 0.0072557\ttotal: 147ms\tremaining: 173ms\n",
      "69:\tlearn: 0.0070641\ttotal: 149ms\tremaining: 171ms\n",
      "70:\tlearn: 0.0068445\ttotal: 151ms\tremaining: 168ms\n",
      "71:\tlearn: 0.0066595\ttotal: 153ms\tremaining: 166ms\n",
      "72:\tlearn: 0.0065530\ttotal: 155ms\tremaining: 163ms\n",
      "73:\tlearn: 0.0063044\ttotal: 156ms\tremaining: 161ms\n",
      "74:\tlearn: 0.0060845\ttotal: 158ms\tremaining: 158ms\n",
      "75:\tlearn: 0.0058539\ttotal: 160ms\tremaining: 156ms\n",
      "76:\tlearn: 0.0056685\ttotal: 162ms\tremaining: 154ms\n",
      "77:\tlearn: 0.0054547\ttotal: 164ms\tremaining: 152ms\n",
      "78:\tlearn: 0.0052671\ttotal: 166ms\tremaining: 149ms\n",
      "79:\tlearn: 0.0051357\ttotal: 168ms\tremaining: 147ms\n",
      "80:\tlearn: 0.0049995\ttotal: 170ms\tremaining: 145ms\n",
      "81:\tlearn: 0.0048659\ttotal: 172ms\tremaining: 142ms\n",
      "82:\tlearn: 0.0046360\ttotal: 174ms\tremaining: 140ms\n",
      "83:\tlearn: 0.0044229\ttotal: 176ms\tremaining: 138ms\n",
      "84:\tlearn: 0.0043117\ttotal: 178ms\tremaining: 136ms\n",
      "85:\tlearn: 0.0041250\ttotal: 180ms\tremaining: 134ms\n",
      "86:\tlearn: 0.0040714\ttotal: 182ms\tremaining: 132ms\n",
      "87:\tlearn: 0.0039953\ttotal: 184ms\tremaining: 129ms\n",
      "88:\tlearn: 0.0039409\ttotal: 185ms\tremaining: 127ms\n",
      "89:\tlearn: 0.0038338\ttotal: 187ms\tremaining: 125ms\n",
      "90:\tlearn: 0.0037120\ttotal: 189ms\tremaining: 123ms\n",
      "91:\tlearn: 0.0035721\ttotal: 192ms\tremaining: 121ms\n",
      "92:\tlearn: 0.0034343\ttotal: 193ms\tremaining: 119ms\n",
      "93:\tlearn: 0.0033672\ttotal: 195ms\tremaining: 116ms\n",
      "94:\tlearn: 0.0033122\ttotal: 197ms\tremaining: 114ms\n",
      "95:\tlearn: 0.0031976\ttotal: 199ms\tremaining: 112ms\n",
      "96:\tlearn: 0.0031079\ttotal: 201ms\tremaining: 110ms\n",
      "97:\tlearn: 0.0030410\ttotal: 202ms\tremaining: 107ms\n",
      "98:\tlearn: 0.0029558\ttotal: 205ms\tremaining: 105ms\n",
      "99:\tlearn: 0.0027832\ttotal: 206ms\tremaining: 103ms\n",
      "100:\tlearn: 0.0026957\ttotal: 208ms\tremaining: 101ms\n",
      "101:\tlearn: 0.0026559\ttotal: 210ms\tremaining: 98.9ms\n",
      "102:\tlearn: 0.0025859\ttotal: 212ms\tremaining: 96.7ms\n",
      "103:\tlearn: 0.0025448\ttotal: 214ms\tremaining: 94.5ms\n",
      "104:\tlearn: 0.0024579\ttotal: 216ms\tremaining: 92.5ms\n",
      "105:\tlearn: 0.0023851\ttotal: 218ms\tremaining: 90.4ms\n",
      "106:\tlearn: 0.0023699\ttotal: 219ms\tremaining: 88.2ms\n",
      "107:\tlearn: 0.0023328\ttotal: 221ms\tremaining: 86ms\n",
      "108:\tlearn: 0.0022864\ttotal: 223ms\tremaining: 83.9ms\n",
      "109:\tlearn: 0.0022391\ttotal: 225ms\tremaining: 81.8ms\n",
      "110:\tlearn: 0.0021624\ttotal: 227ms\tremaining: 79.8ms\n",
      "111:\tlearn: 0.0021367\ttotal: 229ms\tremaining: 77.6ms\n",
      "112:\tlearn: 0.0020798\ttotal: 231ms\tremaining: 75.5ms\n",
      "113:\tlearn: 0.0020496\ttotal: 233ms\tremaining: 73.4ms\n",
      "114:\tlearn: 0.0020263\ttotal: 234ms\tremaining: 71.3ms\n",
      "115:\tlearn: 0.0019777\ttotal: 236ms\tremaining: 69.3ms\n",
      "116:\tlearn: 0.0019492\ttotal: 238ms\tremaining: 67.2ms\n",
      "117:\tlearn: 0.0018880\ttotal: 240ms\tremaining: 65.1ms\n",
      "118:\tlearn: 0.0018503\ttotal: 242ms\tremaining: 63.1ms\n",
      "119:\tlearn: 0.0017791\ttotal: 244ms\tremaining: 61.1ms\n",
      "120:\tlearn: 0.0017465\ttotal: 246ms\tremaining: 59ms\n",
      "121:\tlearn: 0.0017108\ttotal: 248ms\tremaining: 57ms\n",
      "122:\tlearn: 0.0016492\ttotal: 250ms\tremaining: 54.9ms\n",
      "123:\tlearn: 0.0016046\ttotal: 252ms\tremaining: 52.9ms\n",
      "124:\tlearn: 0.0015528\ttotal: 254ms\tremaining: 50.8ms\n",
      "125:\tlearn: 0.0015293\ttotal: 256ms\tremaining: 48.8ms\n",
      "126:\tlearn: 0.0015158\ttotal: 258ms\tremaining: 46.7ms\n",
      "127:\tlearn: 0.0015004\ttotal: 260ms\tremaining: 44.6ms\n",
      "128:\tlearn: 0.0014653\ttotal: 262ms\tremaining: 42.6ms\n",
      "129:\tlearn: 0.0014495\ttotal: 263ms\tremaining: 40.5ms\n",
      "130:\tlearn: 0.0014304\ttotal: 265ms\tremaining: 38.5ms\n",
      "131:\tlearn: 0.0014202\ttotal: 267ms\tremaining: 36.4ms\n",
      "132:\tlearn: 0.0013889\ttotal: 269ms\tremaining: 34.4ms\n",
      "133:\tlearn: 0.0013665\ttotal: 271ms\tremaining: 32.3ms\n",
      "134:\tlearn: 0.0013463\ttotal: 273ms\tremaining: 30.3ms\n",
      "135:\tlearn: 0.0013105\ttotal: 275ms\tremaining: 28.3ms\n",
      "136:\tlearn: 0.0012986\ttotal: 276ms\tremaining: 26.2ms\n",
      "137:\tlearn: 0.0012784\ttotal: 278ms\tremaining: 24.2ms\n",
      "138:\tlearn: 0.0012619\ttotal: 280ms\tremaining: 22.2ms\n",
      "139:\tlearn: 0.0012270\ttotal: 282ms\tremaining: 20.2ms\n",
      "140:\tlearn: 0.0012034\ttotal: 284ms\tremaining: 18.1ms\n",
      "141:\tlearn: 0.0011953\ttotal: 286ms\tremaining: 16.1ms\n",
      "142:\tlearn: 0.0011801\ttotal: 287ms\tremaining: 14.1ms\n",
      "143:\tlearn: 0.0011650\ttotal: 289ms\tremaining: 12ms\n",
      "144:\tlearn: 0.0011472\ttotal: 291ms\tremaining: 10ms\n",
      "145:\tlearn: 0.0011309\ttotal: 293ms\tremaining: 8.02ms\n",
      "146:\tlearn: 0.0011064\ttotal: 295ms\tremaining: 6.01ms\n",
      "147:\tlearn: 0.0011064\ttotal: 296ms\tremaining: 4ms\n",
      "148:\tlearn: 0.0010712\ttotal: 298ms\tremaining: 2ms\n",
      "149:\tlearn: 0.0010402\ttotal: 300ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329edd37ce6146ccb5d9bead57f78226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5583335753519719, Recall = 0.04, Aging Rate = 0.04205418520016175, Precision = 0.08653846153846154, f1 = 0.0547112462006079\n",
      "Epoch 2: Train Loss = 0.45445381847895255, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.4189969574903796, Recall = 0.03111111111111111, Aging Rate = 0.0028305701577031944, Precision = 0, f1 = 0.0\n",
      "Epoch 4: Train Loss = 0.37212188462954615, Recall = 0.12444444444444444, Aging Rate = 0.011726647796198949, Precision = 0.9655172413793104, f1 = 0.2204724409448819\n",
      "Epoch 5: Train Loss = 0.34225375194131147, Recall = 0.29777777777777775, Aging Rate = 0.03194500606550748, Precision = 0.8481012658227848, f1 = 0.44078947368421045\n",
      "Test Loss = 0.31216588868841294, Recall = 0.3688888888888889, Aging Rate = 0.03760614638091387, precision = 0.8924731182795699\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.3002014567723797, Recall = 0.41333333333333333, Aging Rate = 0.043671653861706426, Precision = 0.8611111111111112, f1 = 0.5585585585585585\n",
      "Epoch 7: Train Loss = 0.27280535979450254, Recall = 0.5111111111111111, Aging Rate = 0.05620703598867772, Precision = 0.8273381294964028, f1 = 0.6318681318681318\n",
      "Epoch 8: Train Loss = 0.24847954170596875, Recall = 0.5466666666666666, Aging Rate = 0.061463809138697936, Precision = 0.8092105263157895, f1 = 0.6525198938992042\n",
      "Epoch 9: Train Loss = 0.2269569171243865, Recall = 0.5822222222222222, Aging Rate = 0.06227254346947028, Precision = 0.8506493506493507, f1 = 0.6912928759894459\n",
      "Epoch 10: Train Loss = 0.20934955514229314, Recall = 0.6266666666666667, Aging Rate = 0.06995551961180752, Precision = 0.815028901734104, f1 = 0.7085427135678393\n",
      "Test Loss = 0.19520073528358245, Recall = 0.6844444444444444, Aging Rate = 0.07642539425798625, precision = 0.8148148148148148\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.19140823887419325, Recall = 0.6622222222222223, Aging Rate = 0.06955115244642135, Precision = 0.8662790697674418, f1 = 0.7506297229219143\n",
      "Epoch 12: Train Loss = 0.17716455219175667, Recall = 0.7022222222222222, Aging Rate = 0.07480792559644157, Precision = 0.8540540540540541, f1 = 0.7707317073170732\n",
      "Epoch 13: Train Loss = 0.16154769598967722, Recall = 0.7066666666666667, Aging Rate = 0.07116862110796603, Precision = 0.9034090909090909, f1 = 0.7930174563591023\n",
      "Epoch 14: Train Loss = 0.14902099104145497, Recall = 0.7555555555555555, Aging Rate = 0.07682976142337242, Precision = 0.8947368421052632, f1 = 0.8192771084337349\n",
      "Epoch 15: Train Loss = 0.1363567825854186, Recall = 0.8044444444444444, Aging Rate = 0.07925596441568944, Precision = 0.923469387755102, f1 = 0.8598574821852731\n",
      "Test Loss = 0.12900678721266723, Recall = 0.8577777777777778, Aging Rate = 0.08653457339264052, precision = 0.9018691588785047\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.12689844028420935, Recall = 0.8355555555555556, Aging Rate = 0.08168216740800648, Precision = 0.9306930693069307, f1 = 0.8805620608899298\n",
      "Epoch 17: Train Loss = 0.11828762444705554, Recall = 0.84, Aging Rate = 0.08006469874646178, Precision = 0.9545454545454546, f1 = 0.8936170212765958\n",
      "Epoch 18: Train Loss = 0.10947319921866824, Recall = 0.8755555555555555, Aging Rate = 0.08693894055802669, Precision = 0.9162790697674419, f1 = 0.8954545454545455\n",
      "Epoch 19: Train Loss = 0.0990598312290246, Recall = 0.8977777777777778, Aging Rate = 0.08491710473109583, Precision = 0.9619047619047619, f1 = 0.928735632183908\n",
      "Epoch 20: Train Loss = 0.09401733606294539, Recall = 0.92, Aging Rate = 0.08896077638495754, Precision = 0.9409090909090909, f1 = 0.9303370786516855\n",
      "Test Loss = 0.08574438262602795, Recall = 0.92, Aging Rate = 0.08613020622725434, precision = 0.971830985915493\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.08561464058656322, Recall = 0.9111111111111111, Aging Rate = 0.08572583906186818, Precision = 0.9669811320754716, f1 = 0.9382151029748284\n",
      "Epoch 22: Train Loss = 0.07829038677394415, Recall = 0.9422222222222222, Aging Rate = 0.08855640921957138, Precision = 0.9680365296803652, f1 = 0.9549549549549549\n",
      "Epoch 23: Train Loss = 0.07474480297761411, Recall = 0.9377777777777778, Aging Rate = 0.08774767488879903, Precision = 0.9723502304147466, f1 = 0.9547511312217195\n",
      "Epoch 24: Train Loss = 0.06845541323395427, Recall = 0.96, Aging Rate = 0.09017387788111605, Precision = 0.968609865470852, f1 = 0.9642857142857142\n",
      "Epoch 25: Train Loss = 0.0645897285075044, Recall = 0.96, Aging Rate = 0.09017387788111605, Precision = 0.968609865470852, f1 = 0.9642857142857142\n",
      "Test Loss = 0.059291275036609754, Recall = 0.9688888888888889, Aging Rate = 0.08976951071572989, precision = 0.9819819819819819\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.05991786540916368, Recall = 0.9688888888888889, Aging Rate = 0.09017387788111605, Precision = 0.9775784753363229, f1 = 0.9732142857142856\n",
      "Epoch 27: Train Loss = 0.05476997289060944, Recall = 0.9644444444444444, Aging Rate = 0.08976951071572989, Precision = 0.9774774774774775, f1 = 0.970917225950783\n",
      "Epoch 28: Train Loss = 0.052346811571459366, Recall = 0.9733333333333334, Aging Rate = 0.09017387788111605, Precision = 0.9820627802690582, f1 = 0.9776785714285715\n",
      "Epoch 29: Train Loss = 0.04888687216519731, Recall = 0.9688888888888889, Aging Rate = 0.08976951071572989, Precision = 0.9819819819819819, f1 = 0.9753914988814317\n",
      "Epoch 30: Train Loss = 0.04613716600728671, Recall = 0.9777777777777777, Aging Rate = 0.09057824504650222, Precision = 0.9821428571428571, f1 = 0.9799554565701558\n",
      "Test Loss = 0.04231334627397717, Recall = 0.9866666666666667, Aging Rate = 0.09138697937727457, precision = 0.9823008849557522\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.04285578766376009, Recall = 0.9822222222222222, Aging Rate = 0.0909826122118884, Precision = 0.9822222222222222, f1 = 0.9822222222222222\n",
      "Epoch 32: Train Loss = 0.040949369976172746, Recall = 0.9822222222222222, Aging Rate = 0.0909826122118884, Precision = 0.9822222222222222, f1 = 0.9822222222222222\n",
      "Epoch 33: Train Loss = 0.037532329199300135, Recall = 0.9911111111111112, Aging Rate = 0.09179134654266073, Precision = 0.9823788546255506, f1 = 0.9867256637168141\n",
      "Epoch 34: Train Loss = 0.035253179516060315, Recall = 0.9911111111111112, Aging Rate = 0.09179134654266073, Precision = 0.9823788546255506, f1 = 0.9867256637168141\n",
      "Epoch 35: Train Loss = 0.03318779864203482, Recall = 0.9866666666666667, Aging Rate = 0.09138697937727457, Precision = 0.9823008849557522, f1 = 0.9844789356984479\n",
      "Test Loss = 0.032899962228998166, Recall = 0.9955555555555555, Aging Rate = 0.09219571370804691, precision = 0.9824561403508771\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.03187653141565473, Recall = 0.9911111111111112, Aging Rate = 0.09179134654266073, Precision = 0.9823788546255506, f1 = 0.9867256637168141\n",
      "Epoch 37: Train Loss = 0.02988293786633728, Recall = 0.9911111111111112, Aging Rate = 0.09179134654266073, Precision = 0.9823788546255506, f1 = 0.9867256637168141\n",
      "Epoch 38: Train Loss = 0.028336591684939984, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, Precision = 0.9867256637168141, f1 = 0.9889135254988914\n",
      "Epoch 39: Train Loss = 0.026674857094215083, Recall = 0.9911111111111112, Aging Rate = 0.09179134654266073, Precision = 0.9823788546255506, f1 = 0.9867256637168141\n",
      "Epoch 40: Train Loss = 0.025129202331517612, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, Precision = 0.9867256637168141, f1 = 0.9889135254988914\n",
      "Test Loss = 0.023310838243858177, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, precision = 0.9867256637168141\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.023908586085017423, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, Precision = 0.9867256637168141, f1 = 0.9889135254988914\n",
      "Epoch 42: Train Loss = 0.022689352341875996, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, Precision = 0.9867256637168141, f1 = 0.9889135254988914\n",
      "Epoch 43: Train Loss = 0.021433613130508695, Recall = 0.9955555555555555, Aging Rate = 0.09179134654266073, Precision = 0.986784140969163, f1 = 0.9911504424778761\n",
      "Epoch 44: Train Loss = 0.020214772162212565, Recall = 0.9955555555555555, Aging Rate = 0.09179134654266073, Precision = 0.986784140969163, f1 = 0.9911504424778761\n",
      "Epoch 45: Train Loss = 0.0197670150676639, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, Precision = 0.9867256637168141, f1 = 0.9889135254988914\n",
      "Test Loss = 0.018162009052802713, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, precision = 0.9955555555555555\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.018556130518989914, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 47: Train Loss = 0.017900648155900038, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.01768902263514098, Recall = 1.0, Aging Rate = 0.09179134654266073, Precision = 0.9911894273127754, f1 = 0.9955752212389382\n",
      "Epoch 49: Train Loss = 0.01629017762014295, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.015234654828803625, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.014321485468029397, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.014545223904262175, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.013949028545824131, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 53: Train Loss = 0.013170741773008986, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 54: Train Loss = 0.012554946931337374, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.01229152895715065, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.011490972354492942, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.011688601332567888, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.011329918235142704, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.010886867271922302, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.010671836028031453, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.00995094157051893, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.009514229927623243, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.009572523654530068, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.009274367679526605, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.008956868477915879, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.008823136629757432, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.008420637098479575, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.007883137977748712, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.008090075388252495, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.007931645642145052, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.007646035076504264, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.007346590348770575, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.007143497297949291, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0067510133721401374, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.006850737727556364, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.006663080257974833, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.006519327624454293, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.006286875913096641, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.006140550409074235, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005821635927989916, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.005904050141511191, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.005779220264674655, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.005572983014046712, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.00552935317893815, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.005376632167180805, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00508754125190315, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.005199168835496594, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.005175068203225774, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.004926634055750239, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.004816761807328805, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.004624657372439794, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00451104695506897, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.004584836439533461, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.004473700555846595, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0043590481495096815, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.004294922360506405, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.004201910833877485, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004019222730485247, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.0041010562205077835, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.00399515271172518, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.003915149712167963, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.003853920670553855, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.003808318183795401, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0037282800902203902, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.003705406158347567, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.003633374394039261, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.003572437555589056, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.00353741926687901, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.003463814791545778, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033351212135130847, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5107562\ttotal: 2.23ms\tremaining: 332ms\n",
      "1:\tlearn: 0.3993608\ttotal: 4.95ms\tremaining: 366ms\n",
      "2:\tlearn: 0.3339923\ttotal: 7.1ms\tremaining: 348ms\n",
      "3:\tlearn: 0.2836052\ttotal: 9.34ms\tremaining: 341ms\n",
      "4:\tlearn: 0.2388804\ttotal: 12ms\tremaining: 348ms\n",
      "5:\tlearn: 0.2130064\ttotal: 14.2ms\tremaining: 341ms\n",
      "6:\tlearn: 0.1934843\ttotal: 16.3ms\tremaining: 333ms\n",
      "7:\tlearn: 0.1676082\ttotal: 18.8ms\tremaining: 334ms\n",
      "8:\tlearn: 0.1521021\ttotal: 21.3ms\tremaining: 333ms\n",
      "9:\tlearn: 0.1323115\ttotal: 24.1ms\tremaining: 337ms\n",
      "10:\tlearn: 0.1251913\ttotal: 26.2ms\tremaining: 331ms\n",
      "11:\tlearn: 0.1191047\ttotal: 28.3ms\tremaining: 325ms\n",
      "12:\tlearn: 0.1092533\ttotal: 30.5ms\tremaining: 321ms\n",
      "13:\tlearn: 0.1030886\ttotal: 32.7ms\tremaining: 318ms\n",
      "14:\tlearn: 0.0975671\ttotal: 34.9ms\tremaining: 314ms\n",
      "15:\tlearn: 0.0930259\ttotal: 37ms\tremaining: 310ms\n",
      "16:\tlearn: 0.0872630\ttotal: 39.3ms\tremaining: 307ms\n",
      "17:\tlearn: 0.0823478\ttotal: 41.4ms\tremaining: 304ms\n",
      "18:\tlearn: 0.0735868\ttotal: 44ms\tremaining: 303ms\n",
      "19:\tlearn: 0.0667918\ttotal: 46.4ms\tremaining: 301ms\n",
      "20:\tlearn: 0.0646233\ttotal: 48.4ms\tremaining: 297ms\n",
      "21:\tlearn: 0.0608195\ttotal: 50.7ms\tremaining: 295ms\n",
      "22:\tlearn: 0.0570419\ttotal: 53.1ms\tremaining: 293ms\n",
      "23:\tlearn: 0.0546770\ttotal: 55.1ms\tremaining: 289ms\n",
      "24:\tlearn: 0.0516069\ttotal: 57.1ms\tremaining: 286ms\n",
      "25:\tlearn: 0.0490735\ttotal: 59.2ms\tremaining: 283ms\n",
      "26:\tlearn: 0.0456256\ttotal: 61.7ms\tremaining: 281ms\n",
      "27:\tlearn: 0.0425362\ttotal: 64ms\tremaining: 279ms\n",
      "28:\tlearn: 0.0396836\ttotal: 66.3ms\tremaining: 277ms\n",
      "29:\tlearn: 0.0378096\ttotal: 68.5ms\tremaining: 274ms\n",
      "30:\tlearn: 0.0366052\ttotal: 70.5ms\tremaining: 271ms\n",
      "31:\tlearn: 0.0354116\ttotal: 72.7ms\tremaining: 268ms\n",
      "32:\tlearn: 0.0336914\ttotal: 74.7ms\tremaining: 265ms\n",
      "33:\tlearn: 0.0327636\ttotal: 76.6ms\tremaining: 261ms\n",
      "34:\tlearn: 0.0307872\ttotal: 78.7ms\tremaining: 259ms\n",
      "35:\tlearn: 0.0295118\ttotal: 80.5ms\tremaining: 255ms\n",
      "36:\tlearn: 0.0279774\ttotal: 82.4ms\tremaining: 252ms\n",
      "37:\tlearn: 0.0268478\ttotal: 84.3ms\tremaining: 249ms\n",
      "38:\tlearn: 0.0252462\ttotal: 86.3ms\tremaining: 246ms\n",
      "39:\tlearn: 0.0241559\ttotal: 88.2ms\tremaining: 242ms\n",
      "40:\tlearn: 0.0230199\ttotal: 90.1ms\tremaining: 240ms\n",
      "41:\tlearn: 0.0215762\ttotal: 92.2ms\tremaining: 237ms\n",
      "42:\tlearn: 0.0207136\ttotal: 94ms\tremaining: 234ms\n",
      "43:\tlearn: 0.0197915\ttotal: 95.9ms\tremaining: 231ms\n",
      "44:\tlearn: 0.0189774\ttotal: 97.8ms\tremaining: 228ms\n",
      "45:\tlearn: 0.0179856\ttotal: 99.9ms\tremaining: 226ms\n",
      "46:\tlearn: 0.0168331\ttotal: 102ms\tremaining: 224ms\n",
      "47:\tlearn: 0.0163332\ttotal: 104ms\tremaining: 221ms\n",
      "48:\tlearn: 0.0153064\ttotal: 106ms\tremaining: 218ms\n",
      "49:\tlearn: 0.0146974\ttotal: 108ms\tremaining: 216ms\n",
      "50:\tlearn: 0.0140838\ttotal: 110ms\tremaining: 213ms\n",
      "51:\tlearn: 0.0134797\ttotal: 112ms\tremaining: 211ms\n",
      "52:\tlearn: 0.0131308\ttotal: 114ms\tremaining: 208ms\n",
      "53:\tlearn: 0.0124349\ttotal: 116ms\tremaining: 205ms\n",
      "54:\tlearn: 0.0120777\ttotal: 117ms\tremaining: 203ms\n",
      "55:\tlearn: 0.0116326\ttotal: 119ms\tremaining: 200ms\n",
      "56:\tlearn: 0.0110832\ttotal: 121ms\tremaining: 198ms\n",
      "57:\tlearn: 0.0108580\ttotal: 123ms\tremaining: 195ms\n",
      "58:\tlearn: 0.0103050\ttotal: 125ms\tremaining: 193ms\n",
      "59:\tlearn: 0.0100576\ttotal: 127ms\tremaining: 190ms\n",
      "60:\tlearn: 0.0096114\ttotal: 129ms\tremaining: 188ms\n",
      "61:\tlearn: 0.0094349\ttotal: 130ms\tremaining: 185ms\n",
      "62:\tlearn: 0.0090428\ttotal: 132ms\tremaining: 183ms\n",
      "63:\tlearn: 0.0084847\ttotal: 134ms\tremaining: 180ms\n",
      "64:\tlearn: 0.0082634\ttotal: 136ms\tremaining: 178ms\n",
      "65:\tlearn: 0.0077660\ttotal: 138ms\tremaining: 176ms\n",
      "66:\tlearn: 0.0076935\ttotal: 140ms\tremaining: 173ms\n",
      "67:\tlearn: 0.0072845\ttotal: 141ms\tremaining: 171ms\n",
      "68:\tlearn: 0.0071184\ttotal: 143ms\tremaining: 168ms\n",
      "69:\tlearn: 0.0067541\ttotal: 145ms\tremaining: 166ms\n",
      "70:\tlearn: 0.0064022\ttotal: 147ms\tremaining: 164ms\n",
      "71:\tlearn: 0.0063363\ttotal: 149ms\tremaining: 161ms\n",
      "72:\tlearn: 0.0061913\ttotal: 151ms\tremaining: 159ms\n",
      "73:\tlearn: 0.0060157\ttotal: 152ms\tremaining: 157ms\n",
      "74:\tlearn: 0.0058469\ttotal: 154ms\tremaining: 154ms\n",
      "75:\tlearn: 0.0057484\ttotal: 156ms\tremaining: 152ms\n",
      "76:\tlearn: 0.0055775\ttotal: 158ms\tremaining: 150ms\n",
      "77:\tlearn: 0.0053588\ttotal: 160ms\tremaining: 147ms\n",
      "78:\tlearn: 0.0051583\ttotal: 162ms\tremaining: 145ms\n",
      "79:\tlearn: 0.0050308\ttotal: 163ms\tremaining: 143ms\n",
      "80:\tlearn: 0.0049226\ttotal: 165ms\tremaining: 141ms\n",
      "81:\tlearn: 0.0048474\ttotal: 167ms\tremaining: 138ms\n",
      "82:\tlearn: 0.0047543\ttotal: 169ms\tremaining: 136ms\n",
      "83:\tlearn: 0.0046031\ttotal: 171ms\tremaining: 134ms\n",
      "84:\tlearn: 0.0044513\ttotal: 173ms\tremaining: 132ms\n",
      "85:\tlearn: 0.0044104\ttotal: 174ms\tremaining: 130ms\n",
      "86:\tlearn: 0.0041354\ttotal: 176ms\tremaining: 128ms\n",
      "87:\tlearn: 0.0040897\ttotal: 178ms\tremaining: 125ms\n",
      "88:\tlearn: 0.0039264\ttotal: 180ms\tremaining: 123ms\n",
      "89:\tlearn: 0.0038042\ttotal: 182ms\tremaining: 121ms\n",
      "90:\tlearn: 0.0036962\ttotal: 184ms\tremaining: 119ms\n",
      "91:\tlearn: 0.0036255\ttotal: 186ms\tremaining: 117ms\n",
      "92:\tlearn: 0.0034904\ttotal: 188ms\tremaining: 115ms\n",
      "93:\tlearn: 0.0034185\ttotal: 189ms\tremaining: 113ms\n",
      "94:\tlearn: 0.0033232\ttotal: 191ms\tremaining: 111ms\n",
      "95:\tlearn: 0.0032814\ttotal: 193ms\tremaining: 109ms\n",
      "96:\tlearn: 0.0031968\ttotal: 195ms\tremaining: 106ms\n",
      "97:\tlearn: 0.0031470\ttotal: 197ms\tremaining: 104ms\n",
      "98:\tlearn: 0.0030701\ttotal: 198ms\tremaining: 102ms\n",
      "99:\tlearn: 0.0030115\ttotal: 200ms\tremaining: 100ms\n",
      "100:\tlearn: 0.0029394\ttotal: 202ms\tremaining: 98.1ms\n",
      "101:\tlearn: 0.0028414\ttotal: 204ms\tremaining: 96.1ms\n",
      "102:\tlearn: 0.0027706\ttotal: 206ms\tremaining: 94ms\n",
      "103:\tlearn: 0.0026930\ttotal: 208ms\tremaining: 92ms\n",
      "104:\tlearn: 0.0026300\ttotal: 210ms\tremaining: 90ms\n",
      "105:\tlearn: 0.0025267\ttotal: 212ms\tremaining: 88ms\n",
      "106:\tlearn: 0.0024732\ttotal: 214ms\tremaining: 85.9ms\n",
      "107:\tlearn: 0.0024600\ttotal: 216ms\tremaining: 83.8ms\n",
      "108:\tlearn: 0.0023825\ttotal: 217ms\tremaining: 81.8ms\n",
      "109:\tlearn: 0.0023344\ttotal: 219ms\tremaining: 79.7ms\n",
      "110:\tlearn: 0.0023070\ttotal: 221ms\tremaining: 77.6ms\n",
      "111:\tlearn: 0.0022566\ttotal: 223ms\tremaining: 75.6ms\n",
      "112:\tlearn: 0.0021926\ttotal: 225ms\tremaining: 73.6ms\n",
      "113:\tlearn: 0.0021466\ttotal: 227ms\tremaining: 71.6ms\n",
      "114:\tlearn: 0.0020954\ttotal: 229ms\tremaining: 69.6ms\n",
      "115:\tlearn: 0.0020721\ttotal: 230ms\tremaining: 67.6ms\n",
      "116:\tlearn: 0.0020304\ttotal: 232ms\tremaining: 65.5ms\n",
      "117:\tlearn: 0.0019970\ttotal: 234ms\tremaining: 63.5ms\n",
      "118:\tlearn: 0.0019544\ttotal: 236ms\tremaining: 61.5ms\n",
      "119:\tlearn: 0.0019150\ttotal: 238ms\tremaining: 59.5ms\n",
      "120:\tlearn: 0.0018781\ttotal: 240ms\tremaining: 57.5ms\n",
      "121:\tlearn: 0.0018243\ttotal: 242ms\tremaining: 55.5ms\n",
      "122:\tlearn: 0.0018075\ttotal: 243ms\tremaining: 53.4ms\n",
      "123:\tlearn: 0.0017767\ttotal: 245ms\tremaining: 51.4ms\n",
      "124:\tlearn: 0.0017552\ttotal: 247ms\tremaining: 49.4ms\n",
      "125:\tlearn: 0.0017305\ttotal: 249ms\tremaining: 47.4ms\n",
      "126:\tlearn: 0.0016985\ttotal: 251ms\tremaining: 45.4ms\n",
      "127:\tlearn: 0.0016786\ttotal: 252ms\tremaining: 43.4ms\n",
      "128:\tlearn: 0.0016654\ttotal: 254ms\tremaining: 41.4ms\n",
      "129:\tlearn: 0.0016284\ttotal: 256ms\tremaining: 39.4ms\n",
      "130:\tlearn: 0.0015997\ttotal: 258ms\tremaining: 37.4ms\n",
      "131:\tlearn: 0.0015788\ttotal: 260ms\tremaining: 35.4ms\n",
      "132:\tlearn: 0.0015658\ttotal: 261ms\tremaining: 33.4ms\n",
      "133:\tlearn: 0.0015194\ttotal: 263ms\tremaining: 31.4ms\n",
      "134:\tlearn: 0.0014722\ttotal: 266ms\tremaining: 29.5ms\n",
      "135:\tlearn: 0.0014561\ttotal: 267ms\tremaining: 27.5ms\n",
      "136:\tlearn: 0.0014251\ttotal: 269ms\tremaining: 25.6ms\n",
      "137:\tlearn: 0.0013999\ttotal: 271ms\tremaining: 23.6ms\n",
      "138:\tlearn: 0.0013785\ttotal: 273ms\tremaining: 21.6ms\n",
      "139:\tlearn: 0.0013382\ttotal: 275ms\tremaining: 19.6ms\n",
      "140:\tlearn: 0.0013175\ttotal: 277ms\tremaining: 17.7ms\n",
      "141:\tlearn: 0.0012903\ttotal: 279ms\tremaining: 15.7ms\n",
      "142:\tlearn: 0.0012681\ttotal: 280ms\tremaining: 13.7ms\n",
      "143:\tlearn: 0.0012608\ttotal: 282ms\tremaining: 11.8ms\n",
      "144:\tlearn: 0.0012328\ttotal: 284ms\tremaining: 9.79ms\n",
      "145:\tlearn: 0.0012183\ttotal: 286ms\tremaining: 7.83ms\n",
      "146:\tlearn: 0.0012087\ttotal: 287ms\tremaining: 5.87ms\n",
      "147:\tlearn: 0.0011920\ttotal: 289ms\tremaining: 3.91ms\n",
      "148:\tlearn: 0.0011645\ttotal: 291ms\tremaining: 1.95ms\n",
      "149:\tlearn: 0.0011278\ttotal: 293ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb325cc34d94658b18b138163b07b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5562959120936881, Recall = 0.03111111111111111, Aging Rate = 0.04448038819247877, Precision = 0.06363636363636363, f1 = 0.0417910447761194\n",
      "Epoch 2: Train Loss = 0.4661827172482973, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.4294651485120918, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.39201890529286654, Recall = 0.017777777777777778, Aging Rate = 0.0016174686615446825, Precision = 0, f1 = 0.0\n",
      "Epoch 5: Train Loss = 0.35180617992423874, Recall = 0.19111111111111112, Aging Rate = 0.01819652244237768, Precision = 0.9555555555555556, f1 = 0.31851851851851853\n",
      "Test Loss = 0.32539118453408367, Recall = 0.24, Aging Rate = 0.022240194096239386, precision = 0.9818181818181818\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.31365660258357553, Recall = 0.3288888888888889, Aging Rate = 0.03315810756166599, Precision = 0.9024390243902439, f1 = 0.482084690553746\n",
      "Epoch 7: Train Loss = 0.28835911734283476, Recall = 0.4533333333333333, Aging Rate = 0.05014152850788516, Precision = 0.8225806451612904, f1 = 0.5845272206303725\n",
      "Epoch 8: Train Loss = 0.26294558159359255, Recall = 0.52, Aging Rate = 0.05499393449251921, Precision = 0.8602941176470589, f1 = 0.6481994459833795\n",
      "Epoch 9: Train Loss = 0.2446339123773517, Recall = 0.5777777777777777, Aging Rate = 0.06348564496562879, Precision = 0.8280254777070064, f1 = 0.6806282722513088\n",
      "Epoch 10: Train Loss = 0.22671759838379205, Recall = 0.6222222222222222, Aging Rate = 0.0703598867771937, Precision = 0.8045977011494253, f1 = 0.7017543859649121\n",
      "Test Loss = 0.21526693669929226, Recall = 0.5866666666666667, Aging Rate = 0.055802668823291546, precision = 0.9565217391304348\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.2114702152955209, Recall = 0.6266666666666667, Aging Rate = 0.06389001213101496, Precision = 0.8924050632911392, f1 = 0.7362924281984333\n",
      "Epoch 12: Train Loss = 0.19721203480854335, Recall = 0.6844444444444444, Aging Rate = 0.07238172260412455, Precision = 0.8603351955307262, f1 = 0.7623762376237623\n",
      "Epoch 13: Train Loss = 0.18614669599081887, Recall = 0.6888888888888889, Aging Rate = 0.07197735543873837, Precision = 0.8707865168539326, f1 = 0.7692307692307693\n",
      "Epoch 14: Train Loss = 0.17290922040718557, Recall = 0.7688888888888888, Aging Rate = 0.08046906591184796, Precision = 0.8693467336683417, f1 = 0.8160377358490566\n",
      "Epoch 15: Train Loss = 0.1652868506677518, Recall = 0.7644444444444445, Aging Rate = 0.08006469874646178, Precision = 0.8686868686868687, f1 = 0.8132387706855791\n",
      "Test Loss = 0.15577636906795062, Recall = 0.7422222222222222, Aging Rate = 0.07197735543873837, precision = 0.9382022471910112\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.15507068387709116, Recall = 0.7688888888888888, Aging Rate = 0.07885159725030327, Precision = 0.8871794871794871, f1 = 0.8238095238095239\n",
      "Epoch 17: Train Loss = 0.14486732364930077, Recall = 0.7822222222222223, Aging Rate = 0.07763849575414476, Precision = 0.9166666666666666, f1 = 0.8441247002398081\n",
      "Epoch 18: Train Loss = 0.1377371630589585, Recall = 0.8044444444444444, Aging Rate = 0.08249090173877881, Precision = 0.8872549019607843, f1 = 0.8438228438228437\n",
      "Epoch 19: Train Loss = 0.13075147033810278, Recall = 0.8488888888888889, Aging Rate = 0.08532147189648201, Precision = 0.9052132701421801, f1 = 0.8761467889908257\n",
      "Epoch 20: Train Loss = 0.12340287565943976, Recall = 0.8533333333333334, Aging Rate = 0.08532147189648201, Precision = 0.909952606635071, f1 = 0.8807339449541284\n",
      "Test Loss = 0.11674622320364807, Recall = 0.9066666666666666, Aging Rate = 0.09138697937727457, precision = 0.9026548672566371\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.11642790282972901, Recall = 0.8577777777777778, Aging Rate = 0.08208653457339264, Precision = 0.9507389162561576, f1 = 0.9018691588785047\n",
      "Epoch 22: Train Loss = 0.11137366302091488, Recall = 0.88, Aging Rate = 0.08693894055802669, Precision = 0.9209302325581395, f1 = 0.9\n",
      "Epoch 23: Train Loss = 0.10460145987519069, Recall = 0.8933333333333333, Aging Rate = 0.08774767488879903, Precision = 0.9262672811059908, f1 = 0.9095022624434388\n",
      "Epoch 24: Train Loss = 0.0979827705422756, Recall = 0.9066666666666666, Aging Rate = 0.08734330772341285, Precision = 0.9444444444444444, f1 = 0.9251700680272109\n",
      "Epoch 25: Train Loss = 0.09436370766191796, Recall = 0.9066666666666666, Aging Rate = 0.08734330772341285, Precision = 0.9444444444444444, f1 = 0.9251700680272109\n",
      "Test Loss = 0.0882647076822754, Recall = 0.9155555555555556, Aging Rate = 0.08491710473109583, precision = 0.9809523809523809\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.08831213670754501, Recall = 0.8977777777777778, Aging Rate = 0.0841083704003235, Precision = 0.9711538461538461, f1 = 0.9330254041570438\n",
      "Epoch 27: Train Loss = 0.08466743203041961, Recall = 0.9244444444444444, Aging Rate = 0.08613020622725434, Precision = 0.9765258215962441, f1 = 0.9497716894977168\n",
      "Epoch 28: Train Loss = 0.07971726825820065, Recall = 0.9377777777777778, Aging Rate = 0.0881520420541852, Precision = 0.9678899082568807, f1 = 0.9525959367945823\n",
      "Epoch 29: Train Loss = 0.07506085100428951, Recall = 0.9422222222222222, Aging Rate = 0.08855640921957138, Precision = 0.9680365296803652, f1 = 0.9549549549549549\n",
      "Epoch 30: Train Loss = 0.07258543051896768, Recall = 0.9244444444444444, Aging Rate = 0.08653457339264052, Precision = 0.9719626168224299, f1 = 0.9476082004555808\n",
      "Test Loss = 0.0668590559416631, Recall = 0.9422222222222222, Aging Rate = 0.08855640921957138, precision = 0.9680365296803652\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.06789668792993542, Recall = 0.9422222222222222, Aging Rate = 0.0881520420541852, Precision = 0.9724770642201835, f1 = 0.9571106094808127\n",
      "Epoch 32: Train Loss = 0.06479603423613843, Recall = 0.9466666666666667, Aging Rate = 0.08774767488879903, Precision = 0.9815668202764977, f1 = 0.9638009049773756\n",
      "Epoch 33: Train Loss = 0.06151829403853349, Recall = 0.9466666666666667, Aging Rate = 0.08855640921957138, Precision = 0.9726027397260274, f1 = 0.9594594594594594\n",
      "Epoch 34: Train Loss = 0.05806423021722216, Recall = 0.9555555555555556, Aging Rate = 0.08936514355034371, Precision = 0.9728506787330317, f1 = 0.9641255605381166\n",
      "Epoch 35: Train Loss = 0.055656150973101066, Recall = 0.9555555555555556, Aging Rate = 0.08855640921957138, Precision = 0.9817351598173516, f1 = 0.9684684684684685\n",
      "Test Loss = 0.051379662883809546, Recall = 0.96, Aging Rate = 0.08855640921957138, precision = 0.9863013698630136\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.052203442475065934, Recall = 0.96, Aging Rate = 0.08855640921957138, Precision = 0.9863013698630136, f1 = 0.9729729729729729\n",
      "Epoch 37: Train Loss = 0.05135720868222491, Recall = 0.96, Aging Rate = 0.08936514355034371, Precision = 0.9773755656108597, f1 = 0.9686098654708519\n",
      "Epoch 38: Train Loss = 0.04743055017567867, Recall = 0.9688888888888889, Aging Rate = 0.09017387788111605, Precision = 0.9775784753363229, f1 = 0.9732142857142856\n",
      "Epoch 39: Train Loss = 0.04484085133544019, Recall = 0.9688888888888889, Aging Rate = 0.08936514355034371, Precision = 0.9864253393665159, f1 = 0.9775784753363229\n",
      "Epoch 40: Train Loss = 0.04523895677138426, Recall = 0.9733333333333334, Aging Rate = 0.09017387788111605, Precision = 0.9820627802690582, f1 = 0.9776785714285715\n",
      "Test Loss = 0.04209514350162987, Recall = 0.9733333333333334, Aging Rate = 0.09017387788111605, precision = 0.9820627802690582\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.041184796899364294, Recall = 0.9733333333333334, Aging Rate = 0.08936514355034371, Precision = 0.9909502262443439, f1 = 0.9820627802690584\n",
      "Epoch 42: Train Loss = 0.039054353864923015, Recall = 0.9733333333333334, Aging Rate = 0.08976951071572989, Precision = 0.9864864864864865, f1 = 0.9798657718120806\n",
      "Epoch 43: Train Loss = 0.03718454717348521, Recall = 0.9822222222222222, Aging Rate = 0.09017387788111605, Precision = 0.9910313901345291, f1 = 0.9866071428571429\n",
      "Epoch 44: Train Loss = 0.03632445398038739, Recall = 0.9733333333333334, Aging Rate = 0.08976951071572989, Precision = 0.9864864864864865, f1 = 0.9798657718120806\n",
      "Epoch 45: Train Loss = 0.03319462333168281, Recall = 0.9733333333333334, Aging Rate = 0.08936514355034371, Precision = 0.9909502262443439, f1 = 0.9820627802690584\n",
      "Test Loss = 0.03139485113452284, Recall = 0.9777777777777777, Aging Rate = 0.08976951071572989, precision = 0.990990990990991\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.03283359577937305, Recall = 0.9777777777777777, Aging Rate = 0.09017387788111605, Precision = 0.9865470852017937, f1 = 0.9821428571428572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.031164351528381654, Recall = 0.9866666666666667, Aging Rate = 0.0909826122118884, Precision = 0.9866666666666667, f1 = 0.9866666666666668\n",
      "Epoch 48: Train Loss = 0.030025307587703807, Recall = 0.9822222222222222, Aging Rate = 0.09017387788111605, Precision = 0.9910313901345291, f1 = 0.9866071428571429\n",
      "Epoch 49: Train Loss = 0.02772693225290676, Recall = 0.9866666666666667, Aging Rate = 0.09057824504650222, Precision = 0.9910714285714286, f1 = 0.9888641425389756\n",
      "Epoch 50: Train Loss = 0.026812628631619755, Recall = 0.9866666666666667, Aging Rate = 0.09057824504650222, Precision = 0.9910714285714286, f1 = 0.9888641425389756\n",
      "Test Loss = 0.025310876767583767, Recall = 0.9955555555555555, Aging Rate = 0.09138697937727457, precision = 0.9911504424778761\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.025536038371663898, Recall = 0.9866666666666667, Aging Rate = 0.09057824504650222, Precision = 0.9910714285714286, f1 = 0.9888641425389756\n",
      "Epoch 52: Train Loss = 0.02449916286738209, Recall = 0.9911111111111112, Aging Rate = 0.09138697937727457, Precision = 0.9867256637168141, f1 = 0.9889135254988914\n",
      "Epoch 53: Train Loss = 0.02383592173915087, Recall = 0.9911111111111112, Aging Rate = 0.0909826122118884, Precision = 0.9911111111111112, f1 = 0.9911111111111112\n",
      "Epoch 54: Train Loss = 0.022978946635122247, Recall = 0.9911111111111112, Aging Rate = 0.0909826122118884, Precision = 0.9911111111111112, f1 = 0.9911111111111112\n",
      "Epoch 55: Train Loss = 0.022143363225052726, Recall = 0.9866666666666667, Aging Rate = 0.09057824504650222, Precision = 0.9910714285714286, f1 = 0.9888641425389756\n",
      "Test Loss = 0.020660713407652753, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, precision = 0.9955555555555555\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.02118866400566484, Recall = 0.9866666666666667, Aging Rate = 0.09017387788111605, Precision = 0.9955156950672646, f1 = 0.9910714285714286\n",
      "Epoch 57: Train Loss = 0.02016586881225836, Recall = 0.9866666666666667, Aging Rate = 0.09057824504650222, Precision = 0.9910714285714286, f1 = 0.9888641425389756\n",
      "Epoch 58: Train Loss = 0.018958260764296703, Recall = 0.9955555555555555, Aging Rate = 0.09138697937727457, Precision = 0.9911504424778761, f1 = 0.9933481152993349\n",
      "Epoch 59: Train Loss = 0.01861518982148609, Recall = 1.0, Aging Rate = 0.09179134654266073, Precision = 0.9911894273127754, f1 = 0.9955752212389382\n",
      "Epoch 60: Train Loss = 0.017716022546516465, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.016557225241979945, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.01704034596392369, Recall = 0.9955555555555555, Aging Rate = 0.0909826122118884, Precision = 0.9955555555555555, f1 = 0.9955555555555555\n",
      "Epoch 62: Train Loss = 0.01692691474122602, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 63: Train Loss = 0.01587781283644424, Recall = 1.0, Aging Rate = 0.09179134654266073, Precision = 0.9911894273127754, f1 = 0.9955752212389382\n",
      "Epoch 64: Train Loss = 0.015221627429127693, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 65: Train Loss = 0.014969440090089277, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.014518356603731089, Recall = 1.0, Aging Rate = 0.09179134654266073, precision = 0.9911894273127754\n",
      "\n",
      "Epoch 66: Train Loss = 0.014504880109614343, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 67: Train Loss = 0.013720795808957261, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 68: Train Loss = 0.01308573720795191, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 69: Train Loss = 0.012703729933899783, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 70: Train Loss = 0.012437489831814858, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.011781085077695793, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 71: Train Loss = 0.012127164407835477, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 72: Train Loss = 0.011693480139499583, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 73: Train Loss = 0.011458975755904446, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 74: Train Loss = 0.011033355375526648, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 75: Train Loss = 0.010844825741372102, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.009998840297577211, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 76: Train Loss = 0.010349982567628627, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 77: Train Loss = 0.010065426671891814, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.009739227941296648, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 79: Train Loss = 0.00967347348395854, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 80: Train Loss = 0.009273922683582993, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.008726666191045994, Recall = 1.0, Aging Rate = 0.09138697937727457, precision = 0.995575221238938\n",
      "\n",
      "Epoch 81: Train Loss = 0.008811018608956384, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 82: Train Loss = 0.008767077906243276, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.008678490595682684, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 84: Train Loss = 0.008153282167684487, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 85: Train Loss = 0.00802855853712901, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Test Loss = 0.007565168665365753, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "Model in epoch 85 is saved.\n",
      "\n",
      "Epoch 86: Train Loss = 0.007774397702206015, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0075120598547013, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.007613099198050276, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 89: Train Loss = 0.007168682594322274, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.006918657995313045, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0066367571648501015, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.007035003982753984, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.0066617429239492374, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.006601857128969231, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.006800698690919309, Recall = 1.0, Aging Rate = 0.09138697937727457, Precision = 0.995575221238938, f1 = 0.9977827050997783\n",
      "Epoch 95: Train Loss = 0.006126739732466195, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005908587776324826, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.006004698379466131, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: Train Loss = 0.005973583468817117, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.0058454853116116255, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.00567424496476871, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.005590032515546486, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005345168541943646, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.005531592975836435, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.005471558611368511, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.005272766496508627, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0051579213470711646, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.004983978898645207, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004806323102729985, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.004866786206798652, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.004780335256169308, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.004666075836136497, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.0046605250259256535, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.004559743785654491, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004327571431526615, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.00443822980150008, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.004354788209629888, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.004334286310880611, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.004199267027796436, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.00414870193851769, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003923697856748593, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.004084819691309165, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.004071550484064302, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.0038924829097027078, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.0038596502160845535, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.003944787337850317, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004374719320149061, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.0038939317808103524, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.0036935993850425496, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.0038801733883805016, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.003553500042356825, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.0034528433198762285, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003288226987898434, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.003441282745535093, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 127: Train Loss = 0.003325256099867956, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 128: Train Loss = 0.003286782754573849, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.0033762826511973053, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.0032518485834422115, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003046629992300502, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Epoch 131: Train Loss = 0.003152474679287826, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 132: Train Loss = 0.0031742925934186237, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 133: Train Loss = 0.003062655859477321, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 134: Train Loss = 0.0030065939693534323, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Epoch 135: Train Loss = 0.0029903734421069038, Recall = 1.0, Aging Rate = 0.0909826122118884, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002898814697349591, Recall = 1.0, Aging Rate = 0.0909826122118884, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 135.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5083315\ttotal: 2.54ms\tremaining: 379ms\n",
      "1:\tlearn: 0.3763489\ttotal: 5.68ms\tremaining: 420ms\n",
      "2:\tlearn: 0.3044032\ttotal: 7.97ms\tremaining: 391ms\n",
      "3:\tlearn: 0.2504692\ttotal: 10.5ms\tremaining: 383ms\n",
      "4:\tlearn: 0.2233643\ttotal: 12.7ms\tremaining: 368ms\n",
      "5:\tlearn: 0.2019771\ttotal: 14.8ms\tremaining: 356ms\n",
      "6:\tlearn: 0.1828209\ttotal: 17.1ms\tremaining: 349ms\n",
      "7:\tlearn: 0.1668908\ttotal: 19.4ms\tremaining: 345ms\n",
      "8:\tlearn: 0.1551886\ttotal: 21.6ms\tremaining: 339ms\n",
      "9:\tlearn: 0.1443332\ttotal: 23.8ms\tremaining: 333ms\n",
      "10:\tlearn: 0.1317059\ttotal: 26.2ms\tremaining: 331ms\n",
      "11:\tlearn: 0.1259610\ttotal: 28.3ms\tremaining: 325ms\n",
      "12:\tlearn: 0.1168328\ttotal: 30.6ms\tremaining: 323ms\n",
      "13:\tlearn: 0.1109923\ttotal: 32.8ms\tremaining: 318ms\n",
      "14:\tlearn: 0.1043068\ttotal: 35.1ms\tremaining: 316ms\n",
      "15:\tlearn: 0.0971551\ttotal: 37.5ms\tremaining: 314ms\n",
      "16:\tlearn: 0.0913836\ttotal: 39.7ms\tremaining: 311ms\n",
      "17:\tlearn: 0.0841728\ttotal: 42.2ms\tremaining: 309ms\n",
      "18:\tlearn: 0.0775890\ttotal: 44.6ms\tremaining: 307ms\n",
      "19:\tlearn: 0.0715373\ttotal: 47.1ms\tremaining: 306ms\n",
      "20:\tlearn: 0.0665601\ttotal: 49.4ms\tremaining: 303ms\n",
      "21:\tlearn: 0.0634733\ttotal: 51.6ms\tremaining: 300ms\n",
      "22:\tlearn: 0.0611671\ttotal: 53.8ms\tremaining: 297ms\n",
      "23:\tlearn: 0.0573571\ttotal: 56.1ms\tremaining: 294ms\n",
      "24:\tlearn: 0.0539129\ttotal: 58.3ms\tremaining: 291ms\n",
      "25:\tlearn: 0.0515142\ttotal: 60.7ms\tremaining: 289ms\n",
      "26:\tlearn: 0.0485797\ttotal: 63ms\tremaining: 287ms\n",
      "27:\tlearn: 0.0452602\ttotal: 65.5ms\tremaining: 285ms\n",
      "28:\tlearn: 0.0428959\ttotal: 67.9ms\tremaining: 283ms\n",
      "29:\tlearn: 0.0401314\ttotal: 70.2ms\tremaining: 281ms\n",
      "30:\tlearn: 0.0377026\ttotal: 72.7ms\tremaining: 279ms\n",
      "31:\tlearn: 0.0359058\ttotal: 75.1ms\tremaining: 277ms\n",
      "32:\tlearn: 0.0349741\ttotal: 77.3ms\tremaining: 274ms\n",
      "33:\tlearn: 0.0329747\ttotal: 79.6ms\tremaining: 272ms\n",
      "34:\tlearn: 0.0312037\ttotal: 81.5ms\tremaining: 268ms\n",
      "35:\tlearn: 0.0295668\ttotal: 83.5ms\tremaining: 265ms\n",
      "36:\tlearn: 0.0282895\ttotal: 85.5ms\tremaining: 261ms\n",
      "37:\tlearn: 0.0273169\ttotal: 87.4ms\tremaining: 258ms\n",
      "38:\tlearn: 0.0258941\ttotal: 89.2ms\tremaining: 254ms\n",
      "39:\tlearn: 0.0249987\ttotal: 91.2ms\tremaining: 251ms\n",
      "40:\tlearn: 0.0238780\ttotal: 93.1ms\tremaining: 248ms\n",
      "41:\tlearn: 0.0220485\ttotal: 95.4ms\tremaining: 245ms\n",
      "42:\tlearn: 0.0211501\ttotal: 97.4ms\tremaining: 242ms\n",
      "43:\tlearn: 0.0208496\ttotal: 99.1ms\tremaining: 239ms\n",
      "44:\tlearn: 0.0198684\ttotal: 101ms\tremaining: 236ms\n",
      "45:\tlearn: 0.0192695\ttotal: 103ms\tremaining: 233ms\n",
      "46:\tlearn: 0.0188151\ttotal: 105ms\tremaining: 229ms\n",
      "47:\tlearn: 0.0180290\ttotal: 107ms\tremaining: 227ms\n",
      "48:\tlearn: 0.0173798\ttotal: 109ms\tremaining: 224ms\n",
      "49:\tlearn: 0.0166626\ttotal: 111ms\tremaining: 221ms\n",
      "50:\tlearn: 0.0155469\ttotal: 113ms\tremaining: 219ms\n",
      "51:\tlearn: 0.0149363\ttotal: 115ms\tremaining: 216ms\n",
      "52:\tlearn: 0.0143437\ttotal: 117ms\tremaining: 213ms\n",
      "53:\tlearn: 0.0138746\ttotal: 118ms\tremaining: 210ms\n",
      "54:\tlearn: 0.0132468\ttotal: 120ms\tremaining: 208ms\n",
      "55:\tlearn: 0.0127744\ttotal: 122ms\tremaining: 205ms\n",
      "56:\tlearn: 0.0121980\ttotal: 124ms\tremaining: 203ms\n",
      "57:\tlearn: 0.0117796\ttotal: 126ms\tremaining: 200ms\n",
      "58:\tlearn: 0.0112090\ttotal: 128ms\tremaining: 197ms\n",
      "59:\tlearn: 0.0107575\ttotal: 130ms\tremaining: 195ms\n",
      "60:\tlearn: 0.0103085\ttotal: 132ms\tremaining: 193ms\n",
      "61:\tlearn: 0.0101469\ttotal: 134ms\tremaining: 190ms\n",
      "62:\tlearn: 0.0095562\ttotal: 136ms\tremaining: 187ms\n",
      "63:\tlearn: 0.0092800\ttotal: 138ms\tremaining: 185ms\n",
      "64:\tlearn: 0.0087738\ttotal: 140ms\tremaining: 183ms\n",
      "65:\tlearn: 0.0084642\ttotal: 141ms\tremaining: 180ms\n",
      "66:\tlearn: 0.0083281\ttotal: 143ms\tremaining: 177ms\n",
      "67:\tlearn: 0.0078865\ttotal: 145ms\tremaining: 175ms\n",
      "68:\tlearn: 0.0076366\ttotal: 147ms\tremaining: 173ms\n",
      "69:\tlearn: 0.0075026\ttotal: 149ms\tremaining: 170ms\n",
      "70:\tlearn: 0.0072492\ttotal: 151ms\tremaining: 168ms\n",
      "71:\tlearn: 0.0070623\ttotal: 152ms\tremaining: 165ms\n",
      "72:\tlearn: 0.0068024\ttotal: 154ms\tremaining: 163ms\n",
      "73:\tlearn: 0.0065601\ttotal: 156ms\tremaining: 161ms\n",
      "74:\tlearn: 0.0064344\ttotal: 158ms\tremaining: 158ms\n",
      "75:\tlearn: 0.0062414\ttotal: 160ms\tremaining: 156ms\n",
      "76:\tlearn: 0.0061385\ttotal: 162ms\tremaining: 154ms\n",
      "77:\tlearn: 0.0059932\ttotal: 164ms\tremaining: 151ms\n",
      "78:\tlearn: 0.0058098\ttotal: 166ms\tremaining: 149ms\n",
      "79:\tlearn: 0.0056325\ttotal: 168ms\tremaining: 147ms\n",
      "80:\tlearn: 0.0053373\ttotal: 170ms\tremaining: 145ms\n",
      "81:\tlearn: 0.0050405\ttotal: 172ms\tremaining: 143ms\n",
      "82:\tlearn: 0.0047728\ttotal: 174ms\tremaining: 141ms\n",
      "83:\tlearn: 0.0046767\ttotal: 176ms\tremaining: 138ms\n",
      "84:\tlearn: 0.0044744\ttotal: 178ms\tremaining: 136ms\n",
      "85:\tlearn: 0.0044164\ttotal: 180ms\tremaining: 134ms\n",
      "86:\tlearn: 0.0043607\ttotal: 181ms\tremaining: 131ms\n",
      "87:\tlearn: 0.0043060\ttotal: 183ms\tremaining: 129ms\n",
      "88:\tlearn: 0.0041634\ttotal: 185ms\tremaining: 127ms\n",
      "89:\tlearn: 0.0040771\ttotal: 187ms\tremaining: 125ms\n",
      "90:\tlearn: 0.0039741\ttotal: 189ms\tremaining: 123ms\n",
      "91:\tlearn: 0.0038325\ttotal: 191ms\tremaining: 120ms\n",
      "92:\tlearn: 0.0036525\ttotal: 193ms\tremaining: 118ms\n",
      "93:\tlearn: 0.0035271\ttotal: 195ms\tremaining: 116ms\n",
      "94:\tlearn: 0.0033891\ttotal: 197ms\tremaining: 114ms\n",
      "95:\tlearn: 0.0033291\ttotal: 199ms\tremaining: 112ms\n",
      "96:\tlearn: 0.0032373\ttotal: 201ms\tremaining: 110ms\n",
      "97:\tlearn: 0.0031538\ttotal: 203ms\tremaining: 108ms\n",
      "98:\tlearn: 0.0030381\ttotal: 205ms\tremaining: 106ms\n",
      "99:\tlearn: 0.0029284\ttotal: 207ms\tremaining: 103ms\n",
      "100:\tlearn: 0.0028836\ttotal: 209ms\tremaining: 101ms\n",
      "101:\tlearn: 0.0028302\ttotal: 211ms\tremaining: 99.2ms\n",
      "102:\tlearn: 0.0027339\ttotal: 213ms\tremaining: 97.1ms\n",
      "103:\tlearn: 0.0026885\ttotal: 215ms\tremaining: 94.9ms\n",
      "104:\tlearn: 0.0026419\ttotal: 216ms\tremaining: 92.7ms\n",
      "105:\tlearn: 0.0025826\ttotal: 218ms\tremaining: 90.5ms\n",
      "106:\tlearn: 0.0024426\ttotal: 220ms\tremaining: 88.5ms\n",
      "107:\tlearn: 0.0024263\ttotal: 222ms\tremaining: 86.3ms\n",
      "108:\tlearn: 0.0023627\ttotal: 224ms\tremaining: 84.1ms\n",
      "109:\tlearn: 0.0022863\ttotal: 226ms\tremaining: 82ms\n",
      "110:\tlearn: 0.0022481\ttotal: 227ms\tremaining: 79.9ms\n",
      "111:\tlearn: 0.0021979\ttotal: 229ms\tremaining: 77.8ms\n",
      "112:\tlearn: 0.0021745\ttotal: 231ms\tremaining: 75.7ms\n",
      "113:\tlearn: 0.0021251\ttotal: 233ms\tremaining: 73.6ms\n",
      "114:\tlearn: 0.0020765\ttotal: 235ms\tremaining: 71.5ms\n",
      "115:\tlearn: 0.0020481\ttotal: 237ms\tremaining: 69.4ms\n",
      "116:\tlearn: 0.0020143\ttotal: 239ms\tremaining: 67.3ms\n",
      "117:\tlearn: 0.0019699\ttotal: 240ms\tremaining: 65.2ms\n",
      "118:\tlearn: 0.0019330\ttotal: 242ms\tremaining: 63.1ms\n",
      "119:\tlearn: 0.0018810\ttotal: 244ms\tremaining: 61.1ms\n",
      "120:\tlearn: 0.0018505\ttotal: 246ms\tremaining: 58.9ms\n",
      "121:\tlearn: 0.0018127\ttotal: 248ms\tremaining: 56.9ms\n",
      "122:\tlearn: 0.0017761\ttotal: 250ms\tremaining: 54.8ms\n",
      "123:\tlearn: 0.0017650\ttotal: 251ms\tremaining: 52.7ms\n",
      "124:\tlearn: 0.0017151\ttotal: 253ms\tremaining: 50.6ms\n",
      "125:\tlearn: 0.0016833\ttotal: 255ms\tremaining: 48.6ms\n",
      "126:\tlearn: 0.0016544\ttotal: 257ms\tremaining: 46.5ms\n",
      "127:\tlearn: 0.0015941\ttotal: 259ms\tremaining: 44.5ms\n",
      "128:\tlearn: 0.0015430\ttotal: 261ms\tremaining: 42.5ms\n",
      "129:\tlearn: 0.0014985\ttotal: 263ms\tremaining: 40.5ms\n",
      "130:\tlearn: 0.0014745\ttotal: 265ms\tremaining: 38.4ms\n",
      "131:\tlearn: 0.0014539\ttotal: 267ms\tremaining: 36.4ms\n",
      "132:\tlearn: 0.0014037\ttotal: 269ms\tremaining: 34.4ms\n",
      "133:\tlearn: 0.0013864\ttotal: 271ms\tremaining: 32.3ms\n",
      "134:\tlearn: 0.0013718\ttotal: 273ms\tremaining: 30.3ms\n",
      "135:\tlearn: 0.0013499\ttotal: 274ms\tremaining: 28.2ms\n",
      "136:\tlearn: 0.0013386\ttotal: 276ms\tremaining: 26.2ms\n",
      "137:\tlearn: 0.0013069\ttotal: 278ms\tremaining: 24.2ms\n",
      "138:\tlearn: 0.0012839\ttotal: 280ms\tremaining: 22.1ms\n",
      "139:\tlearn: 0.0012604\ttotal: 281ms\tremaining: 20.1ms\n",
      "140:\tlearn: 0.0012442\ttotal: 283ms\tremaining: 18.1ms\n",
      "141:\tlearn: 0.0012294\ttotal: 285ms\tremaining: 16.1ms\n",
      "142:\tlearn: 0.0012045\ttotal: 287ms\tremaining: 14ms\n",
      "143:\tlearn: 0.0011891\ttotal: 288ms\tremaining: 12ms\n",
      "144:\tlearn: 0.0011733\ttotal: 290ms\tremaining: 10ms\n",
      "145:\tlearn: 0.0011455\ttotal: 292ms\tremaining: 8.01ms\n",
      "146:\tlearn: 0.0011324\ttotal: 294ms\tremaining: 6ms\n",
      "147:\tlearn: 0.0011217\ttotal: 296ms\tremaining: 4ms\n",
      "148:\tlearn: 0.0011060\ttotal: 298ms\tremaining: 2ms\n",
      "149:\tlearn: 0.0010887\ttotal: 299ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c1ee9a626d4e5cb6e46c129c900531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5768331127259337, Recall = 0.09375, Aging Rate = 0.10396440129449838, Precision = 0.08171206225680934, f1 = 0.08731808731808734\n",
      "Epoch 2: Train Loss = 0.4650499146540188, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.4319228738257029, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.40157721198878243, Recall = 0.022321428571428572, Aging Rate = 0.0024271844660194173, Precision = 0.8333333333333334, f1 = 0.043478260869565216\n",
      "Epoch 5: Train Loss = 0.36233357000119476, Recall = 0.12053571428571429, Aging Rate = 0.011326860841423949, Precision = 0.9642857142857143, f1 = 0.2142857142857143\n",
      "Test Loss = 0.3423203799909758, Recall = 0.2857142857142857, Aging Rate = 0.02750809061488673, precision = 0.9411764705882353\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.32911379042180994, Recall = 0.3169642857142857, Aging Rate = 0.032766990291262135, Precision = 0.8765432098765432, f1 = 0.4655737704918032\n",
      "Epoch 7: Train Loss = 0.2974021295899326, Recall = 0.45089285714285715, Aging Rate = 0.04854368932038835, Precision = 0.8416666666666667, f1 = 0.5872093023255814\n",
      "Epoch 8: Train Loss = 0.27166933168485324, Recall = 0.5178571428571429, Aging Rate = 0.05461165048543689, Precision = 0.8592592592592593, f1 = 0.6462395543175488\n",
      "Epoch 9: Train Loss = 0.24844493203371473, Recall = 0.53125, Aging Rate = 0.05622977346278317, Precision = 0.8561151079136691, f1 = 0.6556473829201103\n",
      "Epoch 10: Train Loss = 0.23183006221808275, Recall = 0.5982142857142857, Aging Rate = 0.06432038834951456, Precision = 0.8427672955974843, f1 = 0.6997389033942559\n",
      "Test Loss = 0.21578074615557216, Recall = 0.6294642857142857, Aging Rate = 0.06351132686084142, precision = 0.8980891719745223\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.2139283809268359, Recall = 0.6339285714285714, Aging Rate = 0.06310679611650485, Precision = 0.9102564102564102, f1 = 0.7473684210526315\n",
      "Epoch 12: Train Loss = 0.19685926628344266, Recall = 0.6741071428571429, Aging Rate = 0.07119741100323625, Precision = 0.8579545454545454, f1 = 0.755\n",
      "Epoch 13: Train Loss = 0.18131703557898698, Recall = 0.7321428571428571, Aging Rate = 0.07524271844660194, Precision = 0.8817204301075269, f1 = 0.7999999999999999\n",
      "Epoch 14: Train Loss = 0.16740778920430582, Recall = 0.7321428571428571, Aging Rate = 0.07160194174757281, Precision = 0.9265536723163842, f1 = 0.8179551122194514\n",
      "Epoch 15: Train Loss = 0.1551622855431825, Recall = 0.78125, Aging Rate = 0.07564724919093851, Precision = 0.9358288770053476, f1 = 0.851581508515815\n",
      "Test Loss = 0.14472882304955453, Recall = 0.8125, Aging Rate = 0.07726537216828479, precision = 0.9528795811518325\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.142705349233544, Recall = 0.8214285714285714, Aging Rate = 0.0784789644012945, Precision = 0.9484536082474226, f1 = 0.8803827751196172\n",
      "Epoch 17: Train Loss = 0.13309452053412651, Recall = 0.8392857142857143, Aging Rate = 0.08009708737864078, Precision = 0.9494949494949495, f1 = 0.8909952606635071\n",
      "Epoch 18: Train Loss = 0.12288267317327481, Recall = 0.8839285714285714, Aging Rate = 0.08454692556634304, Precision = 0.9473684210526315, f1 = 0.9145496535796767\n",
      "Epoch 19: Train Loss = 0.11417223267184878, Recall = 0.8794642857142857, Aging Rate = 0.0825242718446602, Precision = 0.9656862745098039, f1 = 0.9205607476635514\n",
      "Epoch 20: Train Loss = 0.10559169365798385, Recall = 0.8928571428571429, Aging Rate = 0.08414239482200647, Precision = 0.9615384615384616, f1 = 0.9259259259259259\n",
      "Test Loss = 0.09787065065601497, Recall = 0.90625, Aging Rate = 0.08373786407766991, precision = 0.9806763285024155\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.09761733467717773, Recall = 0.9017857142857143, Aging Rate = 0.08495145631067962, Precision = 0.9619047619047619, f1 = 0.9308755760368663\n",
      "Epoch 22: Train Loss = 0.09182176869181753, Recall = 0.9151785714285714, Aging Rate = 0.08495145631067962, Precision = 0.9761904761904762, f1 = 0.9447004608294931\n",
      "Epoch 23: Train Loss = 0.084966740417249, Recall = 0.9196428571428571, Aging Rate = 0.08495145631067962, Precision = 0.9809523809523809, f1 = 0.9493087557603686\n",
      "Epoch 24: Train Loss = 0.0788662988439347, Recall = 0.9241071428571429, Aging Rate = 0.08535598705501618, Precision = 0.981042654028436, f1 = 0.9517241379310345\n",
      "Epoch 25: Train Loss = 0.0736671806973161, Recall = 0.9241071428571429, Aging Rate = 0.08495145631067962, Precision = 0.9857142857142858, f1 = 0.9539170506912443\n",
      "Test Loss = 0.07105270078749333, Recall = 0.9598214285714286, Aging Rate = 0.09021035598705501, precision = 0.9641255605381166\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.06870167684352514, Recall = 0.9330357142857143, Aging Rate = 0.08576051779935275, Precision = 0.9858490566037735, f1 = 0.9587155963302751\n",
      "Epoch 27: Train Loss = 0.06368054753368341, Recall = 0.9285714285714286, Aging Rate = 0.08495145631067962, Precision = 0.9904761904761905, f1 = 0.9585253456221199\n",
      "Epoch 28: Train Loss = 0.05970890961081079, Recall = 0.9598214285714286, Aging Rate = 0.08940129449838188, Precision = 0.9728506787330317, f1 = 0.9662921348314607\n",
      "Epoch 29: Train Loss = 0.05540761311800735, Recall = 0.9508928571428571, Aging Rate = 0.0877831715210356, Precision = 0.9815668202764977, f1 = 0.9659863945578231\n",
      "Epoch 30: Train Loss = 0.05192255720640849, Recall = 0.9642857142857143, Aging Rate = 0.08859223300970874, Precision = 0.9863013698630136, f1 = 0.9751693002257336\n",
      "Test Loss = 0.04793439161719628, Recall = 0.96875, Aging Rate = 0.0889967637540453, precision = 0.9863636363636363\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.04797570197616966, Recall = 0.9642857142857143, Aging Rate = 0.08859223300970874, Precision = 0.9863013698630136, f1 = 0.9751693002257336\n",
      "Epoch 32: Train Loss = 0.04514846798864383, Recall = 0.9732142857142857, Aging Rate = 0.08940129449838188, Precision = 0.9864253393665159, f1 = 0.9797752808988764\n",
      "Epoch 33: Train Loss = 0.04216429350662579, Recall = 0.96875, Aging Rate = 0.0889967637540453, Precision = 0.9863636363636363, f1 = 0.9774774774774775\n",
      "Epoch 34: Train Loss = 0.039901053749965235, Recall = 0.9776785714285714, Aging Rate = 0.08980582524271845, Precision = 0.9864864864864865, f1 = 0.9820627802690582\n",
      "Epoch 35: Train Loss = 0.03798816148734208, Recall = 0.9776785714285714, Aging Rate = 0.08980582524271845, Precision = 0.9864864864864865, f1 = 0.9820627802690582\n",
      "Test Loss = 0.03510179303731154, Recall = 0.9732142857142857, Aging Rate = 0.0889967637540453, precision = 0.990909090909091\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.03525478720158628, Recall = 0.9866071428571429, Aging Rate = 0.09061488673139159, Precision = 0.9866071428571429, f1 = 0.9866071428571429\n",
      "Epoch 37: Train Loss = 0.032665751838279, Recall = 0.9866071428571429, Aging Rate = 0.09021035598705501, Precision = 0.9910313901345291, f1 = 0.9888143176733781\n",
      "Epoch 38: Train Loss = 0.030803271578353584, Recall = 0.9866071428571429, Aging Rate = 0.09021035598705501, Precision = 0.9910313901345291, f1 = 0.9888143176733781\n",
      "Epoch 39: Train Loss = 0.029622078339741068, Recall = 0.9866071428571429, Aging Rate = 0.09061488673139159, Precision = 0.9866071428571429, f1 = 0.9866071428571429\n",
      "Epoch 40: Train Loss = 0.027739309280821422, Recall = 0.9866071428571429, Aging Rate = 0.09021035598705501, Precision = 0.9910313901345291, f1 = 0.9888143176733781\n",
      "Test Loss = 0.02611759999423351, Recall = 0.9955357142857143, Aging Rate = 0.09101941747572816, precision = 0.9911111111111112\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.025997357750401913, Recall = 0.9866071428571429, Aging Rate = 0.09021035598705501, Precision = 0.9910313901345291, f1 = 0.9888143176733781\n",
      "Epoch 42: Train Loss = 0.024336208517039284, Recall = 0.9955357142857143, Aging Rate = 0.09101941747572816, Precision = 0.9911111111111112, f1 = 0.9933184855233853\n",
      "Epoch 43: Train Loss = 0.023046985576019703, Recall = 0.9866071428571429, Aging Rate = 0.08980582524271845, Precision = 0.9954954954954955, f1 = 0.9910313901345291\n",
      "Epoch 44: Train Loss = 0.021922400112724998, Recall = 0.9955357142857143, Aging Rate = 0.09061488673139159, Precision = 0.9955357142857143, f1 = 0.9955357142857143\n",
      "Epoch 45: Train Loss = 0.020795880717415255, Recall = 0.9955357142857143, Aging Rate = 0.09061488673139159, Precision = 0.9955357142857143, f1 = 0.9955357142857143\n",
      "Test Loss = 0.0197526371486268, Recall = 0.9955357142857143, Aging Rate = 0.09101941747572816, precision = 0.9911111111111112\n",
      "\n",
      "Epoch 46: Train Loss = 0.019682552068702227, Recall = 0.9955357142857143, Aging Rate = 0.09101941747572816, Precision = 0.9911111111111112, f1 = 0.9933184855233853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.01899891410891003, Recall = 0.9955357142857143, Aging Rate = 0.09101941747572816, Precision = 0.9911111111111112, f1 = 0.9933184855233853\n",
      "Epoch 48: Train Loss = 0.0182304530974152, Recall = 0.9955357142857143, Aging Rate = 0.09101941747572816, Precision = 0.9911111111111112, f1 = 0.9933184855233853\n",
      "Epoch 49: Train Loss = 0.01693887211093046, Recall = 0.9955357142857143, Aging Rate = 0.09061488673139159, Precision = 0.9955357142857143, f1 = 0.9955357142857143\n",
      "Epoch 50: Train Loss = 0.01624400485126139, Recall = 0.9955357142857143, Aging Rate = 0.09061488673139159, Precision = 0.9955357142857143, f1 = 0.9955357142857143\n",
      "Test Loss = 0.015105378850209482, Recall = 0.9955357142857143, Aging Rate = 0.09061488673139159, precision = 0.9955357142857143\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.015369169775722096, Recall = 0.9955357142857143, Aging Rate = 0.09061488673139159, Precision = 0.9955357142857143, f1 = 0.9955357142857143\n",
      "Epoch 52: Train Loss = 0.01494667627532216, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 53: Train Loss = 0.01405006572965858, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 54: Train Loss = 0.013597437659802945, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 55: Train Loss = 0.012862841246052853, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Test Loss = 0.012031643488670437, Recall = 1.0, Aging Rate = 0.09101941747572816, precision = 0.9955555555555555\n",
      "Model in epoch 55 is saved.\n",
      "\n",
      "Epoch 56: Train Loss = 0.012205292714886296, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 57: Train Loss = 0.01185807348816719, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 58: Train Loss = 0.01125983964398648, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 59: Train Loss = 0.010896529098158901, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 60: Train Loss = 0.010553809687567568, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Test Loss = 0.010125784872515687, Recall = 1.0, Aging Rate = 0.09101941747572816, precision = 0.9955555555555555\n",
      "\n",
      "Epoch 61: Train Loss = 0.010265892805549705, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 62: Train Loss = 0.009660030516244254, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 63: Train Loss = 0.009424820388622076, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 64: Train Loss = 0.008957664739991565, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 65: Train Loss = 0.008654696274621105, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Test Loss = 0.00824571133238598, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "Model in epoch 65 is saved.\n",
      "\n",
      "Epoch 66: Train Loss = 0.008344615583570258, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.008107681225225764, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 68: Train Loss = 0.007768613001494442, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.007540818697149025, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.007360370865869291, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.007129686131838312, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.007433271496692329, Recall = 1.0, Aging Rate = 0.09101941747572816, Precision = 0.9955555555555555, f1 = 0.9977728285077951\n",
      "Epoch 72: Train Loss = 0.006921335665997371, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.006603631407962841, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.00640497097148768, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.006337065177345739, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00595997509239151, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.006067596195102895, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.005986024435718083, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.005887184694445539, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.005779806004602065, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.005547823275900581, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0051915299269434025, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.00531161296660605, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.005191769717640958, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.005075025073822262, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0050459644164962385, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.004822289383545229, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004638246848311239, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0047328528830582655, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.004620889165592425, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.00456057590188332, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.004346776373901413, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.004280611095063894, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00407314723701153, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.004208621623229633, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.004162633328165099, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.004015291276957514, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0038967413592685776, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.0038388569554476774, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00367990984283026, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0037744734165014574, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0037860159685346975, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.00365759020793315, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.003562676155198257, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0034914477298813827, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0033664699797563763, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.003423273093202739, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.0034066142561818354, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.003396620666517795, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.00325442477608624, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.0031884819228491447, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0030772406291397453, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106: Train Loss = 0.0031499759185589053, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.003093867490375505, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.003077862458755669, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.003043492912378126, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.0029288793697012858, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0028572849097471794, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.002949659301203142, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.0029232146204856125, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.00283195387608218, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.002803412598367889, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.0027684369734804898, Recall = 1.0, Aging Rate = 0.09061488673139159, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026650997692520178, Recall = 1.0, Aging Rate = 0.09061488673139159, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 115.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5186346\ttotal: 2.54ms\tremaining: 379ms\n",
      "1:\tlearn: 0.4095335\ttotal: 5.03ms\tremaining: 372ms\n",
      "2:\tlearn: 0.3236697\ttotal: 7.65ms\tremaining: 375ms\n",
      "3:\tlearn: 0.2702226\ttotal: 10ms\tremaining: 365ms\n",
      "4:\tlearn: 0.2391464\ttotal: 12.3ms\tremaining: 356ms\n",
      "5:\tlearn: 0.2105551\ttotal: 14.6ms\tremaining: 351ms\n",
      "6:\tlearn: 0.1924775\ttotal: 16.9ms\tremaining: 345ms\n",
      "7:\tlearn: 0.1711308\ttotal: 19.4ms\tremaining: 345ms\n",
      "8:\tlearn: 0.1521025\ttotal: 22ms\tremaining: 344ms\n",
      "9:\tlearn: 0.1382076\ttotal: 24.5ms\tremaining: 343ms\n",
      "10:\tlearn: 0.1299138\ttotal: 26.6ms\tremaining: 336ms\n",
      "11:\tlearn: 0.1205013\ttotal: 29ms\tremaining: 333ms\n",
      "12:\tlearn: 0.1136308\ttotal: 31.1ms\tremaining: 328ms\n",
      "13:\tlearn: 0.1067190\ttotal: 33.3ms\tremaining: 324ms\n",
      "14:\tlearn: 0.0972159\ttotal: 35.9ms\tremaining: 323ms\n",
      "15:\tlearn: 0.0947975\ttotal: 37.8ms\tremaining: 317ms\n",
      "16:\tlearn: 0.0861829\ttotal: 40.4ms\tremaining: 316ms\n",
      "17:\tlearn: 0.0817320\ttotal: 42.6ms\tremaining: 313ms\n",
      "18:\tlearn: 0.0790032\ttotal: 44.8ms\tremaining: 309ms\n",
      "19:\tlearn: 0.0738298\ttotal: 47.1ms\tremaining: 306ms\n",
      "20:\tlearn: 0.0712969\ttotal: 49.1ms\tremaining: 301ms\n",
      "21:\tlearn: 0.0663232\ttotal: 51.4ms\tremaining: 299ms\n",
      "22:\tlearn: 0.0580599\ttotal: 54.6ms\tremaining: 301ms\n",
      "23:\tlearn: 0.0547723\ttotal: 56.9ms\tremaining: 299ms\n",
      "24:\tlearn: 0.0507445\ttotal: 59.4ms\tremaining: 297ms\n",
      "25:\tlearn: 0.0486907\ttotal: 61.6ms\tremaining: 294ms\n",
      "26:\tlearn: 0.0452064\ttotal: 63.9ms\tremaining: 291ms\n",
      "27:\tlearn: 0.0430957\ttotal: 66.2ms\tremaining: 288ms\n",
      "28:\tlearn: 0.0411398\ttotal: 68.4ms\tremaining: 285ms\n",
      "29:\tlearn: 0.0391104\ttotal: 70.7ms\tremaining: 283ms\n",
      "30:\tlearn: 0.0362331\ttotal: 73.2ms\tremaining: 281ms\n",
      "31:\tlearn: 0.0347179\ttotal: 75.6ms\tremaining: 279ms\n",
      "32:\tlearn: 0.0321718\ttotal: 77.8ms\tremaining: 276ms\n",
      "33:\tlearn: 0.0305232\ttotal: 79.8ms\tremaining: 272ms\n",
      "34:\tlearn: 0.0294930\ttotal: 81.4ms\tremaining: 268ms\n",
      "35:\tlearn: 0.0281594\ttotal: 83.5ms\tremaining: 264ms\n",
      "36:\tlearn: 0.0269308\ttotal: 85.5ms\tremaining: 261ms\n",
      "37:\tlearn: 0.0245150\ttotal: 87.9ms\tremaining: 259ms\n",
      "38:\tlearn: 0.0239480\ttotal: 89.5ms\tremaining: 255ms\n",
      "39:\tlearn: 0.0230296\ttotal: 91.3ms\tremaining: 251ms\n",
      "40:\tlearn: 0.0220283\ttotal: 93.2ms\tremaining: 248ms\n",
      "41:\tlearn: 0.0208615\ttotal: 95.1ms\tremaining: 245ms\n",
      "42:\tlearn: 0.0193849\ttotal: 97.2ms\tremaining: 242ms\n",
      "43:\tlearn: 0.0189055\ttotal: 98.9ms\tremaining: 238ms\n",
      "44:\tlearn: 0.0180978\ttotal: 101ms\tremaining: 235ms\n",
      "45:\tlearn: 0.0173465\ttotal: 103ms\tremaining: 232ms\n",
      "46:\tlearn: 0.0166993\ttotal: 105ms\tremaining: 229ms\n",
      "47:\tlearn: 0.0159479\ttotal: 107ms\tremaining: 226ms\n",
      "48:\tlearn: 0.0152476\ttotal: 108ms\tremaining: 223ms\n",
      "49:\tlearn: 0.0146938\ttotal: 110ms\tremaining: 220ms\n",
      "50:\tlearn: 0.0138212\ttotal: 112ms\tremaining: 218ms\n",
      "51:\tlearn: 0.0133343\ttotal: 114ms\tremaining: 215ms\n",
      "52:\tlearn: 0.0127498\ttotal: 116ms\tremaining: 212ms\n",
      "53:\tlearn: 0.0123171\ttotal: 118ms\tremaining: 209ms\n",
      "54:\tlearn: 0.0113775\ttotal: 120ms\tremaining: 207ms\n",
      "55:\tlearn: 0.0110256\ttotal: 122ms\tremaining: 204ms\n",
      "56:\tlearn: 0.0107147\ttotal: 124ms\tremaining: 202ms\n",
      "57:\tlearn: 0.0101168\ttotal: 126ms\tremaining: 199ms\n",
      "58:\tlearn: 0.0097671\ttotal: 127ms\tremaining: 197ms\n",
      "59:\tlearn: 0.0093962\ttotal: 129ms\tremaining: 194ms\n",
      "60:\tlearn: 0.0090865\ttotal: 131ms\tremaining: 191ms\n",
      "61:\tlearn: 0.0087970\ttotal: 133ms\tremaining: 189ms\n",
      "62:\tlearn: 0.0085961\ttotal: 135ms\tremaining: 186ms\n",
      "63:\tlearn: 0.0083796\ttotal: 137ms\tremaining: 183ms\n",
      "64:\tlearn: 0.0081510\ttotal: 138ms\tremaining: 181ms\n",
      "65:\tlearn: 0.0079350\ttotal: 140ms\tremaining: 178ms\n",
      "66:\tlearn: 0.0073678\ttotal: 142ms\tremaining: 176ms\n",
      "67:\tlearn: 0.0072737\ttotal: 144ms\tremaining: 174ms\n",
      "68:\tlearn: 0.0070505\ttotal: 146ms\tremaining: 171ms\n",
      "69:\tlearn: 0.0068647\ttotal: 148ms\tremaining: 169ms\n",
      "70:\tlearn: 0.0064461\ttotal: 150ms\tremaining: 167ms\n",
      "71:\tlearn: 0.0062665\ttotal: 152ms\tremaining: 164ms\n",
      "72:\tlearn: 0.0060820\ttotal: 154ms\tremaining: 162ms\n",
      "73:\tlearn: 0.0057966\ttotal: 155ms\tremaining: 160ms\n",
      "74:\tlearn: 0.0056709\ttotal: 157ms\tremaining: 157ms\n",
      "75:\tlearn: 0.0055433\ttotal: 159ms\tremaining: 155ms\n",
      "76:\tlearn: 0.0054223\ttotal: 161ms\tremaining: 152ms\n",
      "77:\tlearn: 0.0052004\ttotal: 163ms\tremaining: 150ms\n",
      "78:\tlearn: 0.0050907\ttotal: 165ms\tremaining: 148ms\n",
      "79:\tlearn: 0.0050208\ttotal: 166ms\tremaining: 145ms\n",
      "80:\tlearn: 0.0047872\ttotal: 168ms\tremaining: 143ms\n",
      "81:\tlearn: 0.0046772\ttotal: 170ms\tremaining: 141ms\n",
      "82:\tlearn: 0.0045851\ttotal: 172ms\tremaining: 139ms\n",
      "83:\tlearn: 0.0044041\ttotal: 174ms\tremaining: 136ms\n",
      "84:\tlearn: 0.0043776\ttotal: 175ms\tremaining: 134ms\n",
      "85:\tlearn: 0.0042631\ttotal: 177ms\tremaining: 132ms\n",
      "86:\tlearn: 0.0041485\ttotal: 179ms\tremaining: 129ms\n",
      "87:\tlearn: 0.0040573\ttotal: 181ms\tremaining: 127ms\n",
      "88:\tlearn: 0.0039687\ttotal: 183ms\tremaining: 125ms\n",
      "89:\tlearn: 0.0038857\ttotal: 184ms\tremaining: 123ms\n",
      "90:\tlearn: 0.0038100\ttotal: 186ms\tremaining: 121ms\n",
      "91:\tlearn: 0.0036427\ttotal: 188ms\tremaining: 119ms\n",
      "92:\tlearn: 0.0035620\ttotal: 190ms\tremaining: 116ms\n",
      "93:\tlearn: 0.0035245\ttotal: 192ms\tremaining: 114ms\n",
      "94:\tlearn: 0.0034370\ttotal: 194ms\tremaining: 112ms\n",
      "95:\tlearn: 0.0033514\ttotal: 195ms\tremaining: 110ms\n",
      "96:\tlearn: 0.0032345\ttotal: 197ms\tremaining: 108ms\n",
      "97:\tlearn: 0.0031552\ttotal: 199ms\tremaining: 106ms\n",
      "98:\tlearn: 0.0030714\ttotal: 201ms\tremaining: 103ms\n",
      "99:\tlearn: 0.0030001\ttotal: 203ms\tremaining: 101ms\n",
      "100:\tlearn: 0.0029191\ttotal: 205ms\tremaining: 99.3ms\n",
      "101:\tlearn: 0.0028824\ttotal: 206ms\tremaining: 97.1ms\n",
      "102:\tlearn: 0.0028241\ttotal: 208ms\tremaining: 95ms\n",
      "103:\tlearn: 0.0027652\ttotal: 210ms\tremaining: 92.9ms\n",
      "104:\tlearn: 0.0027470\ttotal: 212ms\tremaining: 90.7ms\n",
      "105:\tlearn: 0.0026402\ttotal: 214ms\tremaining: 88.6ms\n",
      "106:\tlearn: 0.0025980\ttotal: 215ms\tremaining: 86.5ms\n",
      "107:\tlearn: 0.0025578\ttotal: 217ms\tremaining: 84.4ms\n",
      "108:\tlearn: 0.0024945\ttotal: 219ms\tremaining: 82.3ms\n",
      "109:\tlearn: 0.0024731\ttotal: 221ms\tremaining: 80.2ms\n",
      "110:\tlearn: 0.0024351\ttotal: 222ms\tremaining: 78.1ms\n",
      "111:\tlearn: 0.0023861\ttotal: 224ms\tremaining: 76ms\n",
      "112:\tlearn: 0.0023586\ttotal: 226ms\tremaining: 74ms\n",
      "113:\tlearn: 0.0022937\ttotal: 228ms\tremaining: 72ms\n",
      "114:\tlearn: 0.0022470\ttotal: 230ms\tremaining: 69.9ms\n",
      "115:\tlearn: 0.0021905\ttotal: 232ms\tremaining: 68ms\n",
      "116:\tlearn: 0.0021481\ttotal: 234ms\tremaining: 65.9ms\n",
      "117:\tlearn: 0.0021208\ttotal: 235ms\tremaining: 63.8ms\n",
      "118:\tlearn: 0.0021004\ttotal: 237ms\tremaining: 61.8ms\n",
      "119:\tlearn: 0.0020833\ttotal: 239ms\tremaining: 59.8ms\n",
      "120:\tlearn: 0.0020338\ttotal: 242ms\tremaining: 57.9ms\n",
      "121:\tlearn: 0.0019926\ttotal: 244ms\tremaining: 56ms\n",
      "122:\tlearn: 0.0019554\ttotal: 246ms\tremaining: 54ms\n",
      "123:\tlearn: 0.0019187\ttotal: 248ms\tremaining: 52ms\n",
      "124:\tlearn: 0.0018859\ttotal: 251ms\tremaining: 50.1ms\n",
      "125:\tlearn: 0.0018434\ttotal: 253ms\tremaining: 48.1ms\n",
      "126:\tlearn: 0.0018186\ttotal: 255ms\tremaining: 46.1ms\n",
      "127:\tlearn: 0.0017998\ttotal: 257ms\tremaining: 44.1ms\n",
      "128:\tlearn: 0.0017362\ttotal: 259ms\tremaining: 42.1ms\n",
      "129:\tlearn: 0.0017082\ttotal: 261ms\tremaining: 40.1ms\n",
      "130:\tlearn: 0.0016808\ttotal: 263ms\tremaining: 38.1ms\n",
      "131:\tlearn: 0.0016234\ttotal: 265ms\tremaining: 36.1ms\n",
      "132:\tlearn: 0.0015891\ttotal: 267ms\tremaining: 34.1ms\n",
      "133:\tlearn: 0.0015545\ttotal: 269ms\tremaining: 32.1ms\n",
      "134:\tlearn: 0.0015177\ttotal: 271ms\tremaining: 30.2ms\n",
      "135:\tlearn: 0.0014915\ttotal: 273ms\tremaining: 28.1ms\n",
      "136:\tlearn: 0.0014646\ttotal: 275ms\tremaining: 26.1ms\n",
      "137:\tlearn: 0.0014471\ttotal: 277ms\tremaining: 24.1ms\n",
      "138:\tlearn: 0.0014322\ttotal: 279ms\tremaining: 22.1ms\n",
      "139:\tlearn: 0.0014027\ttotal: 281ms\tremaining: 20.1ms\n",
      "140:\tlearn: 0.0013762\ttotal: 283ms\tremaining: 18.1ms\n",
      "141:\tlearn: 0.0013606\ttotal: 285ms\tremaining: 16ms\n",
      "142:\tlearn: 0.0013514\ttotal: 287ms\tremaining: 14ms\n",
      "143:\tlearn: 0.0013292\ttotal: 289ms\tremaining: 12ms\n",
      "144:\tlearn: 0.0013145\ttotal: 290ms\tremaining: 10ms\n",
      "145:\tlearn: 0.0012853\ttotal: 293ms\tremaining: 8.02ms\n",
      "146:\tlearn: 0.0012658\ttotal: 295ms\tremaining: 6.01ms\n",
      "147:\tlearn: 0.0012397\ttotal: 296ms\tremaining: 4.01ms\n",
      "148:\tlearn: 0.0012215\ttotal: 298ms\tremaining: 2ms\n",
      "149:\tlearn: 0.0012215\ttotal: 300ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672c1ea2078143619fd6e089be64ed31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6f2a3505ff43fa969c0c35aaff6ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.04213749329448871, Recall = 0.0, Aging Rate = 6.363428105989259e-06, Precision = 0.0, f1 = 0\n",
      "Epoch 2: Train Loss = 0.02645896575981868, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.02583964099916436, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 4: Train Loss = 0.02557642965549665, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 5: Train Loss = 0.025444750080154326, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.024670971747522204, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 6: Train Loss = 0.024733217917791876, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 7: Train Loss = 0.024590228645819737, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 8: Train Loss = 0.02428385153495836, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 9: Train Loss = 0.024158301911544788, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 10: Train Loss = 0.023937983277375913, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02360754161613602, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 11: Train Loss = 0.02371125039623797, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 12: Train Loss = 0.023453772596836734, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 13: Train Loss = 0.023162346303301405, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 14: Train Loss = 0.022934230137445714, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 15: Train Loss = 0.022649588211845317, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.02218545238640158, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 16: Train Loss = 0.02243407548945576, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 17: Train Loss = 0.022029844055302963, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 18: Train Loss = 0.021782551930172984, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 19: Train Loss = 0.021472409163183364, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 20: Train Loss = 0.02113154114853733, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.020604542675915362, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 21: Train Loss = 0.020815388682346794, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 22: Train Loss = 0.02050677767995801, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 23: Train Loss = 0.020177305722002298, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 24: Train Loss = 0.019915876566038975, Recall = 0.0035587188612099642, Aging Rate = 6.363428105989259e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.019638246444199522, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Test Loss = 0.019036886870134856, Recall = 0.021352313167259787, Aging Rate = 3.818056863593555e-05, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.019383968161117735, Recall = 0.0071174377224199285, Aging Rate = 1.9090284317967775e-05, Precision = 0.6666666666666666, f1 = 0.014084507042253521\n",
      "Epoch 27: Train Loss = 0.01903600219787947, Recall = 0.0035587188612099642, Aging Rate = 6.363428105989259e-06, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.01875139494599315, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 29: Train Loss = 0.01843471331592037, Recall = 0.017793594306049824, Aging Rate = 3.181714052994629e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.0181916476522881, Recall = 0.021352313167259787, Aging Rate = 3.818056863593555e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01768478860303067, Recall = 0.0, Aging Rate = 0.0, precision = 0\n",
      "\n",
      "Epoch 31: Train Loss = 0.017967240550567923, Recall = 0.014234875444839857, Aging Rate = 2.5453712423957035e-05, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.01772747148360603, Recall = 0.0071174377224199285, Aging Rate = 1.9090284317967775e-05, Precision = 0.6666666666666666, f1 = 0.014084507042253521\n",
      "Epoch 33: Train Loss = 0.017429632167401926, Recall = 0.021352313167259787, Aging Rate = 4.4543996741924807e-05, Precision = 0.8571428571428571, f1 = 0.04166666666666667\n",
      "Epoch 34: Train Loss = 0.017185350779983188, Recall = 0.02491103202846975, Aging Rate = 5.090742484791407e-05, Precision = 0.875, f1 = 0.04844290657439446\n",
      "Epoch 35: Train Loss = 0.016999410151710668, Recall = 0.046263345195729534, Aging Rate = 8.272456537786036e-05, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01642121623923801, Recall = 0.05693950177935943, Aging Rate = 0.00011454170590780665, precision = 0.8888888888888888\n",
      "\n",
      "Epoch 36: Train Loss = 0.016771584306045292, Recall = 0.03558718861209965, Aging Rate = 7.63611372718711e-05, Precision = 0.8333333333333334, f1 = 0.06825938566552901\n",
      "Epoch 37: Train Loss = 0.016484707384194254, Recall = 0.03914590747330961, Aging Rate = 7.63611372718711e-05, Precision = 0.9166666666666666, f1 = 0.0750853242320819\n",
      "Epoch 38: Train Loss = 0.016359525384905024, Recall = 0.05338078291814947, Aging Rate = 0.00010181484969582814, Precision = 0.9375, f1 = 0.10101010101010101\n",
      "Epoch 39: Train Loss = 0.01608461810392283, Recall = 0.06405693950177936, Aging Rate = 0.00012090513401379592, Precision = 0.9473684210526315, f1 = 0.12000000000000001\n",
      "Epoch 40: Train Loss = 0.015939699086221316, Recall = 0.060498220640569395, Aging Rate = 0.00012090513401379592, Precision = 0.8947368421052632, f1 = 0.11333333333333333\n",
      "Test Loss = 0.01535938410248255, Recall = 0.099644128113879, Aging Rate = 0.00019090284317967775, precision = 0.9333333333333333\n",
      "\n",
      "Epoch 41: Train Loss = 0.01566982860505197, Recall = 0.09252669039145907, Aging Rate = 0.0001845394150736885, Precision = 0.896551724137931, f1 = 0.16774193548387095\n",
      "Epoch 42: Train Loss = 0.015561755288693462, Recall = 0.08185053380782918, Aging Rate = 0.00016544913075572073, Precision = 0.8846153846153846, f1 = 0.1498371335504886\n",
      "Epoch 43: Train Loss = 0.015347215246486126, Recall = 0.07829181494661921, Aging Rate = 0.0001527222745437422, Precision = 0.9166666666666666, f1 = 0.1442622950819672\n",
      "Epoch 44: Train Loss = 0.015203360633709214, Recall = 0.06761565836298933, Aging Rate = 0.00015908570264973148, Precision = 0.76, f1 = 0.12418300653594772\n",
      "Epoch 45: Train Loss = 0.01499124731111988, Recall = 0.10320284697508897, Aging Rate = 0.00020999312749764553, Precision = 0.8787878787878788, f1 = 0.18471337579617833\n",
      "Test Loss = 0.014540869979892544, Recall = 0.06405693950177936, Aging Rate = 0.00012090513401379592, precision = 0.9473684210526315\n",
      "\n",
      "Epoch 46: Train Loss = 0.014857549293529471, Recall = 0.1103202846975089, Aging Rate = 0.0002290834118156133, Precision = 0.8611111111111112, f1 = 0.19558359621451105\n",
      "Epoch 47: Train Loss = 0.014716513767259302, Recall = 0.09608540925266904, Aging Rate = 0.00020362969939165628, Precision = 0.84375, f1 = 0.17252396166134185\n",
      "Epoch 48: Train Loss = 0.014499949991268652, Recall = 0.10676156583629894, Aging Rate = 0.00021635655560363478, Precision = 0.8823529411764706, f1 = 0.19047619047619047\n",
      "Epoch 49: Train Loss = 0.014376391113761124, Recall = 0.09252669039145907, Aging Rate = 0.000197266271285667, Precision = 0.8387096774193549, f1 = 0.16666666666666666\n",
      "Epoch 50: Train Loss = 0.014170454965418532, Recall = 0.11387900355871886, Aging Rate = 0.0002481736961335811, Precision = 0.8205128205128205, f1 = 0.2\n",
      "Test Loss = 0.01388196320978805, Recall = 0.099644128113879, Aging Rate = 0.0001845394150736885, precision = 0.9655172413793104\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.01406935129143293, Recall = 0.12099644128113879, Aging Rate = 0.0002609005523455596, Precision = 0.8292682926829268, f1 = 0.2111801242236025\n",
      "Epoch 52: Train Loss = 0.013926773820518067, Recall = 0.10676156583629894, Aging Rate = 0.00022271998370962406, Precision = 0.8571428571428571, f1 = 0.189873417721519\n",
      "Epoch 53: Train Loss = 0.013770512441045138, Recall = 0.14590747330960854, Aging Rate = 0.0003054445490874844, Precision = 0.8541666666666666, f1 = 0.24924012158054712\n",
      "Epoch 54: Train Loss = 0.013723078691532983, Recall = 0.12099644128113879, Aging Rate = 0.0002799908366635274, Precision = 0.7727272727272727, f1 = 0.20923076923076922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Train Loss = 0.013547540082761874, Recall = 0.12099644128113879, Aging Rate = 0.0002609005523455596, Precision = 0.8292682926829268, f1 = 0.2111801242236025\n",
      "Test Loss = 0.012950649562657556, Recall = 0.13523131672597866, Aging Rate = 0.0003054445490874844, precision = 0.7916666666666666\n",
      "\n",
      "Epoch 56: Train Loss = 0.013391857715898191, Recall = 0.1423487544483986, Aging Rate = 0.00031180797719347365, Precision = 0.8163265306122449, f1 = 0.24242424242424246\n",
      "Epoch 57: Train Loss = 0.01332698140290924, Recall = 0.1387900355871886, Aging Rate = 0.00031180797719347365, Precision = 0.7959183673469388, f1 = 0.23636363636363636\n",
      "Epoch 58: Train Loss = 0.013143302314005961, Recall = 0.1494661921708185, Aging Rate = 0.00031180797719347365, Precision = 0.8571428571428571, f1 = 0.25454545454545446\n",
      "Epoch 59: Train Loss = 0.013071311716135904, Recall = 0.13167259786476868, Aging Rate = 0.00029908112098149514, Precision = 0.7872340425531915, f1 = 0.22560975609756095\n",
      "Epoch 60: Train Loss = 0.012961514538525733, Recall = 0.1494661921708185, Aging Rate = 0.00029908112098149514, Precision = 0.8936170212765957, f1 = 0.2560975609756097\n",
      "Test Loss = 0.012462384993796486, Recall = 0.1601423487544484, Aging Rate = 0.000369078830147377, precision = 0.7758620689655172\n",
      "\n",
      "Epoch 61: Train Loss = 0.012844033223185239, Recall = 0.1601423487544484, Aging Rate = 0.00034362511772341995, Precision = 0.8333333333333334, f1 = 0.26865671641791045\n",
      "Epoch 62: Train Loss = 0.012742436823489437, Recall = 0.15658362989323843, Aging Rate = 0.00031817140529946295, Precision = 0.88, f1 = 0.2658610271903323\n",
      "Epoch 63: Train Loss = 0.012681118090473389, Recall = 0.1387900355871886, Aging Rate = 0.0002927176928755059, Precision = 0.8478260869565217, f1 = 0.2385321100917431\n",
      "Epoch 64: Train Loss = 0.012545871362899658, Recall = 0.16725978647686832, Aging Rate = 0.000369078830147377, Precision = 0.8103448275862069, f1 = 0.2772861356932153\n",
      "Epoch 65: Train Loss = 0.012452687825328913, Recall = 0.17793594306049823, Aging Rate = 0.000369078830147377, Precision = 0.8620689655172413, f1 = 0.2949852507374631\n",
      "Test Loss = 0.011884078055401485, Recall = 0.1708185053380783, Aging Rate = 0.00034362511772341995, precision = 0.8888888888888888\n",
      "\n",
      "Epoch 66: Train Loss = 0.01234429292893266, Recall = 0.1601423487544484, Aging Rate = 0.00038816911446534476, Precision = 0.7377049180327869, f1 = 0.2631578947368421\n",
      "Epoch 67: Train Loss = 0.01221964652551443, Recall = 0.18505338078291814, Aging Rate = 0.0003818056863593555, Precision = 0.8666666666666667, f1 = 0.3049853372434017\n",
      "Epoch 68: Train Loss = 0.012128990973874168, Recall = 0.19572953736654805, Aging Rate = 0.00040725939878331256, Precision = 0.859375, f1 = 0.31884057971014496\n",
      "Epoch 69: Train Loss = 0.012092331363277985, Recall = 0.16725978647686832, Aging Rate = 0.0003499885458294092, Precision = 0.8545454545454545, f1 = 0.2797619047619047\n",
      "Epoch 70: Train Loss = 0.011991932004475563, Recall = 0.19572953736654805, Aging Rate = 0.0004008959706773233, Precision = 0.873015873015873, f1 = 0.3197674418604652\n",
      "Test Loss = 0.011516951614304364, Recall = 0.1494661921708185, Aging Rate = 0.00028635426476951664, precision = 0.9333333333333333\n",
      "\n",
      "Epoch 71: Train Loss = 0.011925369757531624, Recall = 0.15302491103202848, Aging Rate = 0.00031817140529946295, Precision = 0.86, f1 = 0.25981873111782483\n",
      "Epoch 72: Train Loss = 0.011827633052568757, Recall = 0.2099644128113879, Aging Rate = 0.00045180339552523737, Precision = 0.8309859154929577, f1 = 0.33522727272727276\n",
      "Epoch 73: Train Loss = 0.011757505844493218, Recall = 0.199288256227758, Aging Rate = 0.0004263496831012803, Precision = 0.835820895522388, f1 = 0.3218390804597701\n",
      "Epoch 74: Train Loss = 0.011701272018119392, Recall = 0.19572953736654805, Aging Rate = 0.0004263496831012803, Precision = 0.8208955223880597, f1 = 0.3160919540229885\n",
      "Epoch 75: Train Loss = 0.01161931871539324, Recall = 0.199288256227758, Aging Rate = 0.00041998625499529106, Precision = 0.8484848484848485, f1 = 0.3227665706051873\n",
      "Test Loss = 0.01116589064714038, Recall = 0.24555160142348753, Aging Rate = 0.0005090742484791407, precision = 0.8625\n",
      "\n",
      "Epoch 76: Train Loss = 0.011503262958492038, Recall = 0.20284697508896798, Aging Rate = 0.00045180339552523737, Precision = 0.8028169014084507, f1 = 0.32386363636363635\n",
      "Epoch 77: Train Loss = 0.011473258858073616, Recall = 0.20640569395017794, Aging Rate = 0.0004454399674192481, Precision = 0.8285714285714286, f1 = 0.33048433048433046\n",
      "Epoch 78: Train Loss = 0.01136415500974958, Recall = 0.2277580071174377, Aging Rate = 0.0004836205360551837, Precision = 0.8421052631578947, f1 = 0.3585434173669468\n",
      "Epoch 79: Train Loss = 0.011277694454474877, Recall = 0.20640569395017794, Aging Rate = 0.0004454399674192481, Precision = 0.8285714285714286, f1 = 0.33048433048433046\n",
      "Epoch 80: Train Loss = 0.011264616912376757, Recall = 0.21708185053380782, Aging Rate = 0.00046453025173721587, Precision = 0.8356164383561644, f1 = 0.34463276836158185\n",
      "Test Loss = 0.010736863641812696, Recall = 0.23843416370106763, Aging Rate = 0.0004963473922671622, precision = 0.8589743589743589\n",
      "\n",
      "Epoch 81: Train Loss = 0.011268359196286777, Recall = 0.19572953736654805, Aging Rate = 0.0004263496831012803, Precision = 0.8208955223880597, f1 = 0.3160919540229885\n",
      "Epoch 82: Train Loss = 0.011151071948572476, Recall = 0.19217081850533807, Aging Rate = 0.0004263496831012803, Precision = 0.8059701492537313, f1 = 0.3103448275862069\n",
      "Epoch 83: Train Loss = 0.01100254921074593, Recall = 0.21352313167259787, Aging Rate = 0.00045180339552523737, Precision = 0.8450704225352113, f1 = 0.3409090909090909\n",
      "Epoch 84: Train Loss = 0.010992837459679172, Recall = 0.2277580071174377, Aging Rate = 0.0004708936798432051, Precision = 0.8648648648648649, f1 = 0.36056338028169016\n",
      "Epoch 85: Train Loss = 0.010953412171769874, Recall = 0.20284697508896798, Aging Rate = 0.0004263496831012803, Precision = 0.8507462686567164, f1 = 0.32758620689655177\n",
      "Test Loss = 0.010432961204170036, Recall = 0.2526690391459075, Aging Rate = 0.0005345279609030977, precision = 0.8452380952380952\n",
      "\n",
      "Epoch 86: Train Loss = 0.010903082984622737, Recall = 0.21352313167259787, Aging Rate = 0.00045180339552523737, Precision = 0.8450704225352113, f1 = 0.3409090909090909\n",
      "Epoch 87: Train Loss = 0.010862497581370697, Recall = 0.21708185053380782, Aging Rate = 0.0004390765393132588, Precision = 0.8840579710144928, f1 = 0.34857142857142853\n",
      "Epoch 88: Train Loss = 0.010836274208425823, Recall = 0.2206405693950178, Aging Rate = 0.00047725710794919437, Precision = 0.8266666666666667, f1 = 0.34831460674157305\n",
      "Epoch 89: Train Loss = 0.010763125715550747, Recall = 0.2206405693950178, Aging Rate = 0.0004581668236312266, Precision = 0.8611111111111112, f1 = 0.3512747875354108\n",
      "Epoch 90: Train Loss = 0.01069343915678083, Recall = 0.23487544483985764, Aging Rate = 0.0004708936798432051, Precision = 0.8918918918918919, f1 = 0.37183098591549296\n",
      "Test Loss = 0.0103779591886544, Recall = 0.17437722419928825, Aging Rate = 0.0003499885458294092, precision = 0.8909090909090909\n",
      "\n",
      "Epoch 91: Train Loss = 0.010676178551132105, Recall = 0.23487544483985764, Aging Rate = 0.00047725710794919437, Precision = 0.88, f1 = 0.37078651685393255\n",
      "Epoch 92: Train Loss = 0.010570121836526468, Recall = 0.2206405693950178, Aging Rate = 0.00047725710794919437, Precision = 0.8266666666666667, f1 = 0.34831460674157305\n",
      "Epoch 93: Train Loss = 0.010646430580796357, Recall = 0.23487544483985764, Aging Rate = 0.00047725710794919437, Precision = 0.88, f1 = 0.37078651685393255\n",
      "Epoch 94: Train Loss = 0.010497241852622059, Recall = 0.21708185053380782, Aging Rate = 0.00045180339552523737, Precision = 0.8591549295774648, f1 = 0.34659090909090906\n",
      "Epoch 95: Train Loss = 0.010515831252469108, Recall = 0.2277580071174377, Aging Rate = 0.0004708936798432051, Precision = 0.8648648648648649, f1 = 0.36056338028169016\n",
      "Test Loss = 0.010024489637105002, Recall = 0.27402135231316727, Aging Rate = 0.0005790719576450225, precision = 0.8461538461538461\n",
      "\n",
      "Epoch 96: Train Loss = 0.010474635222887385, Recall = 0.2313167259786477, Aging Rate = 0.0004899839641611729, Precision = 0.8441558441558441, f1 = 0.36312849162011174\n",
      "Epoch 97: Train Loss = 0.010467246246195857, Recall = 0.2313167259786477, Aging Rate = 0.0004899839641611729, Precision = 0.8441558441558441, f1 = 0.36312849162011174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: Train Loss = 0.01037117485601417, Recall = 0.23487544483985764, Aging Rate = 0.0004899839641611729, Precision = 0.8571428571428571, f1 = 0.3687150837988827\n",
      "Epoch 99: Train Loss = 0.010370436776600689, Recall = 0.2277580071174377, Aging Rate = 0.00047725710794919437, Precision = 0.8533333333333334, f1 = 0.35955056179775274\n",
      "Epoch 100: Train Loss = 0.010314091075258155, Recall = 0.2277580071174377, Aging Rate = 0.0004836205360551837, Precision = 0.8421052631578947, f1 = 0.3585434173669468\n",
      "Test Loss = 0.009814923604270834, Recall = 0.2597864768683274, Aging Rate = 0.0005281645327971084, precision = 0.8795180722891566\n",
      "\n",
      "Epoch 101: Train Loss = 0.010289194575780286, Recall = 0.2313167259786477, Aging Rate = 0.0004963473922671622, Precision = 0.8333333333333334, f1 = 0.36211699164345407\n",
      "Epoch 102: Train Loss = 0.010229170549761007, Recall = 0.24555160142348753, Aging Rate = 0.0005154376765851299, Precision = 0.8518518518518519, f1 = 0.3812154696132597\n",
      "Epoch 103: Train Loss = 0.010151924037622967, Recall = 0.2526690391459075, Aging Rate = 0.0005345279609030977, Precision = 0.8452380952380952, f1 = 0.3890410958904109\n",
      "Epoch 104: Train Loss = 0.01020121236965732, Recall = 0.23843416370106763, Aging Rate = 0.0005027108203731514, Precision = 0.8481012658227848, f1 = 0.37222222222222223\n",
      "Epoch 105: Train Loss = 0.010168188378569662, Recall = 0.24199288256227758, Aging Rate = 0.0005027108203731514, Precision = 0.8607594936708861, f1 = 0.37777777777777777\n",
      "Test Loss = 0.009621627838034955, Recall = 0.29537366548042704, Aging Rate = 0.0005981622419629903, precision = 0.8829787234042553\n",
      "Model in epoch 105 is saved.\n",
      "\n",
      "Epoch 106: Train Loss = 0.010078671342589569, Recall = 0.2491103202846975, Aging Rate = 0.0005027108203731514, Precision = 0.8860759493670886, f1 = 0.3888888888888889\n",
      "Epoch 107: Train Loss = 0.010030252696852935, Recall = 0.2704626334519573, Aging Rate = 0.0005599816733270548, Precision = 0.8636363636363636, f1 = 0.4119241192411924\n",
      "Epoch 108: Train Loss = 0.01005069051439573, Recall = 0.24199288256227758, Aging Rate = 0.0004963473922671622, Precision = 0.8717948717948718, f1 = 0.37883008356545955\n",
      "Epoch 109: Train Loss = 0.009977675479466883, Recall = 0.24199288256227758, Aging Rate = 0.0005090742484791407, Precision = 0.85, f1 = 0.3767313019390582\n",
      "Epoch 110: Train Loss = 0.009995481967554617, Recall = 0.24555160142348753, Aging Rate = 0.0005345279609030977, Precision = 0.8214285714285714, f1 = 0.3780821917808219\n",
      "Test Loss = 0.009435292441171706, Recall = 0.2669039145907473, Aging Rate = 0.0005345279609030977, precision = 0.8928571428571429\n",
      "\n",
      "Epoch 111: Train Loss = 0.00993669363257929, Recall = 0.2526690391459075, Aging Rate = 0.0005218011046911192, Precision = 0.8658536585365854, f1 = 0.39118457300275483\n",
      "Epoch 112: Train Loss = 0.009886372551484912, Recall = 0.2597864768683274, Aging Rate = 0.0005472548171150763, Precision = 0.8488372093023255, f1 = 0.3978201634877384\n",
      "Epoch 113: Train Loss = 0.009811320137189931, Recall = 0.2526690391459075, Aging Rate = 0.0005218011046911192, Precision = 0.8658536585365854, f1 = 0.39118457300275483\n",
      "Epoch 114: Train Loss = 0.0098109724747167, Recall = 0.25622775800711745, Aging Rate = 0.0005472548171150763, Precision = 0.8372093023255814, f1 = 0.3923705722070845\n",
      "Epoch 115: Train Loss = 0.009819626060355037, Recall = 0.2597864768683274, Aging Rate = 0.0005408913890090869, Precision = 0.8588235294117647, f1 = 0.39890710382513656\n",
      "Test Loss = 0.009263312663486328, Recall = 0.28113879003558717, Aging Rate = 0.000566345101433044, precision = 0.8876404494382022\n",
      "\n",
      "Epoch 116: Train Loss = 0.00974995242654237, Recall = 0.2597864768683274, Aging Rate = 0.0005472548171150763, Precision = 0.8488372093023255, f1 = 0.3978201634877384\n",
      "Epoch 117: Train Loss = 0.009765833131885768, Recall = 0.2491103202846975, Aging Rate = 0.0005281645327971084, Precision = 0.8433734939759037, f1 = 0.3846153846153846\n",
      "Epoch 118: Train Loss = 0.0097185058808832, Recall = 0.2669039145907473, Aging Rate = 0.0005536182452210655, Precision = 0.8620689655172413, f1 = 0.4076086956521739\n",
      "Epoch 119: Train Loss = 0.009608899124138228, Recall = 0.2704626334519573, Aging Rate = 0.0005345279609030977, Precision = 0.9047619047619048, f1 = 0.4164383561643836\n",
      "Epoch 120: Train Loss = 0.009646378833108397, Recall = 0.2918149466192171, Aging Rate = 0.0005854353857510118, Precision = 0.8913043478260869, f1 = 0.43967828418230565\n",
      "Test Loss = 0.009308659735419408, Recall = 0.18861209964412812, Aging Rate = 0.00036271540204138776, precision = 0.9298245614035088\n",
      "\n",
      "Epoch 121: Train Loss = 0.009628178136196593, Recall = 0.24199288256227758, Aging Rate = 0.0005027108203731514, Precision = 0.8607594936708861, f1 = 0.37777777777777777\n",
      "Epoch 122: Train Loss = 0.00956403728773452, Recall = 0.28113879003558717, Aging Rate = 0.0005854353857510118, Precision = 0.8586956521739131, f1 = 0.4235924932975872\n",
      "Epoch 123: Train Loss = 0.009501762322323009, Recall = 0.28113879003558717, Aging Rate = 0.000566345101433044, Precision = 0.8876404494382022, f1 = 0.42702702702702694\n",
      "Epoch 124: Train Loss = 0.009552252721112677, Recall = 0.2704626334519573, Aging Rate = 0.0005536182452210655, Precision = 0.8735632183908046, f1 = 0.41304347826086957\n",
      "Epoch 125: Train Loss = 0.009437446460892646, Recall = 0.2597864768683274, Aging Rate = 0.0005218011046911192, Precision = 0.8902439024390244, f1 = 0.40220385674931125\n",
      "Test Loss = 0.008994810396935454, Recall = 0.30604982206405695, Aging Rate = 0.0006045256700689795, precision = 0.9052631578947369\n",
      "Model in epoch 125 is saved.\n",
      "\n",
      "Epoch 126: Train Loss = 0.009538639252636882, Recall = 0.2846975088967972, Aging Rate = 0.0005790719576450225, Precision = 0.8791208791208791, f1 = 0.4301075268817205\n",
      "Epoch 127: Train Loss = 0.009499777329213126, Recall = 0.2846975088967972, Aging Rate = 0.0005727085295390333, Precision = 0.8888888888888888, f1 = 0.431266846361186\n",
      "Epoch 128: Train Loss = 0.009398727122987362, Recall = 0.2918149466192171, Aging Rate = 0.0005727085295390333, Precision = 0.9111111111111111, f1 = 0.44204851752021557\n",
      "Epoch 129: Train Loss = 0.009420396301562411, Recall = 0.2669039145907473, Aging Rate = 0.0005472548171150763, Precision = 0.872093023255814, f1 = 0.4087193460490463\n",
      "Epoch 130: Train Loss = 0.009360476150131311, Recall = 0.2669039145907473, Aging Rate = 0.0005599816733270548, Precision = 0.8522727272727273, f1 = 0.4065040650406504\n",
      "Test Loss = 0.008953764057601182, Recall = 0.2669039145907473, Aging Rate = 0.0005154376765851299, precision = 0.9259259259259259\n",
      "Model in epoch 130 is saved.\n",
      "\n",
      "Epoch 131: Train Loss = 0.00940952750038401, Recall = 0.298932384341637, Aging Rate = 0.0006045256700689795, Precision = 0.8842105263157894, f1 = 0.44680851063829785\n",
      "Epoch 132: Train Loss = 0.00935029371609639, Recall = 0.2775800711743772, Aging Rate = 0.0005727085295390333, Precision = 0.8666666666666667, f1 = 0.42048517520215634\n",
      "Epoch 133: Train Loss = 0.009336363869224897, Recall = 0.28113879003558717, Aging Rate = 0.0005727085295390333, Precision = 0.8777777777777778, f1 = 0.42587601078167114\n",
      "Epoch 134: Train Loss = 0.00937254146893864, Recall = 0.2846975088967972, Aging Rate = 0.0005727085295390333, Precision = 0.8888888888888888, f1 = 0.431266846361186\n",
      "Epoch 135: Train Loss = 0.009331457054643923, Recall = 0.27402135231316727, Aging Rate = 0.0005536182452210655, Precision = 0.8850574712643678, f1 = 0.4184782608695652\n",
      "Test Loss = 0.008985124954552075, Recall = 0.38434163701067614, Aging Rate = 0.0007699748008247003, precision = 0.8925619834710744\n",
      "Model in epoch 135 is saved.\n",
      "\n",
      "Epoch 136: Train Loss = 0.00931328121340489, Recall = 0.2846975088967972, Aging Rate = 0.0005727085295390333, Precision = 0.8888888888888888, f1 = 0.431266846361186\n",
      "Epoch 137: Train Loss = 0.00929441256782283, Recall = 0.298932384341637, Aging Rate = 0.0006108890981749688, Precision = 0.875, f1 = 0.44562334217506633\n",
      "Epoch 138: Train Loss = 0.009201203482086385, Recall = 0.29537366548042704, Aging Rate = 0.0005790719576450225, Precision = 0.9120879120879121, f1 = 0.44623655913978494\n",
      "Epoch 139: Train Loss = 0.009234320989018021, Recall = 0.28113879003558717, Aging Rate = 0.000566345101433044, Precision = 0.8876404494382022, f1 = 0.42702702702702694\n",
      "Epoch 140: Train Loss = 0.009179420547869358, Recall = 0.28113879003558717, Aging Rate = 0.0005536182452210655, Precision = 0.9080459770114943, f1 = 0.42934782608695654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.008803160599777139, Recall = 0.2526690391459075, Aging Rate = 0.0004899839641611729, precision = 0.922077922077922\n",
      "\n",
      "Epoch 141: Train Loss = 0.009213478239185662, Recall = 0.29537366548042704, Aging Rate = 0.0005981622419629903, Precision = 0.8829787234042553, f1 = 0.44266666666666665\n",
      "Epoch 142: Train Loss = 0.009194568979648605, Recall = 0.2918149466192171, Aging Rate = 0.0005790719576450225, Precision = 0.9010989010989011, f1 = 0.44086021505376344\n",
      "Epoch 143: Train Loss = 0.009105509881637656, Recall = 0.28113879003558717, Aging Rate = 0.0005727085295390333, Precision = 0.8777777777777778, f1 = 0.42587601078167114\n",
      "Epoch 144: Train Loss = 0.009186161536273491, Recall = 0.2918149466192171, Aging Rate = 0.0005981622419629903, Precision = 0.8723404255319149, f1 = 0.4373333333333334\n",
      "Epoch 145: Train Loss = 0.009093183567583875, Recall = 0.2846975088967972, Aging Rate = 0.0005854353857510118, Precision = 0.8695652173913043, f1 = 0.4289544235924933\n",
      "Test Loss = 0.008649611511464076, Recall = 0.31316725978647686, Aging Rate = 0.0006236159543869473, precision = 0.8979591836734694\n",
      "\n",
      "Epoch 146: Train Loss = 0.009123867816566095, Recall = 0.28113879003558717, Aging Rate = 0.0005536182452210655, Precision = 0.9080459770114943, f1 = 0.42934782608695654\n",
      "Epoch 147: Train Loss = 0.009095310689819098, Recall = 0.2775800711743772, Aging Rate = 0.0005408913890090869, Precision = 0.9176470588235294, f1 = 0.4262295081967213\n",
      "Epoch 148: Train Loss = 0.009038893350282783, Recall = 0.27402135231316727, Aging Rate = 0.0005408913890090869, Precision = 0.9058823529411765, f1 = 0.42076502732240434\n",
      "Epoch 149: Train Loss = 0.00902177384439854, Recall = 0.29537366548042704, Aging Rate = 0.0005981622419629903, Precision = 0.8829787234042553, f1 = 0.44266666666666665\n",
      "Epoch 150: Train Loss = 0.009076498089750112, Recall = 0.2846975088967972, Aging Rate = 0.0005727085295390333, Precision = 0.8888888888888888, f1 = 0.431266846361186\n",
      "Test Loss = 0.008591250329356922, Recall = 0.37722419928825623, Aging Rate = 0.0007508845165067325, precision = 0.8983050847457628\n",
      "Model in epoch 150 is saved.\n",
      "\n",
      "Epoch 151: Train Loss = 0.009066733308788487, Recall = 0.31316725978647686, Aging Rate = 0.0006236159543869473, Precision = 0.8979591836734694, f1 = 0.4643799472295514\n",
      "Epoch 152: Train Loss = 0.00899660504294313, Recall = 0.28113879003558717, Aging Rate = 0.0005408913890090869, Precision = 0.9294117647058824, f1 = 0.4316939890710382\n",
      "Epoch 153: Train Loss = 0.008949670374920818, Recall = 0.29537366548042704, Aging Rate = 0.0005854353857510118, Precision = 0.9021739130434783, f1 = 0.44504021447721176\n",
      "Epoch 154: Train Loss = 0.008916417879319408, Recall = 0.2775800711743772, Aging Rate = 0.0005281645327971084, Precision = 0.9397590361445783, f1 = 0.4285714285714286\n",
      "Epoch 155: Train Loss = 0.00891749982856965, Recall = 0.302491103202847, Aging Rate = 0.000591798813857001, Precision = 0.9139784946236559, f1 = 0.45454545454545453\n",
      "Test Loss = 0.008433893936120056, Recall = 0.30604982206405695, Aging Rate = 0.0005981622419629903, precision = 0.9148936170212766\n",
      "\n",
      "Epoch 156: Train Loss = 0.008912139989623516, Recall = 0.28113879003558717, Aging Rate = 0.000566345101433044, Precision = 0.8876404494382022, f1 = 0.42702702702702694\n",
      "Epoch 157: Train Loss = 0.008907836652038164, Recall = 0.2918149466192171, Aging Rate = 0.0006045256700689795, Precision = 0.8631578947368421, f1 = 0.43617021276595747\n",
      "Epoch 158: Train Loss = 0.008908853295807217, Recall = 0.30604982206405695, Aging Rate = 0.0006045256700689795, Precision = 0.9052631578947369, f1 = 0.4574468085106383\n",
      "Epoch 159: Train Loss = 0.00883183258135557, Recall = 0.29537366548042704, Aging Rate = 0.0006045256700689795, Precision = 0.8736842105263158, f1 = 0.4414893617021276\n",
      "Epoch 160: Train Loss = 0.008844255940722636, Recall = 0.30604982206405695, Aging Rate = 0.000591798813857001, Precision = 0.9247311827956989, f1 = 0.45989304812834225\n",
      "Test Loss = 0.008340846954951544, Recall = 0.35587188612099646, Aging Rate = 0.0007063405197648077, precision = 0.9009009009009009\n",
      "\n",
      "Epoch 161: Train Loss = 0.008844346930469397, Recall = 0.302491103202847, Aging Rate = 0.0006108890981749688, Precision = 0.8854166666666666, f1 = 0.45092838196286483\n",
      "Epoch 162: Train Loss = 0.008779122211273112, Recall = 0.3167259786476868, Aging Rate = 0.000617252526280958, Precision = 0.9175257731958762, f1 = 0.47089947089947093\n",
      "Epoch 163: Train Loss = 0.008795517437180591, Recall = 0.30604982206405695, Aging Rate = 0.0006108890981749688, Precision = 0.8958333333333334, f1 = 0.45623342175066317\n",
      "Epoch 164: Train Loss = 0.008740234128715826, Recall = 0.298932384341637, Aging Rate = 0.0005981622419629903, Precision = 0.8936170212765957, f1 = 0.44799999999999995\n",
      "Epoch 165: Train Loss = 0.008731511478110649, Recall = 0.3096085409252669, Aging Rate = 0.0006299793824929365, Precision = 0.8787878787878788, f1 = 0.45789473684210535\n",
      "Test Loss = 0.00835129556515848, Recall = 0.4199288256227758, Aging Rate = 0.0008399725099905821, precision = 0.8939393939393939\n",
      "Model in epoch 165 is saved.\n",
      "\n",
      "Epoch 166: Train Loss = 0.008612438363168324, Recall = 0.31316725978647686, Aging Rate = 0.0006108890981749688, Precision = 0.9166666666666666, f1 = 0.4668435013262599\n",
      "Epoch 167: Train Loss = 0.008692950811208768, Recall = 0.3238434163701068, Aging Rate = 0.0006554330949168937, Precision = 0.883495145631068, f1 = 0.47395833333333337\n",
      "Epoch 168: Train Loss = 0.008653622103959698, Recall = 0.29537366548042704, Aging Rate = 0.0005727085295390333, Precision = 0.9222222222222223, f1 = 0.44743935309973043\n",
      "Epoch 169: Train Loss = 0.008618674964324013, Recall = 0.3167259786476868, Aging Rate = 0.0006617965230228829, Precision = 0.8557692307692307, f1 = 0.46233766233766227\n",
      "Epoch 170: Train Loss = 0.008623709771241198, Recall = 0.31316725978647686, Aging Rate = 0.0006427062387049152, Precision = 0.8712871287128713, f1 = 0.46073298429319376\n",
      "Test Loss = 0.00815086770816438, Recall = 0.3736654804270463, Aging Rate = 0.000738157660294754, precision = 0.9051724137931034\n",
      "\n",
      "Epoch 171: Train Loss = 0.008561799399510921, Recall = 0.3096085409252669, Aging Rate = 0.0006236159543869473, Precision = 0.8877551020408163, f1 = 0.45910290237467016\n",
      "Epoch 172: Train Loss = 0.00849901639923805, Recall = 0.3202846975088968, Aging Rate = 0.0006363428105989259, Precision = 0.9, f1 = 0.4724409448818898\n",
      "Epoch 173: Train Loss = 0.008490273646468203, Recall = 0.33451957295373663, Aging Rate = 0.0006681599511288722, Precision = 0.8952380952380953, f1 = 0.48704663212435234\n",
      "Epoch 174: Train Loss = 0.008548641723840022, Recall = 0.30604982206405695, Aging Rate = 0.0006236159543869473, Precision = 0.8775510204081632, f1 = 0.4538258575197889\n",
      "Epoch 175: Train Loss = 0.008512809973275382, Recall = 0.3096085409252669, Aging Rate = 0.0006108890981749688, Precision = 0.90625, f1 = 0.46153846153846156\n",
      "Test Loss = 0.007946457199157889, Recall = 0.34519572953736655, Aging Rate = 0.0006617965230228829, precision = 0.9326923076923077\n",
      "Model in epoch 175 is saved.\n",
      "\n",
      "Epoch 176: Train Loss = 0.008432312391676291, Recall = 0.3274021352313167, Aging Rate = 0.0006554330949168937, Precision = 0.8932038834951457, f1 = 0.47916666666666663\n",
      "Epoch 177: Train Loss = 0.0084331903007473, Recall = 0.3274021352313167, Aging Rate = 0.0006490696668109044, Precision = 0.9019607843137255, f1 = 0.4804177545691906\n",
      "Epoch 178: Train Loss = 0.008379205346831476, Recall = 0.33451957295373663, Aging Rate = 0.0006490696668109044, Precision = 0.9215686274509803, f1 = 0.4908616187989556\n",
      "Epoch 179: Train Loss = 0.00842775372180747, Recall = 0.3274021352313167, Aging Rate = 0.0006490696668109044, Precision = 0.9019607843137255, f1 = 0.4804177545691906\n",
      "Epoch 180: Train Loss = 0.00838951859324503, Recall = 0.30604982206405695, Aging Rate = 0.0006236159543869473, Precision = 0.8775510204081632, f1 = 0.4538258575197889\n",
      "Test Loss = 0.007855541713776847, Recall = 0.3096085409252669, Aging Rate = 0.000591798813857001, precision = 0.9354838709677419\n",
      "\n",
      "Epoch 181: Train Loss = 0.00837428370119867, Recall = 0.3096085409252669, Aging Rate = 0.0006045256700689795, Precision = 0.9157894736842105, f1 = 0.4627659574468085\n",
      "Epoch 182: Train Loss = 0.008273543628140104, Recall = 0.3487544483985765, Aging Rate = 0.0006936136635528292, Precision = 0.8990825688073395, f1 = 0.5025641025641026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183: Train Loss = 0.008295159878224026, Recall = 0.3274021352313167, Aging Rate = 0.0006490696668109044, Precision = 0.9019607843137255, f1 = 0.4804177545691906\n",
      "Epoch 184: Train Loss = 0.008345752907631843, Recall = 0.31316725978647686, Aging Rate = 0.0006299793824929365, Precision = 0.8888888888888888, f1 = 0.4631578947368421\n",
      "Epoch 185: Train Loss = 0.008229960927012664, Recall = 0.3096085409252669, Aging Rate = 0.0006427062387049152, Precision = 0.8613861386138614, f1 = 0.4554973821989528\n",
      "Test Loss = 0.007748497740745708, Recall = 0.3736654804270463, Aging Rate = 0.0007254308040827755, precision = 0.9210526315789473\n",
      "Model in epoch 185 is saved.\n",
      "\n",
      "Epoch 186: Train Loss = 0.008254968774093714, Recall = 0.33807829181494664, Aging Rate = 0.0006745233792348614, Precision = 0.8962264150943396, f1 = 0.49095607235142125\n",
      "Epoch 187: Train Loss = 0.008185494506318582, Recall = 0.3416370106761566, Aging Rate = 0.0006872502354468399, Precision = 0.8888888888888888, f1 = 0.49357326478149094\n",
      "Epoch 188: Train Loss = 0.008224672512388238, Recall = 0.3416370106761566, Aging Rate = 0.0006808868073408507, Precision = 0.897196261682243, f1 = 0.49484536082474234\n",
      "Epoch 189: Train Loss = 0.008232557294758552, Recall = 0.3202846975088968, Aging Rate = 0.0006617965230228829, Precision = 0.8653846153846154, f1 = 0.4675324675324675\n",
      "Epoch 190: Train Loss = 0.008158258133846932, Recall = 0.31316725978647686, Aging Rate = 0.0006236159543869473, Precision = 0.8979591836734694, f1 = 0.4643799472295514\n",
      "Test Loss = 0.0077080235739426925, Recall = 0.3202846975088968, Aging Rate = 0.0006108890981749688, precision = 0.9375\n",
      "\n",
      "Epoch 191: Train Loss = 0.008144960198578619, Recall = 0.3274021352313167, Aging Rate = 0.0006427062387049152, Precision = 0.9108910891089109, f1 = 0.4816753926701571\n",
      "Epoch 192: Train Loss = 0.008096159214850084, Recall = 0.3202846975088968, Aging Rate = 0.0006363428105989259, Precision = 0.9, f1 = 0.4724409448818898\n",
      "Epoch 193: Train Loss = 0.00811219213189333, Recall = 0.33451957295373663, Aging Rate = 0.0006681599511288722, Precision = 0.8952380952380953, f1 = 0.48704663212435234\n",
      "Epoch 194: Train Loss = 0.008086547151229117, Recall = 0.3238434163701068, Aging Rate = 0.0006427062387049152, Precision = 0.900990099009901, f1 = 0.4764397905759162\n",
      "Epoch 195: Train Loss = 0.008051825643762762, Recall = 0.3487544483985765, Aging Rate = 0.0006936136635528292, Precision = 0.8990825688073395, f1 = 0.5025641025641026\n",
      "Test Loss = 0.007658739178710523, Recall = 0.3096085409252669, Aging Rate = 0.000591798813857001, precision = 0.9354838709677419\n",
      "\n",
      "Epoch 196: Train Loss = 0.008058297293510005, Recall = 0.33451957295373663, Aging Rate = 0.0006554330949168937, Precision = 0.912621359223301, f1 = 0.4895833333333333\n",
      "Epoch 197: Train Loss = 0.00805126566849595, Recall = 0.33451957295373663, Aging Rate = 0.0006299793824929365, Precision = 0.9494949494949495, f1 = 0.49473684210526314\n",
      "Epoch 198: Train Loss = 0.007973842839777739, Recall = 0.35587188612099646, Aging Rate = 0.0006936136635528292, Precision = 0.9174311926605505, f1 = 0.5128205128205129\n",
      "Epoch 199: Train Loss = 0.007991952075641039, Recall = 0.3416370106761566, Aging Rate = 0.0006681599511288722, Precision = 0.9142857142857143, f1 = 0.49740932642487046\n",
      "Epoch 200: Train Loss = 0.007984557301521599, Recall = 0.3416370106761566, Aging Rate = 0.0006617965230228829, Precision = 0.9230769230769231, f1 = 0.4987012987012987\n",
      "Test Loss = 0.0076088314213578, Recall = 0.4377224199288256, Aging Rate = 0.0008399725099905821, precision = 0.9318181818181818\n",
      "Model in epoch 200 is saved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2834823\ttotal: 58.3ms\tremaining: 17.4s\n",
      "1:\tlearn: 0.1248071\ttotal: 89ms\tremaining: 13.3s\n",
      "2:\tlearn: 0.0632787\ttotal: 121ms\tremaining: 12s\n",
      "3:\tlearn: 0.0372976\ttotal: 220ms\tremaining: 16.2s\n",
      "4:\tlearn: 0.0256415\ttotal: 260ms\tremaining: 15.3s\n",
      "5:\tlearn: 0.0196792\ttotal: 337ms\tremaining: 16.5s\n",
      "6:\tlearn: 0.0165642\ttotal: 355ms\tremaining: 14.8s\n",
      "7:\tlearn: 0.0148424\ttotal: 448ms\tremaining: 16.4s\n",
      "8:\tlearn: 0.0134937\ttotal: 494ms\tremaining: 16s\n",
      "9:\tlearn: 0.0128553\ttotal: 537ms\tremaining: 15.6s\n",
      "10:\tlearn: 0.0124583\ttotal: 571ms\tremaining: 15s\n",
      "11:\tlearn: 0.0122136\ttotal: 617ms\tremaining: 14.8s\n",
      "12:\tlearn: 0.0120260\ttotal: 732ms\tremaining: 16.2s\n",
      "13:\tlearn: 0.0116557\ttotal: 762ms\tremaining: 15.6s\n",
      "14:\tlearn: 0.0115480\ttotal: 829ms\tremaining: 15.7s\n",
      "15:\tlearn: 0.0114402\ttotal: 861ms\tremaining: 15.3s\n",
      "16:\tlearn: 0.0113678\ttotal: 874ms\tremaining: 14.5s\n",
      "17:\tlearn: 0.0113026\ttotal: 914ms\tremaining: 14.3s\n",
      "18:\tlearn: 0.0112418\ttotal: 945ms\tremaining: 14s\n",
      "19:\tlearn: 0.0111633\ttotal: 976ms\tremaining: 13.7s\n",
      "20:\tlearn: 0.0111041\ttotal: 1.06s\tremaining: 14s\n",
      "21:\tlearn: 0.0110438\ttotal: 1.09s\tremaining: 13.8s\n",
      "22:\tlearn: 0.0109773\ttotal: 1.13s\tremaining: 13.6s\n",
      "23:\tlearn: 0.0109585\ttotal: 1.16s\tremaining: 13.4s\n",
      "24:\tlearn: 0.0108819\ttotal: 1.19s\tremaining: 13.1s\n",
      "25:\tlearn: 0.0108281\ttotal: 1.27s\tremaining: 13.4s\n",
      "26:\tlearn: 0.0106849\ttotal: 1.29s\tremaining: 13.1s\n",
      "27:\tlearn: 0.0106194\ttotal: 1.32s\tremaining: 12.8s\n",
      "28:\tlearn: 0.0105289\ttotal: 1.35s\tremaining: 12.6s\n",
      "29:\tlearn: 0.0104017\ttotal: 1.38s\tremaining: 12.4s\n",
      "30:\tlearn: 0.0102060\ttotal: 1.41s\tremaining: 12.2s\n",
      "31:\tlearn: 0.0101700\ttotal: 1.44s\tremaining: 12.1s\n",
      "32:\tlearn: 0.0100874\ttotal: 1.49s\tremaining: 12s\n",
      "33:\tlearn: 0.0099940\ttotal: 1.57s\tremaining: 12.3s\n",
      "34:\tlearn: 0.0099482\ttotal: 1.58s\tremaining: 12s\n",
      "35:\tlearn: 0.0098128\ttotal: 1.64s\tremaining: 12s\n",
      "36:\tlearn: 0.0097047\ttotal: 1.74s\tremaining: 12.4s\n",
      "37:\tlearn: 0.0096317\ttotal: 1.78s\tremaining: 12.3s\n",
      "38:\tlearn: 0.0095566\ttotal: 1.82s\tremaining: 12.2s\n",
      "39:\tlearn: 0.0095052\ttotal: 1.86s\tremaining: 12.1s\n",
      "40:\tlearn: 0.0094808\ttotal: 1.93s\tremaining: 12.2s\n",
      "41:\tlearn: 0.0094060\ttotal: 2.01s\tremaining: 12.3s\n",
      "42:\tlearn: 0.0093683\ttotal: 2.04s\tremaining: 12.2s\n",
      "43:\tlearn: 0.0092618\ttotal: 2.08s\tremaining: 12.1s\n",
      "44:\tlearn: 0.0091940\ttotal: 2.11s\tremaining: 12s\n",
      "45:\tlearn: 0.0090863\ttotal: 2.14s\tremaining: 11.8s\n",
      "46:\tlearn: 0.0090168\ttotal: 2.22s\tremaining: 11.9s\n",
      "47:\tlearn: 0.0089925\ttotal: 2.26s\tremaining: 11.9s\n",
      "48:\tlearn: 0.0089460\ttotal: 2.36s\tremaining: 12.1s\n",
      "49:\tlearn: 0.0089010\ttotal: 2.4s\tremaining: 12s\n",
      "50:\tlearn: 0.0088533\ttotal: 2.44s\tremaining: 11.9s\n",
      "51:\tlearn: 0.0087241\ttotal: 2.48s\tremaining: 11.8s\n",
      "52:\tlearn: 0.0086475\ttotal: 2.56s\tremaining: 11.9s\n",
      "53:\tlearn: 0.0085603\ttotal: 2.6s\tremaining: 11.9s\n",
      "54:\tlearn: 0.0084720\ttotal: 2.65s\tremaining: 11.8s\n",
      "55:\tlearn: 0.0083995\ttotal: 2.69s\tremaining: 11.7s\n",
      "56:\tlearn: 0.0083281\ttotal: 2.73s\tremaining: 11.6s\n",
      "57:\tlearn: 0.0082718\ttotal: 2.76s\tremaining: 11.5s\n",
      "58:\tlearn: 0.0081188\ttotal: 2.79s\tremaining: 11.4s\n",
      "59:\tlearn: 0.0080248\ttotal: 2.88s\tremaining: 11.5s\n",
      "60:\tlearn: 0.0078394\ttotal: 2.91s\tremaining: 11.4s\n",
      "61:\tlearn: 0.0077483\ttotal: 2.94s\tremaining: 11.3s\n",
      "62:\tlearn: 0.0076690\ttotal: 2.99s\tremaining: 11.3s\n",
      "63:\tlearn: 0.0075761\ttotal: 3.07s\tremaining: 11.3s\n",
      "64:\tlearn: 0.0074722\ttotal: 3.1s\tremaining: 11.2s\n",
      "65:\tlearn: 0.0073826\ttotal: 3.15s\tremaining: 11.2s\n",
      "66:\tlearn: 0.0073155\ttotal: 3.2s\tremaining: 11.1s\n",
      "67:\tlearn: 0.0072530\ttotal: 3.26s\tremaining: 11.1s\n",
      "68:\tlearn: 0.0071827\ttotal: 3.31s\tremaining: 11.1s\n",
      "69:\tlearn: 0.0071169\ttotal: 3.35s\tremaining: 11s\n",
      "70:\tlearn: 0.0070553\ttotal: 3.46s\tremaining: 11.2s\n",
      "71:\tlearn: 0.0069336\ttotal: 3.51s\tremaining: 11.1s\n",
      "72:\tlearn: 0.0068419\ttotal: 3.56s\tremaining: 11.1s\n",
      "73:\tlearn: 0.0067617\ttotal: 3.64s\tremaining: 11.1s\n",
      "74:\tlearn: 0.0066540\ttotal: 3.66s\tremaining: 11s\n",
      "75:\tlearn: 0.0065932\ttotal: 3.7s\tremaining: 10.9s\n",
      "76:\tlearn: 0.0064960\ttotal: 3.74s\tremaining: 10.8s\n",
      "77:\tlearn: 0.0064075\ttotal: 3.84s\tremaining: 10.9s\n",
      "78:\tlearn: 0.0063515\ttotal: 3.88s\tremaining: 10.9s\n",
      "79:\tlearn: 0.0063100\ttotal: 3.92s\tremaining: 10.8s\n",
      "80:\tlearn: 0.0062126\ttotal: 3.95s\tremaining: 10.7s\n",
      "81:\tlearn: 0.0061900\ttotal: 4s\tremaining: 10.6s\n",
      "82:\tlearn: 0.0061142\ttotal: 4.11s\tremaining: 10.7s\n",
      "83:\tlearn: 0.0060760\ttotal: 4.14s\tremaining: 10.7s\n",
      "84:\tlearn: 0.0060113\ttotal: 4.18s\tremaining: 10.6s\n",
      "85:\tlearn: 0.0059653\ttotal: 4.23s\tremaining: 10.5s\n",
      "86:\tlearn: 0.0059233\ttotal: 4.26s\tremaining: 10.4s\n",
      "87:\tlearn: 0.0058575\ttotal: 4.34s\tremaining: 10.5s\n",
      "88:\tlearn: 0.0058148\ttotal: 4.4s\tremaining: 10.4s\n",
      "89:\tlearn: 0.0057606\ttotal: 4.45s\tremaining: 10.4s\n",
      "90:\tlearn: 0.0057010\ttotal: 4.56s\tremaining: 10.5s\n",
      "91:\tlearn: 0.0056306\ttotal: 4.61s\tremaining: 10.4s\n",
      "92:\tlearn: 0.0055728\ttotal: 4.65s\tremaining: 10.3s\n",
      "93:\tlearn: 0.0055378\ttotal: 4.68s\tremaining: 10.3s\n",
      "94:\tlearn: 0.0054849\ttotal: 4.72s\tremaining: 10.2s\n",
      "95:\tlearn: 0.0054417\ttotal: 4.8s\tremaining: 10.2s\n",
      "96:\tlearn: 0.0053907\ttotal: 4.85s\tremaining: 10.1s\n",
      "97:\tlearn: 0.0053224\ttotal: 4.88s\tremaining: 10.1s\n",
      "98:\tlearn: 0.0052856\ttotal: 4.93s\tremaining: 10s\n",
      "99:\tlearn: 0.0052353\ttotal: 5.02s\tremaining: 10s\n",
      "100:\tlearn: 0.0051794\ttotal: 5.09s\tremaining: 10s\n",
      "101:\tlearn: 0.0051282\ttotal: 5.13s\tremaining: 9.95s\n",
      "102:\tlearn: 0.0050811\ttotal: 5.16s\tremaining: 9.87s\n",
      "103:\tlearn: 0.0050333\ttotal: 5.21s\tremaining: 9.81s\n",
      "104:\tlearn: 0.0049967\ttotal: 5.3s\tremaining: 9.85s\n",
      "105:\tlearn: 0.0049595\ttotal: 5.34s\tremaining: 9.77s\n",
      "106:\tlearn: 0.0048941\ttotal: 5.42s\tremaining: 9.78s\n",
      "107:\tlearn: 0.0048634\ttotal: 5.46s\tremaining: 9.7s\n",
      "108:\tlearn: 0.0048340\ttotal: 5.5s\tremaining: 9.64s\n",
      "109:\tlearn: 0.0047619\ttotal: 5.61s\tremaining: 9.69s\n",
      "110:\tlearn: 0.0047217\ttotal: 5.64s\tremaining: 9.6s\n",
      "111:\tlearn: 0.0046637\ttotal: 5.67s\tremaining: 9.52s\n",
      "112:\tlearn: 0.0046266\ttotal: 5.7s\tremaining: 9.44s\n",
      "113:\tlearn: 0.0045864\ttotal: 5.72s\tremaining: 9.33s\n",
      "114:\tlearn: 0.0045540\ttotal: 5.75s\tremaining: 9.25s\n",
      "115:\tlearn: 0.0045196\ttotal: 5.85s\tremaining: 9.28s\n",
      "116:\tlearn: 0.0044679\ttotal: 5.87s\tremaining: 9.19s\n",
      "117:\tlearn: 0.0044405\ttotal: 5.91s\tremaining: 9.11s\n",
      "118:\tlearn: 0.0043976\ttotal: 5.94s\tremaining: 9.03s\n",
      "119:\tlearn: 0.0043719\ttotal: 6.01s\tremaining: 9.02s\n",
      "120:\tlearn: 0.0043032\ttotal: 6.09s\tremaining: 9s\n",
      "121:\tlearn: 0.0042452\ttotal: 6.12s\tremaining: 8.93s\n",
      "122:\tlearn: 0.0041939\ttotal: 6.15s\tremaining: 8.85s\n",
      "123:\tlearn: 0.0041310\ttotal: 6.18s\tremaining: 8.78s\n",
      "124:\tlearn: 0.0040195\ttotal: 6.25s\tremaining: 8.75s\n",
      "125:\tlearn: 0.0039867\ttotal: 6.34s\tremaining: 8.76s\n",
      "126:\tlearn: 0.0039340\ttotal: 6.38s\tremaining: 8.7s\n",
      "127:\tlearn: 0.0039034\ttotal: 6.42s\tremaining: 8.62s\n",
      "128:\tlearn: 0.0038771\ttotal: 6.5s\tremaining: 8.62s\n",
      "129:\tlearn: 0.0038236\ttotal: 6.54s\tremaining: 8.55s\n",
      "130:\tlearn: 0.0037577\ttotal: 6.57s\tremaining: 8.48s\n",
      "131:\tlearn: 0.0037368\ttotal: 6.62s\tremaining: 8.43s\n",
      "132:\tlearn: 0.0037168\ttotal: 6.67s\tremaining: 8.37s\n",
      "133:\tlearn: 0.0036690\ttotal: 6.7s\tremaining: 8.3s\n",
      "134:\tlearn: 0.0036433\ttotal: 6.77s\tremaining: 8.27s\n",
      "135:\tlearn: 0.0036160\ttotal: 6.82s\tremaining: 8.22s\n",
      "136:\tlearn: 0.0035967\ttotal: 6.87s\tremaining: 8.18s\n",
      "137:\tlearn: 0.0035800\ttotal: 6.97s\tremaining: 8.18s\n",
      "138:\tlearn: 0.0035446\ttotal: 7.01s\tremaining: 8.12s\n",
      "139:\tlearn: 0.0034881\ttotal: 7.04s\tremaining: 8.04s\n",
      "140:\tlearn: 0.0034535\ttotal: 7.08s\tremaining: 7.98s\n",
      "141:\tlearn: 0.0034244\ttotal: 7.12s\tremaining: 7.92s\n",
      "142:\tlearn: 0.0033769\ttotal: 7.15s\tremaining: 7.85s\n",
      "143:\tlearn: 0.0033269\ttotal: 7.23s\tremaining: 7.83s\n",
      "144:\tlearn: 0.0033073\ttotal: 7.27s\tremaining: 7.77s\n",
      "145:\tlearn: 0.0032720\ttotal: 7.31s\tremaining: 7.71s\n",
      "146:\tlearn: 0.0032354\ttotal: 7.34s\tremaining: 7.64s\n",
      "147:\tlearn: 0.0032055\ttotal: 7.42s\tremaining: 7.62s\n",
      "148:\tlearn: 0.0031840\ttotal: 7.43s\tremaining: 7.53s\n",
      "149:\tlearn: 0.0031474\ttotal: 7.46s\tremaining: 7.46s\n",
      "150:\tlearn: 0.0030888\ttotal: 7.56s\tremaining: 7.46s\n",
      "151:\tlearn: 0.0030587\ttotal: 7.59s\tremaining: 7.39s\n",
      "152:\tlearn: 0.0030287\ttotal: 7.69s\tremaining: 7.39s\n",
      "153:\tlearn: 0.0029908\ttotal: 7.73s\tremaining: 7.33s\n",
      "154:\tlearn: 0.0029260\ttotal: 7.75s\tremaining: 7.25s\n",
      "155:\tlearn: 0.0028790\ttotal: 7.79s\tremaining: 7.2s\n",
      "156:\tlearn: 0.0028554\ttotal: 7.84s\tremaining: 7.14s\n",
      "157:\tlearn: 0.0028372\ttotal: 7.95s\tremaining: 7.15s\n",
      "158:\tlearn: 0.0028059\ttotal: 7.99s\tremaining: 7.09s\n",
      "159:\tlearn: 0.0027778\ttotal: 8.04s\tremaining: 7.04s\n",
      "160:\tlearn: 0.0027585\ttotal: 8.07s\tremaining: 6.97s\n",
      "161:\tlearn: 0.0027162\ttotal: 8.17s\tremaining: 6.96s\n",
      "162:\tlearn: 0.0026599\ttotal: 8.19s\tremaining: 6.88s\n",
      "163:\tlearn: 0.0026230\ttotal: 8.21s\tremaining: 6.8s\n",
      "164:\tlearn: 0.0025932\ttotal: 8.24s\tremaining: 6.74s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165:\tlearn: 0.0025596\ttotal: 8.29s\tremaining: 6.69s\n",
      "166:\tlearn: 0.0025135\ttotal: 8.32s\tremaining: 6.63s\n",
      "167:\tlearn: 0.0024655\ttotal: 8.42s\tremaining: 6.62s\n",
      "168:\tlearn: 0.0024426\ttotal: 8.46s\tremaining: 6.56s\n",
      "169:\tlearn: 0.0024048\ttotal: 8.49s\tremaining: 6.49s\n",
      "170:\tlearn: 0.0023815\ttotal: 8.52s\tremaining: 6.43s\n",
      "171:\tlearn: 0.0023425\ttotal: 8.55s\tremaining: 6.37s\n",
      "172:\tlearn: 0.0023048\ttotal: 8.6s\tremaining: 6.32s\n",
      "173:\tlearn: 0.0022759\ttotal: 8.7s\tremaining: 6.3s\n",
      "174:\tlearn: 0.0022512\ttotal: 8.73s\tremaining: 6.24s\n",
      "175:\tlearn: 0.0022424\ttotal: 8.77s\tremaining: 6.18s\n",
      "176:\tlearn: 0.0022143\ttotal: 8.81s\tremaining: 6.12s\n",
      "177:\tlearn: 0.0021823\ttotal: 8.92s\tremaining: 6.11s\n",
      "178:\tlearn: 0.0021542\ttotal: 8.96s\tremaining: 6.06s\n",
      "179:\tlearn: 0.0021344\ttotal: 9s\tremaining: 6s\n",
      "180:\tlearn: 0.0021190\ttotal: 9.05s\tremaining: 5.95s\n",
      "181:\tlearn: 0.0020729\ttotal: 9.13s\tremaining: 5.92s\n",
      "182:\tlearn: 0.0020604\ttotal: 9.18s\tremaining: 5.87s\n",
      "183:\tlearn: 0.0020351\ttotal: 9.22s\tremaining: 5.82s\n",
      "184:\tlearn: 0.0020171\ttotal: 9.28s\tremaining: 5.77s\n",
      "185:\tlearn: 0.0019885\ttotal: 9.32s\tremaining: 5.71s\n",
      "186:\tlearn: 0.0019799\ttotal: 9.35s\tremaining: 5.65s\n",
      "187:\tlearn: 0.0019662\ttotal: 9.44s\tremaining: 5.63s\n",
      "188:\tlearn: 0.0019547\ttotal: 9.47s\tremaining: 5.56s\n",
      "189:\tlearn: 0.0019295\ttotal: 9.51s\tremaining: 5.5s\n",
      "190:\tlearn: 0.0019104\ttotal: 9.57s\tremaining: 5.46s\n",
      "191:\tlearn: 0.0018716\ttotal: 9.68s\tremaining: 5.45s\n",
      "192:\tlearn: 0.0018515\ttotal: 9.71s\tremaining: 5.38s\n",
      "193:\tlearn: 0.0018360\ttotal: 9.76s\tremaining: 5.33s\n",
      "194:\tlearn: 0.0018126\ttotal: 9.85s\tremaining: 5.3s\n",
      "195:\tlearn: 0.0017939\ttotal: 9.88s\tremaining: 5.24s\n",
      "196:\tlearn: 0.0017610\ttotal: 9.97s\tremaining: 5.21s\n",
      "197:\tlearn: 0.0017507\ttotal: 10s\tremaining: 5.16s\n",
      "198:\tlearn: 0.0017361\ttotal: 10.1s\tremaining: 5.12s\n",
      "199:\tlearn: 0.0017243\ttotal: 10.1s\tremaining: 5.05s\n",
      "200:\tlearn: 0.0016998\ttotal: 10.1s\tremaining: 4.99s\n",
      "201:\tlearn: 0.0016846\ttotal: 10.2s\tremaining: 4.93s\n",
      "202:\tlearn: 0.0016670\ttotal: 10.2s\tremaining: 4.88s\n",
      "203:\tlearn: 0.0016396\ttotal: 10.3s\tremaining: 4.83s\n",
      "204:\tlearn: 0.0016198\ttotal: 10.4s\tremaining: 4.8s\n",
      "205:\tlearn: 0.0016033\ttotal: 10.4s\tremaining: 4.75s\n",
      "206:\tlearn: 0.0015772\ttotal: 10.5s\tremaining: 4.7s\n",
      "207:\tlearn: 0.0015696\ttotal: 10.5s\tremaining: 4.65s\n",
      "208:\tlearn: 0.0015311\ttotal: 10.5s\tremaining: 4.59s\n",
      "209:\tlearn: 0.0015197\ttotal: 10.6s\tremaining: 4.53s\n",
      "210:\tlearn: 0.0014910\ttotal: 10.6s\tremaining: 4.48s\n",
      "211:\tlearn: 0.0014797\ttotal: 10.7s\tremaining: 4.43s\n",
      "212:\tlearn: 0.0014652\ttotal: 10.8s\tremaining: 4.4s\n",
      "213:\tlearn: 0.0014481\ttotal: 10.8s\tremaining: 4.34s\n",
      "214:\tlearn: 0.0014376\ttotal: 10.8s\tremaining: 4.29s\n",
      "215:\tlearn: 0.0014168\ttotal: 10.9s\tremaining: 4.24s\n",
      "216:\tlearn: 0.0014065\ttotal: 11s\tremaining: 4.19s\n",
      "217:\tlearn: 0.0013820\ttotal: 11s\tremaining: 4.16s\n",
      "218:\tlearn: 0.0013713\ttotal: 11.1s\tremaining: 4.1s\n",
      "219:\tlearn: 0.0013535\ttotal: 11.1s\tremaining: 4.05s\n",
      "220:\tlearn: 0.0013430\ttotal: 11.2s\tremaining: 3.99s\n",
      "221:\tlearn: 0.0013250\ttotal: 11.2s\tremaining: 3.93s\n",
      "222:\tlearn: 0.0013097\ttotal: 11.2s\tremaining: 3.87s\n",
      "223:\tlearn: 0.0012930\ttotal: 11.3s\tremaining: 3.82s\n",
      "224:\tlearn: 0.0012851\ttotal: 11.3s\tremaining: 3.77s\n",
      "225:\tlearn: 0.0012638\ttotal: 11.4s\tremaining: 3.73s\n",
      "226:\tlearn: 0.0012422\ttotal: 11.5s\tremaining: 3.68s\n",
      "227:\tlearn: 0.0012253\ttotal: 11.5s\tremaining: 3.63s\n",
      "228:\tlearn: 0.0012188\ttotal: 11.6s\tremaining: 3.58s\n",
      "229:\tlearn: 0.0011873\ttotal: 11.6s\tremaining: 3.53s\n",
      "230:\tlearn: 0.0011716\ttotal: 11.6s\tremaining: 3.47s\n",
      "231:\tlearn: 0.0011407\ttotal: 11.7s\tremaining: 3.42s\n",
      "232:\tlearn: 0.0011165\ttotal: 11.7s\tremaining: 3.36s\n",
      "233:\tlearn: 0.0010932\ttotal: 11.7s\tremaining: 3.31s\n",
      "234:\tlearn: 0.0010745\ttotal: 11.8s\tremaining: 3.26s\n",
      "235:\tlearn: 0.0010680\ttotal: 11.9s\tremaining: 3.21s\n",
      "236:\tlearn: 0.0010614\ttotal: 12s\tremaining: 3.18s\n",
      "237:\tlearn: 0.0010454\ttotal: 12s\tremaining: 3.12s\n",
      "238:\tlearn: 0.0010299\ttotal: 12s\tremaining: 3.07s\n",
      "239:\tlearn: 0.0010168\ttotal: 12.1s\tremaining: 3.03s\n",
      "240:\tlearn: 0.0010001\ttotal: 12.2s\tremaining: 2.98s\n",
      "241:\tlearn: 0.0009912\ttotal: 12.2s\tremaining: 2.93s\n",
      "242:\tlearn: 0.0009791\ttotal: 12.3s\tremaining: 2.88s\n",
      "243:\tlearn: 0.0009731\ttotal: 12.3s\tremaining: 2.83s\n",
      "244:\tlearn: 0.0009588\ttotal: 12.4s\tremaining: 2.78s\n",
      "245:\tlearn: 0.0009519\ttotal: 12.4s\tremaining: 2.73s\n",
      "246:\tlearn: 0.0009352\ttotal: 12.5s\tremaining: 2.67s\n",
      "247:\tlearn: 0.0009188\ttotal: 12.6s\tremaining: 2.63s\n",
      "248:\tlearn: 0.0009101\ttotal: 12.6s\tremaining: 2.58s\n",
      "249:\tlearn: 0.0009045\ttotal: 12.7s\tremaining: 2.53s\n",
      "250:\tlearn: 0.0008965\ttotal: 12.7s\tremaining: 2.48s\n",
      "251:\tlearn: 0.0008918\ttotal: 12.7s\tremaining: 2.43s\n",
      "252:\tlearn: 0.0008788\ttotal: 12.8s\tremaining: 2.37s\n",
      "253:\tlearn: 0.0008726\ttotal: 12.8s\tremaining: 2.32s\n",
      "254:\tlearn: 0.0008688\ttotal: 12.9s\tremaining: 2.27s\n",
      "255:\tlearn: 0.0008652\ttotal: 12.9s\tremaining: 2.21s\n",
      "256:\tlearn: 0.0008477\ttotal: 12.9s\tremaining: 2.16s\n",
      "257:\tlearn: 0.0008431\ttotal: 12.9s\tremaining: 2.11s\n",
      "258:\tlearn: 0.0008403\ttotal: 13s\tremaining: 2.05s\n",
      "259:\tlearn: 0.0008345\ttotal: 13s\tremaining: 2s\n",
      "260:\tlearn: 0.0008279\ttotal: 13s\tremaining: 1.95s\n",
      "261:\tlearn: 0.0008247\ttotal: 13.1s\tremaining: 1.9s\n",
      "262:\tlearn: 0.0008114\ttotal: 13.2s\tremaining: 1.85s\n",
      "263:\tlearn: 0.0008059\ttotal: 13.2s\tremaining: 1.8s\n",
      "264:\tlearn: 0.0007991\ttotal: 13.3s\tremaining: 1.75s\n",
      "265:\tlearn: 0.0007875\ttotal: 13.3s\tremaining: 1.7s\n",
      "266:\tlearn: 0.0007830\ttotal: 13.4s\tremaining: 1.65s\n",
      "267:\tlearn: 0.0007774\ttotal: 13.5s\tremaining: 1.61s\n",
      "268:\tlearn: 0.0007671\ttotal: 13.5s\tremaining: 1.56s\n",
      "269:\tlearn: 0.0007631\ttotal: 13.6s\tremaining: 1.51s\n",
      "270:\tlearn: 0.0007530\ttotal: 13.6s\tremaining: 1.46s\n",
      "271:\tlearn: 0.0007368\ttotal: 13.7s\tremaining: 1.41s\n",
      "272:\tlearn: 0.0007303\ttotal: 13.8s\tremaining: 1.36s\n",
      "273:\tlearn: 0.0007247\ttotal: 13.8s\tremaining: 1.31s\n",
      "274:\tlearn: 0.0007152\ttotal: 13.8s\tremaining: 1.26s\n",
      "275:\tlearn: 0.0007047\ttotal: 14s\tremaining: 1.21s\n",
      "276:\tlearn: 0.0006977\ttotal: 14s\tremaining: 1.16s\n",
      "277:\tlearn: 0.0006884\ttotal: 14s\tremaining: 1.11s\n",
      "278:\tlearn: 0.0006848\ttotal: 14s\tremaining: 1.06s\n",
      "279:\tlearn: 0.0006816\ttotal: 14.1s\tremaining: 1.01s\n",
      "280:\tlearn: 0.0006801\ttotal: 14.1s\tremaining: 954ms\n",
      "281:\tlearn: 0.0006729\ttotal: 14.2s\tremaining: 906ms\n",
      "282:\tlearn: 0.0006679\ttotal: 14.2s\tremaining: 855ms\n",
      "283:\tlearn: 0.0006628\ttotal: 14.3s\tremaining: 804ms\n",
      "284:\tlearn: 0.0006588\ttotal: 14.3s\tremaining: 753ms\n",
      "285:\tlearn: 0.0006534\ttotal: 14.3s\tremaining: 702ms\n",
      "286:\tlearn: 0.0006476\ttotal: 14.4s\tremaining: 651ms\n",
      "287:\tlearn: 0.0006432\ttotal: 14.4s\tremaining: 601ms\n",
      "288:\tlearn: 0.0006405\ttotal: 14.5s\tremaining: 552ms\n",
      "289:\tlearn: 0.0006350\ttotal: 14.6s\tremaining: 502ms\n",
      "290:\tlearn: 0.0006299\ttotal: 14.6s\tremaining: 452ms\n",
      "291:\tlearn: 0.0006272\ttotal: 14.7s\tremaining: 402ms\n",
      "292:\tlearn: 0.0006203\ttotal: 14.7s\tremaining: 352ms\n",
      "293:\tlearn: 0.0006175\ttotal: 14.8s\tremaining: 302ms\n",
      "294:\tlearn: 0.0006041\ttotal: 14.9s\tremaining: 252ms\n",
      "295:\tlearn: 0.0005972\ttotal: 15s\tremaining: 202ms\n",
      "296:\tlearn: 0.0005905\ttotal: 15s\tremaining: 151ms\n",
      "297:\tlearn: 0.0005886\ttotal: 15.1s\tremaining: 101ms\n",
      "298:\tlearn: 0.0005860\ttotal: 15.1s\tremaining: 50.6ms\n",
      "299:\tlearn: 0.0005819\ttotal: 15.2s\tremaining: 0us\n",
      "Dataset 1:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5abb99174c44b868a5e3874b7dd3c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.547393340512062, Recall = 0.9006526468455403, Aging Rate = 0.6838288614938361, Precision = 0.6585365853658537, f1 = 0.7607963246554365\n",
      "Epoch 2: Train Loss = 0.34646364651009864, Recall = 0.887962291515591, Aging Rate = 0.5391588107324148, Precision = 0.8234700739744452, f1 = 0.8545010467550593\n",
      "Epoch 3: Train Loss = 0.2583556948276946, Recall = 0.9213197969543148, Aging Rate = 0.5241116751269036, Precision = 0.8789346246973365, f1 = 0.8996282527881041\n",
      "Epoch 4: Train Loss = 0.19684395446908742, Recall = 0.9514140681653372, Aging Rate = 0.5197606961566352, Precision = 0.9152424136728288, f1 = 0.9329777777777778\n",
      "Epoch 5: Train Loss = 0.15272342631400193, Recall = 0.9630166787527193, Aging Rate = 0.5137781000725163, Precision = 0.9371912491178547, f1 = 0.9499284692417739\n",
      "Test Loss = 0.11806820015314266, Recall = 0.9753444525018129, Aging Rate = 0.5005438723712835, precision = 0.9742846794639624\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.10373659679521634, Recall = 0.9869470630891951, Aging Rate = 0.5114213197969543, Precision = 0.9649060616802553, f1 = 0.9758021150743862\n",
      "Epoch 7: Train Loss = 0.0778025983629493, Recall = 0.9938361131254533, Aging Rate = 0.5079767947788252, Precision = 0.9782298358315489, f1 = 0.9859712230215828\n",
      "Epoch 8: Train Loss = 0.059026546411599864, Recall = 0.9949238578680203, Aging Rate = 0.5036258158085569, Precision = 0.9877609791216703, f1 = 0.9913294797687862\n",
      "Epoch 9: Train Loss = 0.046260342328789616, Recall = 0.9960116026105874, Aging Rate = 0.5018129079042785, Precision = 0.9924132947976878, f1 = 0.9942091929062613\n",
      "Epoch 10: Train Loss = 0.03623174546736254, Recall = 0.9978245105148659, Aging Rate = 0.5016316171138506, Precision = 0.9945789663895916, f1 = 0.9961990950226245\n",
      "Test Loss = 0.0374482342888256, Recall = 0.9996374184191443, Aging Rate = 0.5059825960841189, precision = 0.9878179863848083\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.02971056404241926, Recall = 0.9978245105148659, Aging Rate = 0.5007251631617113, Precision = 0.99637943519189, f1 = 0.9971014492753624\n",
      "Epoch 12: Train Loss = 0.02395955221545143, Recall = 0.9981870920957215, Aging Rate = 0.5005438723712835, Precision = 0.997102499094531, f1 = 0.9976445008153652\n",
      "Epoch 13: Train Loss = 0.019975662509693973, Recall = 0.9992748368382887, Aging Rate = 0.5009064539521393, Precision = 0.9974665218964893, f1 = 0.9983698605325123\n",
      "Epoch 14: Train Loss = 0.016932094078970245, Recall = 0.9992748368382887, Aging Rate = 0.5007251631617113, Precision = 0.997827661115134, f1 = 0.9985507246376811\n",
      "Epoch 15: Train Loss = 0.014165692410260935, Recall = 1.0, Aging Rate = 0.5003625815808557, Precision = 0.9992753623188406, f1 = 0.9996375498368975\n",
      "Test Loss = 0.011838411147138232, Recall = 1.0, Aging Rate = 0.5001812907904278, precision = 0.9996375498368975\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.012225030030598667, Recall = 1.0, Aging Rate = 0.5003625815808557, Precision = 0.9992753623188406, f1 = 0.9996375498368975\n",
      "Epoch 17: Train Loss = 0.010319800788105788, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 18: Train Loss = 0.009253216164822787, Recall = 1.0, Aging Rate = 0.5003625815808557, Precision = 0.9992753623188406, f1 = 0.9996375498368975\n",
      "Epoch 19: Train Loss = 0.007873879772954721, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 20: Train Loss = 0.007057373647654377, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Test Loss = 0.006076765375852077, Recall = 1.0, Aging Rate = 0.5001812907904278, precision = 0.9996375498368975\n",
      "\n",
      "Epoch 21: Train Loss = 0.006169922702200846, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 22: Train Loss = 0.005510124512049866, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 23: Train Loss = 0.005082270843527287, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 24: Train Loss = 0.004483723611637755, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 25: Train Loss = 0.004043579085691521, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0035407068120907287, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.0037597219586004977, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.0034900209132009245, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.0031887712804674886, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.0029774439970080007, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.0027950502669694005, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0025037371906694336, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.0026563215828409514, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.0026627852181458954, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.002350897816947896, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.002500944572823032, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.0022001500856712124, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002296861473572568, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.0023023527537852774, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.0021507385173090547, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.0020351843172117007, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0019506400103590966, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.0019770902471766792, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018288891210725248, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.0019169510249318616, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0019197734835592842, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.002131541265327378, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0035864572707275597, Recall = 0.9996374184191443, Aging Rate = 0.49981870920957217, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.002028384839405425, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001361198424722289, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.0014618639651455656, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0015163827601224562, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0015518836845239278, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.001686743085601511, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.0016193669226035432, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015906049527251178, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0016588824723480489, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0015936174576506397, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.0016748377990161297, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.0017061716094096424, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0017712366354793237, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014423138035347847, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.001644223270059156, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.0017182036798783025, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Train Loss = 0.0017402960422960062, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0017971668417408166, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.001695437779267838, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013331340291810696, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0016052599781394771, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.0016388859686950266, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.0020277949895620606, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0019048948393120923, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0014447895585450576, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001389931299486633, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0015607506770875638, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0017944310207265693, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0015318635499957163, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0014717098507163013, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0016854243441481493, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015203214859574907, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0014899429801697528, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.001728595557613832, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.004414633908133449, Recall = 0.9989122552574329, Aging Rate = 0.49963741841914433, Precision = 0.9996371552975326, f1 = 0.9992745738121146\n",
      "Epoch 74: Train Loss = 0.002848538323599329, Recall = 1.0, Aging Rate = 0.5001812907904278, Precision = 0.9996375498368975, f1 = 0.9998187420699656\n",
      "Epoch 75: Train Loss = 0.0012887759449735738, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0011748897617787834, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0012239025607894216, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.001255912057276372, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0013011743016054745, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0013087296683173776, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0014407564463813811, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013021147109067, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0014881375414590157, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0015836044889520192, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0014842865796809701, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0016285891209386607, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0018833995205123975, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001652579159344843, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0015635037925729204, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0014847084747166782, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.001480805285555595, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0015739007274525008, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0017619234615882172, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020707291824244396, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.0015787552879073746, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.001540114971725682, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.0014864801340739825, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0016172133408759689, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.001587593781595011, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015424076520346181, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.001555023948152761, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0017337165771226484, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.001681492718286472, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0014792127495745035, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.001511615625548192, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001418643952934902, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.001639818483914645, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.0015124069518180723, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.0016359699420810578, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0014242737272388267, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.002014017014705493, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026069124543463963, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.0016862677541602622, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.001354512659950348, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.001502483206388471, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.0014444629341280161, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.0015158249531954289, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013559361394836504, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0015825308362789252, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.0020329032513360947, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.0021620267595078707, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.0019481602911496341, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.0012860032598109156, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001215763007176863, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.0012509698259537503, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.0013549400655926895, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.0014412264060026454, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.001455498703193538, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.0015235755168289172, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001270546175018115, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.0016010268017287658, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.001485608073525155, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.0014882047015341558, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.001641506885507288, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.0017210446426694224, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014165925771907036, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6217277\ttotal: 13.1ms\tremaining: 2.6s\n",
      "1:\tlearn: 0.5558744\ttotal: 25.7ms\tremaining: 2.54s\n",
      "2:\tlearn: 0.5028864\ttotal: 38.6ms\tremaining: 2.54s\n",
      "3:\tlearn: 0.4676672\ttotal: 51.2ms\tremaining: 2.51s\n",
      "4:\tlearn: 0.4311154\ttotal: 64.1ms\tremaining: 2.5s\n",
      "5:\tlearn: 0.4056915\ttotal: 76.5ms\tremaining: 2.47s\n",
      "6:\tlearn: 0.3741244\ttotal: 86.9ms\tremaining: 2.4s\n",
      "7:\tlearn: 0.3433703\ttotal: 97.2ms\tremaining: 2.33s\n",
      "8:\tlearn: 0.3183043\ttotal: 108ms\tremaining: 2.3s\n",
      "9:\tlearn: 0.2948963\ttotal: 119ms\tremaining: 2.26s\n",
      "10:\tlearn: 0.2774681\ttotal: 130ms\tremaining: 2.23s\n",
      "11:\tlearn: 0.2585890\ttotal: 141ms\tremaining: 2.2s\n",
      "12:\tlearn: 0.2408625\ttotal: 152ms\tremaining: 2.19s\n",
      "13:\tlearn: 0.2321962\ttotal: 164ms\tremaining: 2.17s\n",
      "14:\tlearn: 0.2179571\ttotal: 175ms\tremaining: 2.16s\n",
      "15:\tlearn: 0.2089641\ttotal: 186ms\tremaining: 2.14s\n",
      "16:\tlearn: 0.1987191\ttotal: 198ms\tremaining: 2.13s\n",
      "17:\tlearn: 0.1869060\ttotal: 210ms\tremaining: 2.12s\n",
      "18:\tlearn: 0.1761924\ttotal: 221ms\tremaining: 2.1s\n",
      "19:\tlearn: 0.1652930\ttotal: 233ms\tremaining: 2.09s\n",
      "20:\tlearn: 0.1559570\ttotal: 245ms\tremaining: 2.08s\n",
      "21:\tlearn: 0.1498867\ttotal: 256ms\tremaining: 2.07s\n",
      "22:\tlearn: 0.1418934\ttotal: 267ms\tremaining: 2.06s\n",
      "23:\tlearn: 0.1374244\ttotal: 279ms\tremaining: 2.04s\n",
      "24:\tlearn: 0.1283307\ttotal: 290ms\tremaining: 2.03s\n",
      "25:\tlearn: 0.1208143\ttotal: 302ms\tremaining: 2.02s\n",
      "26:\tlearn: 0.1133496\ttotal: 313ms\tremaining: 2s\n",
      "27:\tlearn: 0.1086180\ttotal: 325ms\tremaining: 2s\n",
      "28:\tlearn: 0.1063338\ttotal: 336ms\tremaining: 1.98s\n",
      "29:\tlearn: 0.1030617\ttotal: 348ms\tremaining: 1.97s\n",
      "30:\tlearn: 0.0982828\ttotal: 360ms\tremaining: 1.96s\n",
      "31:\tlearn: 0.0949032\ttotal: 371ms\tremaining: 1.95s\n",
      "32:\tlearn: 0.0913369\ttotal: 383ms\tremaining: 1.94s\n",
      "33:\tlearn: 0.0888296\ttotal: 395ms\tremaining: 1.93s\n",
      "34:\tlearn: 0.0852866\ttotal: 407ms\tremaining: 1.92s\n",
      "35:\tlearn: 0.0809667\ttotal: 418ms\tremaining: 1.91s\n",
      "36:\tlearn: 0.0791617\ttotal: 430ms\tremaining: 1.9s\n",
      "37:\tlearn: 0.0754767\ttotal: 442ms\tremaining: 1.88s\n",
      "38:\tlearn: 0.0725052\ttotal: 454ms\tremaining: 1.87s\n",
      "39:\tlearn: 0.0700258\ttotal: 466ms\tremaining: 1.86s\n",
      "40:\tlearn: 0.0670100\ttotal: 477ms\tremaining: 1.85s\n",
      "41:\tlearn: 0.0653388\ttotal: 488ms\tremaining: 1.84s\n",
      "42:\tlearn: 0.0632005\ttotal: 507ms\tremaining: 1.85s\n",
      "43:\tlearn: 0.0611109\ttotal: 527ms\tremaining: 1.87s\n",
      "44:\tlearn: 0.0584798\ttotal: 545ms\tremaining: 1.88s\n",
      "45:\tlearn: 0.0568943\ttotal: 557ms\tremaining: 1.86s\n",
      "46:\tlearn: 0.0558436\ttotal: 569ms\tremaining: 1.85s\n",
      "47:\tlearn: 0.0543255\ttotal: 580ms\tremaining: 1.84s\n",
      "48:\tlearn: 0.0530508\ttotal: 592ms\tremaining: 1.82s\n",
      "49:\tlearn: 0.0505502\ttotal: 603ms\tremaining: 1.81s\n",
      "50:\tlearn: 0.0496543\ttotal: 615ms\tremaining: 1.8s\n",
      "51:\tlearn: 0.0475800\ttotal: 627ms\tremaining: 1.78s\n",
      "52:\tlearn: 0.0466101\ttotal: 638ms\tremaining: 1.77s\n",
      "53:\tlearn: 0.0452318\ttotal: 649ms\tremaining: 1.75s\n",
      "54:\tlearn: 0.0439364\ttotal: 661ms\tremaining: 1.74s\n",
      "55:\tlearn: 0.0423518\ttotal: 673ms\tremaining: 1.73s\n",
      "56:\tlearn: 0.0416480\ttotal: 685ms\tremaining: 1.72s\n",
      "57:\tlearn: 0.0395126\ttotal: 697ms\tremaining: 1.71s\n",
      "58:\tlearn: 0.0388997\ttotal: 708ms\tremaining: 1.69s\n",
      "59:\tlearn: 0.0375280\ttotal: 720ms\tremaining: 1.68s\n",
      "60:\tlearn: 0.0365407\ttotal: 731ms\tremaining: 1.67s\n",
      "61:\tlearn: 0.0359704\ttotal: 743ms\tremaining: 1.65s\n",
      "62:\tlearn: 0.0349891\ttotal: 754ms\tremaining: 1.64s\n",
      "63:\tlearn: 0.0343128\ttotal: 766ms\tremaining: 1.63s\n",
      "64:\tlearn: 0.0334454\ttotal: 778ms\tremaining: 1.61s\n",
      "65:\tlearn: 0.0329788\ttotal: 789ms\tremaining: 1.6s\n",
      "66:\tlearn: 0.0319514\ttotal: 801ms\tremaining: 1.59s\n",
      "67:\tlearn: 0.0313441\ttotal: 813ms\tremaining: 1.58s\n",
      "68:\tlearn: 0.0309449\ttotal: 825ms\tremaining: 1.56s\n",
      "69:\tlearn: 0.0300370\ttotal: 836ms\tremaining: 1.55s\n",
      "70:\tlearn: 0.0296607\ttotal: 848ms\tremaining: 1.54s\n",
      "71:\tlearn: 0.0288666\ttotal: 860ms\tremaining: 1.53s\n",
      "72:\tlearn: 0.0277272\ttotal: 871ms\tremaining: 1.51s\n",
      "73:\tlearn: 0.0270673\ttotal: 883ms\tremaining: 1.5s\n",
      "74:\tlearn: 0.0260224\ttotal: 894ms\tremaining: 1.49s\n",
      "75:\tlearn: 0.0254840\ttotal: 905ms\tremaining: 1.48s\n",
      "76:\tlearn: 0.0248329\ttotal: 917ms\tremaining: 1.46s\n",
      "77:\tlearn: 0.0241636\ttotal: 929ms\tremaining: 1.45s\n",
      "78:\tlearn: 0.0236612\ttotal: 940ms\tremaining: 1.44s\n",
      "79:\tlearn: 0.0231669\ttotal: 952ms\tremaining: 1.43s\n",
      "80:\tlearn: 0.0227811\ttotal: 964ms\tremaining: 1.42s\n",
      "81:\tlearn: 0.0219619\ttotal: 976ms\tremaining: 1.4s\n",
      "82:\tlearn: 0.0215486\ttotal: 988ms\tremaining: 1.39s\n",
      "83:\tlearn: 0.0211772\ttotal: 999ms\tremaining: 1.38s\n",
      "84:\tlearn: 0.0206634\ttotal: 1.01s\tremaining: 1.37s\n",
      "85:\tlearn: 0.0204164\ttotal: 1.02s\tremaining: 1.36s\n",
      "86:\tlearn: 0.0200820\ttotal: 1.03s\tremaining: 1.34s\n",
      "87:\tlearn: 0.0195502\ttotal: 1.05s\tremaining: 1.33s\n",
      "88:\tlearn: 0.0191609\ttotal: 1.06s\tremaining: 1.32s\n",
      "89:\tlearn: 0.0184204\ttotal: 1.07s\tremaining: 1.31s\n",
      "90:\tlearn: 0.0180033\ttotal: 1.08s\tremaining: 1.29s\n",
      "91:\tlearn: 0.0173700\ttotal: 1.09s\tremaining: 1.28s\n",
      "92:\tlearn: 0.0171988\ttotal: 1.1s\tremaining: 1.27s\n",
      "93:\tlearn: 0.0169698\ttotal: 1.12s\tremaining: 1.26s\n",
      "94:\tlearn: 0.0165659\ttotal: 1.13s\tremaining: 1.25s\n",
      "95:\tlearn: 0.0161419\ttotal: 1.14s\tremaining: 1.23s\n",
      "96:\tlearn: 0.0154223\ttotal: 1.15s\tremaining: 1.22s\n",
      "97:\tlearn: 0.0148408\ttotal: 1.16s\tremaining: 1.21s\n",
      "98:\tlearn: 0.0144782\ttotal: 1.18s\tremaining: 1.2s\n",
      "99:\tlearn: 0.0143207\ttotal: 1.19s\tremaining: 1.19s\n",
      "100:\tlearn: 0.0140865\ttotal: 1.2s\tremaining: 1.17s\n",
      "101:\tlearn: 0.0139019\ttotal: 1.21s\tremaining: 1.16s\n",
      "102:\tlearn: 0.0136803\ttotal: 1.22s\tremaining: 1.15s\n",
      "103:\tlearn: 0.0135707\ttotal: 1.23s\tremaining: 1.14s\n",
      "104:\tlearn: 0.0130826\ttotal: 1.25s\tremaining: 1.13s\n",
      "105:\tlearn: 0.0127790\ttotal: 1.26s\tremaining: 1.11s\n",
      "106:\tlearn: 0.0125552\ttotal: 1.27s\tremaining: 1.1s\n",
      "107:\tlearn: 0.0122851\ttotal: 1.28s\tremaining: 1.09s\n",
      "108:\tlearn: 0.0118257\ttotal: 1.29s\tremaining: 1.08s\n",
      "109:\tlearn: 0.0115915\ttotal: 1.3s\tremaining: 1.06s\n",
      "110:\tlearn: 0.0111171\ttotal: 1.31s\tremaining: 1.05s\n",
      "111:\tlearn: 0.0106982\ttotal: 1.33s\tremaining: 1.04s\n",
      "112:\tlearn: 0.0105762\ttotal: 1.34s\tremaining: 1.03s\n",
      "113:\tlearn: 0.0102916\ttotal: 1.35s\tremaining: 1.02s\n",
      "114:\tlearn: 0.0101156\ttotal: 1.36s\tremaining: 1.01s\n",
      "115:\tlearn: 0.0098586\ttotal: 1.37s\tremaining: 996ms\n",
      "116:\tlearn: 0.0096755\ttotal: 1.39s\tremaining: 984ms\n",
      "117:\tlearn: 0.0094037\ttotal: 1.4s\tremaining: 972ms\n",
      "118:\tlearn: 0.0091595\ttotal: 1.41s\tremaining: 960ms\n",
      "119:\tlearn: 0.0090409\ttotal: 1.42s\tremaining: 948ms\n",
      "120:\tlearn: 0.0088256\ttotal: 1.43s\tremaining: 936ms\n",
      "121:\tlearn: 0.0085625\ttotal: 1.45s\tremaining: 924ms\n",
      "122:\tlearn: 0.0083333\ttotal: 1.46s\tremaining: 912ms\n",
      "123:\tlearn: 0.0081875\ttotal: 1.47s\tremaining: 900ms\n",
      "124:\tlearn: 0.0080548\ttotal: 1.48s\tremaining: 888ms\n",
      "125:\tlearn: 0.0078692\ttotal: 1.49s\tremaining: 876ms\n",
      "126:\tlearn: 0.0077839\ttotal: 1.5s\tremaining: 865ms\n",
      "127:\tlearn: 0.0075989\ttotal: 1.52s\tremaining: 857ms\n",
      "128:\tlearn: 0.0075146\ttotal: 1.54s\tremaining: 850ms\n",
      "129:\tlearn: 0.0073686\ttotal: 1.56s\tremaining: 842ms\n",
      "130:\tlearn: 0.0072753\ttotal: 1.58s\tremaining: 834ms\n",
      "131:\tlearn: 0.0071986\ttotal: 1.6s\tremaining: 825ms\n",
      "132:\tlearn: 0.0070281\ttotal: 1.61s\tremaining: 813ms\n",
      "133:\tlearn: 0.0068874\ttotal: 1.62s\tremaining: 800ms\n",
      "134:\tlearn: 0.0067801\ttotal: 1.64s\tremaining: 788ms\n",
      "135:\tlearn: 0.0066727\ttotal: 1.65s\tremaining: 776ms\n",
      "136:\tlearn: 0.0065339\ttotal: 1.66s\tremaining: 763ms\n",
      "137:\tlearn: 0.0064411\ttotal: 1.67s\tremaining: 751ms\n",
      "138:\tlearn: 0.0062602\ttotal: 1.68s\tremaining: 739ms\n",
      "139:\tlearn: 0.0061106\ttotal: 1.69s\tremaining: 726ms\n",
      "140:\tlearn: 0.0060438\ttotal: 1.71s\tremaining: 714ms\n",
      "141:\tlearn: 0.0059701\ttotal: 1.72s\tremaining: 702ms\n",
      "142:\tlearn: 0.0058413\ttotal: 1.73s\tremaining: 689ms\n",
      "143:\tlearn: 0.0057513\ttotal: 1.74s\tremaining: 677ms\n",
      "144:\tlearn: 0.0056614\ttotal: 1.75s\tremaining: 665ms\n",
      "145:\tlearn: 0.0055348\ttotal: 1.76s\tremaining: 653ms\n",
      "146:\tlearn: 0.0054845\ttotal: 1.78s\tremaining: 640ms\n",
      "147:\tlearn: 0.0053688\ttotal: 1.79s\tremaining: 628ms\n",
      "148:\tlearn: 0.0052558\ttotal: 1.8s\tremaining: 616ms\n",
      "149:\tlearn: 0.0051230\ttotal: 1.81s\tremaining: 604ms\n",
      "150:\tlearn: 0.0050154\ttotal: 1.82s\tremaining: 591ms\n",
      "151:\tlearn: 0.0048167\ttotal: 1.83s\tremaining: 579ms\n",
      "152:\tlearn: 0.0047562\ttotal: 1.84s\tremaining: 567ms\n",
      "153:\tlearn: 0.0046592\ttotal: 1.86s\tremaining: 555ms\n",
      "154:\tlearn: 0.0045891\ttotal: 1.87s\tremaining: 542ms\n",
      "155:\tlearn: 0.0045279\ttotal: 1.88s\tremaining: 530ms\n",
      "156:\tlearn: 0.0043849\ttotal: 1.89s\tremaining: 518ms\n",
      "157:\tlearn: 0.0043528\ttotal: 1.9s\tremaining: 506ms\n",
      "158:\tlearn: 0.0042622\ttotal: 1.91s\tremaining: 494ms\n",
      "159:\tlearn: 0.0041841\ttotal: 1.93s\tremaining: 482ms\n",
      "160:\tlearn: 0.0040814\ttotal: 1.94s\tremaining: 470ms\n",
      "161:\tlearn: 0.0040068\ttotal: 1.95s\tremaining: 458ms\n",
      "162:\tlearn: 0.0039672\ttotal: 1.96s\tremaining: 445ms\n",
      "163:\tlearn: 0.0038800\ttotal: 1.97s\tremaining: 433ms\n",
      "164:\tlearn: 0.0038192\ttotal: 1.99s\tremaining: 421ms\n",
      "165:\tlearn: 0.0037550\ttotal: 2s\tremaining: 409ms\n",
      "166:\tlearn: 0.0036809\ttotal: 2.01s\tremaining: 397ms\n",
      "167:\tlearn: 0.0036263\ttotal: 2.02s\tremaining: 385ms\n",
      "168:\tlearn: 0.0035583\ttotal: 2.03s\tremaining: 373ms\n",
      "169:\tlearn: 0.0035222\ttotal: 2.04s\tremaining: 361ms\n",
      "170:\tlearn: 0.0034608\ttotal: 2.06s\tremaining: 349ms\n",
      "171:\tlearn: 0.0034185\ttotal: 2.07s\tremaining: 337ms\n",
      "172:\tlearn: 0.0033146\ttotal: 2.08s\tremaining: 325ms\n",
      "173:\tlearn: 0.0032544\ttotal: 2.09s\tremaining: 312ms\n",
      "174:\tlearn: 0.0032192\ttotal: 2.1s\tremaining: 300ms\n",
      "175:\tlearn: 0.0031892\ttotal: 2.11s\tremaining: 288ms\n",
      "176:\tlearn: 0.0031046\ttotal: 2.13s\tremaining: 276ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177:\tlearn: 0.0030794\ttotal: 2.14s\tremaining: 264ms\n",
      "178:\tlearn: 0.0030241\ttotal: 2.15s\tremaining: 252ms\n",
      "179:\tlearn: 0.0029510\ttotal: 2.16s\tremaining: 240ms\n",
      "180:\tlearn: 0.0028990\ttotal: 2.17s\tremaining: 228ms\n",
      "181:\tlearn: 0.0028763\ttotal: 2.18s\tremaining: 216ms\n",
      "182:\tlearn: 0.0028317\ttotal: 2.19s\tremaining: 204ms\n",
      "183:\tlearn: 0.0028096\ttotal: 2.21s\tremaining: 192ms\n",
      "184:\tlearn: 0.0027619\ttotal: 2.22s\tremaining: 180ms\n",
      "185:\tlearn: 0.0027231\ttotal: 2.23s\tremaining: 168ms\n",
      "186:\tlearn: 0.0026310\ttotal: 2.24s\tremaining: 156ms\n",
      "187:\tlearn: 0.0025908\ttotal: 2.25s\tremaining: 144ms\n",
      "188:\tlearn: 0.0025553\ttotal: 2.27s\tremaining: 132ms\n",
      "189:\tlearn: 0.0025032\ttotal: 2.28s\tremaining: 120ms\n",
      "190:\tlearn: 0.0024293\ttotal: 2.29s\tremaining: 108ms\n",
      "191:\tlearn: 0.0023973\ttotal: 2.3s\tremaining: 95.9ms\n",
      "192:\tlearn: 0.0023661\ttotal: 2.31s\tremaining: 83.9ms\n",
      "193:\tlearn: 0.0023080\ttotal: 2.33s\tremaining: 71.9ms\n",
      "194:\tlearn: 0.0022808\ttotal: 2.34s\tremaining: 59.9ms\n",
      "195:\tlearn: 0.0022545\ttotal: 2.35s\tremaining: 47.9ms\n",
      "196:\tlearn: 0.0022066\ttotal: 2.36s\tremaining: 35.9ms\n",
      "197:\tlearn: 0.0021922\ttotal: 2.37s\tremaining: 24ms\n",
      "198:\tlearn: 0.0021280\ttotal: 2.38s\tremaining: 12ms\n",
      "199:\tlearn: 0.0020654\ttotal: 2.39s\tremaining: 0us\n",
      "Dataset 2:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317ad43e4b79424bb351b41a7ee98434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5144270923674501, Recall = 0.7968211920529802, Aging Rate = 0.5344370860927152, Precision = 0.7454770755885998, f1 = 0.7702944942381562\n",
      "Epoch 2: Train Loss = 0.3238765926629502, Recall = 0.8760264900662251, Aging Rate = 0.5045033112582782, Precision = 0.8682068784457863, f1 = 0.8720991561181433\n",
      "Epoch 3: Train Loss = 0.2420911085131942, Recall = 0.9160264900662252, Aging Rate = 0.5046357615894039, Precision = 0.9076115485564304, f1 = 0.9117996044825313\n",
      "Epoch 4: Train Loss = 0.1949294994564246, Recall = 0.9316556291390728, Aging Rate = 0.4981456953642384, Precision = 0.9351236373304972, f1 = 0.9333864118895966\n",
      "Epoch 5: Train Loss = 0.16170444797206399, Recall = 0.9435761589403974, Aging Rate = 0.4989403973509934, Precision = 0.9455800371648526, f1 = 0.9445770352691594\n",
      "Test Loss = 0.12468023660561896, Recall = 0.9703311258278146, Aging Rate = 0.5042384105960265, precision = 0.9621749408983451\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.11683734150221016, Recall = 0.9631788079470198, Aging Rate = 0.49682119205298014, Precision = 0.9693415089309517, f1 = 0.9662503321817698\n",
      "Epoch 7: Train Loss = 0.09593589583374806, Recall = 0.9705960264900663, Aging Rate = 0.49854304635761587, Precision = 0.973432518597237, f1 = 0.972012203209975\n",
      "Epoch 8: Train Loss = 0.08020581319730803, Recall = 0.9756291390728477, Aging Rate = 0.49841059602649007, Precision = 0.9787403667286739, f1 = 0.977182276465906\n",
      "Epoch 9: Train Loss = 0.06866978685883497, Recall = 0.9788079470198675, Aging Rate = 0.4982781456953642, Precision = 0.9821903242955875, f1 = 0.9804962186546371\n",
      "Epoch 10: Train Loss = 0.05943461228211392, Recall = 0.9856953642384106, Aging Rate = 0.5010596026490066, Precision = 0.9836108908273856, f1 = 0.9846520243450648\n",
      "Test Loss = 0.050177686154743696, Recall = 0.9888741721854305, Aging Rate = 0.5010596026490066, precision = 0.9867829764736982\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.05215562616209715, Recall = 0.9849006622516556, Aging Rate = 0.4996026490066225, Precision = 0.9856839872746553, f1 = 0.9852921690738041\n",
      "Epoch 12: Train Loss = 0.04603221906534094, Recall = 0.9886092715231788, Aging Rate = 0.5, Precision = 0.9886092715231788, f1 = 0.9886092715231788\n",
      "Epoch 13: Train Loss = 0.04087853834822478, Recall = 0.9912582781456953, Aging Rate = 0.5010596026490066, Precision = 0.9891620407084325, f1 = 0.990209050013231\n",
      "Epoch 14: Train Loss = 0.03607037000308763, Recall = 0.9917880794701986, Aging Rate = 0.5002649006622517, Precision = 0.9912629070691025, f1 = 0.9915254237288135\n",
      "Epoch 15: Train Loss = 0.03266660554692248, Recall = 0.992317880794702, Aging Rate = 0.49973509933774835, Precision = 0.9928438908030744, f1 = 0.9925808161102277\n",
      "Test Loss = 0.02743322222512092, Recall = 0.9954966887417218, Aging Rate = 0.5003973509933775, precision = 0.9947061937533086\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.02930119059338475, Recall = 0.9941721854304636, Aging Rate = 0.49973509933774835, Precision = 0.9946991783726478, f1 = 0.9944356120826711\n",
      "Epoch 17: Train Loss = 0.026853164514366364, Recall = 0.9941721854304636, Aging Rate = 0.5002649006622517, Precision = 0.9936457505957109, f1 = 0.9939088983050848\n",
      "Epoch 18: Train Loss = 0.024409394513396236, Recall = 0.9952317880794702, Aging Rate = 0.5003973509933775, Precision = 0.994441503440974, f1 = 0.9948364888123924\n",
      "Epoch 19: Train Loss = 0.022882959209382533, Recall = 0.9960264900662251, Aging Rate = 0.5005298013245033, Precision = 0.9949722148716592, f1 = 0.9954990733386285\n",
      "Epoch 20: Train Loss = 0.02065741604773809, Recall = 0.9968211920529801, Aging Rate = 0.5006622516556292, Precision = 0.9955026455026456, f1 = 0.9961614824619458\n",
      "Test Loss = 0.017121019274588454, Recall = 0.9970860927152317, Aging Rate = 0.49986754966887414, precision = 0.9973502914679385\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.018426923628970488, Recall = 0.9962913907284768, Aging Rate = 0.4996026490066225, Precision = 0.9970837751855779, f1 = 0.996687425467073\n",
      "Epoch 22: Train Loss = 0.018100397861223447, Recall = 0.9968211920529801, Aging Rate = 0.5002649006622517, Precision = 0.9962933545141647, f1 = 0.9965572033898306\n",
      "Epoch 23: Train Loss = 0.016661515735607865, Recall = 0.9960264900662251, Aging Rate = 0.49920529801324504, Precision = 0.9976120986999204, f1 = 0.9968186638388123\n",
      "Epoch 24: Train Loss = 0.01554916472772494, Recall = 0.9968211920529801, Aging Rate = 0.4996026490066225, Precision = 0.9976139978791092, f1 = 0.9972174373923414\n",
      "Epoch 25: Train Loss = 0.013556692686104616, Recall = 0.9978807947019868, Aging Rate = 0.49986754966887414, Precision = 0.998145204027557, f1 = 0.9980129818519009\n",
      "Test Loss = 0.011594503107819927, Recall = 0.9989403973509934, Aging Rate = 0.5002649006622517, precision = 0.9984114376489277\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.013481338246176575, Recall = 0.9970860927152317, Aging Rate = 0.5001324503311259, Precision = 0.996822033898305, f1 = 0.9969540458217454\n",
      "Epoch 27: Train Loss = 0.011924964259594481, Recall = 0.9984105960264901, Aging Rate = 0.5, Precision = 0.9984105960264901, f1 = 0.9984105960264901\n",
      "Epoch 28: Train Loss = 0.012144679563576417, Recall = 0.9981456953642384, Aging Rate = 0.49973509933774835, Precision = 0.9986747945931619, f1 = 0.998410174880763\n",
      "Epoch 29: Train Loss = 0.011019819644310617, Recall = 0.9986754966887417, Aging Rate = 0.5001324503311259, Precision = 0.9984110169491526, f1 = 0.9985432393060522\n",
      "Epoch 30: Train Loss = 0.010199018833011586, Recall = 0.9984105960264901, Aging Rate = 0.5001324503311259, Precision = 0.998146186440678, f1 = 0.9982783737253345\n",
      "Test Loss = 0.009088606078004996, Recall = 0.9973509933774835, Aging Rate = 0.4989403973509934, precision = 0.9994690735333156\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.009312775242604958, Recall = 0.9986754966887417, Aging Rate = 0.49986754966887414, Precision = 0.9989401165871754, f1 = 0.9988077891111405\n",
      "Epoch 32: Train Loss = 0.009182122789989441, Recall = 0.9984105960264901, Aging Rate = 0.49973509933774835, Precision = 0.9989398356745296, f1 = 0.9986751457339692\n",
      "Epoch 33: Train Loss = 0.009308635960178857, Recall = 0.9984105960264901, Aging Rate = 0.5002649006622517, Precision = 0.997881916865237, f1 = 0.998146186440678\n",
      "Epoch 34: Train Loss = 0.00909617171801676, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, Precision = 0.9989406779661016, f1 = 0.9990729704674877\n",
      "Epoch 35: Train Loss = 0.007708988761381302, Recall = 0.9986754966887417, Aging Rate = 0.4996026490066225, Precision = 0.9994697773064687, f1 = 0.9990724791307805\n",
      "Test Loss = 0.007112090924372341, Recall = 0.9989403973509934, Aging Rate = 0.4994701986754967, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.00894297554163842, Recall = 0.9984105960264901, Aging Rate = 0.5001324503311259, Precision = 0.998146186440678, f1 = 0.9982783737253345\n",
      "Epoch 37: Train Loss = 0.007585657918147299, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 38: Train Loss = 0.00791072018426035, Recall = 0.9989403973509934, Aging Rate = 0.5002649006622517, Precision = 0.9984114376489277, f1 = 0.998675847457627\n",
      "Epoch 39: Train Loss = 0.0073627248946700665, Recall = 0.9989403973509934, Aging Rate = 0.5002649006622517, Precision = 0.9984114376489277, f1 = 0.998675847457627\n",
      "Epoch 40: Train Loss = 0.00630943809127102, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Test Loss = 0.012308356743429276, Recall = 0.9933774834437086, Aging Rate = 0.4966887417218543, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.00714037980834972, Recall = 0.9989403973509934, Aging Rate = 0.5, Precision = 0.9989403973509934, f1 = 0.9989403973509934\n",
      "Epoch 42: Train Loss = 0.00873623574251272, Recall = 0.9981456953642384, Aging Rate = 0.5001324503311259, Precision = 0.9978813559322034, f1 = 0.9980135081446165\n",
      "Epoch 43: Train Loss = 0.005906611223464572, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, Precision = 0.9994700582935877, f1 = 0.9993376606173003\n",
      "Epoch 44: Train Loss = 0.0069405852254504794, Recall = 0.9989403973509934, Aging Rate = 0.5001324503311259, Precision = 0.9986758474576272, f1 = 0.9988081048867701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.005876683625731839, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Test Loss = 0.005787092661053337, Recall = 1.0, Aging Rate = 0.5006622516556292, precision = 0.9986772486772487\n",
      "\n",
      "Epoch 46: Train Loss = 0.006511512119508046, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 47: Train Loss = 0.006217662872166823, Recall = 0.9989403973509934, Aging Rate = 0.49986754966887414, Precision = 0.9992050874403816, f1 = 0.9990727248642204\n",
      "Epoch 48: Train Loss = 0.005482703294444656, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 49: Train Loss = 0.005944293022945227, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 50: Train Loss = 0.0053516253962224685, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Test Loss = 0.00558553401306765, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, precision = 0.9989406779661016\n",
      "\n",
      "Epoch 51: Train Loss = 0.005886007962711402, Recall = 0.9989403973509934, Aging Rate = 0.5, Precision = 0.9989403973509934, f1 = 0.9989403973509934\n",
      "Epoch 52: Train Loss = 0.005707895293067031, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, Precision = 0.9989406779661016, f1 = 0.9990729704674877\n",
      "Epoch 53: Train Loss = 0.005598936921713368, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 54: Train Loss = 0.005335371521061027, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 55: Train Loss = 0.006492973579840106, Recall = 0.9986754966887417, Aging Rate = 0.49973509933774835, Precision = 0.9992048767558972, f1 = 0.9989401165871753\n",
      "Test Loss = 0.004376012683963243, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, precision = 0.9992055084745762\n",
      "\n",
      "Epoch 56: Train Loss = 0.004732812216924812, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 57: Train Loss = 0.0051958027612171226, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, Precision = 0.9989406779661016, f1 = 0.9990729704674877\n",
      "Epoch 58: Train Loss = 0.0053287569733817645, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 59: Train Loss = 0.00586871334191239, Recall = 0.999205298013245, Aging Rate = 0.5002649006622517, Precision = 0.9986761980407731, f1 = 0.9989406779661018\n",
      "Epoch 60: Train Loss = 0.0054259496715400885, Recall = 0.9997350993377483, Aging Rate = 0.5005298013245033, Precision = 0.9986768986504366, f1 = 0.9992057188244637\n",
      "Test Loss = 0.004244250078309352, Recall = 0.999205298013245, Aging Rate = 0.4996026490066225, precision = 1.0\n",
      "Model in epoch 60 is saved.\n",
      "\n",
      "Epoch 61: Train Loss = 0.005102626779486752, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 62: Train Loss = 0.005223033088382753, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 63: Train Loss = 0.00557048851340417, Recall = 0.999205298013245, Aging Rate = 0.5001324503311259, Precision = 0.9989406779661016, f1 = 0.9990729704674877\n",
      "Epoch 64: Train Loss = 0.004691944100974214, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 65: Train Loss = 0.004887528271778234, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Test Loss = 0.00552118009061164, Recall = 1.0, Aging Rate = 0.5005298013245033, precision = 0.9989415189203493\n",
      "\n",
      "Epoch 66: Train Loss = 0.004534671091974176, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 67: Train Loss = 0.00496834715880725, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 68: Train Loss = 0.005160926607474438, Recall = 0.9994701986754967, Aging Rate = 0.5003973509933775, Precision = 0.9986765484383272, f1 = 0.9990732159406859\n",
      "Epoch 69: Train Loss = 0.005635535851651371, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 70: Train Loss = 0.004002440143253235, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Test Loss = 0.004496581459491952, Recall = 1.0, Aging Rate = 0.5005298013245033, precision = 0.9989415189203493\n",
      "\n",
      "Epoch 71: Train Loss = 0.0043312224974572065, Recall = 1.0, Aging Rate = 0.5003973509933775, Precision = 0.9992059290629963, f1 = 0.9996028068317225\n",
      "Epoch 72: Train Loss = 0.004464290130464052, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, Precision = 0.9994700582935877, f1 = 0.9993376606173003\n",
      "Epoch 73: Train Loss = 0.0044956597403333284, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 74: Train Loss = 0.0057691348629652075, Recall = 0.9986754966887417, Aging Rate = 0.5, Precision = 0.9986754966887417, f1 = 0.9986754966887417\n",
      "Epoch 75: Train Loss = 0.003902607989676346, Recall = 0.999205298013245, Aging Rate = 0.49973509933774835, Precision = 0.9997349589186324, f1 = 0.9994700582935877\n",
      "Test Loss = 0.0030897618642860146, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 76: Train Loss = 0.004125794545926203, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 77: Train Loss = 0.003807625464404261, Recall = 1.0, Aging Rate = 0.5002649006622517, Precision = 0.9994704792163093, f1 = 0.9997351694915254\n",
      "Epoch 78: Train Loss = 0.004714607955850907, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 79: Train Loss = 0.004571508774348837, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 80: Train Loss = 0.004267256499780034, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Test Loss = 0.0035494961146117243, Recall = 0.9997350993377483, Aging Rate = 0.5003973509933775, precision = 0.9989412387506618\n",
      "\n",
      "Epoch 81: Train Loss = 0.0045959465328715005, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 82: Train Loss = 0.003942886012304105, Recall = 1.0, Aging Rate = 0.5002649006622517, Precision = 0.9994704792163093, f1 = 0.9997351694915254\n",
      "Epoch 83: Train Loss = 0.005054855908680436, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 84: Train Loss = 0.00478343484053598, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 85: Train Loss = 0.003960101935728792, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Test Loss = 0.00414506979607391, Recall = 0.9986754966887417, Aging Rate = 0.49933774834437084, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.00456157143209253, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, Precision = 0.9994700582935877, f1 = 0.9993376606173003\n",
      "Epoch 87: Train Loss = 0.004735328625542241, Recall = 0.9997350993377483, Aging Rate = 0.5003973509933775, Precision = 0.9989412387506618, f1 = 0.9993380113862042\n",
      "Epoch 88: Train Loss = 0.004099711437789809, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 89: Train Loss = 0.004376978252564143, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 90: Train Loss = 0.004255390806826732, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Test Loss = 0.003441954588899944, Recall = 1.0, Aging Rate = 0.5003973509933775, precision = 0.9992059290629963\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss = 0.004139518155993906, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 92: Train Loss = 0.005446844366718306, Recall = 0.9986754966887417, Aging Rate = 0.49986754966887414, Precision = 0.9989401165871754, f1 = 0.9988077891111405\n",
      "Epoch 93: Train Loss = 0.0047455906357912235, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 94: Train Loss = 0.004970071758359474, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 95: Train Loss = 0.004032355463070585, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0029967773305437225, Recall = 0.9994701986754967, Aging Rate = 0.49973509933774835, precision = 1.0\n",
      "Model in epoch 95 is saved.\n",
      "\n",
      "Epoch 96: Train Loss = 0.003869303260430299, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 97: Train Loss = 0.0039448389426083455, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 98: Train Loss = 0.003978847383108261, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 99: Train Loss = 0.004464449483457188, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 100: Train Loss = 0.004508399641918425, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Test Loss = 0.0032177577510056708, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "Model in epoch 100 is saved.\n",
      "\n",
      "Epoch 101: Train Loss = 0.004131210634842614, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 102: Train Loss = 0.004218550056638504, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 103: Train Loss = 0.004176024494859211, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 104: Train Loss = 0.004013900948774736, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 105: Train Loss = 0.0047519933350927, Recall = 0.9997350993377483, Aging Rate = 0.5003973509933775, Precision = 0.9989412387506618, f1 = 0.9993380113862042\n",
      "Test Loss = 0.0030131585553245287, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.004364065654199595, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 107: Train Loss = 0.004136854985741195, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 108: Train Loss = 0.0037840911157626584, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 109: Train Loss = 0.003680688531341537, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 110: Train Loss = 0.005420385649310585, Recall = 0.9989403973509934, Aging Rate = 0.5001324503311259, Precision = 0.9986758474576272, f1 = 0.9988081048867701\n",
      "Test Loss = 0.003529033912160736, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 111: Train Loss = 0.00443992492015048, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 112: Train Loss = 0.004098075545263409, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 113: Train Loss = 0.004191324854800046, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 114: Train Loss = 0.0035315161345168848, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 115: Train Loss = 0.0033143668247861725, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0035227584655069754, Recall = 1.0, Aging Rate = 0.5003973509933775, precision = 0.9992059290629963\n",
      "\n",
      "Epoch 116: Train Loss = 0.004578695685559551, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 117: Train Loss = 0.004122806917012527, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 118: Train Loss = 0.0033919085192167207, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 119: Train Loss = 0.0047285666345735855, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 120: Train Loss = 0.005043891986449625, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Test Loss = 0.006446076617483666, Recall = 0.9986754966887417, Aging Rate = 0.4996026490066225, precision = 0.9994697773064687\n",
      "\n",
      "Epoch 121: Train Loss = 0.00416909737271929, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 122: Train Loss = 0.0038952029625953033, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 123: Train Loss = 0.0033149311561384144, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 124: Train Loss = 0.004401925619605262, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 125: Train Loss = 0.0033283362365543645, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005527986962292714, Recall = 1.0, Aging Rate = 0.5006622516556292, precision = 0.9986772486772487\n",
      "\n",
      "Epoch 126: Train Loss = 0.004629318556649648, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 127: Train Loss = 0.005771418076397123, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 128: Train Loss = 0.0043279596584053425, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 129: Train Loss = 0.004126091389608452, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 130: Train Loss = 0.004700711690620304, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Test Loss = 0.0034117577919761176, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 131: Train Loss = 0.003922780934129132, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 132: Train Loss = 0.0037482638518319816, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 133: Train Loss = 0.003938473432713392, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Epoch 134: Train Loss = 0.004594924795928578, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 135: Train Loss = 0.003601351305749884, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Test Loss = 0.0026170486401125097, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 136: Train Loss = 0.0040867580749119154, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: Train Loss = 0.004060453841129676, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 138: Train Loss = 0.004555129692920589, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 139: Train Loss = 0.00364837887222009, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 140: Train Loss = 0.0041018501671943935, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Test Loss = 0.003061426786739774, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 141: Train Loss = 0.0037615382628182308, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 142: Train Loss = 0.004194815648867693, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 143: Train Loss = 0.00406716800078947, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 144: Train Loss = 0.0043759397921824695, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 145: Train Loss = 0.0048661501106917115, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Test Loss = 0.003312898473999161, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 146: Train Loss = 0.0030252562331294776, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, Precision = 0, f1 = 0.0\n",
      "Epoch 147: Train Loss = 0.0038852934070881333, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, Precision = 0.9994700582935877, f1 = 0.9993376606173003\n",
      "Epoch 148: Train Loss = 0.004063545556072861, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 149: Train Loss = 0.004964010562464003, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 150: Train Loss = 0.003489482797071683, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.004543387749361873, Recall = 0.9989403973509934, Aging Rate = 0.4994701986754967, precision = 1.0\n",
      "\n",
      "Epoch 151: Train Loss = 0.005486596432569998, Recall = 0.9989403973509934, Aging Rate = 0.5001324503311259, Precision = 0.9986758474576272, f1 = 0.9988081048867701\n",
      "Epoch 152: Train Loss = 0.0033336787448813585, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 153: Train Loss = 0.00367459530573622, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 154: Train Loss = 0.004188973753164146, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 155: Train Loss = 0.004036924762601982, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Test Loss = 0.0032301924757610094, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 156: Train Loss = 0.0040222843339210316, Recall = 1.0, Aging Rate = 0.5002649006622517, Precision = 0.9994704792163093, f1 = 0.9997351694915254\n",
      "Epoch 157: Train Loss = 0.0037449491548271763, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 158: Train Loss = 0.0031420299966838086, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 159: Train Loss = 0.003496173901964497, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 160: Train Loss = 0.004521572224561446, Recall = 0.999205298013245, Aging Rate = 0.49986754966887414, Precision = 0.9994700582935877, f1 = 0.9993376606173003\n",
      "Test Loss = 0.004070626173279439, Recall = 1.0, Aging Rate = 0.5003973509933775, precision = 0.9992059290629963\n",
      "\n",
      "Epoch 161: Train Loss = 0.00384993298484987, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Epoch 162: Train Loss = 0.004323776894748606, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 163: Train Loss = 0.0055133100937657204, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 164: Train Loss = 0.0036588049749072815, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 165: Train Loss = 0.0031212942005252307, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003199473787715893, Recall = 1.0, Aging Rate = 0.5003973509933775, precision = 0.9992059290629963\n",
      "\n",
      "Epoch 166: Train Loss = 0.0034447582479294955, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 167: Train Loss = 0.005253971076928178, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 168: Train Loss = 0.0037825926154580534, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 169: Train Loss = 0.0039404580217866315, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Epoch 170: Train Loss = 0.003671843832704109, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0026020096067573555, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 171: Train Loss = 0.004044971125360751, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 172: Train Loss = 0.004577500875360819, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Epoch 173: Train Loss = 0.00453559424034289, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 174: Train Loss = 0.004709503236936023, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 175: Train Loss = 0.003321046464034165, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0030364028274371528, Recall = 1.0, Aging Rate = 0.5003973509933775, precision = 0.9992059290629963\n",
      "\n",
      "Epoch 176: Train Loss = 0.0038589171646471253, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 177: Train Loss = 0.0031582206873136836, Recall = 1.0, Aging Rate = 0.5001324503311259, Precision = 0.9997351694915254, f1 = 0.9998675672096411\n",
      "Epoch 178: Train Loss = 0.003822198613910683, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 179: Train Loss = 0.003868997259657608, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n",
      "Epoch 180: Train Loss = 0.004651283354356589, Recall = 0.999205298013245, Aging Rate = 0.5, Precision = 0.999205298013245, f1 = 0.999205298013245\n",
      "Test Loss = 0.0031506367395971193, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 181: Train Loss = 0.0034428703207645983, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 182: Train Loss = 0.004050302509440492, Recall = 0.9994701986754967, Aging Rate = 0.5, Precision = 0.9994701986754967, f1 = 0.9994701986754967\n",
      "Epoch 183: Train Loss = 0.004095666638964059, Recall = 0.9994701986754967, Aging Rate = 0.49986754966887414, Precision = 0.9997350291467939, f1 = 0.9996025963703802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184: Train Loss = 0.003625555183023017, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 185: Train Loss = 0.0031629861663064816, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0029637642810429565, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 186: Train Loss = 0.0037203136104350257, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 187: Train Loss = 0.006347730471598392, Recall = 0.9984105960264901, Aging Rate = 0.49986754966887414, Precision = 0.9986751457339693, f1 = 0.9985428533580607\n",
      "Epoch 188: Train Loss = 0.004024988554313236, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 189: Train Loss = 0.003557200784184384, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 190: Train Loss = 0.0036481951966585704, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0032557662810809565, Recall = 0.9997350993377483, Aging Rate = 0.49986754966887414, precision = 1.0\n",
      "\n",
      "Epoch 191: Train Loss = 0.0033791203354022756, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 192: Train Loss = 0.004339983680081979, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, Precision = 0.9992057188244639, f1 = 0.9994703389830508\n",
      "Epoch 193: Train Loss = 0.003144298318853688, Recall = 0.9997350993377483, Aging Rate = 0.5001324503311259, Precision = 0.9994703389830508, f1 = 0.9996027016289233\n",
      "Epoch 194: Train Loss = 0.00432481915215092, Recall = 0.9994701986754967, Aging Rate = 0.5002649006622517, Precision = 0.9989409584326184, f1 = 0.9992055084745762\n",
      "Epoch 195: Train Loss = 0.0037301495562720773, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0029718991024234634, Recall = 0.9997350993377483, Aging Rate = 0.5, precision = 0.9997350993377483\n",
      "\n",
      "Epoch 196: Train Loss = 0.0052845813380245936, Recall = 0.9994701986754967, Aging Rate = 0.5003973509933775, Precision = 0.9986765484383272, f1 = 0.9990732159406859\n",
      "Epoch 197: Train Loss = 0.003861323389045845, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 198: Train Loss = 0.004684142708864729, Recall = 0.9994701986754967, Aging Rate = 0.5001324503311259, Precision = 0.9992055084745762, f1 = 0.9993378360482056\n",
      "Epoch 199: Train Loss = 0.0032143255862228525, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Epoch 200: Train Loss = 0.0034144884044464852, Recall = 0.9997350993377483, Aging Rate = 0.5, Precision = 0.9997350993377483, f1 = 0.9997350993377483\n",
      "Test Loss = 0.0033449533520827674, Recall = 0.9997350993377483, Aging Rate = 0.5002649006622517, precision = 0.9992057188244639\n",
      "\n",
      "Training Finished at epoch 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5239266\ttotal: 15.1ms\tremaining: 4.52s\n",
      "1:\tlearn: 0.4361534\ttotal: 30.1ms\tremaining: 4.49s\n",
      "2:\tlearn: 0.3783537\ttotal: 45.2ms\tremaining: 4.47s\n",
      "3:\tlearn: 0.3485698\ttotal: 60.2ms\tremaining: 4.46s\n",
      "4:\tlearn: 0.3173279\ttotal: 74.4ms\tremaining: 4.39s\n",
      "5:\tlearn: 0.2919051\ttotal: 86.8ms\tremaining: 4.25s\n",
      "6:\tlearn: 0.2627729\ttotal: 99.7ms\tremaining: 4.17s\n",
      "7:\tlearn: 0.2461671\ttotal: 112ms\tremaining: 4.11s\n",
      "8:\tlearn: 0.2231313\ttotal: 126ms\tremaining: 4.07s\n",
      "9:\tlearn: 0.2061110\ttotal: 139ms\tremaining: 4.02s\n",
      "10:\tlearn: 0.1878876\ttotal: 151ms\tremaining: 3.98s\n",
      "11:\tlearn: 0.1739924\ttotal: 164ms\tremaining: 3.94s\n",
      "12:\tlearn: 0.1641504\ttotal: 177ms\tremaining: 3.9s\n",
      "13:\tlearn: 0.1564175\ttotal: 190ms\tremaining: 3.88s\n",
      "14:\tlearn: 0.1489377\ttotal: 203ms\tremaining: 3.86s\n",
      "15:\tlearn: 0.1411895\ttotal: 216ms\tremaining: 3.83s\n",
      "16:\tlearn: 0.1323907\ttotal: 228ms\tremaining: 3.8s\n",
      "17:\tlearn: 0.1266166\ttotal: 241ms\tremaining: 3.78s\n",
      "18:\tlearn: 0.1202868\ttotal: 254ms\tremaining: 3.75s\n",
      "19:\tlearn: 0.1165783\ttotal: 275ms\tremaining: 3.84s\n",
      "20:\tlearn: 0.1073039\ttotal: 297ms\tremaining: 3.94s\n",
      "21:\tlearn: 0.1037673\ttotal: 310ms\tremaining: 3.92s\n",
      "22:\tlearn: 0.0983826\ttotal: 323ms\tremaining: 3.88s\n",
      "23:\tlearn: 0.0931397\ttotal: 336ms\tremaining: 3.87s\n",
      "24:\tlearn: 0.0906183\ttotal: 349ms\tremaining: 3.84s\n",
      "25:\tlearn: 0.0854501\ttotal: 362ms\tremaining: 3.81s\n",
      "26:\tlearn: 0.0831515\ttotal: 375ms\tremaining: 3.79s\n",
      "27:\tlearn: 0.0790905\ttotal: 388ms\tremaining: 3.77s\n",
      "28:\tlearn: 0.0752333\ttotal: 401ms\tremaining: 3.75s\n",
      "29:\tlearn: 0.0719957\ttotal: 414ms\tremaining: 3.73s\n",
      "30:\tlearn: 0.0688611\ttotal: 427ms\tremaining: 3.71s\n",
      "31:\tlearn: 0.0659440\ttotal: 440ms\tremaining: 3.69s\n",
      "32:\tlearn: 0.0638614\ttotal: 454ms\tremaining: 3.67s\n",
      "33:\tlearn: 0.0638461\ttotal: 456ms\tremaining: 3.57s\n",
      "34:\tlearn: 0.0615313\ttotal: 469ms\tremaining: 3.55s\n",
      "35:\tlearn: 0.0585174\ttotal: 482ms\tremaining: 3.53s\n",
      "36:\tlearn: 0.0559484\ttotal: 495ms\tremaining: 3.52s\n",
      "37:\tlearn: 0.0545607\ttotal: 508ms\tremaining: 3.5s\n",
      "38:\tlearn: 0.0521222\ttotal: 521ms\tremaining: 3.49s\n",
      "39:\tlearn: 0.0508277\ttotal: 534ms\tremaining: 3.47s\n",
      "40:\tlearn: 0.0497244\ttotal: 548ms\tremaining: 3.46s\n",
      "41:\tlearn: 0.0487943\ttotal: 562ms\tremaining: 3.45s\n",
      "42:\tlearn: 0.0478077\ttotal: 575ms\tremaining: 3.44s\n",
      "43:\tlearn: 0.0460166\ttotal: 588ms\tremaining: 3.42s\n",
      "44:\tlearn: 0.0452099\ttotal: 602ms\tremaining: 3.41s\n",
      "45:\tlearn: 0.0440691\ttotal: 614ms\tremaining: 3.39s\n",
      "46:\tlearn: 0.0428048\ttotal: 628ms\tremaining: 3.38s\n",
      "47:\tlearn: 0.0413305\ttotal: 641ms\tremaining: 3.37s\n",
      "48:\tlearn: 0.0413267\ttotal: 643ms\tremaining: 3.29s\n",
      "49:\tlearn: 0.0399943\ttotal: 657ms\tremaining: 3.28s\n",
      "50:\tlearn: 0.0390555\ttotal: 670ms\tremaining: 3.27s\n",
      "51:\tlearn: 0.0381798\ttotal: 683ms\tremaining: 3.26s\n",
      "52:\tlearn: 0.0377022\ttotal: 696ms\tremaining: 3.24s\n",
      "53:\tlearn: 0.0365696\ttotal: 709ms\tremaining: 3.23s\n",
      "54:\tlearn: 0.0365249\ttotal: 722ms\tremaining: 3.21s\n",
      "55:\tlearn: 0.0364749\ttotal: 735ms\tremaining: 3.2s\n",
      "56:\tlearn: 0.0364748\ttotal: 737ms\tremaining: 3.14s\n",
      "57:\tlearn: 0.0357214\ttotal: 751ms\tremaining: 3.13s\n",
      "58:\tlearn: 0.0348317\ttotal: 764ms\tremaining: 3.12s\n",
      "59:\tlearn: 0.0340530\ttotal: 777ms\tremaining: 3.11s\n",
      "60:\tlearn: 0.0335848\ttotal: 791ms\tremaining: 3.1s\n",
      "61:\tlearn: 0.0327460\ttotal: 804ms\tremaining: 3.08s\n",
      "62:\tlearn: 0.0317063\ttotal: 817ms\tremaining: 3.08s\n",
      "63:\tlearn: 0.0312011\ttotal: 831ms\tremaining: 3.06s\n",
      "64:\tlearn: 0.0304427\ttotal: 844ms\tremaining: 3.05s\n",
      "65:\tlearn: 0.0302487\ttotal: 858ms\tremaining: 3.04s\n",
      "66:\tlearn: 0.0297698\ttotal: 872ms\tremaining: 3.03s\n",
      "67:\tlearn: 0.0297174\ttotal: 885ms\tremaining: 3.02s\n",
      "68:\tlearn: 0.0297173\ttotal: 887ms\tremaining: 2.97s\n",
      "69:\tlearn: 0.0297172\ttotal: 889ms\tremaining: 2.92s\n",
      "70:\tlearn: 0.0290733\ttotal: 902ms\tremaining: 2.91s\n",
      "71:\tlearn: 0.0290733\ttotal: 904ms\tremaining: 2.86s\n",
      "72:\tlearn: 0.0287024\ttotal: 917ms\tremaining: 2.85s\n",
      "73:\tlearn: 0.0281810\ttotal: 930ms\tremaining: 2.84s\n",
      "74:\tlearn: 0.0279252\ttotal: 944ms\tremaining: 2.83s\n",
      "75:\tlearn: 0.0275808\ttotal: 957ms\tremaining: 2.82s\n",
      "76:\tlearn: 0.0273049\ttotal: 971ms\tremaining: 2.81s\n",
      "77:\tlearn: 0.0268641\ttotal: 985ms\tremaining: 2.8s\n",
      "78:\tlearn: 0.0261338\ttotal: 999ms\tremaining: 2.79s\n",
      "79:\tlearn: 0.0253487\ttotal: 1.01s\tremaining: 2.78s\n",
      "80:\tlearn: 0.0248286\ttotal: 1.03s\tremaining: 2.77s\n",
      "81:\tlearn: 0.0245032\ttotal: 1.04s\tremaining: 2.76s\n",
      "82:\tlearn: 0.0240182\ttotal: 1.05s\tremaining: 2.75s\n",
      "83:\tlearn: 0.0236623\ttotal: 1.07s\tremaining: 2.74s\n",
      "84:\tlearn: 0.0231233\ttotal: 1.08s\tremaining: 2.73s\n",
      "85:\tlearn: 0.0229769\ttotal: 1.09s\tremaining: 2.72s\n",
      "86:\tlearn: 0.0227589\ttotal: 1.11s\tremaining: 2.71s\n",
      "87:\tlearn: 0.0223366\ttotal: 1.12s\tremaining: 2.7s\n",
      "88:\tlearn: 0.0218845\ttotal: 1.13s\tremaining: 2.69s\n",
      "89:\tlearn: 0.0215639\ttotal: 1.15s\tremaining: 2.68s\n",
      "90:\tlearn: 0.0215001\ttotal: 1.16s\tremaining: 2.67s\n",
      "91:\tlearn: 0.0213126\ttotal: 1.18s\tremaining: 2.66s\n",
      "92:\tlearn: 0.0211945\ttotal: 1.19s\tremaining: 2.65s\n",
      "93:\tlearn: 0.0208281\ttotal: 1.2s\tremaining: 2.63s\n",
      "94:\tlearn: 0.0204823\ttotal: 1.22s\tremaining: 2.62s\n",
      "95:\tlearn: 0.0202910\ttotal: 1.23s\tremaining: 2.61s\n",
      "96:\tlearn: 0.0201592\ttotal: 1.24s\tremaining: 2.6s\n",
      "97:\tlearn: 0.0200138\ttotal: 1.25s\tremaining: 2.59s\n",
      "98:\tlearn: 0.0198375\ttotal: 1.28s\tremaining: 2.59s\n",
      "99:\tlearn: 0.0195017\ttotal: 1.3s\tremaining: 2.6s\n",
      "100:\tlearn: 0.0192342\ttotal: 1.32s\tremaining: 2.61s\n",
      "101:\tlearn: 0.0190914\ttotal: 1.35s\tremaining: 2.62s\n",
      "102:\tlearn: 0.0190243\ttotal: 1.37s\tremaining: 2.63s\n",
      "103:\tlearn: 0.0187458\ttotal: 1.39s\tremaining: 2.61s\n",
      "104:\tlearn: 0.0185856\ttotal: 1.4s\tremaining: 2.6s\n",
      "105:\tlearn: 0.0183468\ttotal: 1.42s\tremaining: 2.59s\n",
      "106:\tlearn: 0.0181351\ttotal: 1.43s\tremaining: 2.58s\n",
      "107:\tlearn: 0.0177597\ttotal: 1.44s\tremaining: 2.56s\n",
      "108:\tlearn: 0.0174887\ttotal: 1.46s\tremaining: 2.55s\n",
      "109:\tlearn: 0.0172224\ttotal: 1.47s\tremaining: 2.54s\n",
      "110:\tlearn: 0.0169568\ttotal: 1.48s\tremaining: 2.52s\n",
      "111:\tlearn: 0.0165878\ttotal: 1.5s\tremaining: 2.51s\n",
      "112:\tlearn: 0.0165554\ttotal: 1.51s\tremaining: 2.5s\n",
      "113:\tlearn: 0.0165553\ttotal: 1.52s\tremaining: 2.48s\n",
      "114:\tlearn: 0.0163848\ttotal: 1.54s\tremaining: 2.47s\n",
      "115:\tlearn: 0.0160715\ttotal: 1.55s\tremaining: 2.46s\n",
      "116:\tlearn: 0.0158013\ttotal: 1.56s\tremaining: 2.44s\n",
      "117:\tlearn: 0.0157104\ttotal: 1.58s\tremaining: 2.43s\n",
      "118:\tlearn: 0.0155571\ttotal: 1.59s\tremaining: 2.42s\n",
      "119:\tlearn: 0.0153880\ttotal: 1.6s\tremaining: 2.41s\n",
      "120:\tlearn: 0.0152044\ttotal: 1.62s\tremaining: 2.39s\n",
      "121:\tlearn: 0.0150359\ttotal: 1.63s\tremaining: 2.38s\n",
      "122:\tlearn: 0.0149053\ttotal: 1.64s\tremaining: 2.37s\n",
      "123:\tlearn: 0.0147505\ttotal: 1.66s\tremaining: 2.35s\n",
      "124:\tlearn: 0.0145513\ttotal: 1.67s\tremaining: 2.34s\n",
      "125:\tlearn: 0.0144505\ttotal: 1.69s\tremaining: 2.33s\n",
      "126:\tlearn: 0.0141949\ttotal: 1.7s\tremaining: 2.31s\n",
      "127:\tlearn: 0.0141179\ttotal: 1.71s\tremaining: 2.3s\n",
      "128:\tlearn: 0.0140235\ttotal: 1.73s\tremaining: 2.29s\n",
      "129:\tlearn: 0.0137748\ttotal: 1.74s\tremaining: 2.27s\n",
      "130:\tlearn: 0.0136289\ttotal: 1.75s\tremaining: 2.26s\n",
      "131:\tlearn: 0.0135667\ttotal: 1.77s\tremaining: 2.25s\n",
      "132:\tlearn: 0.0134943\ttotal: 1.78s\tremaining: 2.23s\n",
      "133:\tlearn: 0.0134468\ttotal: 1.79s\tremaining: 2.22s\n",
      "134:\tlearn: 0.0133661\ttotal: 1.8s\tremaining: 2.2s\n",
      "135:\tlearn: 0.0132609\ttotal: 1.81s\tremaining: 2.19s\n",
      "136:\tlearn: 0.0131102\ttotal: 1.83s\tremaining: 2.17s\n",
      "137:\tlearn: 0.0130302\ttotal: 1.84s\tremaining: 2.16s\n",
      "138:\tlearn: 0.0129021\ttotal: 1.85s\tremaining: 2.15s\n",
      "139:\tlearn: 0.0127133\ttotal: 1.87s\tremaining: 2.13s\n",
      "140:\tlearn: 0.0126067\ttotal: 1.88s\tremaining: 2.12s\n",
      "141:\tlearn: 0.0124988\ttotal: 1.89s\tremaining: 2.11s\n",
      "142:\tlearn: 0.0124988\ttotal: 1.9s\tremaining: 2.08s\n",
      "143:\tlearn: 0.0123263\ttotal: 1.91s\tremaining: 2.07s\n",
      "144:\tlearn: 0.0122605\ttotal: 1.92s\tremaining: 2.06s\n",
      "145:\tlearn: 0.0121700\ttotal: 1.94s\tremaining: 2.04s\n",
      "146:\tlearn: 0.0120013\ttotal: 1.95s\tremaining: 2.03s\n",
      "147:\tlearn: 0.0118899\ttotal: 1.96s\tremaining: 2.02s\n",
      "148:\tlearn: 0.0118364\ttotal: 1.98s\tremaining: 2s\n",
      "149:\tlearn: 0.0117249\ttotal: 1.99s\tremaining: 1.99s\n",
      "150:\tlearn: 0.0116242\ttotal: 2s\tremaining: 1.98s\n",
      "151:\tlearn: 0.0114673\ttotal: 2.02s\tremaining: 1.96s\n",
      "152:\tlearn: 0.0113574\ttotal: 2.03s\tremaining: 1.95s\n",
      "153:\tlearn: 0.0113091\ttotal: 2.04s\tremaining: 1.94s\n",
      "154:\tlearn: 0.0111287\ttotal: 2.06s\tremaining: 1.93s\n",
      "155:\tlearn: 0.0110458\ttotal: 2.07s\tremaining: 1.91s\n",
      "156:\tlearn: 0.0109223\ttotal: 2.08s\tremaining: 1.9s\n",
      "157:\tlearn: 0.0107513\ttotal: 2.1s\tremaining: 1.89s\n",
      "158:\tlearn: 0.0106850\ttotal: 2.11s\tremaining: 1.87s\n",
      "159:\tlearn: 0.0106031\ttotal: 2.13s\tremaining: 1.86s\n",
      "160:\tlearn: 0.0105156\ttotal: 2.14s\tremaining: 1.85s\n",
      "161:\tlearn: 0.0105155\ttotal: 2.15s\tremaining: 1.83s\n",
      "162:\tlearn: 0.0103917\ttotal: 2.17s\tremaining: 1.82s\n",
      "163:\tlearn: 0.0102780\ttotal: 2.18s\tremaining: 1.81s\n",
      "164:\tlearn: 0.0101509\ttotal: 2.19s\tremaining: 1.79s\n",
      "165:\tlearn: 0.0100334\ttotal: 2.21s\tremaining: 1.78s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166:\tlearn: 0.0099200\ttotal: 2.22s\tremaining: 1.77s\n",
      "167:\tlearn: 0.0098763\ttotal: 2.23s\tremaining: 1.75s\n",
      "168:\tlearn: 0.0097410\ttotal: 2.25s\tremaining: 1.74s\n",
      "169:\tlearn: 0.0096498\ttotal: 2.26s\tremaining: 1.73s\n",
      "170:\tlearn: 0.0096498\ttotal: 2.29s\tremaining: 1.72s\n",
      "171:\tlearn: 0.0095518\ttotal: 2.31s\tremaining: 1.72s\n",
      "172:\tlearn: 0.0094551\ttotal: 2.33s\tremaining: 1.71s\n",
      "173:\tlearn: 0.0093909\ttotal: 2.36s\tremaining: 1.71s\n",
      "174:\tlearn: 0.0093632\ttotal: 2.37s\tremaining: 1.7s\n",
      "175:\tlearn: 0.0093185\ttotal: 2.39s\tremaining: 1.68s\n",
      "176:\tlearn: 0.0092938\ttotal: 2.4s\tremaining: 1.67s\n",
      "177:\tlearn: 0.0092287\ttotal: 2.41s\tremaining: 1.66s\n",
      "178:\tlearn: 0.0091527\ttotal: 2.43s\tremaining: 1.64s\n",
      "179:\tlearn: 0.0090883\ttotal: 2.44s\tremaining: 1.63s\n",
      "180:\tlearn: 0.0090402\ttotal: 2.45s\tremaining: 1.61s\n",
      "181:\tlearn: 0.0089481\ttotal: 2.47s\tremaining: 1.6s\n",
      "182:\tlearn: 0.0088315\ttotal: 2.48s\tremaining: 1.59s\n",
      "183:\tlearn: 0.0088315\ttotal: 2.5s\tremaining: 1.57s\n",
      "184:\tlearn: 0.0088314\ttotal: 2.51s\tremaining: 1.56s\n",
      "185:\tlearn: 0.0087321\ttotal: 2.52s\tremaining: 1.54s\n",
      "186:\tlearn: 0.0087321\ttotal: 2.54s\tremaining: 1.53s\n",
      "187:\tlearn: 0.0086421\ttotal: 2.55s\tremaining: 1.52s\n",
      "188:\tlearn: 0.0084871\ttotal: 2.56s\tremaining: 1.5s\n",
      "189:\tlearn: 0.0084100\ttotal: 2.58s\tremaining: 1.49s\n",
      "190:\tlearn: 0.0083506\ttotal: 2.59s\tremaining: 1.48s\n",
      "191:\tlearn: 0.0082739\ttotal: 2.6s\tremaining: 1.46s\n",
      "192:\tlearn: 0.0081888\ttotal: 2.62s\tremaining: 1.45s\n",
      "193:\tlearn: 0.0081424\ttotal: 2.63s\tremaining: 1.44s\n",
      "194:\tlearn: 0.0081424\ttotal: 2.64s\tremaining: 1.42s\n",
      "195:\tlearn: 0.0081423\ttotal: 2.66s\tremaining: 1.41s\n",
      "196:\tlearn: 0.0080866\ttotal: 2.67s\tremaining: 1.4s\n",
      "197:\tlearn: 0.0080427\ttotal: 2.68s\tremaining: 1.38s\n",
      "198:\tlearn: 0.0079730\ttotal: 2.7s\tremaining: 1.37s\n",
      "199:\tlearn: 0.0079294\ttotal: 2.71s\tremaining: 1.35s\n",
      "200:\tlearn: 0.0078820\ttotal: 2.72s\tremaining: 1.34s\n",
      "201:\tlearn: 0.0078007\ttotal: 2.74s\tremaining: 1.33s\n",
      "202:\tlearn: 0.0077307\ttotal: 2.75s\tremaining: 1.31s\n",
      "203:\tlearn: 0.0076579\ttotal: 2.76s\tremaining: 1.3s\n",
      "204:\tlearn: 0.0075882\ttotal: 2.78s\tremaining: 1.29s\n",
      "205:\tlearn: 0.0075349\ttotal: 2.79s\tremaining: 1.27s\n",
      "206:\tlearn: 0.0074960\ttotal: 2.8s\tremaining: 1.26s\n",
      "207:\tlearn: 0.0074439\ttotal: 2.82s\tremaining: 1.25s\n",
      "208:\tlearn: 0.0073836\ttotal: 2.83s\tremaining: 1.23s\n",
      "209:\tlearn: 0.0073450\ttotal: 2.85s\tremaining: 1.22s\n",
      "210:\tlearn: 0.0073450\ttotal: 2.86s\tremaining: 1.21s\n",
      "211:\tlearn: 0.0073450\ttotal: 2.87s\tremaining: 1.19s\n",
      "212:\tlearn: 0.0073449\ttotal: 2.88s\tremaining: 1.18s\n",
      "213:\tlearn: 0.0073152\ttotal: 2.9s\tremaining: 1.17s\n",
      "214:\tlearn: 0.0073146\ttotal: 2.91s\tremaining: 1.15s\n",
      "215:\tlearn: 0.0072771\ttotal: 2.93s\tremaining: 1.14s\n",
      "216:\tlearn: 0.0072398\ttotal: 2.94s\tremaining: 1.12s\n",
      "217:\tlearn: 0.0072055\ttotal: 2.95s\tremaining: 1.11s\n",
      "218:\tlearn: 0.0072054\ttotal: 2.97s\tremaining: 1.1s\n",
      "219:\tlearn: 0.0071701\ttotal: 2.98s\tremaining: 1.08s\n",
      "220:\tlearn: 0.0071463\ttotal: 2.99s\tremaining: 1.07s\n",
      "221:\tlearn: 0.0071225\ttotal: 3.01s\tremaining: 1.06s\n",
      "222:\tlearn: 0.0071146\ttotal: 3.02s\tremaining: 1.04s\n",
      "223:\tlearn: 0.0070638\ttotal: 3.03s\tremaining: 1.03s\n",
      "224:\tlearn: 0.0070176\ttotal: 3.04s\tremaining: 1.01s\n",
      "225:\tlearn: 0.0069755\ttotal: 3.06s\tremaining: 1s\n",
      "226:\tlearn: 0.0069450\ttotal: 3.07s\tremaining: 988ms\n",
      "227:\tlearn: 0.0069141\ttotal: 3.08s\tremaining: 975ms\n",
      "228:\tlearn: 0.0068775\ttotal: 3.1s\tremaining: 961ms\n",
      "229:\tlearn: 0.0068774\ttotal: 3.11s\tremaining: 947ms\n",
      "230:\tlearn: 0.0068378\ttotal: 3.13s\tremaining: 934ms\n",
      "231:\tlearn: 0.0068084\ttotal: 3.14s\tremaining: 920ms\n",
      "232:\tlearn: 0.0067561\ttotal: 3.15s\tremaining: 905ms\n",
      "233:\tlearn: 0.0067561\ttotal: 3.16s\tremaining: 892ms\n",
      "234:\tlearn: 0.0067182\ttotal: 3.17s\tremaining: 878ms\n",
      "235:\tlearn: 0.0066821\ttotal: 3.19s\tremaining: 865ms\n",
      "236:\tlearn: 0.0066820\ttotal: 3.2s\tremaining: 851ms\n",
      "237:\tlearn: 0.0066242\ttotal: 3.21s\tremaining: 837ms\n",
      "238:\tlearn: 0.0065698\ttotal: 3.23s\tremaining: 824ms\n",
      "239:\tlearn: 0.0065188\ttotal: 3.24s\tremaining: 810ms\n",
      "240:\tlearn: 0.0064793\ttotal: 3.25s\tremaining: 797ms\n",
      "241:\tlearn: 0.0064793\ttotal: 3.28s\tremaining: 785ms\n",
      "242:\tlearn: 0.0064418\ttotal: 3.3s\tremaining: 774ms\n",
      "243:\tlearn: 0.0064049\ttotal: 3.32s\tremaining: 763ms\n",
      "244:\tlearn: 0.0064049\ttotal: 3.35s\tremaining: 751ms\n",
      "245:\tlearn: 0.0064030\ttotal: 3.37s\tremaining: 740ms\n",
      "246:\tlearn: 0.0064028\ttotal: 3.38s\tremaining: 726ms\n",
      "247:\tlearn: 0.0063878\ttotal: 3.4s\tremaining: 712ms\n",
      "248:\tlearn: 0.0063333\ttotal: 3.41s\tremaining: 699ms\n",
      "249:\tlearn: 0.0062954\ttotal: 3.42s\tremaining: 685ms\n",
      "250:\tlearn: 0.0062649\ttotal: 3.44s\tremaining: 671ms\n",
      "251:\tlearn: 0.0062380\ttotal: 3.45s\tremaining: 658ms\n",
      "252:\tlearn: 0.0062380\ttotal: 3.46s\tremaining: 644ms\n",
      "253:\tlearn: 0.0062380\ttotal: 3.48s\tremaining: 630ms\n",
      "254:\tlearn: 0.0062380\ttotal: 3.49s\tremaining: 616ms\n",
      "255:\tlearn: 0.0062379\ttotal: 3.5s\tremaining: 602ms\n",
      "256:\tlearn: 0.0062060\ttotal: 3.51s\tremaining: 587ms\n",
      "257:\tlearn: 0.0062060\ttotal: 3.52s\tremaining: 573ms\n",
      "258:\tlearn: 0.0062060\ttotal: 3.53s\tremaining: 560ms\n",
      "259:\tlearn: 0.0061647\ttotal: 3.55s\tremaining: 546ms\n",
      "260:\tlearn: 0.0061646\ttotal: 3.56s\tremaining: 532ms\n",
      "261:\tlearn: 0.0061225\ttotal: 3.57s\tremaining: 518ms\n",
      "262:\tlearn: 0.0060755\ttotal: 3.59s\tremaining: 505ms\n",
      "263:\tlearn: 0.0060754\ttotal: 3.6s\tremaining: 491ms\n",
      "264:\tlearn: 0.0060754\ttotal: 3.62s\tremaining: 478ms\n",
      "265:\tlearn: 0.0060754\ttotal: 3.63s\tremaining: 464ms\n",
      "266:\tlearn: 0.0060473\ttotal: 3.64s\tremaining: 450ms\n",
      "267:\tlearn: 0.0060472\ttotal: 3.65s\tremaining: 436ms\n",
      "268:\tlearn: 0.0060472\ttotal: 3.67s\tremaining: 423ms\n",
      "269:\tlearn: 0.0060468\ttotal: 3.68s\tremaining: 409ms\n",
      "270:\tlearn: 0.0060468\ttotal: 3.69s\tremaining: 395ms\n",
      "271:\tlearn: 0.0060462\ttotal: 3.71s\tremaining: 382ms\n",
      "272:\tlearn: 0.0060462\ttotal: 3.72s\tremaining: 368ms\n",
      "273:\tlearn: 0.0060338\ttotal: 3.73s\tremaining: 354ms\n",
      "274:\tlearn: 0.0060338\ttotal: 3.75s\tremaining: 341ms\n",
      "275:\tlearn: 0.0059938\ttotal: 3.76s\tremaining: 327ms\n",
      "276:\tlearn: 0.0059657\ttotal: 3.77s\tremaining: 313ms\n",
      "277:\tlearn: 0.0059656\ttotal: 3.79s\tremaining: 300ms\n",
      "278:\tlearn: 0.0059656\ttotal: 3.8s\tremaining: 286ms\n",
      "279:\tlearn: 0.0059317\ttotal: 3.81s\tremaining: 272ms\n",
      "280:\tlearn: 0.0059316\ttotal: 3.83s\tremaining: 259ms\n",
      "281:\tlearn: 0.0059316\ttotal: 3.84s\tremaining: 245ms\n",
      "282:\tlearn: 0.0059316\ttotal: 3.85s\tremaining: 231ms\n",
      "283:\tlearn: 0.0059097\ttotal: 3.86s\tremaining: 218ms\n",
      "284:\tlearn: 0.0059097\ttotal: 3.88s\tremaining: 204ms\n",
      "285:\tlearn: 0.0059096\ttotal: 3.89s\tremaining: 190ms\n",
      "286:\tlearn: 0.0059096\ttotal: 3.9s\tremaining: 177ms\n",
      "287:\tlearn: 0.0058230\ttotal: 3.92s\tremaining: 163ms\n",
      "288:\tlearn: 0.0058230\ttotal: 3.93s\tremaining: 150ms\n",
      "289:\tlearn: 0.0058230\ttotal: 3.94s\tremaining: 136ms\n",
      "290:\tlearn: 0.0058230\ttotal: 3.96s\tremaining: 122ms\n",
      "291:\tlearn: 0.0058229\ttotal: 3.97s\tremaining: 109ms\n",
      "292:\tlearn: 0.0058228\ttotal: 3.98s\tremaining: 95.1ms\n",
      "293:\tlearn: 0.0057910\ttotal: 3.99s\tremaining: 81.5ms\n",
      "294:\tlearn: 0.0057613\ttotal: 4.01s\tremaining: 67.9ms\n",
      "295:\tlearn: 0.0057070\ttotal: 4.02s\tremaining: 54.3ms\n",
      "296:\tlearn: 0.0057070\ttotal: 4.03s\tremaining: 40.8ms\n",
      "297:\tlearn: 0.0057069\ttotal: 4.05s\tremaining: 27.2ms\n",
      "298:\tlearn: 0.0056760\ttotal: 4.06s\tremaining: 13.6ms\n",
      "299:\tlearn: 0.0056760\ttotal: 4.07s\tremaining: 0us\n",
      "Dataset 3:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e778cd78e6d04af1b26f35dd18618de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5971541703906968, Recall = 0.916018158236057, Aging Rate = 0.9212062256809338, Precision = 0.4971840901091165, f1 = 0.6445357061373488\n",
      "Epoch 2: Train Loss = 0.4773924645282879, Recall = 0.9889753566796369, Aging Rate = 0.9215304798962386, Precision = 0.5365939479239972, f1 = 0.6957116788321168\n",
      "Epoch 3: Train Loss = 0.40467838821244145, Recall = 0.9445525291828794, Aging Rate = 0.7159533073929961, Precision = 0.6596467391304348, f1 = 0.7768\n",
      "Epoch 4: Train Loss = 0.35631669217046597, Recall = 0.9315823605706874, Aging Rate = 0.6407263294422828, Precision = 0.7269736842105263, f1 = 0.8166571915861285\n",
      "Epoch 5: Train Loss = 0.3119626613442536, Recall = 0.9361219195849546, Aging Rate = 0.5980869001297017, Precision = 0.7825969097316345, f1 = 0.8525025837885722\n",
      "Test Loss = 0.27468543340259954, Recall = 0.9536316472114138, Aging Rate = 0.5714980544747081, precision = 0.8343262411347517\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.2562508232862569, Recall = 0.9520103761348897, Aging Rate = 0.565337224383917, Precision = 0.8419845139088041, f1 = 0.8936234971845989\n",
      "Epoch 7: Train Loss = 0.21418340376610887, Recall = 0.9649805447470817, Aging Rate = 0.5515564202334631, Precision = 0.8747795414462081, f1 = 0.9176688251618871\n",
      "Epoch 8: Train Loss = 0.17837802955612597, Recall = 0.9737354085603113, Aging Rate = 0.5340466926070039, Precision = 0.9116575591985429, f1 = 0.9416745061147695\n",
      "Epoch 9: Train Loss = 0.14665306220374683, Recall = 0.9831387808041504, Aging Rate = 0.5256160830090791, Precision = 0.9352251696483652, f1 = 0.9585836231425862\n",
      "Epoch 10: Train Loss = 0.12341707391149803, Recall = 0.9873540856031129, Aging Rate = 0.5183203631647212, Precision = 0.9524554269627776, f1 = 0.9695908294857507\n",
      "Test Loss = 0.1086226036402502, Recall = 0.99189364461738, Aging Rate = 0.5139429312581063, precision = 0.9649842271293375\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.1034148166151826, Recall = 0.9909208819714657, Aging Rate = 0.5134565499351491, Precision = 0.9649510577833912, f1 = 0.9777635578307471\n",
      "Epoch 12: Train Loss = 0.08953220808900283, Recall = 0.9931906614785992, Aging Rate = 0.5123216601815823, Precision = 0.9693037974683544, f1 = 0.981101857783472\n",
      "Epoch 13: Train Loss = 0.07678886352230138, Recall = 0.9938391699092088, Aging Rate = 0.5098897535667963, Precision = 0.9745627980922098, f1 = 0.9841065981698507\n",
      "Epoch 14: Train Loss = 0.06589558993100192, Recall = 0.995136186770428, Aging Rate = 0.5079442282749675, Precision = 0.979572294924992, f1 = 0.9872929065465658\n",
      "Epoch 15: Train Loss = 0.057508067166295036, Recall = 0.996757457846952, Aging Rate = 0.5055123216601816, Precision = 0.9858883899935856, f1 = 0.9912931312479846\n",
      "Test Loss = 0.051710813034485284, Recall = 0.998378728923476, Aging Rate = 0.5059987029831388, precision = 0.9865427747516822\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.05158649003749228, Recall = 0.996757457846952, Aging Rate = 0.5050259403372244, Precision = 0.9868378812199037, f1 = 0.9917728665913857\n",
      "Epoch 17: Train Loss = 0.04507969950365649, Recall = 0.998378728923476, Aging Rate = 0.50405317769131, Precision = 0.9903505950466388, f1 = 0.9943484579363797\n",
      "Epoch 18: Train Loss = 0.04030455314782575, Recall = 0.9987029831387808, Aging Rate = 0.5030804150453956, Precision = 0.9925878182404125, f1 = 0.9956360109907871\n",
      "Epoch 19: Train Loss = 0.03623751613242617, Recall = 0.9993514915693904, Aging Rate = 0.5027561608300908, Precision = 0.9938729442115447, f1 = 0.9966046887631367\n",
      "Epoch 20: Train Loss = 0.032373786856808084, Recall = 0.9993514915693904, Aging Rate = 0.5019455252918288, Precision = 0.9954780361757106, f1 = 0.9974110032362459\n",
      "Test Loss = 0.02944073233217117, Recall = 0.9996757457846952, Aging Rate = 0.500810635538262, precision = 0.9980576238264811\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.029343502893174205, Recall = 1.0, Aging Rate = 0.5021076523994812, Precision = 0.9958023894091056, f1 = 0.9978967804562368\n",
      "Epoch 22: Train Loss = 0.02674218909343857, Recall = 1.0, Aging Rate = 0.5014591439688716, Precision = 0.997090203685742, f1 = 0.9985429820301116\n",
      "Epoch 23: Train Loss = 0.02424819658035666, Recall = 1.0, Aging Rate = 0.5009727626459144, Precision = 0.9980582524271845, f1 = 0.9990281827016522\n",
      "Epoch 24: Train Loss = 0.022247607316024574, Recall = 0.9996757457846952, Aging Rate = 0.5004863813229572, Precision = 0.9987042436022028, f1 = 0.9991897585480473\n",
      "Epoch 25: Train Loss = 0.020244968801388943, Recall = 1.0, Aging Rate = 0.5006485084306096, Precision = 0.9987046632124352, f1 = 0.9993519118600129\n",
      "Test Loss = 0.018723356678849994, Recall = 1.0, Aging Rate = 0.5004863813229572, precision = 0.9990281827016521\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.018484882434199525, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.01718113917842913, Recall = 1.0, Aging Rate = 0.5006485084306096, Precision = 0.9987046632124352, f1 = 0.9993519118600129\n",
      "Epoch 28: Train Loss = 0.015795623854614416, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Epoch 29: Train Loss = 0.014783953327381194, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Epoch 30: Train Loss = 0.01363675271664612, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Test Loss = 0.012715208608700608, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.012970228945829177, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Epoch 32: Train Loss = 0.011822207903322775, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Epoch 33: Train Loss = 0.011030458943041383, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.010263434792452054, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.009840174096161754, Recall = 1.0, Aging Rate = 0.5001621271076524, Precision = 0.9996758508914101, f1 = 0.9998378991732858\n",
      "Test Loss = 0.009065267346357268, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.009168117564417972, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.008587409936051076, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.008131908455769376, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.007731547468642665, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.007275508165881328, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006771337204696139, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.006906276039692214, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.00665179882488478, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0062962978252061146, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0059782498070412105, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.005777498457131451, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005427541493905658, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.005488727104955204, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.005241406469058666, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.005011382509124418, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.004962228123526223, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.004668022620089439, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004354049085643041, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.004499180748395564, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.0043964583575290695, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Train Loss = 0.004182200177648768, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.004126095372444229, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0038917426596094895, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003653663713829266, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.003780083402860837, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.003664068406599969, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.0035554432189671852, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.0034463616171380657, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.003390927980463691, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00319648288228415, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.0032661248293817625, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.003186328980731071, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.003122036698625825, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.0030501541456910653, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.002990403698199222, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002817211853854851, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0029162087773085222, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.002848495232803812, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.002839476516984896, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.002735963694620805, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.002725149446859202, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0025540566446027695, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0026479506473770864, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0025812213604450808, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.0025842884548211375, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0025271677379478277, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0025129642152745667, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0023745081934657543, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.0024738826522610878, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0024273512578761413, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0023887322686525635, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.002398756233040345, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0023469840980892285, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021759965275831026, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0023354858124686644, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.002320202228754527, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0023014317869103663, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.002250381598633203, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.002251413986014369, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0021333002606732953, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.00221619667287858, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.002190327337047933, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0021853807847946882, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0021491370921236995, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.002162982327392477, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020541634092629247, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.002157377050138633, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.002168578401947184, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.002129842606028868, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0021074782623927532, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.002078654860592074, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001994041402113954, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0020694673251168594, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0020529638739404223, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.0020308921825691537, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0020626396382887077, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0020380276644870582, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018568169375417704, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.002061292650504219, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.0020351820713242494, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.001996960925347813, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0019977990504361485, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.0020432809690749164, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001983030533177075, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.002013702414375422, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.001961899261309218, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.001954532485248717, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.0019465396588429055, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.0019627073100235326, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017488789839800519, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0019513111749975597, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.001932601862068828, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.001926587666258042, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.0019081512371853632, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.0019207762550664087, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017816613225360557, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.0019315659423659517, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.0018568884641120523, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.0019117193294286815, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.001906070725970579, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.001909446612667249, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017000715568089126, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.0018477574002599263, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.0018805870369569568, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123: Train Loss = 0.0018796440012796495, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.0018954293187236565, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.0018690342984093418, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016750329987649671, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.0018955332735292468, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 127: Train Loss = 0.0018691419984362585, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 128: Train Loss = 0.001889621082454166, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.0018307949563104238, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.0018904269609513334, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017077770253556422, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 130.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2998212\ttotal: 6.48ms\tremaining: 1.94s\n",
      "1:\tlearn: 0.2131963\ttotal: 12.4ms\tremaining: 1.85s\n",
      "2:\tlearn: 0.1739411\ttotal: 18.5ms\tremaining: 1.83s\n",
      "3:\tlearn: 0.1524381\ttotal: 24.5ms\tremaining: 1.81s\n",
      "4:\tlearn: 0.1344535\ttotal: 30.6ms\tremaining: 1.8s\n",
      "5:\tlearn: 0.1200670\ttotal: 36.6ms\tremaining: 1.79s\n",
      "6:\tlearn: 0.1141512\ttotal: 42.6ms\tremaining: 1.78s\n",
      "7:\tlearn: 0.1101780\ttotal: 48.5ms\tremaining: 1.77s\n",
      "8:\tlearn: 0.1054679\ttotal: 54.5ms\tremaining: 1.76s\n",
      "9:\tlearn: 0.1018955\ttotal: 60.6ms\tremaining: 1.76s\n",
      "10:\tlearn: 0.0988926\ttotal: 66.5ms\tremaining: 1.75s\n",
      "11:\tlearn: 0.0913877\ttotal: 72.5ms\tremaining: 1.74s\n",
      "12:\tlearn: 0.0883584\ttotal: 78ms\tremaining: 1.72s\n",
      "13:\tlearn: 0.0834262\ttotal: 83.3ms\tremaining: 1.7s\n",
      "14:\tlearn: 0.0809959\ttotal: 88.6ms\tremaining: 1.68s\n",
      "15:\tlearn: 0.0757636\ttotal: 94ms\tremaining: 1.67s\n",
      "16:\tlearn: 0.0731284\ttotal: 99.3ms\tremaining: 1.65s\n",
      "17:\tlearn: 0.0691026\ttotal: 105ms\tremaining: 1.64s\n",
      "18:\tlearn: 0.0659731\ttotal: 110ms\tremaining: 1.62s\n",
      "19:\tlearn: 0.0644560\ttotal: 115ms\tremaining: 1.61s\n",
      "20:\tlearn: 0.0627866\ttotal: 120ms\tremaining: 1.6s\n",
      "21:\tlearn: 0.0617874\ttotal: 126ms\tremaining: 1.59s\n",
      "22:\tlearn: 0.0574292\ttotal: 131ms\tremaining: 1.58s\n",
      "23:\tlearn: 0.0536303\ttotal: 136ms\tremaining: 1.57s\n",
      "24:\tlearn: 0.0514334\ttotal: 142ms\tremaining: 1.56s\n",
      "25:\tlearn: 0.0500770\ttotal: 147ms\tremaining: 1.55s\n",
      "26:\tlearn: 0.0481108\ttotal: 152ms\tremaining: 1.54s\n",
      "27:\tlearn: 0.0463602\ttotal: 158ms\tremaining: 1.53s\n",
      "28:\tlearn: 0.0443379\ttotal: 163ms\tremaining: 1.52s\n",
      "29:\tlearn: 0.0433284\ttotal: 168ms\tremaining: 1.51s\n",
      "30:\tlearn: 0.0408932\ttotal: 173ms\tremaining: 1.5s\n",
      "31:\tlearn: 0.0393113\ttotal: 178ms\tremaining: 1.49s\n",
      "32:\tlearn: 0.0382912\ttotal: 184ms\tremaining: 1.49s\n",
      "33:\tlearn: 0.0363298\ttotal: 189ms\tremaining: 1.48s\n",
      "34:\tlearn: 0.0351733\ttotal: 195ms\tremaining: 1.47s\n",
      "35:\tlearn: 0.0343632\ttotal: 200ms\tremaining: 1.47s\n",
      "36:\tlearn: 0.0331250\ttotal: 205ms\tremaining: 1.46s\n",
      "37:\tlearn: 0.0319953\ttotal: 210ms\tremaining: 1.45s\n",
      "38:\tlearn: 0.0306821\ttotal: 216ms\tremaining: 1.44s\n",
      "39:\tlearn: 0.0299260\ttotal: 221ms\tremaining: 1.44s\n",
      "40:\tlearn: 0.0283233\ttotal: 226ms\tremaining: 1.43s\n",
      "41:\tlearn: 0.0259972\ttotal: 231ms\tremaining: 1.42s\n",
      "42:\tlearn: 0.0243596\ttotal: 237ms\tremaining: 1.41s\n",
      "43:\tlearn: 0.0227751\ttotal: 242ms\tremaining: 1.41s\n",
      "44:\tlearn: 0.0216208\ttotal: 247ms\tremaining: 1.4s\n",
      "45:\tlearn: 0.0205688\ttotal: 253ms\tremaining: 1.4s\n",
      "46:\tlearn: 0.0194238\ttotal: 258ms\tremaining: 1.39s\n",
      "47:\tlearn: 0.0182470\ttotal: 263ms\tremaining: 1.38s\n",
      "48:\tlearn: 0.0176452\ttotal: 268ms\tremaining: 1.37s\n",
      "49:\tlearn: 0.0169120\ttotal: 273ms\tremaining: 1.37s\n",
      "50:\tlearn: 0.0157904\ttotal: 279ms\tremaining: 1.36s\n",
      "51:\tlearn: 0.0150472\ttotal: 284ms\tremaining: 1.35s\n",
      "52:\tlearn: 0.0143863\ttotal: 289ms\tremaining: 1.35s\n",
      "53:\tlearn: 0.0138239\ttotal: 294ms\tremaining: 1.34s\n",
      "54:\tlearn: 0.0129786\ttotal: 300ms\tremaining: 1.33s\n",
      "55:\tlearn: 0.0125115\ttotal: 305ms\tremaining: 1.33s\n",
      "56:\tlearn: 0.0116516\ttotal: 310ms\tremaining: 1.32s\n",
      "57:\tlearn: 0.0113061\ttotal: 315ms\tremaining: 1.32s\n",
      "58:\tlearn: 0.0107060\ttotal: 321ms\tremaining: 1.31s\n",
      "59:\tlearn: 0.0101682\ttotal: 326ms\tremaining: 1.3s\n",
      "60:\tlearn: 0.0097643\ttotal: 332ms\tremaining: 1.3s\n",
      "61:\tlearn: 0.0094589\ttotal: 337ms\tremaining: 1.29s\n",
      "62:\tlearn: 0.0091876\ttotal: 342ms\tremaining: 1.29s\n",
      "63:\tlearn: 0.0086935\ttotal: 348ms\tremaining: 1.28s\n",
      "64:\tlearn: 0.0084286\ttotal: 353ms\tremaining: 1.27s\n",
      "65:\tlearn: 0.0077150\ttotal: 358ms\tremaining: 1.27s\n",
      "66:\tlearn: 0.0074288\ttotal: 363ms\tremaining: 1.26s\n",
      "67:\tlearn: 0.0070884\ttotal: 369ms\tremaining: 1.26s\n",
      "68:\tlearn: 0.0067537\ttotal: 374ms\tremaining: 1.25s\n",
      "69:\tlearn: 0.0066250\ttotal: 380ms\tremaining: 1.25s\n",
      "70:\tlearn: 0.0064385\ttotal: 385ms\tremaining: 1.24s\n",
      "71:\tlearn: 0.0061311\ttotal: 390ms\tremaining: 1.24s\n",
      "72:\tlearn: 0.0059163\ttotal: 395ms\tremaining: 1.23s\n",
      "73:\tlearn: 0.0057404\ttotal: 401ms\tremaining: 1.22s\n",
      "74:\tlearn: 0.0056578\ttotal: 406ms\tremaining: 1.22s\n",
      "75:\tlearn: 0.0054500\ttotal: 411ms\tremaining: 1.21s\n",
      "76:\tlearn: 0.0052804\ttotal: 416ms\tremaining: 1.21s\n",
      "77:\tlearn: 0.0051278\ttotal: 422ms\tremaining: 1.2s\n",
      "78:\tlearn: 0.0049279\ttotal: 427ms\tremaining: 1.19s\n",
      "79:\tlearn: 0.0048281\ttotal: 432ms\tremaining: 1.19s\n",
      "80:\tlearn: 0.0046443\ttotal: 437ms\tremaining: 1.18s\n",
      "81:\tlearn: 0.0044409\ttotal: 442ms\tremaining: 1.18s\n",
      "82:\tlearn: 0.0042636\ttotal: 448ms\tremaining: 1.17s\n",
      "83:\tlearn: 0.0041218\ttotal: 453ms\tremaining: 1.16s\n",
      "84:\tlearn: 0.0039665\ttotal: 458ms\tremaining: 1.16s\n",
      "85:\tlearn: 0.0037605\ttotal: 464ms\tremaining: 1.15s\n",
      "86:\tlearn: 0.0036761\ttotal: 469ms\tremaining: 1.15s\n",
      "87:\tlearn: 0.0035298\ttotal: 474ms\tremaining: 1.14s\n",
      "88:\tlearn: 0.0034354\ttotal: 479ms\tremaining: 1.14s\n",
      "89:\tlearn: 0.0033541\ttotal: 485ms\tremaining: 1.13s\n",
      "90:\tlearn: 0.0031528\ttotal: 490ms\tremaining: 1.13s\n",
      "91:\tlearn: 0.0030568\ttotal: 495ms\tremaining: 1.12s\n",
      "92:\tlearn: 0.0029558\ttotal: 500ms\tremaining: 1.11s\n",
      "93:\tlearn: 0.0028642\ttotal: 506ms\tremaining: 1.11s\n",
      "94:\tlearn: 0.0027899\ttotal: 511ms\tremaining: 1.1s\n",
      "95:\tlearn: 0.0027153\ttotal: 516ms\tremaining: 1.1s\n",
      "96:\tlearn: 0.0026159\ttotal: 522ms\tremaining: 1.09s\n",
      "97:\tlearn: 0.0025686\ttotal: 527ms\tremaining: 1.08s\n",
      "98:\tlearn: 0.0024663\ttotal: 532ms\tremaining: 1.08s\n",
      "99:\tlearn: 0.0023973\ttotal: 537ms\tremaining: 1.07s\n",
      "100:\tlearn: 0.0023117\ttotal: 543ms\tremaining: 1.07s\n",
      "101:\tlearn: 0.0022518\ttotal: 548ms\tremaining: 1.06s\n",
      "102:\tlearn: 0.0022518\ttotal: 553ms\tremaining: 1.06s\n",
      "103:\tlearn: 0.0021965\ttotal: 558ms\tremaining: 1.05s\n",
      "104:\tlearn: 0.0021419\ttotal: 564ms\tremaining: 1.05s\n",
      "105:\tlearn: 0.0020956\ttotal: 569ms\tremaining: 1.04s\n",
      "106:\tlearn: 0.0020568\ttotal: 574ms\tremaining: 1.03s\n",
      "107:\tlearn: 0.0019866\ttotal: 580ms\tremaining: 1.03s\n",
      "108:\tlearn: 0.0019175\ttotal: 585ms\tremaining: 1.02s\n",
      "109:\tlearn: 0.0018649\ttotal: 590ms\tremaining: 1.02s\n",
      "110:\tlearn: 0.0018161\ttotal: 595ms\tremaining: 1.01s\n",
      "111:\tlearn: 0.0017620\ttotal: 601ms\tremaining: 1.01s\n",
      "112:\tlearn: 0.0017186\ttotal: 606ms\tremaining: 1s\n",
      "113:\tlearn: 0.0016639\ttotal: 611ms\tremaining: 997ms\n",
      "114:\tlearn: 0.0016201\ttotal: 616ms\tremaining: 992ms\n",
      "115:\tlearn: 0.0015696\ttotal: 622ms\tremaining: 986ms\n",
      "116:\tlearn: 0.0015156\ttotal: 627ms\tremaining: 981ms\n",
      "117:\tlearn: 0.0014646\ttotal: 632ms\tremaining: 975ms\n",
      "118:\tlearn: 0.0014369\ttotal: 637ms\tremaining: 969ms\n",
      "119:\tlearn: 0.0014057\ttotal: 643ms\tremaining: 964ms\n",
      "120:\tlearn: 0.0013719\ttotal: 648ms\tremaining: 959ms\n",
      "121:\tlearn: 0.0013253\ttotal: 654ms\tremaining: 954ms\n",
      "122:\tlearn: 0.0012986\ttotal: 659ms\tremaining: 948ms\n",
      "123:\tlearn: 0.0012656\ttotal: 664ms\tremaining: 943ms\n",
      "124:\tlearn: 0.0012339\ttotal: 670ms\tremaining: 938ms\n",
      "125:\tlearn: 0.0012339\ttotal: 675ms\tremaining: 932ms\n",
      "126:\tlearn: 0.0012338\ttotal: 680ms\tremaining: 926ms\n",
      "127:\tlearn: 0.0012338\ttotal: 685ms\tremaining: 920ms\n",
      "128:\tlearn: 0.0012338\ttotal: 690ms\tremaining: 914ms\n",
      "129:\tlearn: 0.0011846\ttotal: 695ms\tremaining: 909ms\n",
      "130:\tlearn: 0.0011348\ttotal: 700ms\tremaining: 903ms\n",
      "131:\tlearn: 0.0011108\ttotal: 705ms\tremaining: 898ms\n",
      "132:\tlearn: 0.0011108\ttotal: 710ms\tremaining: 892ms\n",
      "133:\tlearn: 0.0011108\ttotal: 715ms\tremaining: 886ms\n",
      "134:\tlearn: 0.0010805\ttotal: 721ms\tremaining: 881ms\n",
      "135:\tlearn: 0.0010461\ttotal: 726ms\tremaining: 875ms\n",
      "136:\tlearn: 0.0010102\ttotal: 731ms\tremaining: 870ms\n",
      "137:\tlearn: 0.0010102\ttotal: 736ms\tremaining: 865ms\n",
      "138:\tlearn: 0.0010101\ttotal: 741ms\tremaining: 859ms\n",
      "139:\tlearn: 0.0009960\ttotal: 746ms\tremaining: 853ms\n",
      "140:\tlearn: 0.0009960\ttotal: 751ms\tremaining: 847ms\n",
      "141:\tlearn: 0.0009960\ttotal: 756ms\tremaining: 841ms\n",
      "142:\tlearn: 0.0009628\ttotal: 762ms\tremaining: 836ms\n",
      "143:\tlearn: 0.0009627\ttotal: 766ms\tremaining: 830ms\n",
      "144:\tlearn: 0.0009627\ttotal: 771ms\tremaining: 824ms\n",
      "145:\tlearn: 0.0009627\ttotal: 776ms\tremaining: 819ms\n",
      "146:\tlearn: 0.0009293\ttotal: 781ms\tremaining: 813ms\n",
      "147:\tlearn: 0.0008991\ttotal: 787ms\tremaining: 808ms\n",
      "148:\tlearn: 0.0008990\ttotal: 792ms\tremaining: 802ms\n",
      "149:\tlearn: 0.0008990\ttotal: 797ms\tremaining: 797ms\n",
      "150:\tlearn: 0.0008990\ttotal: 801ms\tremaining: 791ms\n",
      "151:\tlearn: 0.0008798\ttotal: 807ms\tremaining: 785ms\n",
      "152:\tlearn: 0.0008798\ttotal: 812ms\tremaining: 780ms\n",
      "153:\tlearn: 0.0008597\ttotal: 817ms\tremaining: 774ms\n",
      "154:\tlearn: 0.0008597\ttotal: 822ms\tremaining: 769ms\n",
      "155:\tlearn: 0.0008597\ttotal: 827ms\tremaining: 763ms\n",
      "156:\tlearn: 0.0008596\ttotal: 831ms\tremaining: 757ms\n",
      "157:\tlearn: 0.0008596\ttotal: 836ms\tremaining: 752ms\n",
      "158:\tlearn: 0.0008596\ttotal: 841ms\tremaining: 746ms\n",
      "159:\tlearn: 0.0008596\ttotal: 846ms\tremaining: 740ms\n",
      "160:\tlearn: 0.0008596\ttotal: 851ms\tremaining: 735ms\n",
      "161:\tlearn: 0.0008595\ttotal: 856ms\tremaining: 729ms\n",
      "162:\tlearn: 0.0008595\ttotal: 861ms\tremaining: 724ms\n",
      "163:\tlearn: 0.0008595\ttotal: 866ms\tremaining: 718ms\n",
      "164:\tlearn: 0.0008595\ttotal: 871ms\tremaining: 713ms\n",
      "165:\tlearn: 0.0008595\ttotal: 876ms\tremaining: 707ms\n",
      "166:\tlearn: 0.0008595\ttotal: 881ms\tremaining: 702ms\n",
      "167:\tlearn: 0.0008594\ttotal: 886ms\tremaining: 696ms\n",
      "168:\tlearn: 0.0008594\ttotal: 891ms\tremaining: 690ms\n",
      "169:\tlearn: 0.0008594\ttotal: 896ms\tremaining: 685ms\n",
      "170:\tlearn: 0.0008594\ttotal: 901ms\tremaining: 680ms\n",
      "171:\tlearn: 0.0008593\ttotal: 906ms\tremaining: 674ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172:\tlearn: 0.0008593\ttotal: 911ms\tremaining: 669ms\n",
      "173:\tlearn: 0.0008593\ttotal: 916ms\tremaining: 663ms\n",
      "174:\tlearn: 0.0008592\ttotal: 921ms\tremaining: 658ms\n",
      "175:\tlearn: 0.0008592\ttotal: 926ms\tremaining: 652ms\n",
      "176:\tlearn: 0.0008592\ttotal: 931ms\tremaining: 647ms\n",
      "177:\tlearn: 0.0008592\ttotal: 936ms\tremaining: 641ms\n",
      "178:\tlearn: 0.0008592\ttotal: 941ms\tremaining: 636ms\n",
      "179:\tlearn: 0.0008592\ttotal: 946ms\tremaining: 630ms\n",
      "180:\tlearn: 0.0008592\ttotal: 950ms\tremaining: 625ms\n",
      "181:\tlearn: 0.0008592\ttotal: 955ms\tremaining: 619ms\n",
      "182:\tlearn: 0.0008592\ttotal: 961ms\tremaining: 614ms\n",
      "183:\tlearn: 0.0008591\ttotal: 966ms\tremaining: 609ms\n",
      "184:\tlearn: 0.0008591\ttotal: 972ms\tremaining: 604ms\n",
      "185:\tlearn: 0.0008591\ttotal: 977ms\tremaining: 599ms\n",
      "186:\tlearn: 0.0008591\ttotal: 982ms\tremaining: 594ms\n",
      "187:\tlearn: 0.0008591\ttotal: 987ms\tremaining: 588ms\n",
      "188:\tlearn: 0.0008339\ttotal: 993ms\tremaining: 583ms\n",
      "189:\tlearn: 0.0008179\ttotal: 998ms\tremaining: 578ms\n",
      "190:\tlearn: 0.0008179\ttotal: 1s\tremaining: 572ms\n",
      "191:\tlearn: 0.0008179\ttotal: 1.01s\tremaining: 567ms\n",
      "192:\tlearn: 0.0008179\ttotal: 1.01s\tremaining: 561ms\n",
      "193:\tlearn: 0.0008178\ttotal: 1.02s\tremaining: 556ms\n",
      "194:\tlearn: 0.0008177\ttotal: 1.02s\tremaining: 551ms\n",
      "195:\tlearn: 0.0008177\ttotal: 1.03s\tremaining: 545ms\n",
      "196:\tlearn: 0.0008177\ttotal: 1.03s\tremaining: 540ms\n",
      "197:\tlearn: 0.0008177\ttotal: 1.04s\tremaining: 534ms\n",
      "198:\tlearn: 0.0008177\ttotal: 1.04s\tremaining: 529ms\n",
      "199:\tlearn: 0.0008177\ttotal: 1.05s\tremaining: 523ms\n",
      "200:\tlearn: 0.0008177\ttotal: 1.05s\tremaining: 518ms\n",
      "201:\tlearn: 0.0008177\ttotal: 1.06s\tremaining: 513ms\n",
      "202:\tlearn: 0.0008176\ttotal: 1.06s\tremaining: 507ms\n",
      "203:\tlearn: 0.0008176\ttotal: 1.07s\tremaining: 502ms\n",
      "204:\tlearn: 0.0008176\ttotal: 1.07s\tremaining: 496ms\n",
      "205:\tlearn: 0.0008176\ttotal: 1.08s\tremaining: 491ms\n",
      "206:\tlearn: 0.0008176\ttotal: 1.08s\tremaining: 486ms\n",
      "207:\tlearn: 0.0008176\ttotal: 1.09s\tremaining: 480ms\n",
      "208:\tlearn: 0.0008176\ttotal: 1.09s\tremaining: 475ms\n",
      "209:\tlearn: 0.0008176\ttotal: 1.1s\tremaining: 470ms\n",
      "210:\tlearn: 0.0008175\ttotal: 1.1s\tremaining: 464ms\n",
      "211:\tlearn: 0.0008175\ttotal: 1.11s\tremaining: 459ms\n",
      "212:\tlearn: 0.0008175\ttotal: 1.11s\tremaining: 454ms\n",
      "213:\tlearn: 0.0008175\ttotal: 1.12s\tremaining: 449ms\n",
      "214:\tlearn: 0.0008175\ttotal: 1.12s\tremaining: 443ms\n",
      "215:\tlearn: 0.0008175\ttotal: 1.13s\tremaining: 438ms\n",
      "216:\tlearn: 0.0008175\ttotal: 1.13s\tremaining: 433ms\n",
      "217:\tlearn: 0.0008175\ttotal: 1.14s\tremaining: 427ms\n",
      "218:\tlearn: 0.0008175\ttotal: 1.14s\tremaining: 422ms\n",
      "219:\tlearn: 0.0008175\ttotal: 1.15s\tremaining: 417ms\n",
      "220:\tlearn: 0.0008174\ttotal: 1.15s\tremaining: 411ms\n",
      "221:\tlearn: 0.0008174\ttotal: 1.16s\tremaining: 406ms\n",
      "222:\tlearn: 0.0008174\ttotal: 1.16s\tremaining: 401ms\n",
      "223:\tlearn: 0.0008174\ttotal: 1.17s\tremaining: 395ms\n",
      "224:\tlearn: 0.0008174\ttotal: 1.17s\tremaining: 390ms\n",
      "225:\tlearn: 0.0008174\ttotal: 1.17s\tremaining: 385ms\n",
      "226:\tlearn: 0.0008174\ttotal: 1.18s\tremaining: 379ms\n",
      "227:\tlearn: 0.0008174\ttotal: 1.18s\tremaining: 374ms\n",
      "228:\tlearn: 0.0008174\ttotal: 1.19s\tremaining: 369ms\n",
      "229:\tlearn: 0.0008174\ttotal: 1.19s\tremaining: 363ms\n",
      "230:\tlearn: 0.0008174\ttotal: 1.2s\tremaining: 358ms\n",
      "231:\tlearn: 0.0008174\ttotal: 1.2s\tremaining: 353ms\n",
      "232:\tlearn: 0.0008174\ttotal: 1.21s\tremaining: 348ms\n",
      "233:\tlearn: 0.0008174\ttotal: 1.21s\tremaining: 342ms\n",
      "234:\tlearn: 0.0008173\ttotal: 1.22s\tremaining: 337ms\n",
      "235:\tlearn: 0.0008173\ttotal: 1.22s\tremaining: 332ms\n",
      "236:\tlearn: 0.0008173\ttotal: 1.23s\tremaining: 327ms\n",
      "237:\tlearn: 0.0008173\ttotal: 1.23s\tremaining: 321ms\n",
      "238:\tlearn: 0.0008173\ttotal: 1.24s\tremaining: 316ms\n",
      "239:\tlearn: 0.0008172\ttotal: 1.24s\tremaining: 311ms\n",
      "240:\tlearn: 0.0008172\ttotal: 1.25s\tremaining: 305ms\n",
      "241:\tlearn: 0.0008172\ttotal: 1.25s\tremaining: 300ms\n",
      "242:\tlearn: 0.0008172\ttotal: 1.26s\tremaining: 295ms\n",
      "243:\tlearn: 0.0008172\ttotal: 1.26s\tremaining: 290ms\n",
      "244:\tlearn: 0.0008172\ttotal: 1.27s\tremaining: 284ms\n",
      "245:\tlearn: 0.0008172\ttotal: 1.27s\tremaining: 279ms\n",
      "246:\tlearn: 0.0008172\ttotal: 1.28s\tremaining: 274ms\n",
      "247:\tlearn: 0.0008172\ttotal: 1.28s\tremaining: 269ms\n",
      "248:\tlearn: 0.0008172\ttotal: 1.29s\tremaining: 264ms\n",
      "249:\tlearn: 0.0008172\ttotal: 1.29s\tremaining: 258ms\n",
      "250:\tlearn: 0.0008172\ttotal: 1.3s\tremaining: 253ms\n",
      "251:\tlearn: 0.0008172\ttotal: 1.3s\tremaining: 248ms\n",
      "252:\tlearn: 0.0008171\ttotal: 1.31s\tremaining: 243ms\n",
      "253:\tlearn: 0.0008171\ttotal: 1.31s\tremaining: 237ms\n",
      "254:\tlearn: 0.0008171\ttotal: 1.32s\tremaining: 232ms\n",
      "255:\tlearn: 0.0008171\ttotal: 1.32s\tremaining: 227ms\n",
      "256:\tlearn: 0.0008171\ttotal: 1.33s\tremaining: 222ms\n",
      "257:\tlearn: 0.0007931\ttotal: 1.33s\tremaining: 217ms\n",
      "258:\tlearn: 0.0007658\ttotal: 1.34s\tremaining: 212ms\n",
      "259:\tlearn: 0.0007658\ttotal: 1.34s\tremaining: 206ms\n",
      "260:\tlearn: 0.0007658\ttotal: 1.35s\tremaining: 201ms\n",
      "261:\tlearn: 0.0007658\ttotal: 1.35s\tremaining: 196ms\n",
      "262:\tlearn: 0.0007498\ttotal: 1.36s\tremaining: 191ms\n",
      "263:\tlearn: 0.0007498\ttotal: 1.36s\tremaining: 186ms\n",
      "264:\tlearn: 0.0007498\ttotal: 1.37s\tremaining: 180ms\n",
      "265:\tlearn: 0.0007498\ttotal: 1.37s\tremaining: 175ms\n",
      "266:\tlearn: 0.0007498\ttotal: 1.38s\tremaining: 170ms\n",
      "267:\tlearn: 0.0007498\ttotal: 1.38s\tremaining: 165ms\n",
      "268:\tlearn: 0.0007497\ttotal: 1.39s\tremaining: 160ms\n",
      "269:\tlearn: 0.0007497\ttotal: 1.39s\tremaining: 155ms\n",
      "270:\tlearn: 0.0007497\ttotal: 1.4s\tremaining: 149ms\n",
      "271:\tlearn: 0.0007497\ttotal: 1.4s\tremaining: 144ms\n",
      "272:\tlearn: 0.0007497\ttotal: 1.41s\tremaining: 139ms\n",
      "273:\tlearn: 0.0007496\ttotal: 1.41s\tremaining: 134ms\n",
      "274:\tlearn: 0.0007496\ttotal: 1.41s\tremaining: 129ms\n",
      "275:\tlearn: 0.0007496\ttotal: 1.42s\tremaining: 123ms\n",
      "276:\tlearn: 0.0007496\ttotal: 1.42s\tremaining: 118ms\n",
      "277:\tlearn: 0.0007496\ttotal: 1.43s\tremaining: 113ms\n",
      "278:\tlearn: 0.0007496\ttotal: 1.43s\tremaining: 108ms\n",
      "279:\tlearn: 0.0007496\ttotal: 1.44s\tremaining: 103ms\n",
      "280:\tlearn: 0.0007495\ttotal: 1.44s\tremaining: 97.7ms\n",
      "281:\tlearn: 0.0007495\ttotal: 1.45s\tremaining: 92.5ms\n",
      "282:\tlearn: 0.0007495\ttotal: 1.45s\tremaining: 87.4ms\n",
      "283:\tlearn: 0.0007495\ttotal: 1.46s\tremaining: 82.2ms\n",
      "284:\tlearn: 0.0007495\ttotal: 1.46s\tremaining: 77.1ms\n",
      "285:\tlearn: 0.0007495\ttotal: 1.47s\tremaining: 71.9ms\n",
      "286:\tlearn: 0.0007495\ttotal: 1.47s\tremaining: 66.8ms\n",
      "287:\tlearn: 0.0007494\ttotal: 1.48s\tremaining: 61.6ms\n",
      "288:\tlearn: 0.0007494\ttotal: 1.48s\tremaining: 56.5ms\n",
      "289:\tlearn: 0.0007494\ttotal: 1.49s\tremaining: 51.4ms\n",
      "290:\tlearn: 0.0007494\ttotal: 1.49s\tremaining: 46.2ms\n",
      "291:\tlearn: 0.0007494\ttotal: 1.5s\tremaining: 41.1ms\n",
      "292:\tlearn: 0.0007494\ttotal: 1.5s\tremaining: 35.9ms\n",
      "293:\tlearn: 0.0007494\ttotal: 1.51s\tremaining: 30.8ms\n",
      "294:\tlearn: 0.0007494\ttotal: 1.51s\tremaining: 25.7ms\n",
      "295:\tlearn: 0.0007494\ttotal: 1.52s\tremaining: 20.5ms\n",
      "296:\tlearn: 0.0007494\ttotal: 1.52s\tremaining: 15.4ms\n",
      "297:\tlearn: 0.0007494\ttotal: 1.53s\tremaining: 10.3ms\n",
      "298:\tlearn: 0.0007494\ttotal: 1.53s\tremaining: 5.13ms\n",
      "299:\tlearn: 0.0007494\ttotal: 1.54s\tremaining: 0us\n",
      "Dataset 4:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1332914b394d5e9c4647e519f936bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5817514204520609, Recall = 0.6753292986828052, Aging Rate = 0.44037023851904594, Precision = 0.7667744543249798, f1 = 0.7181525648305888\n",
      "Epoch 2: Train Loss = 0.3912727597579705, Recall = 0.8273406906372375, Aging Rate = 0.4877180491278035, Precision = 0.8481751824817518, f1 = 0.8376284015137863\n",
      "Epoch 3: Train Loss = 0.29842621514533246, Recall = 0.8924884300462799, Aging Rate = 0.5021359914560342, Precision = 0.8886919532080823, f1 = 0.8905861456483126\n",
      "Epoch 4: Train Loss = 0.23933875154943898, Recall = 0.9209683161267355, Aging Rate = 0.5003559985760057, Precision = 0.92031305585201, f1 = 0.9206405693950178\n",
      "Epoch 5: Train Loss = 0.20078300941177177, Recall = 0.9334282662869349, Aging Rate = 0.49608401566393734, Precision = 0.9407965554359526, f1 = 0.9370979270907792\n",
      "Test Loss = 0.1677174916701115, Recall = 0.9412602349590602, Aging Rate = 0.489498042007832, precision = 0.9614545454545455\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.15607680580865535, Recall = 0.9512281950872197, Aging Rate = 0.49359202563189747, Precision = 0.9635773530472412, f1 = 0.9573629523468292\n",
      "Epoch 7: Train Loss = 0.13066167512496793, Recall = 0.9661801352794589, Aging Rate = 0.49537201851192597, Precision = 0.9752066115702479, f1 = 0.9706723891273247\n",
      "Epoch 8: Train Loss = 0.11010445580125153, Recall = 0.968316126735493, Aging Rate = 0.4943040227839089, Precision = 0.9794742527907814, f1 = 0.9738632295023273\n",
      "Epoch 9: Train Loss = 0.09468475301130339, Recall = 0.9715201139195443, Aging Rate = 0.4928800284798861, Precision = 0.9855543517515348, f1 = 0.9784869128719972\n",
      "Epoch 10: Train Loss = 0.08198357067389521, Recall = 0.9779280882876469, Aging Rate = 0.49501601993592026, Precision = 0.9877741819489392, f1 = 0.9828264758497316\n",
      "Test Loss = 0.07320178110893828, Recall = 0.9786400854396582, Aging Rate = 0.49323602705589176, precision = 0.9920606279321544\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07330577185423233, Recall = 0.9779280882876469, Aging Rate = 0.49341402634389464, Precision = 0.990981240981241, f1 = 0.984411395807203\n",
      "Epoch 12: Train Loss = 0.06507467179190819, Recall = 0.9804200783196867, Aging Rate = 0.4937700249199003, Precision = 0.992790194664744, f1 = 0.9865663621708759\n",
      "Epoch 13: Train Loss = 0.05874964764594353, Recall = 0.9825560697757209, Aging Rate = 0.49466002135991455, Precision = 0.9931630082763584, f1 = 0.987831066571224\n",
      "Epoch 14: Train Loss = 0.05240980637332391, Recall = 0.9850480598077608, Aging Rate = 0.49466002135991455, Precision = 0.9956818999640158, f1 = 0.9903364352183249\n",
      "Epoch 15: Train Loss = 0.048438816133391904, Recall = 0.9864720541117835, Aging Rate = 0.49537201851192597, Precision = 0.9956881063600431, f1 = 0.9910586552217454\n",
      "Test Loss = 0.04533912763980921, Recall = 0.9925240299038803, Aging Rate = 0.49875400498398004, precision = 0.9950035688793719\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.044109775535245095, Recall = 0.9871840512637949, Aging Rate = 0.49537201851192597, Precision = 0.996406755300036, f1 = 0.9917739628040059\n",
      "Epoch 17: Train Loss = 0.040266445091597, Recall = 0.9886080455678178, Aging Rate = 0.49608401566393734, Precision = 0.9964119124506639, f1 = 0.992494639027877\n",
      "Epoch 18: Train Loss = 0.03798976528231849, Recall = 0.9900320398718405, Aging Rate = 0.4966180135279459, Precision = 0.9967741935483871, f1 = 0.9933916770851938\n",
      "Epoch 19: Train Loss = 0.03495449736852288, Recall = 0.991812032751869, Aging Rate = 0.4973300106799573, Precision = 0.9971367215461704, f1 = 0.9944672496876673\n",
      "Epoch 20: Train Loss = 0.03302132431649788, Recall = 0.9932360270558918, Aging Rate = 0.4982200071199715, Precision = 0.9967845659163987, f1 = 0.9950071326676178\n",
      "Test Loss = 0.030281847496024653, Recall = 0.9943040227839088, Aging Rate = 0.49750800996796013, precision = 0.9992844364937388\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.03094741737157711, Recall = 0.9928800284798861, Aging Rate = 0.4973300106799573, Precision = 0.9982104509663565, f1 = 0.9955381045868286\n",
      "Epoch 22: Train Loss = 0.029975449439932567, Recall = 0.9943040227839088, Aging Rate = 0.49804200783196867, Precision = 0.9982130092923517, f1 = 0.9962546816479401\n",
      "Epoch 23: Train Loss = 0.028011631913517625, Recall = 0.9939480242079032, Aging Rate = 0.4973300106799573, Precision = 0.9992841803865425, f1 = 0.9966089594859896\n",
      "Epoch 24: Train Loss = 0.026516574882859436, Recall = 0.9953720185119259, Aging Rate = 0.4983980064079744, Precision = 0.9985714285714286, f1 = 0.9969691567124264\n",
      "Epoch 25: Train Loss = 0.025323655913842873, Recall = 0.9960840156639373, Aging Rate = 0.4982200071199715, Precision = 0.9996427295462665, f1 = 0.9978601997146933\n",
      "Test Loss = 0.023194833353941617, Recall = 0.9971520113919544, Aging Rate = 0.4985760056959772, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.024524027471065565, Recall = 0.996440014239943, Aging Rate = 0.4982200071199715, Precision = 0, f1 = 0.0\n",
      "Epoch 27: Train Loss = 0.023460788706011856, Recall = 0.996440014239943, Aging Rate = 0.4985760056959772, Precision = 0.9992859692966798, f1 = 0.9978609625668449\n",
      "Epoch 28: Train Loss = 0.022728554683648934, Recall = 0.996440014239943, Aging Rate = 0.4982200071199715, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.022028493216535126, Recall = 0.9978640085439658, Aging Rate = 0.4989320042719829, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.021739822948645255, Recall = 0.9971520113919544, Aging Rate = 0.4985760056959772, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.02087896512327308, Recall = 0.996440014239943, Aging Rate = 0.4982200071199715, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.020926462988352258, Recall = 0.9982200071199715, Aging Rate = 0.49911000355998575, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.020059417722360143, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 33: Train Loss = 0.020116562808205028, Recall = 0.9971520113919544, Aging Rate = 0.49875400498398004, Precision = 0.9996431120628123, f1 = 0.9983960078417395\n",
      "Epoch 34: Train Loss = 0.01962168255390519, Recall = 0.9975080099679601, Aging Rate = 0.4989320042719829, Precision = 0.9996432393863718, f1 = 0.9985744832501781\n",
      "Epoch 35: Train Loss = 0.01960780338147816, Recall = 0.9985760056959773, Aging Rate = 0.49946600213599146, Precision = 0.9996436208125445, f1 = 0.9991095280498665\n",
      "Test Loss = 0.018692530897847483, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, precision = 1.0\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.018741685475712463, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.018409583733498987, Recall = 0.9985760056959773, Aging Rate = 0.49946600213599146, Precision = 0.9996436208125445, f1 = 0.9991095280498665\n",
      "Epoch 38: Train Loss = 0.018240943367716658, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 39: Train Loss = 0.018593103313555324, Recall = 0.9982200071199715, Aging Rate = 0.49911000355998575, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.018426315984948965, Recall = 0.9985760056959773, Aging Rate = 0.49946600213599146, Precision = 0.9996436208125445, f1 = 0.9991095280498665\n",
      "Test Loss = 0.016467655953861994, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.01755090992421644, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.017540435821955826, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.01742560139019407, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Epoch 44: Train Loss = 0.017036112118877098, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.01733611517275152, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015746174993881364, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016629813679939234, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.01720909833385559, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016630610038986214, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016693611871669962, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.016503063366400596, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01506510623238368, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.01632429378146313, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.016531707236895402, Recall = 0.9985760056959773, Aging Rate = 0.49928800284798863, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.016732845359258737, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Epoch 54: Train Loss = 0.016091290708855406, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0165461833605688, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014992871601234003, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.016057098055594583, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Epoch 57: Train Loss = 0.016023969019794006, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.01671934614787538, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 59: Train Loss = 0.01656406790736881, Recall = 0.9985760056959773, Aging Rate = 0.49946600213599146, Precision = 0.9996436208125445, f1 = 0.9991095280498665\n",
      "Epoch 60: Train Loss = 0.016098033669631796, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01531991060795352, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015773679693745057, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 62: Train Loss = 0.015619841862792177, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015781370230647712, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015705109393869314, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Epoch 65: Train Loss = 0.016236209913348232, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015122902568995995, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, precision = 0.9996438746438746\n",
      "\n",
      "Epoch 66: Train Loss = 0.015919133812592266, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.016356063469907554, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 68: Train Loss = 0.015800375799058637, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 69: Train Loss = 0.015570423322779168, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.015635377299915848, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014091789244348233, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.015371826569851118, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.015587104538813016, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.015641396448159397, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.015541487428117324, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.015719608746896055, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015588444857548123, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.015691994706645538, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.015863257371772434, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 78: Train Loss = 0.01617243908738254, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0156654172406879, Recall = 0.9992880028479886, Aging Rate = 0.49982200071199717, Precision = 0.9996438746438746, f1 = 0.9994659070678298\n",
      "Epoch 80: Train Loss = 0.015293753744378531, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01517598898388411, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.015163071671067799, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0151658313393317, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.015635524731197045, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Epoch 84: Train Loss = 0.01553973526620608, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.015432405427108884, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014092493015376274, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.015492091646988903, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.015247654815656344, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.01527547670330026, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.015551033767650414, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Epoch 90: Train Loss = 0.015425142042349458, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014061110423749856, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014981842087896725, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.015895993491563776, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.015420538554980625, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Epoch 94: Train Loss = 0.01526209174686212, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.015331454063812453, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01364741109320864, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.015166527689121273, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.015323528404595929, Recall = 0.9989320042719829, Aging Rate = 0.4996440014239943, Precision = 0.9996437477734236, f1 = 0.9992877492877492\n",
      "Epoch 98: Train Loss = 0.015345202358396464, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.015369224320300091, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.01511550544976297, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016110789401462683, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.015028892401917501, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: Train Loss = 0.015060181708607906, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.015351446192272621, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.015652679794144062, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 105: Train Loss = 0.015691481702409, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013918533134474772, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.015097262059302253, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.01515730021500893, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.015060139003704773, Recall = 0.9992880028479886, Aging Rate = 0.4996440014239943, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.015017708010199698, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.014826164487384634, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013597488358892677, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.014913323905946267, Recall = 0.9996440014239943, Aging Rate = 0.5001779992880029, Precision = 0.999288256227758, f1 = 0.999466097170315\n",
      "Epoch 112: Train Loss = 0.014911694781063672, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.014976835664097802, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.014929252295052297, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.01504020201259128, Recall = 0.9996440014239943, Aging Rate = 0.5, Precision = 0.9996440014239943, f1 = 0.9996440014239943\n",
      "Test Loss = 0.014193536481634711, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.014910849672634435, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.015700834754545437, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.015055398884782719, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.014993825111101852, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.015062396715293367, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014021590335480935, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.015214889949196257, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Epoch 122: Train Loss = 0.015214744990445533, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.014659377444905503, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.01460708420718372, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.015031488540679502, Recall = 1.0, Aging Rate = 0.5001779992880029, Precision = 0.999644128113879, f1 = 0.999822032390105\n",
      "Test Loss = 0.01343585824213946, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.015592597751228697, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 127: Train Loss = 0.015328875555527215, Recall = 0.9982200071199715, Aging Rate = 0.49928800284798863, Precision = 0.9996434937611408, f1 = 0.9989312433202707\n",
      "Epoch 128: Train Loss = 0.014822482272330108, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.014750469076467413, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.014936384999903761, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01398959993452746, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 131: Train Loss = 0.014874147794658007, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 132: Train Loss = 0.015143819283939375, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 133: Train Loss = 0.014899796866286306, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 134: Train Loss = 0.014873306061402785, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 135: Train Loss = 0.014928048244160599, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013442095637324125, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 136: Train Loss = 0.015025935817447558, Recall = 0.9989320042719829, Aging Rate = 0.49946600213599146, Precision = 0, f1 = 0.0\n",
      "Epoch 137: Train Loss = 0.014488341148190755, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 138: Train Loss = 0.014746880188621461, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 139: Train Loss = 0.01518166654578474, Recall = 0.9996440014239943, Aging Rate = 0.49982200071199717, Precision = 0, f1 = 0.0\n",
      "Epoch 140: Train Loss = 0.014617464446451749, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013911836189454923, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 140.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5131520\ttotal: 7.82ms\tremaining: 1.17s\n",
      "1:\tlearn: 0.3928250\ttotal: 14.9ms\tremaining: 1.1s\n",
      "2:\tlearn: 0.2955745\ttotal: 22.4ms\tremaining: 1.1s\n",
      "3:\tlearn: 0.2175775\ttotal: 29.4ms\tremaining: 1.07s\n",
      "4:\tlearn: 0.1887298\ttotal: 35.2ms\tremaining: 1.02s\n",
      "5:\tlearn: 0.1505980\ttotal: 42.5ms\tremaining: 1.02s\n",
      "6:\tlearn: 0.1236562\ttotal: 48.9ms\tremaining: 999ms\n",
      "7:\tlearn: 0.1074408\ttotal: 55.1ms\tremaining: 978ms\n",
      "8:\tlearn: 0.0912632\ttotal: 61.2ms\tremaining: 959ms\n",
      "9:\tlearn: 0.0767034\ttotal: 67.6ms\tremaining: 946ms\n",
      "10:\tlearn: 0.0662883\ttotal: 73.2ms\tremaining: 925ms\n",
      "11:\tlearn: 0.0632227\ttotal: 77.1ms\tremaining: 886ms\n",
      "12:\tlearn: 0.0563567\ttotal: 81.7ms\tremaining: 861ms\n",
      "13:\tlearn: 0.0522696\ttotal: 85.4ms\tremaining: 830ms\n",
      "14:\tlearn: 0.0469742\ttotal: 90ms\tremaining: 810ms\n",
      "15:\tlearn: 0.0422809\ttotal: 94.9ms\tremaining: 795ms\n",
      "16:\tlearn: 0.0384536\ttotal: 99.7ms\tremaining: 780ms\n",
      "17:\tlearn: 0.0329255\ttotal: 105ms\tremaining: 768ms\n",
      "18:\tlearn: 0.0310740\ttotal: 109ms\tremaining: 750ms\n",
      "19:\tlearn: 0.0285486\ttotal: 113ms\tremaining: 737ms\n",
      "20:\tlearn: 0.0257773\ttotal: 118ms\tremaining: 727ms\n",
      "21:\tlearn: 0.0226614\ttotal: 124ms\tremaining: 719ms\n",
      "22:\tlearn: 0.0203757\ttotal: 129ms\tremaining: 713ms\n",
      "23:\tlearn: 0.0193155\ttotal: 134ms\tremaining: 702ms\n",
      "24:\tlearn: 0.0178758\ttotal: 138ms\tremaining: 692ms\n",
      "25:\tlearn: 0.0164813\ttotal: 143ms\tremaining: 682ms\n",
      "26:\tlearn: 0.0150471\ttotal: 148ms\tremaining: 676ms\n",
      "27:\tlearn: 0.0138326\ttotal: 153ms\tremaining: 668ms\n",
      "28:\tlearn: 0.0124978\ttotal: 158ms\tremaining: 660ms\n",
      "29:\tlearn: 0.0117284\ttotal: 163ms\tremaining: 651ms\n",
      "30:\tlearn: 0.0108156\ttotal: 168ms\tremaining: 643ms\n",
      "31:\tlearn: 0.0097308\ttotal: 173ms\tremaining: 638ms\n",
      "32:\tlearn: 0.0093095\ttotal: 177ms\tremaining: 626ms\n",
      "33:\tlearn: 0.0087677\ttotal: 181ms\tremaining: 616ms\n",
      "34:\tlearn: 0.0084007\ttotal: 184ms\tremaining: 606ms\n",
      "35:\tlearn: 0.0079567\ttotal: 189ms\tremaining: 598ms\n",
      "36:\tlearn: 0.0075452\ttotal: 193ms\tremaining: 589ms\n",
      "37:\tlearn: 0.0072544\ttotal: 197ms\tremaining: 580ms\n",
      "38:\tlearn: 0.0069966\ttotal: 201ms\tremaining: 572ms\n",
      "39:\tlearn: 0.0067237\ttotal: 205ms\tremaining: 562ms\n",
      "40:\tlearn: 0.0065129\ttotal: 208ms\tremaining: 554ms\n",
      "41:\tlearn: 0.0062720\ttotal: 212ms\tremaining: 546ms\n",
      "42:\tlearn: 0.0059225\ttotal: 217ms\tremaining: 540ms\n",
      "43:\tlearn: 0.0055214\ttotal: 221ms\tremaining: 533ms\n",
      "44:\tlearn: 0.0051685\ttotal: 226ms\tremaining: 528ms\n",
      "45:\tlearn: 0.0050325\ttotal: 229ms\tremaining: 519ms\n",
      "46:\tlearn: 0.0048534\ttotal: 233ms\tremaining: 511ms\n",
      "47:\tlearn: 0.0044089\ttotal: 239ms\tremaining: 507ms\n",
      "48:\tlearn: 0.0040992\ttotal: 244ms\tremaining: 502ms\n",
      "49:\tlearn: 0.0038720\ttotal: 248ms\tremaining: 495ms\n",
      "50:\tlearn: 0.0036980\ttotal: 252ms\tremaining: 489ms\n",
      "51:\tlearn: 0.0035509\ttotal: 256ms\tremaining: 482ms\n",
      "52:\tlearn: 0.0034283\ttotal: 260ms\tremaining: 475ms\n",
      "53:\tlearn: 0.0033070\ttotal: 263ms\tremaining: 468ms\n",
      "54:\tlearn: 0.0031831\ttotal: 267ms\tremaining: 462ms\n",
      "55:\tlearn: 0.0030282\ttotal: 272ms\tremaining: 457ms\n",
      "56:\tlearn: 0.0029082\ttotal: 276ms\tremaining: 451ms\n",
      "57:\tlearn: 0.0027720\ttotal: 280ms\tremaining: 445ms\n",
      "58:\tlearn: 0.0026632\ttotal: 285ms\tremaining: 439ms\n",
      "59:\tlearn: 0.0025271\ttotal: 290ms\tremaining: 434ms\n",
      "60:\tlearn: 0.0024720\ttotal: 293ms\tremaining: 428ms\n",
      "61:\tlearn: 0.0023872\ttotal: 297ms\tremaining: 421ms\n",
      "62:\tlearn: 0.0023387\ttotal: 300ms\tremaining: 414ms\n",
      "63:\tlearn: 0.0022522\ttotal: 304ms\tremaining: 408ms\n",
      "64:\tlearn: 0.0021479\ttotal: 308ms\tremaining: 403ms\n",
      "65:\tlearn: 0.0020614\ttotal: 312ms\tremaining: 397ms\n",
      "66:\tlearn: 0.0019990\ttotal: 315ms\tremaining: 391ms\n",
      "67:\tlearn: 0.0019312\ttotal: 319ms\tremaining: 385ms\n",
      "68:\tlearn: 0.0018694\ttotal: 323ms\tremaining: 379ms\n",
      "69:\tlearn: 0.0017855\ttotal: 327ms\tremaining: 373ms\n",
      "70:\tlearn: 0.0017385\ttotal: 330ms\tremaining: 368ms\n",
      "71:\tlearn: 0.0016619\ttotal: 334ms\tremaining: 362ms\n",
      "72:\tlearn: 0.0016321\ttotal: 337ms\tremaining: 356ms\n",
      "73:\tlearn: 0.0015845\ttotal: 342ms\tremaining: 351ms\n",
      "74:\tlearn: 0.0015643\ttotal: 345ms\tremaining: 345ms\n",
      "75:\tlearn: 0.0015643\ttotal: 347ms\tremaining: 338ms\n",
      "76:\tlearn: 0.0015330\ttotal: 351ms\tremaining: 332ms\n",
      "77:\tlearn: 0.0014819\ttotal: 355ms\tremaining: 327ms\n",
      "78:\tlearn: 0.0014451\ttotal: 358ms\tremaining: 322ms\n",
      "79:\tlearn: 0.0014043\ttotal: 361ms\tremaining: 316ms\n",
      "80:\tlearn: 0.0013676\ttotal: 365ms\tremaining: 311ms\n",
      "81:\tlearn: 0.0013351\ttotal: 368ms\tremaining: 305ms\n",
      "82:\tlearn: 0.0013036\ttotal: 371ms\tremaining: 300ms\n",
      "83:\tlearn: 0.0012626\ttotal: 375ms\tremaining: 295ms\n",
      "84:\tlearn: 0.0012262\ttotal: 379ms\tremaining: 290ms\n",
      "85:\tlearn: 0.0012107\ttotal: 382ms\tremaining: 284ms\n",
      "86:\tlearn: 0.0011892\ttotal: 385ms\tremaining: 279ms\n",
      "87:\tlearn: 0.0011634\ttotal: 388ms\tremaining: 274ms\n",
      "88:\tlearn: 0.0011458\ttotal: 391ms\tremaining: 268ms\n",
      "89:\tlearn: 0.0011457\ttotal: 395ms\tremaining: 263ms\n",
      "90:\tlearn: 0.0011178\ttotal: 398ms\tremaining: 258ms\n",
      "91:\tlearn: 0.0011029\ttotal: 401ms\tremaining: 253ms\n",
      "92:\tlearn: 0.0011029\ttotal: 403ms\tremaining: 247ms\n",
      "93:\tlearn: 0.0011029\ttotal: 406ms\tremaining: 242ms\n",
      "94:\tlearn: 0.0011028\ttotal: 408ms\tremaining: 236ms\n",
      "95:\tlearn: 0.0011028\ttotal: 411ms\tremaining: 231ms\n",
      "96:\tlearn: 0.0011028\ttotal: 413ms\tremaining: 226ms\n",
      "97:\tlearn: 0.0011028\ttotal: 416ms\tremaining: 221ms\n",
      "98:\tlearn: 0.0011028\ttotal: 419ms\tremaining: 216ms\n",
      "99:\tlearn: 0.0011028\ttotal: 421ms\tremaining: 211ms\n",
      "100:\tlearn: 0.0011027\ttotal: 424ms\tremaining: 206ms\n",
      "101:\tlearn: 0.0010858\ttotal: 427ms\tremaining: 201ms\n",
      "102:\tlearn: 0.0010470\ttotal: 431ms\tremaining: 197ms\n",
      "103:\tlearn: 0.0010068\ttotal: 435ms\tremaining: 192ms\n",
      "104:\tlearn: 0.0009758\ttotal: 439ms\tremaining: 188ms\n",
      "105:\tlearn: 0.0009758\ttotal: 442ms\tremaining: 183ms\n",
      "106:\tlearn: 0.0009758\ttotal: 444ms\tremaining: 179ms\n",
      "107:\tlearn: 0.0009758\ttotal: 447ms\tremaining: 174ms\n",
      "108:\tlearn: 0.0009758\ttotal: 449ms\tremaining: 169ms\n",
      "109:\tlearn: 0.0009758\ttotal: 452ms\tremaining: 164ms\n",
      "110:\tlearn: 0.0009758\ttotal: 454ms\tremaining: 160ms\n",
      "111:\tlearn: 0.0009758\ttotal: 457ms\tremaining: 155ms\n",
      "112:\tlearn: 0.0009758\ttotal: 460ms\tremaining: 151ms\n",
      "113:\tlearn: 0.0009758\ttotal: 462ms\tremaining: 146ms\n",
      "114:\tlearn: 0.0009757\ttotal: 465ms\tremaining: 141ms\n",
      "115:\tlearn: 0.0009757\ttotal: 467ms\tremaining: 137ms\n",
      "116:\tlearn: 0.0009757\ttotal: 470ms\tremaining: 133ms\n",
      "117:\tlearn: 0.0009757\ttotal: 473ms\tremaining: 128ms\n",
      "118:\tlearn: 0.0009757\ttotal: 475ms\tremaining: 124ms\n",
      "119:\tlearn: 0.0009757\ttotal: 478ms\tremaining: 119ms\n",
      "120:\tlearn: 0.0009757\ttotal: 480ms\tremaining: 115ms\n",
      "121:\tlearn: 0.0009757\ttotal: 483ms\tremaining: 111ms\n",
      "122:\tlearn: 0.0009757\ttotal: 485ms\tremaining: 107ms\n",
      "123:\tlearn: 0.0009757\ttotal: 488ms\tremaining: 102ms\n",
      "124:\tlearn: 0.0009757\ttotal: 491ms\tremaining: 98.1ms\n",
      "125:\tlearn: 0.0009757\ttotal: 493ms\tremaining: 93.9ms\n",
      "126:\tlearn: 0.0009757\ttotal: 496ms\tremaining: 89.7ms\n",
      "127:\tlearn: 0.0009757\ttotal: 498ms\tremaining: 85.6ms\n",
      "128:\tlearn: 0.0009757\ttotal: 501ms\tremaining: 81.5ms\n",
      "129:\tlearn: 0.0009757\ttotal: 503ms\tremaining: 77.4ms\n",
      "130:\tlearn: 0.0009757\ttotal: 506ms\tremaining: 73.4ms\n",
      "131:\tlearn: 0.0009757\ttotal: 508ms\tremaining: 69.3ms\n",
      "132:\tlearn: 0.0009756\ttotal: 511ms\tremaining: 65.3ms\n",
      "133:\tlearn: 0.0009756\ttotal: 513ms\tremaining: 61.3ms\n",
      "134:\tlearn: 0.0009756\ttotal: 516ms\tremaining: 57.3ms\n",
      "135:\tlearn: 0.0009756\ttotal: 519ms\tremaining: 53.4ms\n",
      "136:\tlearn: 0.0009755\ttotal: 521ms\tremaining: 49.5ms\n",
      "137:\tlearn: 0.0009755\ttotal: 524ms\tremaining: 45.6ms\n",
      "138:\tlearn: 0.0009755\ttotal: 526ms\tremaining: 41.7ms\n",
      "139:\tlearn: 0.0009755\ttotal: 529ms\tremaining: 37.8ms\n",
      "140:\tlearn: 0.0009755\ttotal: 531ms\tremaining: 33.9ms\n",
      "141:\tlearn: 0.0009755\ttotal: 534ms\tremaining: 30.1ms\n",
      "142:\tlearn: 0.0009549\ttotal: 537ms\tremaining: 26.3ms\n",
      "143:\tlearn: 0.0009385\ttotal: 540ms\tremaining: 22.5ms\n",
      "144:\tlearn: 0.0009385\ttotal: 543ms\tremaining: 18.7ms\n",
      "145:\tlearn: 0.0009385\ttotal: 546ms\tremaining: 15ms\n",
      "146:\tlearn: 0.0009385\ttotal: 548ms\tremaining: 11.2ms\n",
      "147:\tlearn: 0.0009385\ttotal: 551ms\tremaining: 7.45ms\n",
      "148:\tlearn: 0.0009385\ttotal: 554ms\tremaining: 3.72ms\n",
      "149:\tlearn: 0.0009385\ttotal: 557ms\tremaining: 0us\n",
      "Dataset 5:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef1e0bb11fc464aaa08e669907b0d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6097173896439382, Recall = 0.4827586206896552, Aging Rate = 0.2979978548444762, Precision = 0.8062387522495501, f1 = 0.603909233880027\n",
      "Epoch 2: Train Loss = 0.41545198108197795, Recall = 0.8013649425287356, Aging Rate = 0.4680014301036825, Precision = 0.8521772345301757, f1 = 0.8259903739355794\n",
      "Epoch 3: Train Loss = 0.31112851331947444, Recall = 0.8663793103448276, Aging Rate = 0.48140865212727924, Precision = 0.8956554028963981, f1 = 0.8807741464305276\n",
      "Epoch 4: Train Loss = 0.24330271541283818, Recall = 0.9109195402298851, Aging Rate = 0.4908830890239542, Precision = 0.9235251274581209, f1 = 0.9171790235081374\n",
      "Epoch 5: Train Loss = 0.19339415763728227, Recall = 0.9267241379310345, Aging Rate = 0.487665355738291, Precision = 0.9457478005865103, f1 = 0.9361393323657474\n",
      "Test Loss = 0.15873229303848416, Recall = 0.9748563218390804, Aging Rate = 0.5117983553807651, precision = 0.9479566887879847\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14214940884836155, Recall = 0.9612068965517241, Aging Rate = 0.4969610296746514, Precision = 0.962589928057554, f1 = 0.9618979151689432\n",
      "Epoch 7: Train Loss = 0.1101658527882857, Recall = 0.975933908045977, Aging Rate = 0.4953521630318198, Precision = 0.9805124503789245, f1 = 0.9782178217821782\n",
      "Epoch 8: Train Loss = 0.08876662069121173, Recall = 0.9841954022988506, Aging Rate = 0.4976760815159099, Precision = 0.9841954022988506, f1 = 0.9841954022988506\n",
      "Epoch 9: Train Loss = 0.07147423004012983, Recall = 0.9906609195402298, Aging Rate = 0.4983911333571684, Precision = 0.9892395982783357, f1 = 0.9899497487437187\n",
      "Epoch 10: Train Loss = 0.060223626308502747, Recall = 0.992816091954023, Aging Rate = 0.4982123703968538, Precision = 0.9917473986365267, f1 = 0.9922814575480167\n",
      "Test Loss = 0.05094106064647873, Recall = 0.9949712643678161, Aging Rate = 0.49749731855559526, precision = 0.9953287818900467\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.048894368477690464, Recall = 0.9946120689655172, Aging Rate = 0.49731855559528065, Precision = 0.9953271028037384, f1 = 0.9949694574200504\n",
      "Epoch 12: Train Loss = 0.04091143249682243, Recall = 0.9960488505747126, Aging Rate = 0.49749731855559526, Precision = 0.996406755300036, f1 = 0.9962277707921681\n",
      "Epoch 13: Train Loss = 0.03493394035397149, Recall = 0.9964080459770115, Aging Rate = 0.49749731855559526, Precision = 0.9967660797700323, f1 = 0.9965870307167236\n",
      "Epoch 14: Train Loss = 0.03053802507538262, Recall = 0.9967672413793104, Aging Rate = 0.49731855559528065, Precision = 0.9974838245866283, f1 = 0.9971254042400287\n",
      "Epoch 15: Train Loss = 0.02547509627983448, Recall = 0.9989224137931034, Aging Rate = 0.4978548444762245, Precision = 0.9985637342908438, f1 = 0.9987430418387502\n",
      "Test Loss = 0.02280012798862859, Recall = 0.9989224137931034, Aging Rate = 0.4976760815159099, precision = 0.9989224137931034\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.02224609446250672, Recall = 0.9992816091954023, Aging Rate = 0.4980336074365391, Precision = 0.9985642498205313, f1 = 0.9989228007181329\n",
      "Epoch 17: Train Loss = 0.019422203345003834, Recall = 0.9989224137931034, Aging Rate = 0.4976760815159099, Precision = 0.9989224137931034, f1 = 0.9989224137931034\n",
      "Epoch 18: Train Loss = 0.01746328494587795, Recall = 0.9985632183908046, Aging Rate = 0.49749731855559526, Precision = 0.9989220265900107, f1 = 0.9987425902640561\n",
      "Epoch 19: Train Loss = 0.0157434199508819, Recall = 1.0, Aging Rate = 0.4980336074365391, Precision = 0.9992821249102656, f1 = 0.9996409335727109\n",
      "Epoch 20: Train Loss = 0.013843595241900689, Recall = 0.9996408045977011, Aging Rate = 0.4980336074365391, Precision = 0.9989231873653984, f1 = 0.9992818671454219\n",
      "Test Loss = 0.012281054001891554, Recall = 1.0, Aging Rate = 0.4978548444762245, precision = 0.999640933572711\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.012320299008025424, Recall = 0.9996408045977011, Aging Rate = 0.4976760815159099, Precision = 0.9996408045977011, f1 = 0.9996408045977011\n",
      "Epoch 22: Train Loss = 0.011127588361126102, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Epoch 23: Train Loss = 0.010327752388235944, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 24: Train Loss = 0.009409017591598361, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Epoch 25: Train Loss = 0.008380814117982499, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Test Loss = 0.007693776918021102, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.00775575716382117, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Epoch 27: Train Loss = 0.007399897953238329, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 28: Train Loss = 0.0069395941290917845, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 29: Train Loss = 0.0062045678021875736, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.005824188779057948, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005553965715003527, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 31: Train Loss = 0.005387748283103327, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Epoch 32: Train Loss = 0.005026157927913488, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.004725427590383828, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.004419472230955276, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 35: Train Loss = 0.00422914054419234, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003825663429697257, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.004050265599413774, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.003768699373690292, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.0036366212256332677, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 39: Train Loss = 0.0035968896444929545, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.003330838412857552, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003203897949236235, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.00313688943916409, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.0030832633866505975, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.0029351040900744496, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.0028389954451181216, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.002768703595101487, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002595347467201243, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.0027137384167248047, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 47: Train Loss = 0.0025879221250881463, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.0025310787225117483, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.002501045255234194, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.002419730908283015, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022057738442481374, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.0022890330898293456, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.002261677167711671, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 53: Train Loss = 0.002227374082349619, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 54: Train Loss = 0.002174852017768082, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.0022196901169604555, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001959612868922041, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.002080756239676272, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.002052049812669994, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 58: Train Loss = 0.002028825956941667, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.00200552548856891, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.0019309832313266831, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018769524744530347, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.002066387481502003, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 62: Train Loss = 0.001991964180615501, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.001899150185100368, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.001828284024192346, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.0018450859522871196, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018119233799353276, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0018381847013810282, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.0018273839010488446, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 68: Train Loss = 0.0018935846058271552, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.0018617486742739345, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.0018497852421764246, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017010026660834066, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.0017568913289996095, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.0018433863560152685, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.002755104007895443, Recall = 1.0, Aging Rate = 0.4978548444762245, Precision = 0.999640933572711, f1 = 0.9998204345483929\n",
      "Epoch 74: Train Loss = 0.0017200217494131077, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.0016329673340815143, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001596523171708968, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.001615117913781984, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.0016131719028591193, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.001647382377023207, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.0016807927586478546, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.0016101136187878615, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015730931047179687, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.0016140997280627437, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.0016817316569172813, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.0016298884480777417, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0018027642154769788, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0017871508636151587, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014891826389437362, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0016596651040710186, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.0016146679073379036, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0015584900724665955, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.0017310303282861731, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0016187163211190843, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014415437114732336, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.0015654703186812264, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.0016772124432173849, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.0016351652219355464, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0016592746142006299, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.001804255366556461, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019847667631371865, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0016474053471771722, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.0015891504322149027, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.0020871379199691857, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0017280803933326112, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.0014891973253796367, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013466219153537584, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.0015021065759652688, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.0014631207860056623, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.001544090098503965, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0017003419203533896, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.0020137473233783347, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013152384982102383, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.0014609117001172889, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.0014364566750279043, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.001491240520412512, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.0016155534272884946, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.0016291868996306652, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0014036754403022925, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0016488252847941913, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112: Train Loss = 0.001923627891698649, Recall = 0.9996408045977011, Aging Rate = 0.49749731855559526, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.0021748217187347713, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.0014397440930464848, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.0014041158092475583, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0013159676470777468, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.0015467035282062933, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.0014008800762189066, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.0015256458056909503, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.001496062085971777, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.0014958649946238164, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001404761983354569, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.0015222656696736493, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.0015255167239492452, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.0015146086249842809, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.0015116138263668133, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.001668712349983467, Recall = 1.0, Aging Rate = 0.4976760815159099, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017211975198982282, Recall = 1.0, Aging Rate = 0.4976760815159099, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5006929\ttotal: 12.2ms\tremaining: 2.44s\n",
      "1:\tlearn: 0.4032661\ttotal: 24.3ms\tremaining: 2.41s\n",
      "2:\tlearn: 0.3372670\ttotal: 36.3ms\tremaining: 2.38s\n",
      "3:\tlearn: 0.2917970\ttotal: 48.3ms\tremaining: 2.37s\n",
      "4:\tlearn: 0.2560554\ttotal: 60.2ms\tremaining: 2.35s\n",
      "5:\tlearn: 0.2103897\ttotal: 72.2ms\tremaining: 2.33s\n",
      "6:\tlearn: 0.1827419\ttotal: 83.5ms\tremaining: 2.3s\n",
      "7:\tlearn: 0.1551266\ttotal: 93ms\tremaining: 2.23s\n",
      "8:\tlearn: 0.1377657\ttotal: 103ms\tremaining: 2.19s\n",
      "9:\tlearn: 0.1202142\ttotal: 113ms\tremaining: 2.15s\n",
      "10:\tlearn: 0.1059172\ttotal: 123ms\tremaining: 2.12s\n",
      "11:\tlearn: 0.0953903\ttotal: 133ms\tremaining: 2.09s\n",
      "12:\tlearn: 0.0856898\ttotal: 144ms\tremaining: 2.06s\n",
      "13:\tlearn: 0.0756888\ttotal: 154ms\tremaining: 2.05s\n",
      "14:\tlearn: 0.0640122\ttotal: 165ms\tremaining: 2.03s\n",
      "15:\tlearn: 0.0598210\ttotal: 176ms\tremaining: 2.02s\n",
      "16:\tlearn: 0.0549438\ttotal: 186ms\tremaining: 2s\n",
      "17:\tlearn: 0.0507439\ttotal: 196ms\tremaining: 1.99s\n",
      "18:\tlearn: 0.0473636\ttotal: 207ms\tremaining: 1.97s\n",
      "19:\tlearn: 0.0440237\ttotal: 218ms\tremaining: 1.96s\n",
      "20:\tlearn: 0.0409764\ttotal: 228ms\tremaining: 1.94s\n",
      "21:\tlearn: 0.0385548\ttotal: 238ms\tremaining: 1.93s\n",
      "22:\tlearn: 0.0362587\ttotal: 250ms\tremaining: 1.92s\n",
      "23:\tlearn: 0.0352472\ttotal: 260ms\tremaining: 1.9s\n",
      "24:\tlearn: 0.0324246\ttotal: 270ms\tremaining: 1.89s\n",
      "25:\tlearn: 0.0306316\ttotal: 281ms\tremaining: 1.88s\n",
      "26:\tlearn: 0.0290408\ttotal: 291ms\tremaining: 1.86s\n",
      "27:\tlearn: 0.0266176\ttotal: 302ms\tremaining: 1.85s\n",
      "28:\tlearn: 0.0255145\ttotal: 313ms\tremaining: 1.84s\n",
      "29:\tlearn: 0.0235125\ttotal: 324ms\tremaining: 1.84s\n",
      "30:\tlearn: 0.0221579\ttotal: 335ms\tremaining: 1.83s\n",
      "31:\tlearn: 0.0205218\ttotal: 346ms\tremaining: 1.82s\n",
      "32:\tlearn: 0.0186092\ttotal: 357ms\tremaining: 1.8s\n",
      "33:\tlearn: 0.0176274\ttotal: 367ms\tremaining: 1.79s\n",
      "34:\tlearn: 0.0163884\ttotal: 377ms\tremaining: 1.78s\n",
      "35:\tlearn: 0.0159104\ttotal: 388ms\tremaining: 1.76s\n",
      "36:\tlearn: 0.0154255\ttotal: 399ms\tremaining: 1.76s\n",
      "37:\tlearn: 0.0146354\ttotal: 410ms\tremaining: 1.75s\n",
      "38:\tlearn: 0.0135799\ttotal: 421ms\tremaining: 1.74s\n",
      "39:\tlearn: 0.0117494\ttotal: 431ms\tremaining: 1.72s\n",
      "40:\tlearn: 0.0110500\ttotal: 441ms\tremaining: 1.71s\n",
      "41:\tlearn: 0.0102407\ttotal: 453ms\tremaining: 1.7s\n",
      "42:\tlearn: 0.0096148\ttotal: 464ms\tremaining: 1.69s\n",
      "43:\tlearn: 0.0092713\ttotal: 476ms\tremaining: 1.69s\n",
      "44:\tlearn: 0.0087918\ttotal: 487ms\tremaining: 1.68s\n",
      "45:\tlearn: 0.0083423\ttotal: 498ms\tremaining: 1.67s\n",
      "46:\tlearn: 0.0079521\ttotal: 508ms\tremaining: 1.65s\n",
      "47:\tlearn: 0.0077396\ttotal: 518ms\tremaining: 1.64s\n",
      "48:\tlearn: 0.0070679\ttotal: 528ms\tremaining: 1.63s\n",
      "49:\tlearn: 0.0069018\ttotal: 538ms\tremaining: 1.61s\n",
      "50:\tlearn: 0.0063785\ttotal: 550ms\tremaining: 1.6s\n",
      "51:\tlearn: 0.0061160\ttotal: 560ms\tremaining: 1.59s\n",
      "52:\tlearn: 0.0059081\ttotal: 571ms\tremaining: 1.58s\n",
      "53:\tlearn: 0.0056662\ttotal: 583ms\tremaining: 1.58s\n",
      "54:\tlearn: 0.0053541\ttotal: 594ms\tremaining: 1.57s\n",
      "55:\tlearn: 0.0049982\ttotal: 606ms\tremaining: 1.56s\n",
      "56:\tlearn: 0.0047937\ttotal: 616ms\tremaining: 1.55s\n",
      "57:\tlearn: 0.0044686\ttotal: 628ms\tremaining: 1.54s\n",
      "58:\tlearn: 0.0043060\ttotal: 638ms\tremaining: 1.52s\n",
      "59:\tlearn: 0.0042169\ttotal: 650ms\tremaining: 1.52s\n",
      "60:\tlearn: 0.0040175\ttotal: 660ms\tremaining: 1.5s\n",
      "61:\tlearn: 0.0036740\ttotal: 672ms\tremaining: 1.49s\n",
      "62:\tlearn: 0.0035773\ttotal: 682ms\tremaining: 1.48s\n",
      "63:\tlearn: 0.0034387\ttotal: 693ms\tremaining: 1.47s\n",
      "64:\tlearn: 0.0033065\ttotal: 703ms\tremaining: 1.46s\n",
      "65:\tlearn: 0.0029838\ttotal: 714ms\tremaining: 1.45s\n",
      "66:\tlearn: 0.0028127\ttotal: 725ms\tremaining: 1.44s\n",
      "67:\tlearn: 0.0027368\ttotal: 735ms\tremaining: 1.43s\n",
      "68:\tlearn: 0.0026431\ttotal: 746ms\tremaining: 1.42s\n",
      "69:\tlearn: 0.0024987\ttotal: 757ms\tremaining: 1.41s\n",
      "70:\tlearn: 0.0023366\ttotal: 768ms\tremaining: 1.4s\n",
      "71:\tlearn: 0.0022822\ttotal: 779ms\tremaining: 1.38s\n",
      "72:\tlearn: 0.0022187\ttotal: 790ms\tremaining: 1.37s\n",
      "73:\tlearn: 0.0020129\ttotal: 801ms\tremaining: 1.36s\n",
      "74:\tlearn: 0.0019453\ttotal: 812ms\tremaining: 1.35s\n",
      "75:\tlearn: 0.0018652\ttotal: 823ms\tremaining: 1.34s\n",
      "76:\tlearn: 0.0017833\ttotal: 834ms\tremaining: 1.33s\n",
      "77:\tlearn: 0.0017344\ttotal: 845ms\tremaining: 1.32s\n",
      "78:\tlearn: 0.0016928\ttotal: 856ms\tremaining: 1.31s\n",
      "79:\tlearn: 0.0015833\ttotal: 866ms\tremaining: 1.3s\n",
      "80:\tlearn: 0.0015420\ttotal: 884ms\tremaining: 1.3s\n",
      "81:\tlearn: 0.0014783\ttotal: 903ms\tremaining: 1.3s\n",
      "82:\tlearn: 0.0014407\ttotal: 922ms\tremaining: 1.3s\n",
      "83:\tlearn: 0.0014157\ttotal: 940ms\tremaining: 1.3s\n",
      "84:\tlearn: 0.0013730\ttotal: 951ms\tremaining: 1.29s\n",
      "85:\tlearn: 0.0013296\ttotal: 963ms\tremaining: 1.28s\n",
      "86:\tlearn: 0.0012622\ttotal: 973ms\tremaining: 1.26s\n",
      "87:\tlearn: 0.0011994\ttotal: 984ms\tremaining: 1.25s\n",
      "88:\tlearn: 0.0011596\ttotal: 996ms\tremaining: 1.24s\n",
      "89:\tlearn: 0.0010883\ttotal: 1.01s\tremaining: 1.23s\n",
      "90:\tlearn: 0.0010508\ttotal: 1.02s\tremaining: 1.22s\n",
      "91:\tlearn: 0.0010155\ttotal: 1.03s\tremaining: 1.21s\n",
      "92:\tlearn: 0.0009832\ttotal: 1.04s\tremaining: 1.2s\n",
      "93:\tlearn: 0.0009533\ttotal: 1.05s\tremaining: 1.19s\n",
      "94:\tlearn: 0.0009173\ttotal: 1.06s\tremaining: 1.17s\n",
      "95:\tlearn: 0.0008966\ttotal: 1.07s\tremaining: 1.16s\n",
      "96:\tlearn: 0.0008740\ttotal: 1.08s\tremaining: 1.15s\n",
      "97:\tlearn: 0.0008426\ttotal: 1.09s\tremaining: 1.14s\n",
      "98:\tlearn: 0.0008160\ttotal: 1.1s\tremaining: 1.13s\n",
      "99:\tlearn: 0.0007962\ttotal: 1.11s\tremaining: 1.11s\n",
      "100:\tlearn: 0.0007843\ttotal: 1.13s\tremaining: 1.1s\n",
      "101:\tlearn: 0.0007513\ttotal: 1.14s\tremaining: 1.09s\n",
      "102:\tlearn: 0.0007251\ttotal: 1.15s\tremaining: 1.08s\n",
      "103:\tlearn: 0.0007141\ttotal: 1.16s\tremaining: 1.07s\n",
      "104:\tlearn: 0.0006590\ttotal: 1.17s\tremaining: 1.06s\n",
      "105:\tlearn: 0.0006398\ttotal: 1.18s\tremaining: 1.04s\n",
      "106:\tlearn: 0.0006207\ttotal: 1.19s\tremaining: 1.03s\n",
      "107:\tlearn: 0.0005912\ttotal: 1.2s\tremaining: 1.02s\n",
      "108:\tlearn: 0.0005749\ttotal: 1.21s\tremaining: 1.01s\n",
      "109:\tlearn: 0.0005421\ttotal: 1.22s\tremaining: 1s\n",
      "110:\tlearn: 0.0005106\ttotal: 1.23s\tremaining: 989ms\n",
      "111:\tlearn: 0.0004986\ttotal: 1.25s\tremaining: 978ms\n",
      "112:\tlearn: 0.0004780\ttotal: 1.25s\tremaining: 967ms\n",
      "113:\tlearn: 0.0004663\ttotal: 1.27s\tremaining: 955ms\n",
      "114:\tlearn: 0.0004560\ttotal: 1.28s\tremaining: 944ms\n",
      "115:\tlearn: 0.0004418\ttotal: 1.29s\tremaining: 933ms\n",
      "116:\tlearn: 0.0004304\ttotal: 1.3s\tremaining: 922ms\n",
      "117:\tlearn: 0.0004244\ttotal: 1.31s\tremaining: 910ms\n",
      "118:\tlearn: 0.0004114\ttotal: 1.32s\tremaining: 899ms\n",
      "119:\tlearn: 0.0004010\ttotal: 1.33s\tremaining: 888ms\n",
      "120:\tlearn: 0.0003897\ttotal: 1.34s\tremaining: 877ms\n",
      "121:\tlearn: 0.0003741\ttotal: 1.35s\tremaining: 866ms\n",
      "122:\tlearn: 0.0003588\ttotal: 1.36s\tremaining: 854ms\n",
      "123:\tlearn: 0.0003433\ttotal: 1.38s\tremaining: 843ms\n",
      "124:\tlearn: 0.0003350\ttotal: 1.39s\tremaining: 832ms\n",
      "125:\tlearn: 0.0003302\ttotal: 1.4s\tremaining: 821ms\n",
      "126:\tlearn: 0.0003073\ttotal: 1.41s\tremaining: 809ms\n",
      "127:\tlearn: 0.0002997\ttotal: 1.42s\tremaining: 798ms\n",
      "128:\tlearn: 0.0002942\ttotal: 1.43s\tremaining: 787ms\n",
      "129:\tlearn: 0.0002830\ttotal: 1.44s\tremaining: 776ms\n",
      "130:\tlearn: 0.0002755\ttotal: 1.45s\tremaining: 764ms\n",
      "131:\tlearn: 0.0002689\ttotal: 1.46s\tremaining: 753ms\n",
      "132:\tlearn: 0.0002689\ttotal: 1.47s\tremaining: 742ms\n",
      "133:\tlearn: 0.0002592\ttotal: 1.48s\tremaining: 730ms\n",
      "134:\tlearn: 0.0002592\ttotal: 1.49s\tremaining: 719ms\n",
      "135:\tlearn: 0.0002454\ttotal: 1.5s\tremaining: 708ms\n",
      "136:\tlearn: 0.0002454\ttotal: 1.51s\tremaining: 696ms\n",
      "137:\tlearn: 0.0002454\ttotal: 1.52s\tremaining: 685ms\n",
      "138:\tlearn: 0.0002454\ttotal: 1.53s\tremaining: 674ms\n",
      "139:\tlearn: 0.0002454\ttotal: 1.55s\tremaining: 663ms\n",
      "140:\tlearn: 0.0002454\ttotal: 1.56s\tremaining: 652ms\n",
      "141:\tlearn: 0.0002413\ttotal: 1.57s\tremaining: 640ms\n",
      "142:\tlearn: 0.0002413\ttotal: 1.58s\tremaining: 629ms\n",
      "143:\tlearn: 0.0002413\ttotal: 1.59s\tremaining: 618ms\n",
      "144:\tlearn: 0.0002362\ttotal: 1.6s\tremaining: 606ms\n",
      "145:\tlearn: 0.0002362\ttotal: 1.61s\tremaining: 595ms\n",
      "146:\tlearn: 0.0002362\ttotal: 1.62s\tremaining: 584ms\n",
      "147:\tlearn: 0.0002362\ttotal: 1.63s\tremaining: 573ms\n",
      "148:\tlearn: 0.0002362\ttotal: 1.64s\tremaining: 561ms\n",
      "149:\tlearn: 0.0002362\ttotal: 1.65s\tremaining: 550ms\n",
      "150:\tlearn: 0.0002296\ttotal: 1.66s\tremaining: 539ms\n",
      "151:\tlearn: 0.0002296\ttotal: 1.67s\tremaining: 528ms\n",
      "152:\tlearn: 0.0002296\ttotal: 1.68s\tremaining: 517ms\n",
      "153:\tlearn: 0.0002296\ttotal: 1.69s\tremaining: 505ms\n",
      "154:\tlearn: 0.0002296\ttotal: 1.7s\tremaining: 494ms\n",
      "155:\tlearn: 0.0002296\ttotal: 1.71s\tremaining: 483ms\n",
      "156:\tlearn: 0.0002296\ttotal: 1.72s\tremaining: 472ms\n",
      "157:\tlearn: 0.0002296\ttotal: 1.73s\tremaining: 461ms\n",
      "158:\tlearn: 0.0002225\ttotal: 1.74s\tremaining: 450ms\n",
      "159:\tlearn: 0.0002225\ttotal: 1.75s\tremaining: 439ms\n",
      "160:\tlearn: 0.0002225\ttotal: 1.76s\tremaining: 427ms\n",
      "161:\tlearn: 0.0002225\ttotal: 1.77s\tremaining: 417ms\n",
      "162:\tlearn: 0.0002225\ttotal: 1.78s\tremaining: 405ms\n",
      "163:\tlearn: 0.0002225\ttotal: 1.8s\tremaining: 394ms\n",
      "164:\tlearn: 0.0002225\ttotal: 1.81s\tremaining: 383ms\n",
      "165:\tlearn: 0.0002225\ttotal: 1.82s\tremaining: 373ms\n",
      "166:\tlearn: 0.0002099\ttotal: 1.83s\tremaining: 362ms\n",
      "167:\tlearn: 0.0002099\ttotal: 1.84s\tremaining: 351ms\n",
      "168:\tlearn: 0.0002058\ttotal: 1.85s\tremaining: 340ms\n",
      "169:\tlearn: 0.0002058\ttotal: 1.86s\tremaining: 329ms\n",
      "170:\tlearn: 0.0002058\ttotal: 1.88s\tremaining: 319ms\n",
      "171:\tlearn: 0.0002058\ttotal: 1.9s\tremaining: 309ms\n",
      "172:\tlearn: 0.0002058\ttotal: 1.92s\tremaining: 299ms\n",
      "173:\tlearn: 0.0002058\ttotal: 1.93s\tremaining: 289ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174:\tlearn: 0.0002058\ttotal: 1.95s\tremaining: 278ms\n",
      "175:\tlearn: 0.0002058\ttotal: 1.96s\tremaining: 267ms\n",
      "176:\tlearn: 0.0002058\ttotal: 1.97s\tremaining: 256ms\n",
      "177:\tlearn: 0.0002058\ttotal: 1.98s\tremaining: 245ms\n",
      "178:\tlearn: 0.0002058\ttotal: 1.99s\tremaining: 233ms\n",
      "179:\tlearn: 0.0002058\ttotal: 2s\tremaining: 222ms\n",
      "180:\tlearn: 0.0002058\ttotal: 2.01s\tremaining: 211ms\n",
      "181:\tlearn: 0.0002058\ttotal: 2.02s\tremaining: 200ms\n",
      "182:\tlearn: 0.0002058\ttotal: 2.03s\tremaining: 189ms\n",
      "183:\tlearn: 0.0002058\ttotal: 2.04s\tremaining: 178ms\n",
      "184:\tlearn: 0.0002058\ttotal: 2.05s\tremaining: 167ms\n",
      "185:\tlearn: 0.0002058\ttotal: 2.06s\tremaining: 155ms\n",
      "186:\tlearn: 0.0002058\ttotal: 2.08s\tremaining: 144ms\n",
      "187:\tlearn: 0.0002058\ttotal: 2.09s\tremaining: 133ms\n",
      "188:\tlearn: 0.0002058\ttotal: 2.1s\tremaining: 122ms\n",
      "189:\tlearn: 0.0002058\ttotal: 2.11s\tremaining: 111ms\n",
      "190:\tlearn: 0.0002023\ttotal: 2.12s\tremaining: 99.7ms\n",
      "191:\tlearn: 0.0002023\ttotal: 2.13s\tremaining: 88.6ms\n",
      "192:\tlearn: 0.0002023\ttotal: 2.14s\tremaining: 77.5ms\n",
      "193:\tlearn: 0.0002023\ttotal: 2.15s\tremaining: 66.5ms\n",
      "194:\tlearn: 0.0001985\ttotal: 2.16s\tremaining: 55.4ms\n",
      "195:\tlearn: 0.0001985\ttotal: 2.17s\tremaining: 44.3ms\n",
      "196:\tlearn: 0.0001985\ttotal: 2.18s\tremaining: 33.2ms\n",
      "197:\tlearn: 0.0001955\ttotal: 2.19s\tremaining: 22.1ms\n",
      "198:\tlearn: 0.0001955\ttotal: 2.2s\tremaining: 11.1ms\n",
      "199:\tlearn: 0.0001955\ttotal: 2.21s\tremaining: 0us\n",
      "Dataset 6:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1d1e9b47d2467790c7fbd8c83ad008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.4655147044799357, Recall = 0.9620548744892002, Aging Rate = 0.8574406670942912, Precision = 0.616420422666916, f1 = 0.7513963296477829\n",
      "Epoch 2: Train Loss = 0.29018172661871233, Recall = 0.963806187974314, Aging Rate = 0.6767158434894163, Precision = 0.7824644549763033, f1 = 0.8637195919434998\n",
      "Epoch 3: Train Loss = 0.2246409068989249, Recall = 0.9705195563339171, Aging Rate = 0.6395125080179602, Precision = 0.8337512537612839, f1 = 0.8969517129754518\n",
      "Epoch 4: Train Loss = 0.17613071792969878, Recall = 0.9772329246935202, Aging Rate = 0.6148171905067351, Precision = 0.8732394366197183, f1 = 0.9223140495867769\n",
      "Epoch 5: Train Loss = 0.14429402415419024, Recall = 0.9813193228254524, Aging Rate = 0.5973380372033354, Precision = 0.9025503355704698, f1 = 0.9402880715983779\n",
      "Test Loss = 0.11368307837830513, Recall = 0.985697606538237, Aging Rate = 0.5800192431045542, precision = 0.9336466685098148\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.10478741964749452, Recall = 0.9877408056042032, Aging Rate = 0.5800192431045542, Precision = 0.9355819740116118, f1 = 0.9609541388612807\n",
      "Epoch 7: Train Loss = 0.08260588101311477, Recall = 0.9915353181552832, Aging Rate = 0.5718409236690186, Precision = 0.9526079641054402, f1 = 0.9716819221967964\n",
      "Epoch 8: Train Loss = 0.0672939639846777, Recall = 0.9950379451255108, Aging Rate = 0.5684733803720333, Precision = 0.9616361071932299, f1 = 0.9780519294218907\n",
      "Epoch 9: Train Loss = 0.05532816367855892, Recall = 0.9935785172212492, Aging Rate = 0.5630211674150096, Precision = 0.9695243520364568, f1 = 0.9814040651578493\n",
      "Epoch 10: Train Loss = 0.04667299057788922, Recall = 0.9953298307063632, Aging Rate = 0.559012187299551, Precision = 0.9781985083189902, f1 = 0.9866898148148148\n",
      "Test Loss = 0.03954434851477206, Recall = 0.9976649153531816, Aging Rate = 0.558531109685696, precision = 0.9813379270743612\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.038980550936174134, Recall = 0.9979568009340338, Aging Rate = 0.5583707504810776, Precision = 0.9819069500287191, f1 = 0.9898668210770122\n",
      "Epoch 12: Train Loss = 0.0349845040620635, Recall = 0.9964973730297724, Aging Rate = 0.5553239255933291, Precision = 0.9858504187120993, f1 = 0.9911453041079983\n",
      "Epoch 13: Train Loss = 0.029638426543247477, Recall = 0.9976649153531816, Aging Rate = 0.553880692751764, Precision = 0.9895773016792125, f1 = 0.9936046511627908\n",
      "Epoch 14: Train Loss = 0.0262903629361895, Recall = 0.9988324576765908, Aging Rate = 0.5540410519563823, Precision = 0.9904486251808973, f1 = 0.9946228745821829\n",
      "Epoch 15: Train Loss = 0.022656539480146343, Recall = 0.9988324576765908, Aging Rate = 0.5527581783194355, Precision = 0.9927473165071077, f1 = 0.9957805907172996\n",
      "Test Loss = 0.020327153873424965, Recall = 0.9997081144191476, Aging Rate = 0.5529185375240538, precision = 0.9933294663573086\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.020149891854355242, Recall = 0.9994162288382954, Aging Rate = 0.5522771007055804, Precision = 0.9941927990708479, f1 = 0.996797671033479\n",
      "Epoch 17: Train Loss = 0.019139611309201884, Recall = 0.9997081144191476, Aging Rate = 0.5525978191148172, Precision = 0.993905977945444, f1 = 0.9967986030267754\n",
      "Epoch 18: Train Loss = 0.01691023245420381, Recall = 0.999124343257443, Aging Rate = 0.5521167415009621, Precision = 0.994191112401975, f1 = 0.996651623234823\n",
      "Epoch 19: Train Loss = 0.014743879975092725, Recall = 0.9997081144191476, Aging Rate = 0.5517960230917255, Precision = 0.995350188898576, f1 = 0.9975243920198049\n",
      "Epoch 20: Train Loss = 0.01348735380832959, Recall = 0.9997081144191476, Aging Rate = 0.5514753046824887, Precision = 0.9959290491421925, f1 = 0.9978150036416606\n",
      "Test Loss = 0.01161852415060867, Recall = 1.0, Aging Rate = 0.5503527902501604, precision = 0.9982517482517482\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.011851046251096047, Recall = 1.0, Aging Rate = 0.550673508659397, Precision = 0.9976703552708212, f1 = 0.9988338192419826\n",
      "Epoch 22: Train Loss = 0.011092541615685987, Recall = 1.0, Aging Rate = 0.5511545862732521, Precision = 0.9967995344777422, f1 = 0.9983972023896255\n",
      "Epoch 23: Train Loss = 0.011003037357871314, Recall = 0.9994162288382954, Aging Rate = 0.5509942270686338, Precision = 0.9965075669383003, f1 = 0.9979597784902361\n",
      "Epoch 24: Train Loss = 0.010322792717208458, Recall = 0.9994162288382954, Aging Rate = 0.5505131494547787, Precision = 0.9973783862510923, f1 = 0.9983962676775039\n",
      "Epoch 25: Train Loss = 0.009218128251701574, Recall = 1.0, Aging Rate = 0.5508338678640154, Precision = 0.9973799126637555, f1 = 0.9986882378662003\n",
      "Test Loss = 0.007105534024073039, Recall = 1.0, Aging Rate = 0.550192431045542, precision = 0.9985426989215972\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.008218176427701724, Recall = 1.0, Aging Rate = 0.5508338678640154, Precision = 0.9973799126637555, f1 = 0.9986882378662003\n",
      "Epoch 27: Train Loss = 0.007267211503922733, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Epoch 28: Train Loss = 0.007957558647918828, Recall = 1.0, Aging Rate = 0.550673508659397, Precision = 0.9976703552708212, f1 = 0.9988338192419826\n",
      "Epoch 29: Train Loss = 0.0066102611594262934, Recall = 1.0, Aging Rate = 0.550192431045542, Precision = 0.9985426989215972, f1 = 0.9992708181420447\n",
      "Epoch 30: Train Loss = 0.006826147674257324, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Test Loss = 0.00540686887255831, Recall = 1.0, Aging Rate = 0.550192431045542, precision = 0.9985426989215972\n",
      "\n",
      "Epoch 31: Train Loss = 0.0060247330989980155, Recall = 0.9997081144191476, Aging Rate = 0.5498717126363053, Precision = 0.9988334791484398, f1 = 0.9992706053975201\n",
      "Epoch 32: Train Loss = 0.005595493697303851, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Epoch 33: Train Loss = 0.005943269420605132, Recall = 1.0, Aging Rate = 0.550192431045542, Precision = 0.9985426989215972, f1 = 0.9992708181420447\n",
      "Epoch 34: Train Loss = 0.005770715434287004, Recall = 0.9997081144191476, Aging Rate = 0.5498717126363053, Precision = 0.9988334791484398, f1 = 0.9992706053975201\n",
      "Epoch 35: Train Loss = 0.0050131009703901115, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Test Loss = 0.004078520643820531, Recall = 1.0, Aging Rate = 0.550192431045542, precision = 0.9985426989215972\n",
      "\n",
      "Epoch 36: Train Loss = 0.004879381191319977, Recall = 1.0, Aging Rate = 0.550192431045542, Precision = 0.9985426989215972, f1 = 0.9992708181420447\n",
      "Epoch 37: Train Loss = 0.004447663133659874, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 38: Train Loss = 0.004701842191898532, Recall = 0.9997081144191476, Aging Rate = 0.5498717126363053, Precision = 0.9988334791484398, f1 = 0.9992706053975201\n",
      "Epoch 39: Train Loss = 0.0054701480816158445, Recall = 1.0, Aging Rate = 0.5503527902501604, Precision = 0.9982517482517482, f1 = 0.9991251093613298\n",
      "Epoch 40: Train Loss = 0.00409715598685974, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Test Loss = 0.0042804942636274525, Recall = 1.0, Aging Rate = 0.549711353431687, precision = 0.9994165694282381\n",
      "Model in epoch 40 is saved.\n",
      "\n",
      "Epoch 41: Train Loss = 0.004001522256029176, Recall = 0.9997081144191476, Aging Rate = 0.549711353431687, Precision = 0.9991248541423571, f1 = 0.9994163991829588\n",
      "Epoch 42: Train Loss = 0.004302966548816909, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Epoch 43: Train Loss = 0.0035560720570617242, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 44: Train Loss = 0.0033347582679918134, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 45: Train Loss = 0.003189122683062165, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.003406072582038761, Recall = 0.9997081144191476, Aging Rate = 0.5495509942270687, precision = 0.9994163991829589\n",
      "\n",
      "Epoch 46: Train Loss = 0.0035274077057205213, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.003317654434074197, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 48: Train Loss = 0.0033720749226650265, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 49: Train Loss = 0.003450756233049326, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 50: Train Loss = 0.004057109731532074, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Test Loss = 0.002440842516607389, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.0028137579384648775, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 52: Train Loss = 0.0038603080927637466, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 53: Train Loss = 0.0041893730785356, Recall = 0.9997081144191476, Aging Rate = 0.549711353431687, Precision = 0.9991248541423571, f1 = 0.9994163991829588\n",
      "Epoch 54: Train Loss = 0.0030157703699751964, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 55: Train Loss = 0.003538388869227248, Recall = 0.9997081144191476, Aging Rate = 0.5495509942270687, Precision = 0.9994163991829589, f1 = 0.9995622355172917\n",
      "Test Loss = 0.002132143911315968, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 56: Train Loss = 0.0029552298062734874, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 57: Train Loss = 0.00298799512919213, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 58: Train Loss = 0.0029962814503813533, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 59: Train Loss = 0.0026928473539074562, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 60: Train Loss = 0.0027314922890331384, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Test Loss = 0.002402509492030457, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 61: Train Loss = 0.0025048414387873414, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 62: Train Loss = 0.0025371594119738164, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 63: Train Loss = 0.003481928708442177, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 64: Train Loss = 0.0023794422743661594, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 65: Train Loss = 0.002465131139559546, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.00399937505808117, Recall = 0.9988324576765908, Aging Rate = 0.5487491982039769, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.0027608840908219584, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 67: Train Loss = 0.0028085292397918822, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 68: Train Loss = 0.0035861802701536633, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 69: Train Loss = 0.002571340409872756, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 70: Train Loss = 0.0027323051196277487, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.0029373054538696353, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, precision = 0.9997081144191476\n",
      "\n",
      "Epoch 71: Train Loss = 0.0035112979489565614, Recall = 0.9997081144191476, Aging Rate = 0.5495509942270687, Precision = 0.9994163991829589, f1 = 0.9995622355172917\n",
      "Epoch 72: Train Loss = 0.002762391840278292, Recall = 1.0, Aging Rate = 0.5500320718409236, Precision = 0.9988338192419826, f1 = 0.9994165694282381\n",
      "Epoch 73: Train Loss = 0.0025609770528834752, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.002897842296604079, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 75: Train Loss = 0.003158182321526134, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Test Loss = 0.002625062385922856, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 76: Train Loss = 0.0028194488809372923, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 77: Train Loss = 0.0026776962587251457, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 78: Train Loss = 0.002505417265633576, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 79: Train Loss = 0.002293630051793981, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 80: Train Loss = 0.0024211870205025307, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002166558781846631, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 81: Train Loss = 0.002415451032469184, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 82: Train Loss = 0.0028015662492757677, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 83: Train Loss = 0.002640980536881607, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.0023599452703666705, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.0033668958662033577, Recall = 0.9997081144191476, Aging Rate = 0.5495509942270687, Precision = 0.9994163991829589, f1 = 0.9995622355172917\n",
      "Test Loss = 0.00399412512268472, Recall = 0.9997081144191476, Aging Rate = 0.5498717126363053, precision = 0.9988334791484398\n",
      "\n",
      "Epoch 86: Train Loss = 0.006759323842146669, Recall = 0.999124343257443, Aging Rate = 0.5495509942270687, Precision = 0.9988327983659178, f1 = 0.9989785495403473\n",
      "Epoch 87: Train Loss = 0.0026383912526823775, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 88: Train Loss = 0.0024234391284438665, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 89: Train Loss = 0.002374362818100905, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 90: Train Loss = 0.0020538203821788142, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019880598586445733, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "Model in epoch 90 is saved.\n",
      "\n",
      "Epoch 91: Train Loss = 0.0021360225798090237, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.002228641433977642, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 93: Train Loss = 0.0023582111146525646, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.002540809994624991, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 95: Train Loss = 0.0023690783445597307, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.0031153701085312323, Recall = 0.9997081144191476, Aging Rate = 0.5492302758178319, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0026353315272230494, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.002455455756276091, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 98: Train Loss = 0.0022723931135045813, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.002311668988535894, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 100: Train Loss = 0.002440340970203787, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.004246510518087531, Recall = 0.9994162288382954, Aging Rate = 0.5490699166132136, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.002537568786116992, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.0025006368209089967, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 103: Train Loss = 0.002933921379567679, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 104: Train Loss = 0.0028438350436504095, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 105: Train Loss = 0.00249513855512834, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0025208500394107197, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 106: Train Loss = 0.0025652547071510455, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 107: Train Loss = 0.0021510006499376967, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.0025831957203123986, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 109: Train Loss = 0.0027877065775595783, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.002945428891878066, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.002191492832102111, Recall = 0.9997081144191476, Aging Rate = 0.5492302758178319, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0023073587665779992, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.002393663297374473, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 113: Train Loss = 0.003554106251210586, Recall = 0.9997081144191476, Aging Rate = 0.5498717126363053, Precision = 0.9988334791484398, f1 = 0.9992706053975201\n",
      "Epoch 114: Train Loss = 0.003172901955240832, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 115: Train Loss = 0.002168581305182825, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001931125377967949, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.002070261093365498, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 117: Train Loss = 0.0026973568648577806, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 118: Train Loss = 0.0027100829608592662, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 119: Train Loss = 0.002302466653648231, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 120: Train Loss = 0.002419680715708816, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.002188120464895296, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.002212188284123714, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.0027240983681243113, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 123: Train Loss = 0.0030406748654868638, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 124: Train Loss = 0.0021360378095317415, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 125: Train Loss = 0.002416096265323823, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0024639638192322038, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.0025202410666432143, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 127: Train Loss = 0.0021766850351002914, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 128: Train Loss = 0.002146040819507066, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.0022200068926069507, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.0038301326929471523, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.0028689600132415314, Recall = 1.0, Aging Rate = 0.5498717126363053, precision = 0.9991251093613298\n",
      "\n",
      "Epoch 131: Train Loss = 0.0030152918624550087, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 132: Train Loss = 0.002122183678258925, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 133: Train Loss = 0.002309166284106586, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 134: Train Loss = 0.0022805082080664734, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 135: Train Loss = 0.002231130786263996, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0022406423818517175, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 136: Train Loss = 0.0034389097940695254, Recall = 0.9997081144191476, Aging Rate = 0.5495509942270687, Precision = 0.9994163991829589, f1 = 0.9995622355172917\n",
      "Epoch 137: Train Loss = 0.0026218937847326015, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Epoch 138: Train Loss = 0.002139535061032359, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 139: Train Loss = 0.0020038290708723062, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 140: Train Loss = 0.002310139316595933, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.002904113443781018, Recall = 0.9994162288382954, Aging Rate = 0.5490699166132136, precision = 1.0\n",
      "\n",
      "Epoch 141: Train Loss = 0.002537882088870527, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 142: Train Loss = 0.002494398477463968, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 143: Train Loss = 0.0025215212069332332, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 144: Train Loss = 0.002489633925409654, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 145: Train Loss = 0.0021021571487885487, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.002479165278266425, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 146: Train Loss = 0.00252818592616152, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147: Train Loss = 0.0028922736429761093, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 148: Train Loss = 0.0023864503242577244, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 149: Train Loss = 0.0025625086882959027, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 150: Train Loss = 0.0026478921226031644, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0021812309261000427, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 151: Train Loss = 0.002176846502554281, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 152: Train Loss = 0.002439046509485068, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 153: Train Loss = 0.002330036478513174, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 154: Train Loss = 0.002556934419271463, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 155: Train Loss = 0.0023594361589833225, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.001917295333017751, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 156: Train Loss = 0.0026132343048701745, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 157: Train Loss = 0.0022744662022353116, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 158: Train Loss = 0.00273395187969393, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 159: Train Loss = 0.002599998402652391, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 160: Train Loss = 0.0024962636103466477, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.002734330740925749, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 161: Train Loss = 0.0022212698022345697, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 162: Train Loss = 0.0022989570818371914, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 163: Train Loss = 0.002422976905552908, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 164: Train Loss = 0.00221358956915468, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 165: Train Loss = 0.002226318829203642, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018517592833888184, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 166: Train Loss = 0.0022200788841705297, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 167: Train Loss = 0.0029336992839997665, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 168: Train Loss = 0.0023765474972461493, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 169: Train Loss = 0.0022573786572032896, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 170: Train Loss = 0.0027242460646687496, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0021253592293468174, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 171: Train Loss = 0.002570231027487185, Recall = 0.9997081144191476, Aging Rate = 0.5493906350224503, Precision = 0.9997081144191476, f1 = 0.9997081144191476\n",
      "Epoch 172: Train Loss = 0.002255768022924719, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 173: Train Loss = 0.002209808163161463, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 174: Train Loss = 0.00236280751899972, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 175: Train Loss = 0.00208130514358527, Recall = 1.0, Aging Rate = 0.549711353431687, Precision = 0.9994165694282381, f1 = 0.9997081995914795\n",
      "Test Loss = 0.0033904625937125196, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 176: Train Loss = 0.0033197206808098802, Recall = 1.0, Aging Rate = 0.5498717126363053, Precision = 0.9991251093613298, f1 = 0.999562363238512\n",
      "Epoch 177: Train Loss = 0.0024122140420190967, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 178: Train Loss = 0.0022638050430544517, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 179: Train Loss = 0.0024792227044736223, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 180: Train Loss = 0.0023408615491977243, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0027861318813405113, Recall = 1.0, Aging Rate = 0.5493906350224503, precision = 1.0\n",
      "\n",
      "Epoch 181: Train Loss = 0.0022984399793148664, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 182: Train Loss = 0.002314780886877015, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 183: Train Loss = 0.002725225542191084, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 184: Train Loss = 0.002413494350995589, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 185: Train Loss = 0.0022907376863155575, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002009663231595586, Recall = 1.0, Aging Rate = 0.5495509942270687, precision = 0.9997081995914794\n",
      "\n",
      "Epoch 186: Train Loss = 0.0026727188602591683, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 187: Train Loss = 0.002611372155848845, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 188: Train Loss = 0.0023822170287081546, Recall = 1.0, Aging Rate = 0.5493906350224503, Precision = 0, f1 = 0.0\n",
      "Epoch 189: Train Loss = 0.0021424138472870115, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Epoch 190: Train Loss = 0.003149224047515018, Recall = 1.0, Aging Rate = 0.5495509942270687, Precision = 0.9997081995914794, f1 = 0.9998540785057639\n",
      "Test Loss = 0.0020307984589608033, Recall = 1.0, Aging Rate = 0.549711353431687, precision = 0.9994165694282381\n",
      "\n",
      "Training Finished at epoch 190.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6034859\ttotal: 20.8ms\tremaining: 5.17s\n",
      "1:\tlearn: 0.5388492\ttotal: 36.4ms\tremaining: 4.51s\n",
      "2:\tlearn: 0.4717767\ttotal: 51.1ms\tremaining: 4.21s\n",
      "3:\tlearn: 0.4426738\ttotal: 66.9ms\tremaining: 4.12s\n",
      "4:\tlearn: 0.4188943\ttotal: 81.4ms\tremaining: 3.99s\n",
      "5:\tlearn: 0.3910881\ttotal: 96.4ms\tremaining: 3.92s\n",
      "6:\tlearn: 0.3653530\ttotal: 112ms\tremaining: 3.89s\n",
      "7:\tlearn: 0.3450609\ttotal: 127ms\tremaining: 3.84s\n",
      "8:\tlearn: 0.3288592\ttotal: 142ms\tremaining: 3.81s\n",
      "9:\tlearn: 0.3164987\ttotal: 147ms\tremaining: 3.53s\n",
      "10:\tlearn: 0.2973079\ttotal: 152ms\tremaining: 3.29s\n",
      "11:\tlearn: 0.2857455\ttotal: 158ms\tremaining: 3.14s\n",
      "12:\tlearn: 0.2779026\ttotal: 174ms\tremaining: 3.18s\n",
      "13:\tlearn: 0.2635030\ttotal: 189ms\tremaining: 3.19s\n",
      "14:\tlearn: 0.2508685\ttotal: 194ms\tremaining: 3.04s\n",
      "15:\tlearn: 0.2427965\ttotal: 199ms\tremaining: 2.91s\n",
      "16:\tlearn: 0.2308789\ttotal: 204ms\tremaining: 2.8s\n",
      "17:\tlearn: 0.2232026\ttotal: 210ms\tremaining: 2.7s\n",
      "18:\tlearn: 0.2185667\ttotal: 214ms\tremaining: 2.61s\n",
      "19:\tlearn: 0.2102039\ttotal: 235ms\tremaining: 2.7s\n",
      "20:\tlearn: 0.2056388\ttotal: 240ms\tremaining: 2.61s\n",
      "21:\tlearn: 0.1989529\ttotal: 244ms\tremaining: 2.53s\n",
      "22:\tlearn: 0.1924926\ttotal: 249ms\tremaining: 2.46s\n",
      "23:\tlearn: 0.1879554\ttotal: 254ms\tremaining: 2.39s\n",
      "24:\tlearn: 0.1826304\ttotal: 259ms\tremaining: 2.33s\n",
      "25:\tlearn: 0.1797650\ttotal: 263ms\tremaining: 2.27s\n",
      "26:\tlearn: 0.1751234\ttotal: 281ms\tremaining: 2.32s\n",
      "27:\tlearn: 0.1680354\ttotal: 286ms\tremaining: 2.27s\n",
      "28:\tlearn: 0.1637007\ttotal: 291ms\tremaining: 2.22s\n",
      "29:\tlearn: 0.1589391\ttotal: 296ms\tremaining: 2.17s\n",
      "30:\tlearn: 0.1539478\ttotal: 301ms\tremaining: 2.13s\n",
      "31:\tlearn: 0.1463096\ttotal: 306ms\tremaining: 2.08s\n",
      "32:\tlearn: 0.1409240\ttotal: 310ms\tremaining: 2.04s\n",
      "33:\tlearn: 0.1370816\ttotal: 329ms\tremaining: 2.09s\n",
      "34:\tlearn: 0.1332626\ttotal: 344ms\tremaining: 2.11s\n",
      "35:\tlearn: 0.1298107\ttotal: 349ms\tremaining: 2.07s\n",
      "36:\tlearn: 0.1258895\ttotal: 354ms\tremaining: 2.04s\n",
      "37:\tlearn: 0.1223690\ttotal: 375ms\tremaining: 2.09s\n",
      "38:\tlearn: 0.1189936\ttotal: 380ms\tremaining: 2.06s\n",
      "39:\tlearn: 0.1161134\ttotal: 385ms\tremaining: 2.02s\n",
      "40:\tlearn: 0.1120060\ttotal: 390ms\tremaining: 1.99s\n",
      "41:\tlearn: 0.1081278\ttotal: 395ms\tremaining: 1.96s\n",
      "42:\tlearn: 0.1057072\ttotal: 400ms\tremaining: 1.93s\n",
      "43:\tlearn: 0.1026751\ttotal: 405ms\tremaining: 1.9s\n",
      "44:\tlearn: 0.1015972\ttotal: 408ms\tremaining: 1.86s\n",
      "45:\tlearn: 0.0994587\ttotal: 420ms\tremaining: 1.86s\n",
      "46:\tlearn: 0.0977234\ttotal: 424ms\tremaining: 1.83s\n",
      "47:\tlearn: 0.0964111\ttotal: 429ms\tremaining: 1.8s\n",
      "48:\tlearn: 0.0952434\ttotal: 434ms\tremaining: 1.78s\n",
      "49:\tlearn: 0.0929695\ttotal: 438ms\tremaining: 1.75s\n",
      "50:\tlearn: 0.0919456\ttotal: 443ms\tremaining: 1.73s\n",
      "51:\tlearn: 0.0913812\ttotal: 465ms\tremaining: 1.77s\n",
      "52:\tlearn: 0.0882283\ttotal: 480ms\tremaining: 1.78s\n",
      "53:\tlearn: 0.0869229\ttotal: 485ms\tremaining: 1.76s\n",
      "54:\tlearn: 0.0853543\ttotal: 489ms\tremaining: 1.74s\n",
      "55:\tlearn: 0.0831338\ttotal: 495ms\tremaining: 1.71s\n",
      "56:\tlearn: 0.0805962\ttotal: 499ms\tremaining: 1.69s\n",
      "57:\tlearn: 0.0780337\ttotal: 504ms\tremaining: 1.67s\n",
      "58:\tlearn: 0.0772411\ttotal: 509ms\tremaining: 1.65s\n",
      "59:\tlearn: 0.0759405\ttotal: 526ms\tremaining: 1.67s\n",
      "60:\tlearn: 0.0742481\ttotal: 531ms\tremaining: 1.65s\n",
      "61:\tlearn: 0.0724113\ttotal: 536ms\tremaining: 1.62s\n",
      "62:\tlearn: 0.0702370\ttotal: 541ms\tremaining: 1.6s\n",
      "63:\tlearn: 0.0683039\ttotal: 546ms\tremaining: 1.59s\n",
      "64:\tlearn: 0.0674565\ttotal: 551ms\tremaining: 1.57s\n",
      "65:\tlearn: 0.0664408\ttotal: 555ms\tremaining: 1.55s\n",
      "66:\tlearn: 0.0650602\ttotal: 560ms\tremaining: 1.53s\n",
      "67:\tlearn: 0.0640110\ttotal: 565ms\tremaining: 1.51s\n",
      "68:\tlearn: 0.0629005\ttotal: 570ms\tremaining: 1.49s\n",
      "69:\tlearn: 0.0611816\ttotal: 575ms\tremaining: 1.48s\n",
      "70:\tlearn: 0.0602526\ttotal: 587ms\tremaining: 1.48s\n",
      "71:\tlearn: 0.0594426\ttotal: 592ms\tremaining: 1.46s\n",
      "72:\tlearn: 0.0573323\ttotal: 596ms\tremaining: 1.45s\n",
      "73:\tlearn: 0.0566965\ttotal: 617ms\tremaining: 1.47s\n",
      "74:\tlearn: 0.0557913\ttotal: 622ms\tremaining: 1.45s\n",
      "75:\tlearn: 0.0553722\ttotal: 632ms\tremaining: 1.45s\n",
      "76:\tlearn: 0.0542378\ttotal: 637ms\tremaining: 1.43s\n",
      "77:\tlearn: 0.0527379\ttotal: 642ms\tremaining: 1.42s\n",
      "78:\tlearn: 0.0521632\ttotal: 647ms\tremaining: 1.4s\n",
      "79:\tlearn: 0.0515451\ttotal: 652ms\tremaining: 1.39s\n",
      "80:\tlearn: 0.0506845\ttotal: 657ms\tremaining: 1.37s\n",
      "81:\tlearn: 0.0504694\ttotal: 661ms\tremaining: 1.35s\n",
      "82:\tlearn: 0.0496788\ttotal: 666ms\tremaining: 1.34s\n",
      "83:\tlearn: 0.0490805\ttotal: 671ms\tremaining: 1.33s\n",
      "84:\tlearn: 0.0484275\ttotal: 676ms\tremaining: 1.31s\n",
      "85:\tlearn: 0.0483655\ttotal: 694ms\tremaining: 1.32s\n",
      "86:\tlearn: 0.0469787\ttotal: 699ms\tremaining: 1.31s\n",
      "87:\tlearn: 0.0463941\ttotal: 704ms\tremaining: 1.29s\n",
      "88:\tlearn: 0.0450513\ttotal: 709ms\tremaining: 1.28s\n",
      "89:\tlearn: 0.0445501\ttotal: 714ms\tremaining: 1.27s\n",
      "90:\tlearn: 0.0436480\ttotal: 718ms\tremaining: 1.25s\n",
      "91:\tlearn: 0.0430060\ttotal: 725ms\tremaining: 1.24s\n",
      "92:\tlearn: 0.0423264\ttotal: 741ms\tremaining: 1.25s\n",
      "93:\tlearn: 0.0418125\ttotal: 745ms\tremaining: 1.24s\n",
      "94:\tlearn: 0.0410068\ttotal: 750ms\tremaining: 1.22s\n",
      "95:\tlearn: 0.0407539\ttotal: 786ms\tremaining: 1.26s\n",
      "96:\tlearn: 0.0400992\ttotal: 791ms\tremaining: 1.25s\n",
      "97:\tlearn: 0.0399423\ttotal: 796ms\tremaining: 1.24s\n",
      "98:\tlearn: 0.0391288\ttotal: 802ms\tremaining: 1.22s\n",
      "99:\tlearn: 0.0382741\ttotal: 817ms\tremaining: 1.22s\n",
      "100:\tlearn: 0.0377154\ttotal: 822ms\tremaining: 1.21s\n",
      "101:\tlearn: 0.0369829\ttotal: 827ms\tremaining: 1.2s\n",
      "102:\tlearn: 0.0362353\ttotal: 850ms\tremaining: 1.21s\n",
      "103:\tlearn: 0.0356955\ttotal: 855ms\tremaining: 1.2s\n",
      "104:\tlearn: 0.0352531\ttotal: 861ms\tremaining: 1.19s\n",
      "105:\tlearn: 0.0350561\ttotal: 866ms\tremaining: 1.18s\n",
      "106:\tlearn: 0.0342751\ttotal: 879ms\tremaining: 1.17s\n",
      "107:\tlearn: 0.0342750\ttotal: 881ms\tremaining: 1.16s\n",
      "108:\tlearn: 0.0342750\ttotal: 882ms\tremaining: 1.14s\n",
      "109:\tlearn: 0.0342746\ttotal: 884ms\tremaining: 1.12s\n",
      "110:\tlearn: 0.0339588\ttotal: 888ms\tremaining: 1.11s\n",
      "111:\tlearn: 0.0339588\ttotal: 890ms\tremaining: 1.1s\n",
      "112:\tlearn: 0.0336084\ttotal: 910ms\tremaining: 1.1s\n",
      "113:\tlearn: 0.0331940\ttotal: 916ms\tremaining: 1.09s\n",
      "114:\tlearn: 0.0330008\ttotal: 921ms\tremaining: 1.08s\n",
      "115:\tlearn: 0.0328635\ttotal: 926ms\tremaining: 1.07s\n",
      "116:\tlearn: 0.0326376\ttotal: 931ms\tremaining: 1.06s\n",
      "117:\tlearn: 0.0326376\ttotal: 932ms\tremaining: 1.04s\n",
      "118:\tlearn: 0.0322818\ttotal: 937ms\tremaining: 1.03s\n",
      "119:\tlearn: 0.0319326\ttotal: 942ms\tremaining: 1.02s\n",
      "120:\tlearn: 0.0315946\ttotal: 957ms\tremaining: 1.02s\n",
      "121:\tlearn: 0.0310531\ttotal: 962ms\tremaining: 1.01s\n",
      "122:\tlearn: 0.0305812\ttotal: 967ms\tremaining: 999ms\n",
      "123:\tlearn: 0.0304425\ttotal: 989ms\tremaining: 1s\n",
      "124:\tlearn: 0.0301181\ttotal: 994ms\tremaining: 994ms\n",
      "125:\tlearn: 0.0296283\ttotal: 999ms\tremaining: 983ms\n",
      "126:\tlearn: 0.0291809\ttotal: 1.02s\tremaining: 987ms\n",
      "127:\tlearn: 0.0289679\ttotal: 1.02s\tremaining: 976ms\n",
      "128:\tlearn: 0.0286149\ttotal: 1.03s\tremaining: 965ms\n",
      "129:\tlearn: 0.0283916\ttotal: 1.03s\tremaining: 954ms\n",
      "130:\tlearn: 0.0281622\ttotal: 1.04s\tremaining: 944ms\n",
      "131:\tlearn: 0.0277468\ttotal: 1.04s\tremaining: 934ms\n",
      "132:\tlearn: 0.0274320\ttotal: 1.06s\tremaining: 937ms\n",
      "133:\tlearn: 0.0271395\ttotal: 1.07s\tremaining: 927ms\n",
      "134:\tlearn: 0.0268625\ttotal: 1.09s\tremaining: 930ms\n",
      "135:\tlearn: 0.0263883\ttotal: 1.11s\tremaining: 934ms\n",
      "136:\tlearn: 0.0263879\ttotal: 1.12s\tremaining: 924ms\n",
      "137:\tlearn: 0.0261618\ttotal: 1.13s\tremaining: 914ms\n",
      "138:\tlearn: 0.0259170\ttotal: 1.13s\tremaining: 903ms\n",
      "139:\tlearn: 0.0258379\ttotal: 1.14s\tremaining: 892ms\n",
      "140:\tlearn: 0.0255868\ttotal: 1.16s\tremaining: 895ms\n",
      "141:\tlearn: 0.0255868\ttotal: 1.16s\tremaining: 882ms\n",
      "142:\tlearn: 0.0254617\ttotal: 1.16s\tremaining: 871ms\n",
      "143:\tlearn: 0.0253584\ttotal: 1.17s\tremaining: 860ms\n",
      "144:\tlearn: 0.0250178\ttotal: 1.17s\tremaining: 850ms\n",
      "145:\tlearn: 0.0246677\ttotal: 1.18s\tremaining: 840ms\n",
      "146:\tlearn: 0.0243358\ttotal: 1.2s\tremaining: 838ms\n",
      "147:\tlearn: 0.0240989\ttotal: 1.21s\tremaining: 832ms\n",
      "148:\tlearn: 0.0239597\ttotal: 1.21s\tremaining: 823ms\n",
      "149:\tlearn: 0.0236598\ttotal: 1.22s\tremaining: 814ms\n",
      "150:\tlearn: 0.0233023\ttotal: 1.23s\tremaining: 804ms\n",
      "151:\tlearn: 0.0230089\ttotal: 1.23s\tremaining: 794ms\n",
      "152:\tlearn: 0.0227068\ttotal: 1.25s\tremaining: 793ms\n",
      "153:\tlearn: 0.0225431\ttotal: 1.26s\tremaining: 783ms\n",
      "154:\tlearn: 0.0223770\ttotal: 1.26s\tremaining: 773ms\n",
      "155:\tlearn: 0.0223154\ttotal: 1.27s\tremaining: 763ms\n",
      "156:\tlearn: 0.0223114\ttotal: 1.27s\tremaining: 753ms\n",
      "157:\tlearn: 0.0222300\ttotal: 1.28s\tremaining: 743ms\n",
      "158:\tlearn: 0.0222300\ttotal: 1.28s\tremaining: 731ms\n",
      "159:\tlearn: 0.0220688\ttotal: 1.32s\tremaining: 743ms\n",
      "160:\tlearn: 0.0220687\ttotal: 1.33s\tremaining: 733ms\n",
      "161:\tlearn: 0.0218863\ttotal: 1.34s\tremaining: 726ms\n",
      "162:\tlearn: 0.0217427\ttotal: 1.34s\tremaining: 717ms\n",
      "163:\tlearn: 0.0216420\ttotal: 1.35s\tremaining: 708ms\n",
      "164:\tlearn: 0.0213790\ttotal: 1.36s\tremaining: 699ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165:\tlearn: 0.0212760\ttotal: 1.36s\tremaining: 689ms\n",
      "166:\tlearn: 0.0209928\ttotal: 1.38s\tremaining: 687ms\n",
      "167:\tlearn: 0.0207083\ttotal: 1.39s\tremaining: 678ms\n",
      "168:\tlearn: 0.0207082\ttotal: 1.39s\tremaining: 667ms\n",
      "169:\tlearn: 0.0204101\ttotal: 1.4s\tremaining: 657ms\n",
      "170:\tlearn: 0.0202648\ttotal: 1.4s\tremaining: 647ms\n",
      "171:\tlearn: 0.0202645\ttotal: 1.41s\tremaining: 638ms\n",
      "172:\tlearn: 0.0202276\ttotal: 1.43s\tremaining: 636ms\n",
      "173:\tlearn: 0.0200730\ttotal: 1.44s\tremaining: 627ms\n",
      "174:\tlearn: 0.0198856\ttotal: 1.44s\tremaining: 618ms\n",
      "175:\tlearn: 0.0197451\ttotal: 1.45s\tremaining: 608ms\n",
      "176:\tlearn: 0.0194124\ttotal: 1.45s\tremaining: 599ms\n",
      "177:\tlearn: 0.0192559\ttotal: 1.46s\tremaining: 589ms\n",
      "178:\tlearn: 0.0190967\ttotal: 1.46s\tremaining: 580ms\n",
      "179:\tlearn: 0.0190866\ttotal: 1.47s\tremaining: 570ms\n",
      "180:\tlearn: 0.0189379\ttotal: 1.47s\tremaining: 561ms\n",
      "181:\tlearn: 0.0187816\ttotal: 1.48s\tremaining: 554ms\n",
      "182:\tlearn: 0.0185548\ttotal: 1.49s\tremaining: 545ms\n",
      "183:\tlearn: 0.0185548\ttotal: 1.49s\tremaining: 534ms\n",
      "184:\tlearn: 0.0185102\ttotal: 1.5s\tremaining: 526ms\n",
      "185:\tlearn: 0.0185102\ttotal: 1.51s\tremaining: 520ms\n",
      "186:\tlearn: 0.0185102\ttotal: 1.51s\tremaining: 509ms\n",
      "187:\tlearn: 0.0185102\ttotal: 1.51s\tremaining: 499ms\n",
      "188:\tlearn: 0.0184501\ttotal: 1.52s\tremaining: 490ms\n",
      "189:\tlearn: 0.0180571\ttotal: 1.53s\tremaining: 483ms\n",
      "190:\tlearn: 0.0179476\ttotal: 1.53s\tremaining: 474ms\n",
      "191:\tlearn: 0.0178288\ttotal: 1.54s\tremaining: 465ms\n",
      "192:\tlearn: 0.0177971\ttotal: 1.54s\tremaining: 456ms\n",
      "193:\tlearn: 0.0177615\ttotal: 1.55s\tremaining: 447ms\n",
      "194:\tlearn: 0.0175646\ttotal: 1.55s\tremaining: 438ms\n",
      "195:\tlearn: 0.0174444\ttotal: 1.56s\tremaining: 430ms\n",
      "196:\tlearn: 0.0173144\ttotal: 1.57s\tremaining: 424ms\n",
      "197:\tlearn: 0.0170995\ttotal: 1.58s\tremaining: 415ms\n",
      "198:\tlearn: 0.0170309\ttotal: 1.58s\tremaining: 406ms\n",
      "199:\tlearn: 0.0169226\ttotal: 1.59s\tremaining: 397ms\n",
      "200:\tlearn: 0.0167516\ttotal: 1.59s\tremaining: 389ms\n",
      "201:\tlearn: 0.0166819\ttotal: 1.6s\tremaining: 382ms\n",
      "202:\tlearn: 0.0165098\ttotal: 1.61s\tremaining: 373ms\n",
      "203:\tlearn: 0.0164541\ttotal: 1.61s\tremaining: 364ms\n",
      "204:\tlearn: 0.0164541\ttotal: 1.62s\tremaining: 356ms\n",
      "205:\tlearn: 0.0164279\ttotal: 1.64s\tremaining: 349ms\n",
      "206:\tlearn: 0.0164279\ttotal: 1.64s\tremaining: 341ms\n",
      "207:\tlearn: 0.0163736\ttotal: 1.65s\tremaining: 332ms\n",
      "208:\tlearn: 0.0162407\ttotal: 1.67s\tremaining: 327ms\n",
      "209:\tlearn: 0.0159752\ttotal: 1.67s\tremaining: 318ms\n",
      "210:\tlearn: 0.0158834\ttotal: 1.68s\tremaining: 310ms\n",
      "211:\tlearn: 0.0156690\ttotal: 1.68s\tremaining: 301ms\n",
      "212:\tlearn: 0.0156245\ttotal: 1.7s\tremaining: 295ms\n",
      "213:\tlearn: 0.0155533\ttotal: 1.7s\tremaining: 286ms\n",
      "214:\tlearn: 0.0153073\ttotal: 1.71s\tremaining: 278ms\n",
      "215:\tlearn: 0.0152862\ttotal: 1.71s\tremaining: 270ms\n",
      "216:\tlearn: 0.0150827\ttotal: 1.72s\tremaining: 261ms\n",
      "217:\tlearn: 0.0150445\ttotal: 1.72s\tremaining: 253ms\n",
      "218:\tlearn: 0.0150049\ttotal: 1.74s\tremaining: 247ms\n",
      "219:\tlearn: 0.0148839\ttotal: 1.75s\tremaining: 238ms\n",
      "220:\tlearn: 0.0148372\ttotal: 1.75s\tremaining: 230ms\n",
      "221:\tlearn: 0.0146223\ttotal: 1.76s\tremaining: 222ms\n",
      "222:\tlearn: 0.0145137\ttotal: 1.76s\tremaining: 213ms\n",
      "223:\tlearn: 0.0144351\ttotal: 1.77s\tremaining: 206ms\n",
      "224:\tlearn: 0.0144169\ttotal: 1.79s\tremaining: 199ms\n",
      "225:\tlearn: 0.0143295\ttotal: 1.79s\tremaining: 191ms\n",
      "226:\tlearn: 0.0142446\ttotal: 1.8s\tremaining: 182ms\n",
      "227:\tlearn: 0.0141234\ttotal: 1.8s\tremaining: 174ms\n",
      "228:\tlearn: 0.0139591\ttotal: 1.82s\tremaining: 167ms\n",
      "229:\tlearn: 0.0138983\ttotal: 1.82s\tremaining: 159ms\n",
      "230:\tlearn: 0.0138423\ttotal: 1.83s\tremaining: 151ms\n",
      "231:\tlearn: 0.0137350\ttotal: 1.84s\tremaining: 143ms\n",
      "232:\tlearn: 0.0136701\ttotal: 1.84s\tremaining: 134ms\n",
      "233:\tlearn: 0.0135971\ttotal: 1.85s\tremaining: 126ms\n",
      "234:\tlearn: 0.0134663\ttotal: 1.85s\tremaining: 118ms\n",
      "235:\tlearn: 0.0133945\ttotal: 1.86s\tremaining: 110ms\n",
      "236:\tlearn: 0.0133506\ttotal: 1.86s\tremaining: 102ms\n",
      "237:\tlearn: 0.0132242\ttotal: 1.87s\tremaining: 94.2ms\n",
      "238:\tlearn: 0.0131259\ttotal: 1.88s\tremaining: 86.7ms\n",
      "239:\tlearn: 0.0130531\ttotal: 1.89s\tremaining: 78.7ms\n",
      "240:\tlearn: 0.0129798\ttotal: 1.89s\tremaining: 70.7ms\n",
      "241:\tlearn: 0.0129210\ttotal: 1.9s\tremaining: 62.8ms\n",
      "242:\tlearn: 0.0128246\ttotal: 1.9s\tremaining: 54.8ms\n",
      "243:\tlearn: 0.0128245\ttotal: 1.91s\tremaining: 46.9ms\n",
      "244:\tlearn: 0.0127904\ttotal: 1.92s\tremaining: 39.1ms\n",
      "245:\tlearn: 0.0127763\ttotal: 1.93s\tremaining: 31.4ms\n",
      "246:\tlearn: 0.0127090\ttotal: 1.94s\tremaining: 23.5ms\n",
      "247:\tlearn: 0.0126618\ttotal: 1.94s\tremaining: 15.6ms\n",
      "248:\tlearn: 0.0125540\ttotal: 1.96s\tremaining: 7.88ms\n",
      "249:\tlearn: 0.0124410\ttotal: 1.97s\tremaining: 0us\n",
      "Dataset 7:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cb4c2d60e64133a5b73acda1a4befd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.535990360771634, Recall = 0.8612099644128114, Aging Rate = 0.6699288256227758, Precision = 0.6427622841965471, f1 = 0.7361216730038024\n",
      "Epoch 2: Train Loss = 0.3524850057749562, Recall = 0.8857651245551601, Aging Rate = 0.5380782918149466, Precision = 0.8230820105820106, f1 = 0.8532739115529654\n",
      "Epoch 3: Train Loss = 0.270522571405482, Recall = 0.9103202846975089, Aging Rate = 0.5128113879003559, Precision = 0.8875780707841776, f1 = 0.8988053408292339\n",
      "Epoch 4: Train Loss = 0.2161562804437617, Recall = 0.9306049822064056, Aging Rate = 0.507473309608541, Precision = 0.9169004207573632, f1 = 0.9237018721299892\n",
      "Epoch 5: Train Loss = 0.17545221656879073, Recall = 0.9498220640569395, Aging Rate = 0.5096085409252669, Precision = 0.931913407821229, f1 = 0.9407825167430384\n",
      "Test Loss = 0.131690814709324, Recall = 0.9793594306049822, Aging Rate = 0.5120996441281139, precision = 0.9562195969423211\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.12203020811399107, Recall = 0.9740213523131672, Aging Rate = 0.5030249110320285, Precision = 0.9681641315882561, f1 = 0.9710839098811425\n",
      "Epoch 7: Train Loss = 0.09384653051764939, Recall = 0.9836298932384342, Aging Rate = 0.5, Precision = 0.9836298932384342, f1 = 0.9836298932384342\n",
      "Epoch 8: Train Loss = 0.07557452069694885, Recall = 0.9871886120996441, Aging Rate = 0.4991103202846975, Precision = 0.9889483065953654, f1 = 0.9880676758682101\n",
      "Epoch 9: Train Loss = 0.06270362991985477, Recall = 0.9914590747330961, Aging Rate = 0.5005338078291814, Precision = 0.9904017063633132, f1 = 0.9909301084830162\n",
      "Epoch 10: Train Loss = 0.0529233161134652, Recall = 0.9971530249110321, Aging Rate = 0.5016014234875444, Precision = 0.9939694927279177, f1 = 0.9955587138035176\n",
      "Test Loss = 0.04626400458006672, Recall = 0.996797153024911, Aging Rate = 0.4991103202846975, precision = 0.9985739750445632\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.04478128944056314, Recall = 0.998576512455516, Aging Rate = 0.5023131672597865, Precision = 0.9939780375487071, f1 = 0.9962719687555477\n",
      "Epoch 12: Train Loss = 0.03888501407333328, Recall = 0.997864768683274, Aging Rate = 0.5001779359430605, Precision = 0.9975097829953753, f1 = 0.9976872442625867\n",
      "Epoch 13: Train Loss = 0.034327025369499074, Recall = 0.999644128113879, Aging Rate = 0.5005338078291814, Precision = 0.9985780305723427, f1 = 0.9991107949493153\n",
      "Epoch 14: Train Loss = 0.031125763769251597, Recall = 0.999644128113879, Aging Rate = 0.501067615658363, Precision = 0.9975142045454546, f1 = 0.9985780305723427\n",
      "Epoch 15: Train Loss = 0.028416137723417893, Recall = 0.999644128113879, Aging Rate = 0.5008896797153025, Precision = 0.9978685612788633, f1 = 0.9987555555555555\n",
      "Test Loss = 0.026774794988837954, Recall = 1.0, Aging Rate = 0.5008896797153025, precision = 0.9982238010657194\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.026406862295076942, Recall = 0.999288256227758, Aging Rate = 0.5, Precision = 0.999288256227758, f1 = 0.999288256227758\n",
      "Epoch 17: Train Loss = 0.024264162758968477, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 18: Train Loss = 0.023298104767591504, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 19: Train Loss = 0.02227436755502139, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Epoch 20: Train Loss = 0.02099493873037159, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Test Loss = 0.018837092733308938, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.020353949246298377, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 22: Train Loss = 0.019982894997897946, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 23: Train Loss = 0.01986004796767362, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 24: Train Loss = 0.01908277627478397, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Epoch 25: Train Loss = 0.01884712849833151, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0174337198507054, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 26: Train Loss = 0.018531755563468272, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 27: Train Loss = 0.018438827845655727, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 28: Train Loss = 0.018678283260545273, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 29: Train Loss = 0.01809248865591992, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.01859425567687088, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.016570157285289197, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 31: Train Loss = 0.01743066109239631, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 32: Train Loss = 0.01709570843143077, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.01753387009108512, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.017079512877025214, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 35: Train Loss = 0.01744857209592103, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.01677280128718059, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.017506340332992136, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.017095286306325227, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 38: Train Loss = 0.0166491233236637, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 39: Train Loss = 0.016899799533002743, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 40: Train Loss = 0.01671336708703075, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.015132837065783895, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.016907907836802065, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 42: Train Loss = 0.017053230153893872, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 43: Train Loss = 0.017240915932689272, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 44: Train Loss = 0.017154520255549945, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016895917724831247, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.0145325451240845, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016661943849242456, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 47: Train Loss = 0.016666945364339495, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.016211521772674393, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 49: Train Loss = 0.016874195635318757, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 50: Train Loss = 0.016533421376368755, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016227051813248215, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.01680546288189621, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss = 0.016740111122404977, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 53: Train Loss = 0.016199776733101898, Recall = 1.0, Aging Rate = 0.500355871886121, Precision = 0.9992887624466572, f1 = 0.999644254713625\n",
      "Epoch 54: Train Loss = 0.016208000217972064, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.016853824687385898, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.015525486574133732, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 56: Train Loss = 0.01612820656459739, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 57: Train Loss = 0.016814737753061002, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 58: Train Loss = 0.0159969420255132, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.016081165107384696, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 60: Train Loss = 0.017143058535904754, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.01412583750563266, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015862442334298562, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 62: Train Loss = 0.016201898870572076, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.016515106529501198, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 64: Train Loss = 0.01644395873202442, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 65: Train Loss = 0.016381085304762122, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013985061424111557, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.01623978279265857, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 67: Train Loss = 0.015926747810877728, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 68: Train Loss = 0.015862139990533375, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.01597602416850706, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.016258468807698146, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.01620512876222142, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.015895028609212394, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.01619204253926078, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.016318954370942405, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.01634878945732456, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.016196421349547088, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.01419977431990922, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.01590183220299962, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 77: Train Loss = 0.016223498942907172, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.0164941401177772, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 79: Train Loss = 0.01567147668862046, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.01583360923301707, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.015216845729358467, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.016453630119243976, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 82: Train Loss = 0.016232978076644214, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.015995144987233593, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 84: Train Loss = 0.015735715292261587, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.01584087104347677, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014294610492010133, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.015814279491791098, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.01622377614692861, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 88: Train Loss = 0.01590242640801384, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 89: Train Loss = 0.015416060091125583, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.01565803503120497, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014153924085252862, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 91: Train Loss = 0.01649254225194454, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.016488295274511563, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 93: Train Loss = 0.015265166867537643, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.016339420405252974, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.015830745282864656, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013791986240001124, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.0165614893772636, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 97: Train Loss = 0.015180827235783121, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 98: Train Loss = 0.015610120961528763, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.016281769422025444, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.01607767505325987, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.015184780846605097, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.015451699336228421, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.016051319566989495, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.015680083155207785, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.01580357823481127, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.01586125047870802, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.014017664615953097, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.015822447439344117, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.016125576365805392, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.01621585597028936, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 109: Train Loss = 0.015540007674847846, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110: Train Loss = 0.01595949806352527, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.014625721296885472, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.015585129266584894, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.015648018859751072, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.01574049139017524, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.015758938607478058, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.015942509485627407, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.016035452537450715, Recall = 1.0, Aging Rate = 0.5001779359430605, precision = 0.9996442547136251\n",
      "\n",
      "Epoch 116: Train Loss = 0.01573381079659131, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.015476824996787458, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.015924649546420023, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.016008180606566714, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 120: Train Loss = 0.0154172193829063, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.014721079095137502, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 120.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3188763\ttotal: 8.29ms\tremaining: 1.65s\n",
      "1:\tlearn: 0.2021354\ttotal: 17ms\tremaining: 1.68s\n",
      "2:\tlearn: 0.1575709\ttotal: 25.5ms\tremaining: 1.68s\n",
      "3:\tlearn: 0.1374007\ttotal: 34.4ms\tremaining: 1.68s\n",
      "4:\tlearn: 0.1211656\ttotal: 43.6ms\tremaining: 1.7s\n",
      "5:\tlearn: 0.1083718\ttotal: 53.3ms\tremaining: 1.72s\n",
      "6:\tlearn: 0.0914915\ttotal: 61.3ms\tremaining: 1.69s\n",
      "7:\tlearn: 0.0818295\ttotal: 70ms\tremaining: 1.68s\n",
      "8:\tlearn: 0.0735558\ttotal: 79.3ms\tremaining: 1.68s\n",
      "9:\tlearn: 0.0663877\ttotal: 87.8ms\tremaining: 1.67s\n",
      "10:\tlearn: 0.0609215\ttotal: 96.6ms\tremaining: 1.66s\n",
      "11:\tlearn: 0.0558362\ttotal: 105ms\tremaining: 1.64s\n",
      "12:\tlearn: 0.0521226\ttotal: 113ms\tremaining: 1.63s\n",
      "13:\tlearn: 0.0467107\ttotal: 121ms\tremaining: 1.61s\n",
      "14:\tlearn: 0.0426878\ttotal: 129ms\tremaining: 1.59s\n",
      "15:\tlearn: 0.0391303\ttotal: 138ms\tremaining: 1.58s\n",
      "16:\tlearn: 0.0355233\ttotal: 146ms\tremaining: 1.57s\n",
      "17:\tlearn: 0.0333281\ttotal: 154ms\tremaining: 1.55s\n",
      "18:\tlearn: 0.0304135\ttotal: 162ms\tremaining: 1.54s\n",
      "19:\tlearn: 0.0275092\ttotal: 169ms\tremaining: 1.52s\n",
      "20:\tlearn: 0.0253696\ttotal: 176ms\tremaining: 1.5s\n",
      "21:\tlearn: 0.0226913\ttotal: 184ms\tremaining: 1.49s\n",
      "22:\tlearn: 0.0213690\ttotal: 192ms\tremaining: 1.48s\n",
      "23:\tlearn: 0.0196868\ttotal: 199ms\tremaining: 1.46s\n",
      "24:\tlearn: 0.0180059\ttotal: 208ms\tremaining: 1.45s\n",
      "25:\tlearn: 0.0167615\ttotal: 215ms\tremaining: 1.44s\n",
      "26:\tlearn: 0.0159383\ttotal: 223ms\tremaining: 1.43s\n",
      "27:\tlearn: 0.0145833\ttotal: 231ms\tremaining: 1.42s\n",
      "28:\tlearn: 0.0136455\ttotal: 238ms\tremaining: 1.4s\n",
      "29:\tlearn: 0.0127823\ttotal: 246ms\tremaining: 1.39s\n",
      "30:\tlearn: 0.0116146\ttotal: 254ms\tremaining: 1.38s\n",
      "31:\tlearn: 0.0108268\ttotal: 261ms\tremaining: 1.37s\n",
      "32:\tlearn: 0.0099730\ttotal: 269ms\tremaining: 1.36s\n",
      "33:\tlearn: 0.0092553\ttotal: 277ms\tremaining: 1.35s\n",
      "34:\tlearn: 0.0085943\ttotal: 284ms\tremaining: 1.34s\n",
      "35:\tlearn: 0.0079135\ttotal: 291ms\tremaining: 1.33s\n",
      "36:\tlearn: 0.0074629\ttotal: 299ms\tremaining: 1.32s\n",
      "37:\tlearn: 0.0069152\ttotal: 307ms\tremaining: 1.31s\n",
      "38:\tlearn: 0.0064745\ttotal: 314ms\tremaining: 1.3s\n",
      "39:\tlearn: 0.0061033\ttotal: 322ms\tremaining: 1.29s\n",
      "40:\tlearn: 0.0056308\ttotal: 330ms\tremaining: 1.28s\n",
      "41:\tlearn: 0.0051831\ttotal: 338ms\tremaining: 1.27s\n",
      "42:\tlearn: 0.0048697\ttotal: 345ms\tremaining: 1.26s\n",
      "43:\tlearn: 0.0045181\ttotal: 352ms\tremaining: 1.25s\n",
      "44:\tlearn: 0.0041919\ttotal: 360ms\tremaining: 1.24s\n",
      "45:\tlearn: 0.0039672\ttotal: 367ms\tremaining: 1.23s\n",
      "46:\tlearn: 0.0037655\ttotal: 375ms\tremaining: 1.22s\n",
      "47:\tlearn: 0.0035710\ttotal: 383ms\tremaining: 1.21s\n",
      "48:\tlearn: 0.0033835\ttotal: 390ms\tremaining: 1.2s\n",
      "49:\tlearn: 0.0032561\ttotal: 398ms\tremaining: 1.19s\n",
      "50:\tlearn: 0.0030471\ttotal: 405ms\tremaining: 1.18s\n",
      "51:\tlearn: 0.0028868\ttotal: 413ms\tremaining: 1.18s\n",
      "52:\tlearn: 0.0027453\ttotal: 420ms\tremaining: 1.17s\n",
      "53:\tlearn: 0.0025923\ttotal: 429ms\tremaining: 1.16s\n",
      "54:\tlearn: 0.0024213\ttotal: 436ms\tremaining: 1.15s\n",
      "55:\tlearn: 0.0022782\ttotal: 444ms\tremaining: 1.14s\n",
      "56:\tlearn: 0.0021489\ttotal: 452ms\tremaining: 1.13s\n",
      "57:\tlearn: 0.0020552\ttotal: 460ms\tremaining: 1.13s\n",
      "58:\tlearn: 0.0019160\ttotal: 468ms\tremaining: 1.12s\n",
      "59:\tlearn: 0.0018393\ttotal: 475ms\tremaining: 1.11s\n",
      "60:\tlearn: 0.0017622\ttotal: 482ms\tremaining: 1.1s\n",
      "61:\tlearn: 0.0016788\ttotal: 490ms\tremaining: 1.09s\n",
      "62:\tlearn: 0.0015887\ttotal: 498ms\tremaining: 1.08s\n",
      "63:\tlearn: 0.0015349\ttotal: 505ms\tremaining: 1.07s\n",
      "64:\tlearn: 0.0014578\ttotal: 513ms\tremaining: 1.06s\n",
      "65:\tlearn: 0.0014054\ttotal: 520ms\tremaining: 1.05s\n",
      "66:\tlearn: 0.0013446\ttotal: 528ms\tremaining: 1.05s\n",
      "67:\tlearn: 0.0012965\ttotal: 535ms\tremaining: 1.04s\n",
      "68:\tlearn: 0.0012337\ttotal: 543ms\tremaining: 1.03s\n",
      "69:\tlearn: 0.0011815\ttotal: 551ms\tremaining: 1.02s\n",
      "70:\tlearn: 0.0011815\ttotal: 559ms\tremaining: 1.01s\n",
      "71:\tlearn: 0.0011352\ttotal: 566ms\tremaining: 1.01s\n",
      "72:\tlearn: 0.0010833\ttotal: 574ms\tremaining: 998ms\n",
      "73:\tlearn: 0.0010832\ttotal: 581ms\tremaining: 989ms\n",
      "74:\tlearn: 0.0010832\ttotal: 589ms\tremaining: 981ms\n",
      "75:\tlearn: 0.0010398\ttotal: 595ms\tremaining: 971ms\n",
      "76:\tlearn: 0.0010033\ttotal: 603ms\tremaining: 963ms\n",
      "77:\tlearn: 0.0010033\ttotal: 611ms\tremaining: 955ms\n",
      "78:\tlearn: 0.0010033\ttotal: 618ms\tremaining: 946ms\n",
      "79:\tlearn: 0.0010033\ttotal: 622ms\tremaining: 934ms\n",
      "80:\tlearn: 0.0010032\ttotal: 630ms\tremaining: 926ms\n",
      "81:\tlearn: 0.0009564\ttotal: 638ms\tremaining: 918ms\n",
      "82:\tlearn: 0.0009173\ttotal: 645ms\tremaining: 909ms\n",
      "83:\tlearn: 0.0008771\ttotal: 653ms\tremaining: 902ms\n",
      "84:\tlearn: 0.0008770\ttotal: 660ms\tremaining: 893ms\n",
      "85:\tlearn: 0.0008770\ttotal: 668ms\tremaining: 885ms\n",
      "86:\tlearn: 0.0008770\ttotal: 674ms\tremaining: 876ms\n",
      "87:\tlearn: 0.0008770\ttotal: 681ms\tremaining: 867ms\n",
      "88:\tlearn: 0.0008770\ttotal: 688ms\tremaining: 858ms\n",
      "89:\tlearn: 0.0008770\ttotal: 695ms\tremaining: 849ms\n",
      "90:\tlearn: 0.0008770\ttotal: 701ms\tremaining: 840ms\n",
      "91:\tlearn: 0.0008509\ttotal: 708ms\tremaining: 832ms\n",
      "92:\tlearn: 0.0008509\ttotal: 716ms\tremaining: 823ms\n",
      "93:\tlearn: 0.0008509\ttotal: 723ms\tremaining: 815ms\n",
      "94:\tlearn: 0.0008509\ttotal: 729ms\tremaining: 806ms\n",
      "95:\tlearn: 0.0008509\ttotal: 736ms\tremaining: 798ms\n",
      "96:\tlearn: 0.0008191\ttotal: 744ms\tremaining: 790ms\n",
      "97:\tlearn: 0.0008191\ttotal: 751ms\tremaining: 782ms\n",
      "98:\tlearn: 0.0008191\ttotal: 759ms\tremaining: 774ms\n",
      "99:\tlearn: 0.0008191\ttotal: 766ms\tremaining: 766ms\n",
      "100:\tlearn: 0.0008190\ttotal: 773ms\tremaining: 758ms\n",
      "101:\tlearn: 0.0008190\ttotal: 780ms\tremaining: 749ms\n",
      "102:\tlearn: 0.0008190\ttotal: 787ms\tremaining: 741ms\n",
      "103:\tlearn: 0.0007811\ttotal: 794ms\tremaining: 733ms\n",
      "104:\tlearn: 0.0007811\ttotal: 801ms\tremaining: 724ms\n",
      "105:\tlearn: 0.0007810\ttotal: 807ms\tremaining: 716ms\n",
      "106:\tlearn: 0.0007810\ttotal: 815ms\tremaining: 708ms\n",
      "107:\tlearn: 0.0007810\ttotal: 821ms\tremaining: 700ms\n",
      "108:\tlearn: 0.0007810\ttotal: 828ms\tremaining: 692ms\n",
      "109:\tlearn: 0.0007809\ttotal: 835ms\tremaining: 683ms\n",
      "110:\tlearn: 0.0007809\ttotal: 842ms\tremaining: 675ms\n",
      "111:\tlearn: 0.0007809\ttotal: 849ms\tremaining: 667ms\n",
      "112:\tlearn: 0.0007808\ttotal: 856ms\tremaining: 659ms\n",
      "113:\tlearn: 0.0007807\ttotal: 863ms\tremaining: 651ms\n",
      "114:\tlearn: 0.0007807\ttotal: 871ms\tremaining: 644ms\n",
      "115:\tlearn: 0.0007807\ttotal: 878ms\tremaining: 636ms\n",
      "116:\tlearn: 0.0007807\ttotal: 885ms\tremaining: 628ms\n",
      "117:\tlearn: 0.0007806\ttotal: 892ms\tremaining: 620ms\n",
      "118:\tlearn: 0.0007806\ttotal: 899ms\tremaining: 612ms\n",
      "119:\tlearn: 0.0007805\ttotal: 906ms\tremaining: 604ms\n",
      "120:\tlearn: 0.0007805\ttotal: 912ms\tremaining: 596ms\n",
      "121:\tlearn: 0.0007804\ttotal: 920ms\tremaining: 588ms\n",
      "122:\tlearn: 0.0007804\ttotal: 927ms\tremaining: 580ms\n",
      "123:\tlearn: 0.0007804\ttotal: 935ms\tremaining: 573ms\n",
      "124:\tlearn: 0.0007803\ttotal: 941ms\tremaining: 565ms\n",
      "125:\tlearn: 0.0007803\ttotal: 948ms\tremaining: 557ms\n",
      "126:\tlearn: 0.0007802\ttotal: 955ms\tremaining: 549ms\n",
      "127:\tlearn: 0.0007802\ttotal: 962ms\tremaining: 541ms\n",
      "128:\tlearn: 0.0007802\ttotal: 969ms\tremaining: 533ms\n",
      "129:\tlearn: 0.0007802\ttotal: 975ms\tremaining: 525ms\n",
      "130:\tlearn: 0.0007802\ttotal: 982ms\tremaining: 517ms\n",
      "131:\tlearn: 0.0007394\ttotal: 990ms\tremaining: 510ms\n",
      "132:\tlearn: 0.0007393\ttotal: 997ms\tremaining: 502ms\n",
      "133:\tlearn: 0.0007393\ttotal: 1s\tremaining: 495ms\n",
      "134:\tlearn: 0.0007393\ttotal: 1.01s\tremaining: 487ms\n",
      "135:\tlearn: 0.0007393\ttotal: 1.02s\tremaining: 478ms\n",
      "136:\tlearn: 0.0007393\ttotal: 1.02s\tremaining: 471ms\n",
      "137:\tlearn: 0.0007392\ttotal: 1.03s\tremaining: 463ms\n",
      "138:\tlearn: 0.0007392\ttotal: 1.04s\tremaining: 455ms\n",
      "139:\tlearn: 0.0007392\ttotal: 1.04s\tremaining: 448ms\n",
      "140:\tlearn: 0.0007392\ttotal: 1.05s\tremaining: 440ms\n",
      "141:\tlearn: 0.0007392\ttotal: 1.06s\tremaining: 432ms\n",
      "142:\tlearn: 0.0007392\ttotal: 1.06s\tremaining: 424ms\n",
      "143:\tlearn: 0.0007392\ttotal: 1.07s\tremaining: 416ms\n",
      "144:\tlearn: 0.0007391\ttotal: 1.08s\tremaining: 409ms\n",
      "145:\tlearn: 0.0007391\ttotal: 1.08s\tremaining: 401ms\n",
      "146:\tlearn: 0.0007391\ttotal: 1.09s\tremaining: 393ms\n",
      "147:\tlearn: 0.0007390\ttotal: 1.1s\tremaining: 386ms\n",
      "148:\tlearn: 0.0007390\ttotal: 1.1s\tremaining: 378ms\n",
      "149:\tlearn: 0.0007389\ttotal: 1.11s\tremaining: 371ms\n",
      "150:\tlearn: 0.0007389\ttotal: 1.12s\tremaining: 363ms\n",
      "151:\tlearn: 0.0007389\ttotal: 1.13s\tremaining: 356ms\n",
      "152:\tlearn: 0.0007389\ttotal: 1.13s\tremaining: 348ms\n",
      "153:\tlearn: 0.0007389\ttotal: 1.14s\tremaining: 340ms\n",
      "154:\tlearn: 0.0007388\ttotal: 1.15s\tremaining: 332ms\n",
      "155:\tlearn: 0.0007388\ttotal: 1.15s\tremaining: 325ms\n",
      "156:\tlearn: 0.0007387\ttotal: 1.16s\tremaining: 317ms\n",
      "157:\tlearn: 0.0007387\ttotal: 1.17s\tremaining: 310ms\n",
      "158:\tlearn: 0.0007387\ttotal: 1.17s\tremaining: 303ms\n",
      "159:\tlearn: 0.0007387\ttotal: 1.18s\tremaining: 295ms\n",
      "160:\tlearn: 0.0007387\ttotal: 1.19s\tremaining: 288ms\n",
      "161:\tlearn: 0.0007123\ttotal: 1.19s\tremaining: 280ms\n",
      "162:\tlearn: 0.0007123\ttotal: 1.2s\tremaining: 273ms\n",
      "163:\tlearn: 0.0007123\ttotal: 1.21s\tremaining: 265ms\n",
      "164:\tlearn: 0.0007123\ttotal: 1.21s\tremaining: 258ms\n",
      "165:\tlearn: 0.0007123\ttotal: 1.22s\tremaining: 250ms\n",
      "166:\tlearn: 0.0007123\ttotal: 1.23s\tremaining: 243ms\n",
      "167:\tlearn: 0.0007123\ttotal: 1.23s\tremaining: 235ms\n",
      "168:\tlearn: 0.0007123\ttotal: 1.24s\tremaining: 228ms\n",
      "169:\tlearn: 0.0007123\ttotal: 1.25s\tremaining: 220ms\n",
      "170:\tlearn: 0.0007122\ttotal: 1.25s\tremaining: 213ms\n",
      "171:\tlearn: 0.0007122\ttotal: 1.26s\tremaining: 206ms\n",
      "172:\tlearn: 0.0007122\ttotal: 1.27s\tremaining: 198ms\n",
      "173:\tlearn: 0.0007122\ttotal: 1.28s\tremaining: 191ms\n",
      "174:\tlearn: 0.0007121\ttotal: 1.28s\tremaining: 183ms\n",
      "175:\tlearn: 0.0007121\ttotal: 1.29s\tremaining: 176ms\n",
      "176:\tlearn: 0.0007121\ttotal: 1.3s\tremaining: 169ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177:\tlearn: 0.0007121\ttotal: 1.3s\tremaining: 161ms\n",
      "178:\tlearn: 0.0007121\ttotal: 1.31s\tremaining: 154ms\n",
      "179:\tlearn: 0.0007120\ttotal: 1.32s\tremaining: 146ms\n",
      "180:\tlearn: 0.0007120\ttotal: 1.32s\tremaining: 139ms\n",
      "181:\tlearn: 0.0007120\ttotal: 1.33s\tremaining: 132ms\n",
      "182:\tlearn: 0.0007120\ttotal: 1.34s\tremaining: 124ms\n",
      "183:\tlearn: 0.0007120\ttotal: 1.34s\tremaining: 117ms\n",
      "184:\tlearn: 0.0007120\ttotal: 1.35s\tremaining: 110ms\n",
      "185:\tlearn: 0.0007120\ttotal: 1.36s\tremaining: 102ms\n",
      "186:\tlearn: 0.0007120\ttotal: 1.36s\tremaining: 95ms\n",
      "187:\tlearn: 0.0007120\ttotal: 1.37s\tremaining: 87.6ms\n",
      "188:\tlearn: 0.0007119\ttotal: 1.38s\tremaining: 80.3ms\n",
      "189:\tlearn: 0.0007119\ttotal: 1.39s\tremaining: 72.9ms\n",
      "190:\tlearn: 0.0007119\ttotal: 1.39s\tremaining: 65.6ms\n",
      "191:\tlearn: 0.0007119\ttotal: 1.4s\tremaining: 58.3ms\n",
      "192:\tlearn: 0.0007119\ttotal: 1.41s\tremaining: 51ms\n",
      "193:\tlearn: 0.0007118\ttotal: 1.41s\tremaining: 43.7ms\n",
      "194:\tlearn: 0.0007118\ttotal: 1.42s\tremaining: 36.4ms\n",
      "195:\tlearn: 0.0007118\ttotal: 1.43s\tremaining: 29.1ms\n",
      "196:\tlearn: 0.0007118\ttotal: 1.43s\tremaining: 21.8ms\n",
      "197:\tlearn: 0.0007118\ttotal: 1.44s\tremaining: 14.6ms\n",
      "198:\tlearn: 0.0007118\ttotal: 1.45s\tremaining: 7.27ms\n",
      "199:\tlearn: 0.0007118\ttotal: 1.45s\tremaining: 0us\n",
      "Dataset 8:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d383b775ad64ac28025c68ae6483165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5519530393899124, Recall = 0.7427046263345196, Aging Rate = 0.4729537366548043, Precision = 0.7851768246802107, f1 = 0.7633504023408925\n",
      "Epoch 2: Train Loss = 0.33332649057870234, Recall = 0.8811387900355871, Aging Rate = 0.5005338078291814, Precision = 0.880199075719872, f1 = 0.8806686821981149\n",
      "Epoch 3: Train Loss = 0.24944565673739885, Recall = 0.9249110320284698, Aging Rate = 0.5078291814946619, Precision = 0.9106517168885775, f1 = 0.917725988700565\n",
      "Epoch 4: Train Loss = 0.20912668829074535, Recall = 0.9327402135231316, Aging Rate = 0.4987544483985765, Precision = 0.9350695683196575, f1 = 0.9339034384464636\n",
      "Epoch 5: Train Loss = 0.17963098086072032, Recall = 0.9384341637010676, Aging Rate = 0.49661921708185053, Precision = 0.9448226442135436, f1 = 0.9416175682913767\n",
      "Test Loss = 0.15554952731230082, Recall = 0.9412811387900356, Aging Rate = 0.48523131672597863, precision = 0.9699303263659699\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.14558160315098712, Recall = 0.9430604982206405, Aging Rate = 0.48772241992882565, Precision = 0.9668004377964247, f1 = 0.9547829219960368\n",
      "Epoch 7: Train Loss = 0.12477823243446622, Recall = 0.9494661921708185, Aging Rate = 0.48825622775800714, Precision = 0.9723032069970845, f1 = 0.9607490097227224\n",
      "Epoch 8: Train Loss = 0.10668528326041334, Recall = 0.9590747330960854, Aging Rate = 0.4887900355871886, Precision = 0.9810702584637787, f1 = 0.9699478135684723\n",
      "Epoch 9: Train Loss = 0.09203408718427306, Recall = 0.9665480427046264, Aging Rate = 0.4900355871886121, Precision = 0.9862018881626725, f1 = 0.97627606038821\n",
      "Epoch 10: Train Loss = 0.08055331208527725, Recall = 0.9733096085409253, Aging Rate = 0.4925266903914591, Precision = 0.9880780346820809, f1 = 0.9806382215847974\n",
      "Test Loss = 0.07171420990572282, Recall = 0.9758007117437723, Aging Rate = 0.4907473309608541, precision = 0.994198694706309\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.07036109857287695, Recall = 0.9761565836298932, Aging Rate = 0.4925266903914591, Precision = 0.9909682080924855, f1 = 0.9835066332018644\n",
      "Epoch 12: Train Loss = 0.06239086818811732, Recall = 0.9779359430604982, Aging Rate = 0.49217081850533806, Precision = 0.9934924078091106, f1 = 0.9856527977044477\n",
      "Epoch 13: Train Loss = 0.05632161416405036, Recall = 0.9825622775800712, Aging Rate = 0.49483985765124555, Precision = 0.9928083423229054, f1 = 0.9876587372563047\n",
      "Epoch 14: Train Loss = 0.05036492486388234, Recall = 0.9846975088967972, Aging Rate = 0.49501779359430603, Precision = 0.994608195542775, f1 = 0.9896280400572247\n",
      "Epoch 15: Train Loss = 0.04563072545062817, Recall = 0.9861209964412812, Aging Rate = 0.49519572953736657, Precision = 0.9956881063600431, f1 = 0.9908814589665654\n",
      "Test Loss = 0.04095835224642448, Recall = 0.9889679715302491, Aging Rate = 0.49697508896797155, precision = 0.9949874686716792\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.04224040851368174, Recall = 0.9889679715302491, Aging Rate = 0.49661921708185053, Precision = 0.9957004657828735, f1 = 0.9923227995000893\n",
      "Epoch 17: Train Loss = 0.0392496332022431, Recall = 0.9907473309608541, Aging Rate = 0.4973309608540925, Precision = 0.9960644007155635, f1 = 0.9933987511150758\n",
      "Epoch 18: Train Loss = 0.03546927556050842, Recall = 0.9914590747330961, Aging Rate = 0.49750889679715304, Precision = 0.9964234620886981, f1 = 0.9939350695683197\n",
      "Epoch 19: Train Loss = 0.03310164536922852, Recall = 0.9911032028469751, Aging Rate = 0.49661921708185053, Precision = 0.9978502328914367, f1 = 0.9944652740582038\n",
      "Epoch 20: Train Loss = 0.031166756369200042, Recall = 0.9928825622775801, Aging Rate = 0.49750889679715304, Precision = 0.9978540772532188, f1 = 0.9953621120228325\n",
      "Test Loss = 0.028364647815527652, Recall = 0.994661921708185, Aging Rate = 0.4976868327402135, precision = 0.9992849481587415\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.030239247862130298, Recall = 0.9928825622775801, Aging Rate = 0.497153024911032, Precision = 0.9985683607730852, f1 = 0.9957173447537474\n",
      "Epoch 22: Train Loss = 0.027785808172779574, Recall = 0.9950177935943061, Aging Rate = 0.498220640569395, Precision = 0.9985714285714286, f1 = 0.9967914438502673\n",
      "Epoch 23: Train Loss = 0.026760734763539983, Recall = 0.994306049822064, Aging Rate = 0.49804270462633454, Precision = 0.9982136477313326, f1 = 0.9962560171153504\n",
      "Epoch 24: Train Loss = 0.024857371781034615, Recall = 0.996797153024911, Aging Rate = 0.4991103202846975, Precision = 0.9985739750445632, f1 = 0.9976847729296527\n",
      "Epoch 25: Train Loss = 0.024028333334842188, Recall = 0.994661921708185, Aging Rate = 0.4976868327402135, Precision = 0.9992849481587415, f1 = 0.996968075619761\n",
      "Test Loss = 0.022120812679790092, Recall = 0.999288256227758, Aging Rate = 0.5, precision = 0.999288256227758\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.023176877197952032, Recall = 0.996797153024911, Aging Rate = 0.49857651245551604, Precision = 0.9996431120628123, f1 = 0.9982181040627227\n",
      "Epoch 27: Train Loss = 0.022563698162652843, Recall = 0.9971530249110321, Aging Rate = 0.498932384341637, Precision = 0.9992867332382311, f1 = 0.998218738867118\n",
      "Epoch 28: Train Loss = 0.021490673924998458, Recall = 0.996797153024911, Aging Rate = 0.49857651245551604, Precision = 0.9996431120628123, f1 = 0.9982181040627227\n",
      "Epoch 29: Train Loss = 0.020393198132647526, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 30: Train Loss = 0.020050467513634217, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.018641100065563923, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.019445320046457108, Recall = 0.997508896797153, Aging Rate = 0.4991103202846975, Precision = 0.9992869875222816, f1 = 0.9983971504897596\n",
      "Epoch 32: Train Loss = 0.01899443855078301, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 33: Train Loss = 0.018436424626415312, Recall = 0.997508896797153, Aging Rate = 0.4987544483985765, Precision = 0, f1 = 0.0\n",
      "Epoch 34: Train Loss = 0.018889027912156436, Recall = 0.997864768683274, Aging Rate = 0.499288256227758, Precision = 0.9992872416250891, f1 = 0.9985754985754985\n",
      "Epoch 35: Train Loss = 0.018097392899179797, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n",
      "Test Loss = 0.01624538375866795, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 36: Train Loss = 0.017989581656307513, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 37: Train Loss = 0.017688520623907925, Recall = 0.998220640569395, Aging Rate = 0.499288256227758, Precision = 0.9996436208125445, f1 = 0.9989316239316239\n",
      "Epoch 38: Train Loss = 0.01722864544556022, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n",
      "Epoch 39: Train Loss = 0.017063561012948536, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Epoch 40: Train Loss = 0.016591842098698496, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Test Loss = 0.015330579607777324, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 41: Train Loss = 0.017017075720365784, Recall = 0.998576512455516, Aging Rate = 0.499644128113879, Precision = 0.9992877492877493, f1 = 0.9989320042719829\n",
      "Epoch 42: Train Loss = 0.016499011006431648, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 43: Train Loss = 0.016439706071203715, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 44: Train Loss = 0.016090269596233064, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 45: Train Loss = 0.016399987786345424, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Test Loss = 0.014814165874891434, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, precision = 1.0\n",
      "\n",
      "Epoch 46: Train Loss = 0.016455130226035133, Recall = 0.998576512455516, Aging Rate = 0.4994661921708185, Precision = 0.9996437477734236, f1 = 0.9991098451130497\n",
      "Epoch 47: Train Loss = 0.015945953589423273, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 48: Train Loss = 0.01597094000764589, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.016072637876794006, Recall = 0.998220640569395, Aging Rate = 0.4991103202846975, Precision = 0, f1 = 0.0\n",
      "Epoch 50: Train Loss = 0.01574432749393996, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01526683125631665, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 51: Train Loss = 0.015619300715492713, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 52: Train Loss = 0.016043871133405968, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 53: Train Loss = 0.015501452789532544, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 54: Train Loss = 0.01572505722611188, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 55: Train Loss = 0.015263772100952597, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014067961666003877, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, precision = 1.0\n",
      "\n",
      "Epoch 56: Train Loss = 0.015809528052382622, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 57: Train Loss = 0.015460001250390905, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Epoch 58: Train Loss = 0.015449793281504268, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 59: Train Loss = 0.015476275822048297, Recall = 0.998220640569395, Aging Rate = 0.499288256227758, Precision = 0.9996436208125445, f1 = 0.9989316239316239\n",
      "Epoch 60: Train Loss = 0.015263798083858134, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.013746935228595106, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 61: Train Loss = 0.015130566394626033, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 62: Train Loss = 0.015594559687842678, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 63: Train Loss = 0.015125647413932132, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 64: Train Loss = 0.015243569018445194, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 65: Train Loss = 0.014900774238266554, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.01403391803896512, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 66: Train Loss = 0.01577657801347695, Recall = 0.998220640569395, Aging Rate = 0.499288256227758, Precision = 0.9996436208125445, f1 = 0.9989316239316239\n",
      "Epoch 67: Train Loss = 0.015488978442189109, Recall = 0.998932384341637, Aging Rate = 0.499644128113879, Precision = 0.9996438746438746, f1 = 0.9992880028479887\n",
      "Epoch 68: Train Loss = 0.014917370072463229, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 69: Train Loss = 0.015083576816389357, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.01470082084585339, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.013331494017693072, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 71: Train Loss = 0.015033924889177402, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.014714209958115506, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.014820442587349339, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.01509735282656349, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 75: Train Loss = 0.014796902532308127, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.013475591005273561, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.014995187281024414, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 77: Train Loss = 0.014644877173775456, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 78: Train Loss = 0.014467243330215113, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 79: Train Loss = 0.014878130743936287, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 80: Train Loss = 0.0150118883460548, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013222485711778186, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.015161294679677783, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.014749579598258823, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 83: Train Loss = 0.014683298107727142, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 84: Train Loss = 0.014699095039499187, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.01531137867674921, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.013901112104345153, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.01505786755728764, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.01458434662069078, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 88: Train Loss = 0.014676869729553678, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 89: Train Loss = 0.014148922501880927, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.014580268259044219, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.014215018556807919, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.014592798889425726, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.014957877033757994, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.014493049115764501, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 94: Train Loss = 0.014641553601018051, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.014631129619648872, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.013044624116762252, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.014450242373018502, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 97: Train Loss = 0.014856100437692052, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.014842173473054403, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.015219362952424962, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 100: Train Loss = 0.014171210934948242, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013113176652306551, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.01458041886815609, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.014486045485819787, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.014441227759118606, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: Train Loss = 0.014387403260850185, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.01484272242242013, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Test Loss = 0.013179548767484804, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.014524184573057283, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 107: Train Loss = 0.014577188889008824, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.014115903618019671, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.01425189239857884, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.014991079206541975, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.013088196541225783, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.014214960168848259, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.014480667127780218, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.014546646042544647, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.014250231658860881, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 115: Train Loss = 0.014543196165874548, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.01303247572105126, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.01440578337984153, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Epoch 117: Train Loss = 0.01418223578111663, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.014166957551606822, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.014452465589148294, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 120: Train Loss = 0.014068186836602213, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.014179642145850055, Recall = 0.998576512455516, Aging Rate = 0.499288256227758, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.015921424412801595, Recall = 0.997864768683274, Aging Rate = 0.4991103202846975, Precision = 0.9996434937611408, f1 = 0.9987533392698129\n",
      "Epoch 122: Train Loss = 0.014898873558035949, Recall = 0.999288256227758, Aging Rate = 0.499644128113879, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.014150701931866674, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 124: Train Loss = 0.01379170526826615, Recall = 0.999644128113879, Aging Rate = 0.4998220640569395, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.014157491962150322, Recall = 0.999644128113879, Aging Rate = 0.5, Precision = 0.999644128113879, f1 = 0.999644128113879\n",
      "Test Loss = 0.01309322204997637, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.014609949592686527, Recall = 0.999288256227758, Aging Rate = 0.4998220640569395, Precision = 0.9996440014239943, f1 = 0.999466097170315\n",
      "Epoch 127: Train Loss = 0.014059317015190989, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Epoch 128: Train Loss = 0.014150159971728868, Recall = 1.0, Aging Rate = 0.5, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.014394613305020587, Recall = 0.998932384341637, Aging Rate = 0.4994661921708185, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.013979819550898151, Recall = 1.0, Aging Rate = 0.5001779359430605, Precision = 0.9996442547136251, f1 = 0.9998220957125067\n",
      "Test Loss = 0.012659972426862691, Recall = 1.0, Aging Rate = 0.5, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 130.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5309592\ttotal: 6.34ms\tremaining: 1.26s\n",
      "1:\tlearn: 0.4163358\ttotal: 13.2ms\tremaining: 1.31s\n",
      "2:\tlearn: 0.3457194\ttotal: 19.3ms\tremaining: 1.27s\n",
      "3:\tlearn: 0.2809792\ttotal: 26.5ms\tremaining: 1.3s\n",
      "4:\tlearn: 0.2343100\ttotal: 32.7ms\tremaining: 1.28s\n",
      "5:\tlearn: 0.1977526\ttotal: 39.1ms\tremaining: 1.26s\n",
      "6:\tlearn: 0.1684060\ttotal: 45.4ms\tremaining: 1.25s\n",
      "7:\tlearn: 0.1451046\ttotal: 51.7ms\tremaining: 1.24s\n",
      "8:\tlearn: 0.1234846\ttotal: 58.2ms\tremaining: 1.24s\n",
      "9:\tlearn: 0.1090801\ttotal: 64.7ms\tremaining: 1.23s\n",
      "10:\tlearn: 0.0960310\ttotal: 71.1ms\tremaining: 1.22s\n",
      "11:\tlearn: 0.0842607\ttotal: 76.3ms\tremaining: 1.2s\n",
      "12:\tlearn: 0.0739585\ttotal: 81.7ms\tremaining: 1.17s\n",
      "13:\tlearn: 0.0667074\ttotal: 86.8ms\tremaining: 1.15s\n",
      "14:\tlearn: 0.0599628\ttotal: 91.8ms\tremaining: 1.13s\n",
      "15:\tlearn: 0.0544621\ttotal: 96.9ms\tremaining: 1.11s\n",
      "16:\tlearn: 0.0478604\ttotal: 102ms\tremaining: 1.1s\n",
      "17:\tlearn: 0.0423019\ttotal: 107ms\tremaining: 1.08s\n",
      "18:\tlearn: 0.0399054\ttotal: 112ms\tremaining: 1.06s\n",
      "19:\tlearn: 0.0363920\ttotal: 117ms\tremaining: 1.05s\n",
      "20:\tlearn: 0.0329076\ttotal: 122ms\tremaining: 1.04s\n",
      "21:\tlearn: 0.0298541\ttotal: 127ms\tremaining: 1.03s\n",
      "22:\tlearn: 0.0280556\ttotal: 132ms\tremaining: 1.01s\n",
      "23:\tlearn: 0.0255687\ttotal: 137ms\tremaining: 1s\n",
      "24:\tlearn: 0.0235675\ttotal: 142ms\tremaining: 996ms\n",
      "25:\tlearn: 0.0215266\ttotal: 147ms\tremaining: 986ms\n",
      "26:\tlearn: 0.0198366\ttotal: 152ms\tremaining: 977ms\n",
      "27:\tlearn: 0.0187148\ttotal: 157ms\tremaining: 964ms\n",
      "28:\tlearn: 0.0174770\ttotal: 162ms\tremaining: 956ms\n",
      "29:\tlearn: 0.0161888\ttotal: 167ms\tremaining: 948ms\n",
      "30:\tlearn: 0.0151663\ttotal: 172ms\tremaining: 939ms\n",
      "31:\tlearn: 0.0134526\ttotal: 178ms\tremaining: 932ms\n",
      "32:\tlearn: 0.0124927\ttotal: 183ms\tremaining: 924ms\n",
      "33:\tlearn: 0.0116916\ttotal: 188ms\tremaining: 917ms\n",
      "34:\tlearn: 0.0106302\ttotal: 193ms\tremaining: 911ms\n",
      "35:\tlearn: 0.0100335\ttotal: 198ms\tremaining: 900ms\n",
      "36:\tlearn: 0.0094630\ttotal: 203ms\tremaining: 892ms\n",
      "37:\tlearn: 0.0089776\ttotal: 208ms\tremaining: 885ms\n",
      "38:\tlearn: 0.0082962\ttotal: 213ms\tremaining: 880ms\n",
      "39:\tlearn: 0.0077785\ttotal: 218ms\tremaining: 872ms\n",
      "40:\tlearn: 0.0072050\ttotal: 224ms\tremaining: 867ms\n",
      "41:\tlearn: 0.0067341\ttotal: 229ms\tremaining: 860ms\n",
      "42:\tlearn: 0.0062054\ttotal: 234ms\tremaining: 853ms\n",
      "43:\tlearn: 0.0058163\ttotal: 239ms\tremaining: 846ms\n",
      "44:\tlearn: 0.0053802\ttotal: 244ms\tremaining: 840ms\n",
      "45:\tlearn: 0.0049721\ttotal: 249ms\tremaining: 835ms\n",
      "46:\tlearn: 0.0046735\ttotal: 254ms\tremaining: 828ms\n",
      "47:\tlearn: 0.0043971\ttotal: 259ms\tremaining: 821ms\n",
      "48:\tlearn: 0.0041073\ttotal: 265ms\tremaining: 815ms\n",
      "49:\tlearn: 0.0038682\ttotal: 269ms\tremaining: 808ms\n",
      "50:\tlearn: 0.0034013\ttotal: 275ms\tremaining: 802ms\n",
      "51:\tlearn: 0.0032169\ttotal: 279ms\tremaining: 795ms\n",
      "52:\tlearn: 0.0030214\ttotal: 284ms\tremaining: 789ms\n",
      "53:\tlearn: 0.0028760\ttotal: 289ms\tremaining: 782ms\n",
      "54:\tlearn: 0.0026754\ttotal: 294ms\tremaining: 776ms\n",
      "55:\tlearn: 0.0024743\ttotal: 300ms\tremaining: 771ms\n",
      "56:\tlearn: 0.0023765\ttotal: 303ms\tremaining: 760ms\n",
      "57:\tlearn: 0.0022610\ttotal: 308ms\tremaining: 753ms\n",
      "58:\tlearn: 0.0021536\ttotal: 312ms\tremaining: 746ms\n",
      "59:\tlearn: 0.0020177\ttotal: 318ms\tremaining: 741ms\n",
      "60:\tlearn: 0.0019173\ttotal: 322ms\tremaining: 735ms\n",
      "61:\tlearn: 0.0017972\ttotal: 328ms\tremaining: 729ms\n",
      "62:\tlearn: 0.0017345\ttotal: 331ms\tremaining: 721ms\n",
      "63:\tlearn: 0.0016405\ttotal: 336ms\tremaining: 714ms\n",
      "64:\tlearn: 0.0015431\ttotal: 341ms\tremaining: 708ms\n",
      "65:\tlearn: 0.0014940\ttotal: 345ms\tremaining: 701ms\n",
      "66:\tlearn: 0.0014031\ttotal: 351ms\tremaining: 696ms\n",
      "67:\tlearn: 0.0013588\ttotal: 355ms\tremaining: 688ms\n",
      "68:\tlearn: 0.0013366\ttotal: 358ms\tremaining: 680ms\n",
      "69:\tlearn: 0.0012565\ttotal: 363ms\tremaining: 675ms\n",
      "70:\tlearn: 0.0011966\ttotal: 368ms\tremaining: 669ms\n",
      "71:\tlearn: 0.0011427\ttotal: 373ms\tremaining: 663ms\n",
      "72:\tlearn: 0.0010900\ttotal: 378ms\tremaining: 658ms\n",
      "73:\tlearn: 0.0010132\ttotal: 383ms\tremaining: 652ms\n",
      "74:\tlearn: 0.0009584\ttotal: 388ms\tremaining: 647ms\n",
      "75:\tlearn: 0.0009213\ttotal: 393ms\tremaining: 641ms\n",
      "76:\tlearn: 0.0008826\ttotal: 398ms\tremaining: 635ms\n",
      "77:\tlearn: 0.0008536\ttotal: 402ms\tremaining: 628ms\n",
      "78:\tlearn: 0.0008287\ttotal: 406ms\tremaining: 622ms\n",
      "79:\tlearn: 0.0007933\ttotal: 411ms\tremaining: 616ms\n",
      "80:\tlearn: 0.0007696\ttotal: 415ms\tremaining: 610ms\n",
      "81:\tlearn: 0.0007425\ttotal: 420ms\tremaining: 605ms\n",
      "82:\tlearn: 0.0007170\ttotal: 425ms\tremaining: 599ms\n",
      "83:\tlearn: 0.0007170\ttotal: 428ms\tremaining: 591ms\n",
      "84:\tlearn: 0.0006948\ttotal: 432ms\tremaining: 584ms\n",
      "85:\tlearn: 0.0006641\ttotal: 437ms\tremaining: 579ms\n",
      "86:\tlearn: 0.0006479\ttotal: 441ms\tremaining: 572ms\n",
      "87:\tlearn: 0.0006269\ttotal: 445ms\tremaining: 566ms\n",
      "88:\tlearn: 0.0006213\ttotal: 448ms\tremaining: 559ms\n",
      "89:\tlearn: 0.0006213\ttotal: 452ms\tremaining: 552ms\n",
      "90:\tlearn: 0.0006213\ttotal: 455ms\tremaining: 545ms\n",
      "91:\tlearn: 0.0006060\ttotal: 459ms\tremaining: 539ms\n",
      "92:\tlearn: 0.0006060\ttotal: 463ms\tremaining: 533ms\n",
      "93:\tlearn: 0.0006060\ttotal: 466ms\tremaining: 526ms\n",
      "94:\tlearn: 0.0006060\ttotal: 470ms\tremaining: 519ms\n",
      "95:\tlearn: 0.0006060\ttotal: 473ms\tremaining: 512ms\n",
      "96:\tlearn: 0.0006060\ttotal: 476ms\tremaining: 506ms\n",
      "97:\tlearn: 0.0006060\ttotal: 480ms\tremaining: 499ms\n",
      "98:\tlearn: 0.0006060\ttotal: 483ms\tremaining: 493ms\n",
      "99:\tlearn: 0.0006060\ttotal: 486ms\tremaining: 486ms\n",
      "100:\tlearn: 0.0006060\ttotal: 490ms\tremaining: 480ms\n",
      "101:\tlearn: 0.0006060\ttotal: 494ms\tremaining: 474ms\n",
      "102:\tlearn: 0.0006060\ttotal: 497ms\tremaining: 468ms\n",
      "103:\tlearn: 0.0006060\ttotal: 500ms\tremaining: 462ms\n",
      "104:\tlearn: 0.0006060\ttotal: 503ms\tremaining: 456ms\n",
      "105:\tlearn: 0.0006060\ttotal: 507ms\tremaining: 449ms\n",
      "106:\tlearn: 0.0006060\ttotal: 510ms\tremaining: 443ms\n",
      "107:\tlearn: 0.0006059\ttotal: 513ms\tremaining: 437ms\n",
      "108:\tlearn: 0.0006059\ttotal: 517ms\tremaining: 432ms\n",
      "109:\tlearn: 0.0006059\ttotal: 521ms\tremaining: 426ms\n",
      "110:\tlearn: 0.0006059\ttotal: 524ms\tremaining: 420ms\n",
      "111:\tlearn: 0.0006059\ttotal: 528ms\tremaining: 414ms\n",
      "112:\tlearn: 0.0006058\ttotal: 531ms\tremaining: 409ms\n",
      "113:\tlearn: 0.0006058\ttotal: 534ms\tremaining: 403ms\n",
      "114:\tlearn: 0.0006058\ttotal: 538ms\tremaining: 397ms\n",
      "115:\tlearn: 0.0006058\ttotal: 541ms\tremaining: 392ms\n",
      "116:\tlearn: 0.0006058\ttotal: 544ms\tremaining: 386ms\n",
      "117:\tlearn: 0.0006058\ttotal: 548ms\tremaining: 381ms\n",
      "118:\tlearn: 0.0006058\ttotal: 553ms\tremaining: 376ms\n",
      "119:\tlearn: 0.0006058\ttotal: 556ms\tremaining: 371ms\n",
      "120:\tlearn: 0.0006058\ttotal: 559ms\tremaining: 365ms\n",
      "121:\tlearn: 0.0006058\ttotal: 563ms\tremaining: 360ms\n",
      "122:\tlearn: 0.0006058\ttotal: 566ms\tremaining: 355ms\n",
      "123:\tlearn: 0.0006058\ttotal: 570ms\tremaining: 349ms\n",
      "124:\tlearn: 0.0006058\ttotal: 573ms\tremaining: 344ms\n",
      "125:\tlearn: 0.0006058\ttotal: 577ms\tremaining: 339ms\n",
      "126:\tlearn: 0.0005891\ttotal: 580ms\tremaining: 334ms\n",
      "127:\tlearn: 0.0005891\ttotal: 585ms\tremaining: 329ms\n",
      "128:\tlearn: 0.0005891\ttotal: 588ms\tremaining: 324ms\n",
      "129:\tlearn: 0.0005891\ttotal: 591ms\tremaining: 318ms\n",
      "130:\tlearn: 0.0005759\ttotal: 595ms\tremaining: 313ms\n",
      "131:\tlearn: 0.0005759\ttotal: 599ms\tremaining: 309ms\n",
      "132:\tlearn: 0.0005759\ttotal: 602ms\tremaining: 303ms\n",
      "133:\tlearn: 0.0005759\ttotal: 605ms\tremaining: 298ms\n",
      "134:\tlearn: 0.0005759\ttotal: 609ms\tremaining: 293ms\n",
      "135:\tlearn: 0.0005759\ttotal: 612ms\tremaining: 288ms\n",
      "136:\tlearn: 0.0005759\ttotal: 615ms\tremaining: 283ms\n",
      "137:\tlearn: 0.0005759\ttotal: 618ms\tremaining: 278ms\n",
      "138:\tlearn: 0.0005759\ttotal: 621ms\tremaining: 272ms\n",
      "139:\tlearn: 0.0005759\ttotal: 624ms\tremaining: 267ms\n",
      "140:\tlearn: 0.0005759\ttotal: 627ms\tremaining: 262ms\n",
      "141:\tlearn: 0.0005759\ttotal: 631ms\tremaining: 258ms\n",
      "142:\tlearn: 0.0005759\ttotal: 634ms\tremaining: 253ms\n",
      "143:\tlearn: 0.0005759\ttotal: 637ms\tremaining: 248ms\n",
      "144:\tlearn: 0.0005759\ttotal: 640ms\tremaining: 243ms\n",
      "145:\tlearn: 0.0005758\ttotal: 643ms\tremaining: 238ms\n",
      "146:\tlearn: 0.0005758\ttotal: 646ms\tremaining: 233ms\n",
      "147:\tlearn: 0.0005758\ttotal: 649ms\tremaining: 228ms\n",
      "148:\tlearn: 0.0005758\ttotal: 652ms\tremaining: 223ms\n",
      "149:\tlearn: 0.0005758\ttotal: 656ms\tremaining: 219ms\n",
      "150:\tlearn: 0.0005758\ttotal: 659ms\tremaining: 214ms\n",
      "151:\tlearn: 0.0005758\ttotal: 662ms\tremaining: 209ms\n",
      "152:\tlearn: 0.0005758\ttotal: 665ms\tremaining: 204ms\n",
      "153:\tlearn: 0.0005758\ttotal: 668ms\tremaining: 200ms\n",
      "154:\tlearn: 0.0005758\ttotal: 672ms\tremaining: 195ms\n",
      "155:\tlearn: 0.0005758\ttotal: 675ms\tremaining: 190ms\n",
      "156:\tlearn: 0.0005758\ttotal: 678ms\tremaining: 186ms\n",
      "157:\tlearn: 0.0005758\ttotal: 681ms\tremaining: 181ms\n",
      "158:\tlearn: 0.0005758\ttotal: 684ms\tremaining: 176ms\n",
      "159:\tlearn: 0.0005625\ttotal: 688ms\tremaining: 172ms\n",
      "160:\tlearn: 0.0005625\ttotal: 691ms\tremaining: 167ms\n",
      "161:\tlearn: 0.0005625\ttotal: 695ms\tremaining: 163ms\n",
      "162:\tlearn: 0.0005625\ttotal: 698ms\tremaining: 158ms\n",
      "163:\tlearn: 0.0005625\ttotal: 701ms\tremaining: 154ms\n",
      "164:\tlearn: 0.0005625\ttotal: 704ms\tremaining: 149ms\n",
      "165:\tlearn: 0.0005625\ttotal: 707ms\tremaining: 145ms\n",
      "166:\tlearn: 0.0005625\ttotal: 710ms\tremaining: 140ms\n",
      "167:\tlearn: 0.0005625\ttotal: 714ms\tremaining: 136ms\n",
      "168:\tlearn: 0.0005625\ttotal: 717ms\tremaining: 132ms\n",
      "169:\tlearn: 0.0005625\ttotal: 720ms\tremaining: 127ms\n",
      "170:\tlearn: 0.0005625\ttotal: 723ms\tremaining: 123ms\n",
      "171:\tlearn: 0.0005625\ttotal: 727ms\tremaining: 118ms\n",
      "172:\tlearn: 0.0005625\ttotal: 730ms\tremaining: 114ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173:\tlearn: 0.0005624\ttotal: 733ms\tremaining: 109ms\n",
      "174:\tlearn: 0.0005624\ttotal: 736ms\tremaining: 105ms\n",
      "175:\tlearn: 0.0005624\ttotal: 739ms\tremaining: 101ms\n",
      "176:\tlearn: 0.0005624\ttotal: 742ms\tremaining: 96.5ms\n",
      "177:\tlearn: 0.0005624\ttotal: 745ms\tremaining: 92.1ms\n",
      "178:\tlearn: 0.0005624\ttotal: 749ms\tremaining: 87.8ms\n",
      "179:\tlearn: 0.0005624\ttotal: 752ms\tremaining: 83.5ms\n",
      "180:\tlearn: 0.0005624\ttotal: 755ms\tremaining: 79.3ms\n",
      "181:\tlearn: 0.0005624\ttotal: 758ms\tremaining: 75ms\n",
      "182:\tlearn: 0.0005624\ttotal: 762ms\tremaining: 70.8ms\n",
      "183:\tlearn: 0.0005623\ttotal: 766ms\tremaining: 66.6ms\n",
      "184:\tlearn: 0.0005623\ttotal: 768ms\tremaining: 62.3ms\n",
      "185:\tlearn: 0.0005623\ttotal: 772ms\tremaining: 58.1ms\n",
      "186:\tlearn: 0.0005623\ttotal: 776ms\tremaining: 53.9ms\n",
      "187:\tlearn: 0.0005623\ttotal: 779ms\tremaining: 49.7ms\n",
      "188:\tlearn: 0.0005623\ttotal: 782ms\tremaining: 45.5ms\n",
      "189:\tlearn: 0.0005623\ttotal: 786ms\tremaining: 41.4ms\n",
      "190:\tlearn: 0.0005623\ttotal: 789ms\tremaining: 37.2ms\n",
      "191:\tlearn: 0.0005623\ttotal: 792ms\tremaining: 33ms\n",
      "192:\tlearn: 0.0005623\ttotal: 795ms\tremaining: 28.8ms\n",
      "193:\tlearn: 0.0005623\ttotal: 799ms\tremaining: 24.7ms\n",
      "194:\tlearn: 0.0005623\ttotal: 801ms\tremaining: 20.6ms\n",
      "195:\tlearn: 0.0005623\ttotal: 805ms\tremaining: 16.4ms\n",
      "196:\tlearn: 0.0005623\ttotal: 808ms\tremaining: 12.3ms\n",
      "197:\tlearn: 0.0005623\ttotal: 811ms\tremaining: 8.19ms\n",
      "198:\tlearn: 0.0005623\ttotal: 814ms\tremaining: 4.09ms\n",
      "199:\tlearn: 0.0005623\ttotal: 817ms\tremaining: 0us\n",
      "Dataset 9:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21094f61cc374d819085d954d5d5619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5504778214968049, Recall = 0.08896797153024912, Aging Rate = 0.06890973794888386, Precision = 0.11737089201877934, f1 = 0.10121457489878541\n",
      "Epoch 2: Train Loss = 0.451286964800627, Recall = 0.0, Aging Rate = 0.0, Precision = 0, f1 = 0\n",
      "Epoch 3: Train Loss = 0.4158428333668368, Recall = 0.0071174377224199285, Aging Rate = 0.0006470397929472663, Precision = 0, f1 = 0.0\n",
      "Epoch 4: Train Loss = 0.3766719401750469, Recall = 0.09252669039145907, Aging Rate = 0.008411517308314461, Precision = 0, f1 = 0.0\n",
      "Epoch 5: Train Loss = 0.3355842155185343, Recall = 0.29537366548042704, Aging Rate = 0.030734390164995146, Precision = 0.8736842105263158, f1 = 0.4414893617021276\n",
      "Test Loss = 0.30597724615852645, Recall = 0.3807829181494662, Aging Rate = 0.03623422840504691, precision = 0.9553571428571429\n",
      "Model in epoch 5 is saved.\n",
      "\n",
      "Epoch 6: Train Loss = 0.29388899823746534, Recall = 0.40213523131672596, Aging Rate = 0.04076350695567777, Precision = 0.8968253968253969, f1 = 0.5552825552825552\n",
      "Epoch 7: Train Loss = 0.26540129990426903, Recall = 0.501779359430605, Aging Rate = 0.0527337431252022, Precision = 0.8650306748466258, f1 = 0.6351351351351351\n",
      "Epoch 8: Train Loss = 0.24073422283888712, Recall = 0.5729537366548043, Aging Rate = 0.06082174053704303, Precision = 0.8563829787234043, f1 = 0.6865671641791046\n",
      "Epoch 9: Train Loss = 0.21843464681483982, Recall = 0.6370106761565836, Aging Rate = 0.0682626981559366, Precision = 0.8483412322274881, f1 = 0.7276422764227641\n",
      "Epoch 10: Train Loss = 0.19724969610702023, Recall = 0.6690391459074733, Aging Rate = 0.06858621805241022, Precision = 0.8867924528301887, f1 = 0.7626774847870181\n",
      "Test Loss = 0.1824504413850791, Recall = 0.7402135231316725, Aging Rate = 0.07635069556777742, precision = 0.8813559322033898\n",
      "Model in epoch 10 is saved.\n",
      "\n",
      "Epoch 11: Train Loss = 0.17935337279075098, Recall = 0.7153024911032029, Aging Rate = 0.07505661598188289, Precision = 0.8663793103448276, f1 = 0.783625730994152\n",
      "Epoch 12: Train Loss = 0.16560147382109267, Recall = 0.7793594306049823, Aging Rate = 0.08120349401488192, Precision = 0.8725099601593626, f1 = 0.8233082706766917\n",
      "Epoch 13: Train Loss = 0.14912315299213222, Recall = 0.7758007117437722, Aging Rate = 0.07732125525719832, Precision = 0.9121338912133892, f1 = 0.8384615384615385\n",
      "Epoch 14: Train Loss = 0.13702923207703782, Recall = 0.7900355871886121, Aging Rate = 0.07699773536072468, Precision = 0.9327731092436975, f1 = 0.8554913294797688\n",
      "Epoch 15: Train Loss = 0.12462697515823971, Recall = 0.8327402135231317, Aging Rate = 0.08120349401488192, Precision = 0.9322709163346613, f1 = 0.8796992481203008\n",
      "Test Loss = 0.11563616890287909, Recall = 0.8220640569395018, Aging Rate = 0.07732125525719832, precision = 0.9665271966527197\n",
      "Model in epoch 15 is saved.\n",
      "\n",
      "Epoch 16: Train Loss = 0.11429490784164464, Recall = 0.8612099644128114, Aging Rate = 0.08314461339372371, Precision = 0.9416342412451362, f1 = 0.8996282527881041\n",
      "Epoch 17: Train Loss = 0.1050513432596502, Recall = 0.8540925266903915, Aging Rate = 0.08055645422193465, Precision = 0.963855421686747, f1 = 0.9056603773584907\n",
      "Epoch 18: Train Loss = 0.09553158042915659, Recall = 0.896797153024911, Aging Rate = 0.08605629246198641, Precision = 0.9473684210526315, f1 = 0.9213893967093235\n",
      "Epoch 19: Train Loss = 0.08833943194471991, Recall = 0.9181494661921709, Aging Rate = 0.08637981235846004, Precision = 0.9662921348314607, f1 = 0.9416058394160584\n",
      "Epoch 20: Train Loss = 0.08098768594363522, Recall = 0.9359430604982206, Aging Rate = 0.08735037204788094, Precision = 0.9740740740740741, f1 = 0.9546279491833031\n",
      "Test Loss = 0.07496623429992522, Recall = 0.9288256227758007, Aging Rate = 0.08573277256551277, precision = 0.9849056603773585\n",
      "Model in epoch 20 is saved.\n",
      "\n",
      "Epoch 21: Train Loss = 0.07398735880080413, Recall = 0.9430604982206405, Aging Rate = 0.08799741184082821, Precision = 0.9742647058823529, f1 = 0.9584086799276672\n",
      "Epoch 22: Train Loss = 0.06824209990624046, Recall = 0.9359430604982206, Aging Rate = 0.0870268521514073, Precision = 0.9776951672862454, f1 = 0.9563636363636363\n",
      "Epoch 23: Train Loss = 0.06544017884548872, Recall = 0.9501779359430605, Aging Rate = 0.08832093173730185, Precision = 0.978021978021978, f1 = 0.9638989169675091\n",
      "Epoch 24: Train Loss = 0.05913959607115574, Recall = 0.9572953736654805, Aging Rate = 0.08896797153024912, Precision = 0.9781818181818182, f1 = 0.9676258992805755\n",
      "Epoch 25: Train Loss = 0.0538535578178187, Recall = 0.9644128113879004, Aging Rate = 0.08961501132319638, Precision = 0.9783393501805054, f1 = 0.971326164874552\n",
      "Test Loss = 0.04968105211000696, Recall = 0.9715302491103203, Aging Rate = 0.08993853121967, precision = 0.9820143884892086\n",
      "Model in epoch 25 is saved.\n",
      "\n",
      "Epoch 26: Train Loss = 0.05013979804608398, Recall = 0.9572953736654805, Aging Rate = 0.08864445163377548, Precision = 0.9817518248175182, f1 = 0.9693693693693693\n",
      "Epoch 27: Train Loss = 0.04703903446755568, Recall = 0.9715302491103203, Aging Rate = 0.08993853121967, Precision = 0.9820143884892086, f1 = 0.9767441860465116\n",
      "Epoch 28: Train Loss = 0.04303756330719092, Recall = 0.9715302491103203, Aging Rate = 0.08961501132319638, Precision = 0.9855595667870036, f1 = 0.978494623655914\n",
      "Epoch 29: Train Loss = 0.040182073708154825, Recall = 0.9786476868327402, Aging Rate = 0.09026205111614365, Precision = 0.985663082437276, f1 = 0.9821428571428571\n",
      "Epoch 30: Train Loss = 0.03685165214614743, Recall = 0.9857651245551602, Aging Rate = 0.09090909090909091, Precision = 0.9857651245551602, f1 = 0.9857651245551602\n",
      "Test Loss = 0.03427463921738486, Recall = 0.9928825622775801, Aging Rate = 0.09123261080556454, precision = 0.9893617021276596\n",
      "Model in epoch 30 is saved.\n",
      "\n",
      "Epoch 31: Train Loss = 0.034890161612077426, Recall = 0.9822064056939501, Aging Rate = 0.09026205111614365, Precision = 0.989247311827957, f1 = 0.9857142857142857\n",
      "Epoch 32: Train Loss = 0.032825136999531844, Recall = 0.9928825622775801, Aging Rate = 0.09123261080556454, Precision = 0.9893617021276596, f1 = 0.9911190053285969\n",
      "Epoch 33: Train Loss = 0.03105243570820601, Recall = 0.9893238434163701, Aging Rate = 0.09058557101261727, Precision = 0.9928571428571429, f1 = 0.9910873440285205\n",
      "Epoch 34: Train Loss = 0.027902799466548327, Recall = 0.9928825622775801, Aging Rate = 0.09123261080556454, Precision = 0.9893617021276596, f1 = 0.9911190053285969\n",
      "Epoch 35: Train Loss = 0.027499640720228438, Recall = 0.9928825622775801, Aging Rate = 0.09123261080556454, Precision = 0.9893617021276596, f1 = 0.9911190053285969\n",
      "Test Loss = 0.02439516296671099, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, precision = 0.9928825622775801\n",
      "Model in epoch 35 is saved.\n",
      "\n",
      "Epoch 36: Train Loss = 0.024730948282966757, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 37: Train Loss = 0.02294156023047187, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 38: Train Loss = 0.02178384972069158, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 39: Train Loss = 0.020549815219824887, Recall = 0.9928825622775801, Aging Rate = 0.09058557101261727, Precision = 0.9964285714285714, f1 = 0.9946524064171123\n",
      "Epoch 40: Train Loss = 0.01953315546610177, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Test Loss = 0.018427362855655192, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, precision = 0.9928825622775801\n",
      "\n",
      "Epoch 41: Train Loss = 0.018483267055293507, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 42: Train Loss = 0.017290994667772605, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 43: Train Loss = 0.016434998823970374, Recall = 0.9928825622775801, Aging Rate = 0.09090909090909091, Precision = 0.9928825622775801, f1 = 0.9928825622775801\n",
      "Epoch 44: Train Loss = 0.01577918123621379, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 45: Train Loss = 0.014892086481590539, Recall = 0.99644128113879, Aging Rate = 0.09090909090909091, Precision = 0.99644128113879, f1 = 0.99644128113879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.013883830111718455, Recall = 0.9928825622775801, Aging Rate = 0.09058557101261727, precision = 0.9964285714285714\n",
      "Model in epoch 45 is saved.\n",
      "\n",
      "Epoch 46: Train Loss = 0.014343300252786465, Recall = 0.9928825622775801, Aging Rate = 0.09058557101261727, Precision = 0.9964285714285714, f1 = 0.9946524064171123\n",
      "Epoch 47: Train Loss = 0.013509754179179939, Recall = 0.99644128113879, Aging Rate = 0.09123261080556454, Precision = 0.9929078014184397, f1 = 0.9946714031971581\n",
      "Epoch 48: Train Loss = 0.012464684597691096, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 49: Train Loss = 0.012068469137006384, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 50: Train Loss = 0.011577978793475012, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.010779663478393398, Recall = 1.0, Aging Rate = 0.09123261080556454, precision = 0.9964539007092199\n",
      "Model in epoch 50 is saved.\n",
      "\n",
      "Epoch 51: Train Loss = 0.011074803118777484, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 52: Train Loss = 0.010790152068782281, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 53: Train Loss = 0.010232006462694024, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 54: Train Loss = 0.009833901159438798, Recall = 1.0, Aging Rate = 0.09155613070203818, Precision = 0.9929328621908127, f1 = 0.9964539007092198\n",
      "Epoch 55: Train Loss = 0.009324736628483767, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.008695092502064263, Recall = 1.0, Aging Rate = 0.09123261080556454, precision = 0.9964539007092199\n",
      "\n",
      "Epoch 56: Train Loss = 0.008990082378486856, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 57: Train Loss = 0.008766928527700654, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 58: Train Loss = 0.008215960022092194, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 59: Train Loss = 0.008007998405604715, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 60: Train Loss = 0.007537890326735494, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.007197674646020292, Recall = 1.0, Aging Rate = 0.09123261080556454, precision = 0.9964539007092199\n",
      "\n",
      "Epoch 61: Train Loss = 0.007440260389528382, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 62: Train Loss = 0.007150627567970041, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 63: Train Loss = 0.006908854942926091, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 64: Train Loss = 0.006645340119033271, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 65: Train Loss = 0.00641426436882677, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.006153763114136955, Recall = 1.0, Aging Rate = 0.09123261080556454, precision = 0.9964539007092199\n",
      "\n",
      "Epoch 66: Train Loss = 0.006146441578361275, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 67: Train Loss = 0.006037037411511224, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 68: Train Loss = 0.005905083897371425, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 69: Train Loss = 0.005696757302617408, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 70: Train Loss = 0.005531475972278759, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.005226876756534299, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "Model in epoch 70 is saved.\n",
      "\n",
      "Epoch 71: Train Loss = 0.005308805386049523, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 72: Train Loss = 0.005198245042671677, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 73: Train Loss = 0.005076018913216052, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 74: Train Loss = 0.0053827363998601736, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 75: Train Loss = 0.004890058442271718, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Test Loss = 0.00456625073247287, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 76: Train Loss = 0.004614512089896611, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 77: Train Loss = 0.004562751665128083, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 78: Train Loss = 0.004515348319434273, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 79: Train Loss = 0.004314118018840864, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 80: Train Loss = 0.004416328828344354, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.004005121138122804, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 81: Train Loss = 0.004111103852506515, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 82: Train Loss = 0.004078777475084896, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 83: Train Loss = 0.003978076081485549, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 84: Train Loss = 0.003792395833632642, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 85: Train Loss = 0.003698239843709083, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0035593932550939825, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 86: Train Loss = 0.0036906677247252554, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 87: Train Loss = 0.003590559582061769, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 88: Train Loss = 0.0035539179157094494, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 89: Train Loss = 0.003415656433006419, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 90: Train Loss = 0.0034273164872585067, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.003212906098139687, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 91: Train Loss = 0.0033086788095172287, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 92: Train Loss = 0.003229432990753421, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 93: Train Loss = 0.0031901597072780423, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 94: Train Loss = 0.0032641877086103366, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 95: Train Loss = 0.0031460780561192066, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.00292965637029437, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 96: Train Loss = 0.003259404174940114, Recall = 1.0, Aging Rate = 0.09123261080556454, Precision = 0.9964539007092199, f1 = 0.9982238010657194\n",
      "Epoch 97: Train Loss = 0.0030695272363126954, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 98: Train Loss = 0.002939937968629306, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 99: Train Loss = 0.0028771971906717615, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss = 0.0028457557289526397, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002741607995750518, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 101: Train Loss = 0.002759112843918843, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 102: Train Loss = 0.002719047789521267, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 103: Train Loss = 0.002664362328818212, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 104: Train Loss = 0.0026637451790709472, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 105: Train Loss = 0.002630845536617969, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0026719977242561175, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 106: Train Loss = 0.0026162439961479305, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 107: Train Loss = 0.0025651702064606813, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 108: Train Loss = 0.0025105355374687704, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 109: Train Loss = 0.002462982653657686, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 110: Train Loss = 0.002427046338701274, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002324210308998822, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 111: Train Loss = 0.0024142169829977594, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 112: Train Loss = 0.0024070888107647955, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 113: Train Loss = 0.0023499383804738154, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 114: Train Loss = 0.0023615010592598976, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 115: Train Loss = 0.002285483311087409, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.002196674134229156, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 116: Train Loss = 0.002305490439634171, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 117: Train Loss = 0.002252771851610584, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 118: Train Loss = 0.002261262522585237, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 119: Train Loss = 0.0021978813103660493, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 120: Train Loss = 0.002141866813650622, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0020811489756637595, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 121: Train Loss = 0.0021797977523490743, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 122: Train Loss = 0.0022461654422385904, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 123: Train Loss = 0.002127470031273089, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 124: Train Loss = 0.0021404761128076054, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 125: Train Loss = 0.0020855198980493463, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0019845332627509192, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 126: Train Loss = 0.0020562750448173605, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 127: Train Loss = 0.002026813787422334, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 128: Train Loss = 0.0020208853322244875, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 129: Train Loss = 0.002019840188532952, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 130: Train Loss = 0.002001113207156302, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018915687152885072, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 131: Train Loss = 0.0020379101876299003, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 132: Train Loss = 0.002032520177022987, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 133: Train Loss = 0.0019498800631222054, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 134: Train Loss = 0.001951948620159925, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 135: Train Loss = 0.0018932190296622503, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0018228550789716877, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 136: Train Loss = 0.0019057047824644696, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 137: Train Loss = 0.0019106361247914437, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 138: Train Loss = 0.0018674237947005313, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 139: Train Loss = 0.0018907707942024824, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 140: Train Loss = 0.0018620172695011812, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0017724622069271681, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 141: Train Loss = 0.001834346795246648, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 142: Train Loss = 0.001843908341964258, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 143: Train Loss = 0.0018535308845387002, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 144: Train Loss = 0.001850707986383343, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 145: Train Loss = 0.0018011791881960848, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001695419342170609, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 146: Train Loss = 0.0017709370885319725, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 147: Train Loss = 0.0017896884295604918, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 148: Train Loss = 0.00174967948373242, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 149: Train Loss = 0.0017420599828840305, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 150: Train Loss = 0.001722507218536368, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0016360354127855903, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 151: Train Loss = 0.0017435039707785819, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 152: Train Loss = 0.0017234790807036096, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 153: Train Loss = 0.001756509550326163, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 154: Train Loss = 0.0017213161159620118, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 155: Train Loss = 0.0017187798020105408, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015950601448847144, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 156: Train Loss = 0.0016853727296881188, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 157: Train Loss = 0.0016702644459768256, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 158: Train Loss = 0.0017042272464037472, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 159: Train Loss = 0.0016545335480033995, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 160: Train Loss = 0.0017855293293724897, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.00157846792918018, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 161: Train Loss = 0.0016499419015461945, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 162: Train Loss = 0.0016251020386588491, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 163: Train Loss = 0.0016453653395544123, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 164: Train Loss = 0.0016236215397406432, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 165: Train Loss = 0.001619319618968531, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.001516744922460325, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Epoch 166: Train Loss = 0.0016555269544058944, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 167: Train Loss = 0.0016727231253711046, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 168: Train Loss = 0.0016314114133768032, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 169: Train Loss = 0.0016367654031397493, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Epoch 170: Train Loss = 0.0018005075471521517, Recall = 1.0, Aging Rate = 0.09090909090909091, Precision = 0, f1 = 0.0\n",
      "Test Loss = 0.0015519001997678277, Recall = 1.0, Aging Rate = 0.09090909090909091, precision = 1.0\n",
      "\n",
      "Training Finished at epoch 170.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5003079\ttotal: 3.06ms\tremaining: 456ms\n",
      "1:\tlearn: 0.4004641\ttotal: 5.7ms\tremaining: 422ms\n",
      "2:\tlearn: 0.3278842\ttotal: 8.42ms\tremaining: 412ms\n",
      "3:\tlearn: 0.2779032\ttotal: 11.1ms\tremaining: 404ms\n",
      "4:\tlearn: 0.2342335\ttotal: 14.1ms\tremaining: 410ms\n",
      "5:\tlearn: 0.1994568\ttotal: 16.9ms\tremaining: 405ms\n",
      "6:\tlearn: 0.1770106\ttotal: 19.5ms\tremaining: 398ms\n",
      "7:\tlearn: 0.1582141\ttotal: 22.3ms\tremaining: 395ms\n",
      "8:\tlearn: 0.1464022\ttotal: 24.6ms\tremaining: 385ms\n",
      "9:\tlearn: 0.1306479\ttotal: 27.4ms\tremaining: 384ms\n",
      "10:\tlearn: 0.1215850\ttotal: 29.8ms\tremaining: 376ms\n",
      "11:\tlearn: 0.1121166\ttotal: 32.3ms\tremaining: 372ms\n",
      "12:\tlearn: 0.1024371\ttotal: 35ms\tremaining: 369ms\n",
      "13:\tlearn: 0.0965709\ttotal: 37.6ms\tremaining: 365ms\n",
      "14:\tlearn: 0.0910979\ttotal: 40.1ms\tremaining: 361ms\n",
      "15:\tlearn: 0.0873191\ttotal: 42.4ms\tremaining: 355ms\n",
      "16:\tlearn: 0.0826915\ttotal: 44.9ms\tremaining: 352ms\n",
      "17:\tlearn: 0.0788651\ttotal: 47.3ms\tremaining: 347ms\n",
      "18:\tlearn: 0.0711240\ttotal: 49.9ms\tremaining: 344ms\n",
      "19:\tlearn: 0.0649211\ttotal: 52.7ms\tremaining: 342ms\n",
      "20:\tlearn: 0.0581144\ttotal: 55.5ms\tremaining: 341ms\n",
      "21:\tlearn: 0.0533106\ttotal: 58.4ms\tremaining: 340ms\n",
      "22:\tlearn: 0.0482244\ttotal: 61.1ms\tremaining: 337ms\n",
      "23:\tlearn: 0.0459973\ttotal: 63.6ms\tremaining: 334ms\n",
      "24:\tlearn: 0.0418830\ttotal: 66.4ms\tremaining: 332ms\n",
      "25:\tlearn: 0.0380660\ttotal: 69.3ms\tremaining: 330ms\n",
      "26:\tlearn: 0.0353417\ttotal: 71.7ms\tremaining: 326ms\n",
      "27:\tlearn: 0.0339237\ttotal: 74ms\tremaining: 322ms\n",
      "28:\tlearn: 0.0318378\ttotal: 76.4ms\tremaining: 319ms\n",
      "29:\tlearn: 0.0308981\ttotal: 78.4ms\tremaining: 314ms\n",
      "30:\tlearn: 0.0298153\ttotal: 80.6ms\tremaining: 309ms\n",
      "31:\tlearn: 0.0283187\ttotal: 83.1ms\tremaining: 306ms\n",
      "32:\tlearn: 0.0271944\ttotal: 85.4ms\tremaining: 303ms\n",
      "33:\tlearn: 0.0260666\ttotal: 87.5ms\tremaining: 298ms\n",
      "34:\tlearn: 0.0250902\ttotal: 89.6ms\tremaining: 294ms\n",
      "35:\tlearn: 0.0238679\ttotal: 91.9ms\tremaining: 291ms\n",
      "36:\tlearn: 0.0218828\ttotal: 94.4ms\tremaining: 288ms\n",
      "37:\tlearn: 0.0205946\ttotal: 96.6ms\tremaining: 285ms\n",
      "38:\tlearn: 0.0192815\ttotal: 98.9ms\tremaining: 282ms\n",
      "39:\tlearn: 0.0186864\ttotal: 101ms\tremaining: 278ms\n",
      "40:\tlearn: 0.0173855\ttotal: 103ms\tremaining: 275ms\n",
      "41:\tlearn: 0.0165504\ttotal: 106ms\tremaining: 272ms\n",
      "42:\tlearn: 0.0154258\ttotal: 108ms\tremaining: 269ms\n",
      "43:\tlearn: 0.0151131\ttotal: 110ms\tremaining: 265ms\n",
      "44:\tlearn: 0.0136775\ttotal: 113ms\tremaining: 263ms\n",
      "45:\tlearn: 0.0131648\ttotal: 115ms\tremaining: 259ms\n",
      "46:\tlearn: 0.0126054\ttotal: 117ms\tremaining: 256ms\n",
      "47:\tlearn: 0.0117872\ttotal: 119ms\tremaining: 253ms\n",
      "48:\tlearn: 0.0113431\ttotal: 121ms\tremaining: 249ms\n",
      "49:\tlearn: 0.0110404\ttotal: 123ms\tremaining: 246ms\n",
      "50:\tlearn: 0.0102631\ttotal: 125ms\tremaining: 243ms\n",
      "51:\tlearn: 0.0100567\ttotal: 127ms\tremaining: 239ms\n",
      "52:\tlearn: 0.0096484\ttotal: 129ms\tremaining: 237ms\n",
      "53:\tlearn: 0.0092815\ttotal: 131ms\tremaining: 233ms\n",
      "54:\tlearn: 0.0088646\ttotal: 133ms\tremaining: 231ms\n",
      "55:\tlearn: 0.0086517\ttotal: 135ms\tremaining: 227ms\n",
      "56:\tlearn: 0.0083211\ttotal: 137ms\tremaining: 224ms\n",
      "57:\tlearn: 0.0081013\ttotal: 140ms\tremaining: 221ms\n",
      "58:\tlearn: 0.0079286\ttotal: 141ms\tremaining: 218ms\n",
      "59:\tlearn: 0.0076698\ttotal: 143ms\tremaining: 215ms\n",
      "60:\tlearn: 0.0073722\ttotal: 146ms\tremaining: 212ms\n",
      "61:\tlearn: 0.0070678\ttotal: 148ms\tremaining: 210ms\n",
      "62:\tlearn: 0.0067551\ttotal: 150ms\tremaining: 207ms\n",
      "63:\tlearn: 0.0066231\ttotal: 152ms\tremaining: 204ms\n",
      "64:\tlearn: 0.0064685\ttotal: 154ms\tremaining: 201ms\n",
      "65:\tlearn: 0.0063327\ttotal: 156ms\tremaining: 198ms\n",
      "66:\tlearn: 0.0061374\ttotal: 158ms\tremaining: 196ms\n",
      "67:\tlearn: 0.0059449\ttotal: 160ms\tremaining: 193ms\n",
      "68:\tlearn: 0.0057648\ttotal: 162ms\tremaining: 190ms\n",
      "69:\tlearn: 0.0053245\ttotal: 164ms\tremaining: 188ms\n",
      "70:\tlearn: 0.0051867\ttotal: 166ms\tremaining: 185ms\n",
      "71:\tlearn: 0.0050365\ttotal: 168ms\tremaining: 182ms\n",
      "72:\tlearn: 0.0049665\ttotal: 170ms\tremaining: 179ms\n",
      "73:\tlearn: 0.0048744\ttotal: 172ms\tremaining: 177ms\n",
      "74:\tlearn: 0.0048009\ttotal: 174ms\tremaining: 174ms\n",
      "75:\tlearn: 0.0046497\ttotal: 176ms\tremaining: 171ms\n",
      "76:\tlearn: 0.0045549\ttotal: 178ms\tremaining: 169ms\n",
      "77:\tlearn: 0.0045025\ttotal: 180ms\tremaining: 166ms\n",
      "78:\tlearn: 0.0043279\ttotal: 182ms\tremaining: 164ms\n",
      "79:\tlearn: 0.0041697\ttotal: 184ms\tremaining: 161ms\n",
      "80:\tlearn: 0.0040201\ttotal: 186ms\tremaining: 159ms\n",
      "81:\tlearn: 0.0039979\ttotal: 188ms\tremaining: 156ms\n",
      "82:\tlearn: 0.0038631\ttotal: 190ms\tremaining: 154ms\n",
      "83:\tlearn: 0.0037086\ttotal: 192ms\tremaining: 151ms\n",
      "84:\tlearn: 0.0035800\ttotal: 195ms\tremaining: 149ms\n",
      "85:\tlearn: 0.0034624\ttotal: 197ms\tremaining: 146ms\n",
      "86:\tlearn: 0.0033616\ttotal: 199ms\tremaining: 144ms\n",
      "87:\tlearn: 0.0032872\ttotal: 201ms\tremaining: 142ms\n",
      "88:\tlearn: 0.0031397\ttotal: 203ms\tremaining: 139ms\n",
      "89:\tlearn: 0.0030862\ttotal: 205ms\tremaining: 137ms\n",
      "90:\tlearn: 0.0030548\ttotal: 207ms\tremaining: 134ms\n",
      "91:\tlearn: 0.0029688\ttotal: 209ms\tremaining: 132ms\n",
      "92:\tlearn: 0.0028518\ttotal: 211ms\tremaining: 129ms\n",
      "93:\tlearn: 0.0027561\ttotal: 213ms\tremaining: 127ms\n",
      "94:\tlearn: 0.0026778\ttotal: 215ms\tremaining: 125ms\n",
      "95:\tlearn: 0.0025852\ttotal: 218ms\tremaining: 122ms\n",
      "96:\tlearn: 0.0025657\ttotal: 219ms\tremaining: 120ms\n",
      "97:\tlearn: 0.0025274\ttotal: 221ms\tremaining: 117ms\n",
      "98:\tlearn: 0.0024233\ttotal: 224ms\tremaining: 115ms\n",
      "99:\tlearn: 0.0023732\ttotal: 226ms\tremaining: 113ms\n",
      "100:\tlearn: 0.0023416\ttotal: 228ms\tremaining: 111ms\n",
      "101:\tlearn: 0.0022848\ttotal: 230ms\tremaining: 108ms\n",
      "102:\tlearn: 0.0022109\ttotal: 232ms\tremaining: 106ms\n",
      "103:\tlearn: 0.0021636\ttotal: 234ms\tremaining: 103ms\n",
      "104:\tlearn: 0.0021351\ttotal: 236ms\tremaining: 101ms\n",
      "105:\tlearn: 0.0021131\ttotal: 238ms\tremaining: 98.7ms\n",
      "106:\tlearn: 0.0020740\ttotal: 240ms\tremaining: 96.3ms\n",
      "107:\tlearn: 0.0020368\ttotal: 242ms\tremaining: 94ms\n",
      "108:\tlearn: 0.0019977\ttotal: 244ms\tremaining: 91.7ms\n",
      "109:\tlearn: 0.0019389\ttotal: 246ms\tremaining: 89.5ms\n",
      "110:\tlearn: 0.0019177\ttotal: 248ms\tremaining: 87.1ms\n",
      "111:\tlearn: 0.0018797\ttotal: 250ms\tremaining: 84.8ms\n",
      "112:\tlearn: 0.0018005\ttotal: 252ms\tremaining: 82.6ms\n",
      "113:\tlearn: 0.0017350\ttotal: 254ms\tremaining: 80.3ms\n",
      "114:\tlearn: 0.0016804\ttotal: 257ms\tremaining: 78.1ms\n",
      "115:\tlearn: 0.0016315\ttotal: 259ms\tremaining: 75.8ms\n",
      "116:\tlearn: 0.0015982\ttotal: 261ms\tremaining: 73.6ms\n",
      "117:\tlearn: 0.0015838\ttotal: 263ms\tremaining: 71.3ms\n",
      "118:\tlearn: 0.0015655\ttotal: 265ms\tremaining: 69ms\n",
      "119:\tlearn: 0.0015385\ttotal: 267ms\tremaining: 66.7ms\n",
      "120:\tlearn: 0.0015049\ttotal: 269ms\tremaining: 64.4ms\n",
      "121:\tlearn: 0.0014591\ttotal: 271ms\tremaining: 62.2ms\n",
      "122:\tlearn: 0.0014272\ttotal: 273ms\tremaining: 60ms\n",
      "123:\tlearn: 0.0014050\ttotal: 275ms\tremaining: 57.7ms\n",
      "124:\tlearn: 0.0013900\ttotal: 277ms\tremaining: 55.4ms\n",
      "125:\tlearn: 0.0013683\ttotal: 279ms\tremaining: 53.2ms\n",
      "126:\tlearn: 0.0013396\ttotal: 281ms\tremaining: 50.9ms\n",
      "127:\tlearn: 0.0013122\ttotal: 283ms\tremaining: 48.7ms\n",
      "128:\tlearn: 0.0012993\ttotal: 285ms\tremaining: 46.4ms\n",
      "129:\tlearn: 0.0012800\ttotal: 287ms\tremaining: 44.2ms\n",
      "130:\tlearn: 0.0012633\ttotal: 289ms\tremaining: 41.9ms\n",
      "131:\tlearn: 0.0012562\ttotal: 291ms\tremaining: 39.6ms\n",
      "132:\tlearn: 0.0012216\ttotal: 293ms\tremaining: 37.4ms\n",
      "133:\tlearn: 0.0011622\ttotal: 295ms\tremaining: 35.3ms\n",
      "134:\tlearn: 0.0011527\ttotal: 297ms\tremaining: 33ms\n",
      "135:\tlearn: 0.0011187\ttotal: 299ms\tremaining: 30.8ms\n",
      "136:\tlearn: 0.0010984\ttotal: 301ms\tremaining: 28.6ms\n",
      "137:\tlearn: 0.0010861\ttotal: 303ms\tremaining: 26.4ms\n",
      "138:\tlearn: 0.0010802\ttotal: 305ms\tremaining: 24.1ms\n",
      "139:\tlearn: 0.0010719\ttotal: 307ms\tremaining: 21.9ms\n",
      "140:\tlearn: 0.0010482\ttotal: 309ms\tremaining: 19.7ms\n",
      "141:\tlearn: 0.0010295\ttotal: 311ms\tremaining: 17.5ms\n",
      "142:\tlearn: 0.0010217\ttotal: 313ms\tremaining: 15.3ms\n",
      "143:\tlearn: 0.0010117\ttotal: 315ms\tremaining: 13.1ms\n",
      "144:\tlearn: 0.0009953\ttotal: 317ms\tremaining: 10.9ms\n",
      "145:\tlearn: 0.0009747\ttotal: 319ms\tremaining: 8.74ms\n",
      "146:\tlearn: 0.0009599\ttotal: 321ms\tremaining: 6.55ms\n",
      "147:\tlearn: 0.0009418\ttotal: 323ms\tremaining: 4.37ms\n",
      "148:\tlearn: 0.0009254\ttotal: 325ms\tremaining: 2.18ms\n",
      "149:\tlearn: 0.0009167\ttotal: 327ms\tremaining: 0us\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      "Labels of  10 datasets are divided.\n"
     ]
    }
   ],
   "source": [
    "train_firstC = transform_train(run_train, mode = 'C', base_param = base_paramC, cv = 5)\n",
    "test_firstC = transform_test(run_train, run_test, mode = 'C', base_param = base_paramC)\n",
    "train_firstC_x, train_firstC_y = train_set(train_firstC, num_set = 10)\n",
    "test_firstC_x, test_firstC_y = train_set(test_firstC, num_set = 10) \n",
    "\n",
    "# train_firstR = transform_train(run_train, mode = 'R', base_param = base_paramR, cv = 5)\n",
    "# test_firstR = transform_test(run_train, run_test, mode = 'R', base_param = base_paramR)\n",
    "# train_firstR_x, train_firstR_y = train_set(train_firstR, num_set = 10)\n",
    "# test_firstR_x, test_firstR_y = train_set(test_firstR, num_set = 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### searching for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:56:20.166517Z",
     "start_time": "2021-12-05T10:50:37.431773Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e3b1b23f554bf0b3dd0d873be9209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:50:37,445]\u001b[0m A new study created in memory with name: no-name-abf5987d-a161-4ae5-a4ec-7ecc42c6d975\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f2293ac3f442dc9f35f148f920cb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9333333333333333 \n",
      "Recall: 0.5915492957746479 \n",
      "Aging Rate: 0.0011453879047037263\n",
      "Precision: 0.9245283018867925 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013490124210955\n",
      "Precision: 0.9411764705882353 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0012981062919975566\n",
      "\u001b[32m[I 2021-12-05 18:50:40,924]\u001b[0m Trial 0 finished with value: 2.5186075634966536 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 0 with value: 2.5186075634966536.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9523809523809523 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.001603543066585217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.98 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0012726532274485848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9074074074074074 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "\u001b[32m[I 2021-12-05 18:50:44,116]\u001b[0m Trial 1 finished with value: 2.6349762774175916 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.9487179487179487 \n",
      "Recall: 0.5211267605633803 \n",
      "Aging Rate: 0.0009926695174098962\n",
      "Precision: 0.95 \n",
      "Recall: 0.5352112676056338 \n",
      "Aging Rate: 0.0010181225819588678\n",
      "Precision: 0.9743589743589743 \n",
      "Recall: 0.5352112676056338 \n",
      "Aging Rate: 0.0009926695174098962\n",
      "\u001b[32m[I 2021-12-05 18:50:46,949]\u001b[0m Trial 2 finished with value: 2.4459010473094978 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.92 \n",
      "Recall: 0.647887323943662 \n",
      "Aging Rate: 0.0012726532274485848\n",
      "Precision: 0.9298245614035088 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9423076923076923 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "\u001b[32m[I 2021-12-05 18:50:49,285]\u001b[0m Trial 3 finished with value: 2.556257183225308 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 0.0\n",
      "\u001b[32m[I 2021-12-05 18:50:52,120]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.9259259259259259 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "Precision: 0.8870967741935484 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0015780900020362452\n",
      "Precision: 0.9454545454545454 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "\u001b[32m[I 2021-12-05 18:50:55,167]\u001b[0m Trial 5 finished with value: 2.5760740322606144 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.9285714285714286 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9807692307692307 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "Precision: 0.9074074074074074 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "\u001b[32m[I 2021-12-05 18:50:58,253]\u001b[0m Trial 6 finished with value: 2.5914470679728896 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.8727272727272727 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0013999185501934433\n",
      "Precision: 0.9622641509433962 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013490124210955\n",
      "Precision: 0.98 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0012726532274485848\n",
      "\u001b[32m[I 2021-12-05 18:51:06,627]\u001b[0m Trial 7 finished with value: 2.571496629864953 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.9464285714285714 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9629629629629629 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "Precision: 0.9107142857142857 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "\u001b[32m[I 2021-12-05 18:51:09,425]\u001b[0m Trial 8 finished with value: 2.612464912934396 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.9107142857142857 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9230769230769231 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "Precision: 0.9166666666666666 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "\u001b[32m[I 2021-12-05 18:51:11,903]\u001b[0m Trial 9 finished with value: 2.5566432784742643 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n",
      "Precision: 0.8571428571428571 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.001603543066585217\n",
      "Precision: 0.9615384615384616 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "Precision: 0.9245283018867925 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013490124210955\n",
      "\u001b[32m[I 2021-12-05 18:51:16,475]\u001b[0m Trial 10 finished with value: 2.547116272867004 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9310344827586207 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0014762777438403585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0013235593565465282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.92 \n",
      "Recall: 0.647887323943662 \n",
      "Aging Rate: 0.0012726532274485848\n",
      "\u001b[32m[I 2021-12-05 18:51:18,295]\u001b[0m Trial 11 finished with value: 2.614304678646592 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9259259259259259 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013744654856444715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9661016949152542 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.00150173080838933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8888888888888888 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "\u001b[32m[I 2021-12-05 18:51:20,147]\u001b[0m Trial 12 finished with value: 2.581643870336478 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 1 with value: 2.6349762774175916.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.95 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0015271838729383018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9491525423728814 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.00150173080838933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9423076923076923 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "\u001b[32m[I 2021-12-05 18:51:22,004]\u001b[0m Trial 13 finished with value: 2.6548702034020724 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 13 with value: 2.6548702034020724.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.94 \n",
      "Recall: 0.6619718309859155 \n",
      "Aging Rate: 0.0012726532274485848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9803921568627451 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0012981062919975566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8947368421052632 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "\u001b[32m[I 2021-12-05 18:51:23,879]\u001b[0m Trial 14 finished with value: 2.5715883467298455 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 13 with value: 2.6548702034020724.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9056603773584906 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0013490124210955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9464285714285714 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014253716147424149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9491525423728814 \n",
      "Recall: 0.7887323943661971 \n",
      "Aging Rate: 0.00150173080838933\n",
      "\u001b[32m[I 2021-12-05 18:51:25,745]\u001b[0m Trial 15 finished with value: 2.6045835293178965 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 13 with value: 2.6548702034020724.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.98 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0012726532274485848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9444444444444444 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013744654856444715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9615384615384616 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "\u001b[32m[I 2021-12-05 18:51:27,656]\u001b[0m Trial 16 finished with value: 2.62821395610128 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 13 with value: 2.6548702034020724.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9322033898305084 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.00150173080838933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9245283018867925 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013490124210955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9411764705882353 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0012981062919975566\n",
      "\u001b[32m[I 2021-12-05 18:51:29,498]\u001b[0m Trial 17 finished with value: 2.578887131677869 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 13 with value: 2.6548702034020724.\u001b[0m\n",
      "Precision: 0.9137931034482759 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.9827586206896551 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.9375 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.0016289961311341885\n",
      "\u001b[32m[I 2021-12-05 18:51:32,315]\u001b[0m Trial 18 finished with value: 2.6874898818196535 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Precision: 0.8888888888888888 \n",
      "Recall: 0.676056338028169 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "Precision: 0.9464285714285714 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9322033898305084 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.00150173080838933\n",
      "\u001b[32m[I 2021-12-05 18:51:35,244]\u001b[0m Trial 19 finished with value: 2.5774082662958286 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Precision: 0.9016393442622951 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0015526369374872734\n",
      "Precision: 0.9423076923076923 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0013235593565465282\n",
      "Precision: 0.9 \n",
      "Recall: 0.7605633802816901 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "\u001b[32m[I 2021-12-05 18:51:39,025]\u001b[0m Trial 20 finished with value: 2.571082061938677 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 9}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Precision: 0.9821428571428571 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9285714285714286 \n",
      "Recall: 0.7323943661971831 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9433962264150944 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013490124210955\n",
      "\u001b[32m[I 2021-12-05 18:51:42,853]\u001b[0m Trial 21 finished with value: 2.639829543297521 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Precision: 0.9464285714285714 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "Precision: 0.9298245614035088 \n",
      "Recall: 0.7464788732394366 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.875 \n",
      "Recall: 0.6901408450704225 \n",
      "Aging Rate: 0.0014253716147424149\n",
      "\u001b[32m[I 2021-12-05 18:51:46,789]\u001b[0m Trial 22 finished with value: 2.5618682857378183 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Precision: 0.9387755102040817 \n",
      "Recall: 0.647887323943662 \n",
      "Aging Rate: 0.001247200162899613\n",
      "Precision: 0.9827586206896551 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0014762777438403585\n",
      "Precision: 0.9259259259259259 \n",
      "Recall: 0.704225352112676 \n",
      "Aging Rate: 0.0013744654856444715\n",
      "\u001b[32m[I 2021-12-05 18:51:50,492]\u001b[0m Trial 23 finished with value: 2.6166165637013714 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Precision: 0.9649122807017544 \n",
      "Recall: 0.7746478873239436 \n",
      "Aging Rate: 0.0014508246792913867\n",
      "Precision: 0.9444444444444444 \n",
      "Recall: 0.7183098591549296 \n",
      "Aging Rate: 0.0013744654856444715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:51:52,836]\u001b[0m A new study created in memory with name: no-name-88fed6ab-1fe2-4ebb-ba3b-f2d71198309f\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.95 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.0015271838729383018\n",
      "\u001b[32m[I 2021-12-05 18:51:52,747]\u001b[0m Trial 24 finished with value: 2.671496032726574 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 18 with value: 2.6874898818196535.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfa326ad87f48b6a010499847d910db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.9840579710144928 \n",
      "Aging Rate: 0.4920289855072464\n",
      "Precision: 1.0 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9985358711566618 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.49492753623188407\n",
      "\u001b[32m[I 2021-12-05 18:51:54,491]\u001b[0m Trial 0 finished with value: 2.9888789865682095 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9956268221574344 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9956140350877193 \n",
      "Recall: 0.9869565217391304 \n",
      "Aging Rate: 0.4956521739130435\n",
      "Precision: 0.9985380116959064 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4956521739130435\n",
      "\u001b[32m[I 2021-12-05 18:51:56,243]\u001b[0m Trial 1 finished with value: 2.982074801516262 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9970717423133236 \n",
      "Recall: 0.9869565217391304 \n",
      "Aging Rate: 0.49492753623188407\n",
      "\u001b[32m[I 2021-12-05 18:51:56,371]\u001b[0m Trial 2 finished with value: 2.983055130759157 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9941860465116279 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9927641099855282 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.5007246376811594\n",
      "Precision: 0.9985380116959064 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4956521739130435\n",
      "\u001b[32m[I 2021-12-05 18:51:58,073]\u001b[0m Trial 3 finished with value: 2.9821128850755687 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 18}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9985358711566618 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.49492753623188407\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.998546511627907 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 18:52:00,258]\u001b[0m Trial 4 finished with value: 2.98789872687321 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9985380116959064 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4956521739130435\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 1.0 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.49420289855072463\n",
      "\u001b[32m[I 2021-12-05 18:52:01,964]\u001b[0m Trial 5 finished with value: 2.986936778997677 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9985380116959064 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4956521739130435\n",
      "Precision: 0.9956268221574344 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 18:52:03,689]\u001b[0m Trial 6 finished with value: 2.9859593279758596 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9941944847605225 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9956204379562044 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9942028985507246 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:52:03,818]\u001b[0m Trial 7 finished with value: 2.981132653791827 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9913544668587896 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5028985507246376\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9985401459854014 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4963768115942029\n",
      "\u001b[32m[I 2021-12-05 18:52:03,953]\u001b[0m Trial 8 finished with value: 2.9860110646943974 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.9888789865682095.\u001b[0m\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "\u001b[32m[I 2021-12-05 18:52:04,082]\u001b[0m Trial 9 finished with value: 2.9908142322996647 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 9 with value: 2.9908142322996647.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9941775836972343 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.997080291970803 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 18:52:04,216]\u001b[0m Trial 10 finished with value: 2.982566446766537 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 9 with value: 2.9908142322996647.\u001b[0m\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9956521739130435 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:52:04,352]\u001b[0m Trial 11 finished with value: 2.9864593865857767 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 9 with value: 2.9908142322996647.\u001b[0m\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9985443959243085 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.9985401459854014 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4963768115942029\n",
      "\u001b[32m[I 2021-12-05 18:52:05,588]\u001b[0m Trial 12 finished with value: 2.991291673950619 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 15}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9941690962099126 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9956331877729258 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.49782608695652175\n",
      "\u001b[32m[I 2021-12-05 18:52:05,718]\u001b[0m Trial 13 finished with value: 2.983053790522259 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.997080291970803 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9956458635703919 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.998533724340176 \n",
      "Recall: 0.9869565217391304 \n",
      "Aging Rate: 0.49420289855072463\n",
      "\u001b[32m[I 2021-12-05 18:52:06,981]\u001b[0m Trial 14 finished with value: 2.984511417505455 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 18}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9985401459854014 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9970760233918129 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4956521739130435\n",
      "Precision: 1.0 \n",
      "Recall: 0.9971014492753624 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 18:52:07,113]\u001b[0m Trial 15 finished with value: 2.9893479776524425 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9942028985507246 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.5\n",
      "Precision: 1.0 \n",
      "Recall: 0.9840579710144928 \n",
      "Aging Rate: 0.4920289855072464\n",
      "\u001b[32m[I 2021-12-05 18:52:08,394]\u001b[0m Trial 16 finished with value: 2.985501612653343 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 15}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9956521739130435 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9956204379562044 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4963768115942029\n",
      "Precision: 0.9956331877729258 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.49782608695652175\n",
      "\u001b[32m[I 2021-12-05 18:52:08,522]\u001b[0m Trial 17 finished with value: 2.983057972708309 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9985380116959064 \n",
      "Recall: 0.9898550724637681 \n",
      "Aging Rate: 0.4956521739130435\n",
      "Precision: 0.9985315712187959 \n",
      "Recall: 0.9855072463768116 \n",
      "Aging Rate: 0.4934782608695652\n",
      "Precision: 0.9941690962099126 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 18:52:09,805]\u001b[0m Trial 18 finished with value: 2.9820818247304195 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9956458635703919 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9941690962099126 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.991304347826087 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:52:09,938]\u001b[0m Trial 19 finished with value: 2.978717219563681 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.49420289855072463\n",
      "Precision: 0.9941434846266471 \n",
      "Recall: 0.9840579710144928 \n",
      "Aging Rate: 0.49492753623188407\n",
      "Precision: 0.9956458635703919 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.49927536231884057\n",
      "\u001b[32m[I 2021-12-05 18:52:12,147]\u001b[0m Trial 20 finished with value: 2.9820817876869143 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 21}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "Precision: 0.9985358711566618 \n",
      "Recall: 0.9884057971014493 \n",
      "Aging Rate: 0.49492753623188407\n",
      "\u001b[32m[I 2021-12-05 18:52:12,285]\u001b[0m Trial 21 finished with value: 2.986446642837708 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9970887918486172 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.9956011730205279 \n",
      "Recall: 0.9840579710144928 \n",
      "Aging Rate: 0.49420289855072463\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 18:52:12,423]\u001b[0m Trial 22 finished with value: 2.9825549893257297 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9985507246376811 \n",
      "Aging Rate: 0.5007246376811594\n",
      "Precision: 0.997093023255814 \n",
      "Recall: 0.9942028985507246 \n",
      "Aging Rate: 0.4985507246376812\n",
      "\u001b[32m[I 2021-12-05 18:52:12,554]\u001b[0m Trial 23 finished with value: 2.9903297096606063 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:52:12,764]\u001b[0m A new study created in memory with name: no-name-1c63be58-3b4f-47be-b222-549b28db5247\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9956521739130435 \n",
      "Aging Rate: 0.49927536231884057\n",
      "Precision: 0.9956331877729258 \n",
      "Recall: 0.991304347826087 \n",
      "Aging Rate: 0.49782608695652175\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9927536231884058 \n",
      "Aging Rate: 0.4971014492753623\n",
      "\u001b[32m[I 2021-12-05 18:52:12,685]\u001b[0m Trial 24 finished with value: 2.987418517779622 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 12 with value: 2.991291673950619.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238b7c7ef3074ec5bddaaabbf60644fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9947089947089947 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9894291754756871 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.989406779661017 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:52:12,838]\u001b[0m Trial 0 finished with value: 2.9745949383152115 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.9745949383152115.\u001b[0m\n",
      "Precision: 0.9904661016949152 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9852941176470589 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5042372881355932\n",
      "Precision: 0.9883966244725738 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5021186440677966\n",
      "\u001b[32m[I 2021-12-05 18:52:14,075]\u001b[0m Trial 1 finished with value: 2.968336200961111 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 0 with value: 2.9745949383152115.\u001b[0m\n",
      "Precision: 0.9946865037194474 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4984110169491525\n",
      "Precision: 0.9915522703273495 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.989451476793249 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5021186440677966\n",
      "\u001b[32m[I 2021-12-05 18:52:15,346]\u001b[0m Trial 2 finished with value: 2.977084461012008 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 2 with value: 2.977084461012008.\u001b[0m\n",
      "Precision: 0.9968119022316685 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4984110169491525\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4978813559322034\n",
      "Precision: 0.989451476793249 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5021186440677966\n",
      "\u001b[32m[I 2021-12-05 18:52:17,551]\u001b[0m Trial 3 finished with value: 2.97988089915521 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 3 with value: 2.97988089915521.\u001b[0m\n",
      "Precision: 0.9884088514225501 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.990506329113924 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5021186440677966\n",
      "Precision: 0.9842767295597484 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5052966101694916\n",
      "\u001b[32m[I 2021-12-05 18:52:19,274]\u001b[0m Trial 4 finished with value: 2.969811555883357 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 3 with value: 2.97988089915521.\u001b[0m\n",
      "Precision: 0.9936507936507937 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9925690021231423 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9968119022316685 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.4984110169491525\n",
      "\u001b[32m[I 2021-12-05 18:52:20,516]\u001b[0m Trial 5 finished with value: 2.9816256517777475 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9925925925925926 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9873949579831933 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5042372881355932\n",
      "Precision: 0.9873817034700315 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.503707627118644\n",
      "\u001b[32m[I 2021-12-05 18:52:20,650]\u001b[0m Trial 6 finished with value: 2.9729495591943866 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9853403141361257 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5058262711864406\n",
      "Precision: 0.9915700737618546 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9926082365364308 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5015889830508474\n",
      "\u001b[32m[I 2021-12-05 18:52:21,927]\u001b[0m Trial 7 finished with value: 2.9761480095099464 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 15}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9936102236421726 \n",
      "Recall: 0.9883474576271186 \n",
      "Aging Rate: 0.4973516949152542\n",
      "Precision: 0.994692144373673 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9925373134328358 \n",
      "Recall: 0.986228813559322 \n",
      "Aging Rate: 0.4968220338983051\n",
      "\u001b[32m[I 2021-12-05 18:52:23,680]\u001b[0m Trial 8 finished with value: 2.976280126615505 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9925690021231423 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.990405117270789 \n",
      "Recall: 0.9841101694915254 \n",
      "Aging Rate: 0.4968220338983051\n",
      "Precision: 0.9915522703273495 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5015889830508474\n",
      "\u001b[32m[I 2021-12-05 18:52:24,942]\u001b[0m Trial 9 finished with value: 2.97277748015317 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9915164369034994 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9925847457627118 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.989406779661017 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:52:25,079]\u001b[0m Trial 10 finished with value: 2.973157850591034 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9883720930232558 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9883966244725738 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5021186440677966\n",
      "Precision: 0.9905263157894737 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.503177966101695\n",
      "\u001b[32m[I 2021-12-05 18:52:27,811]\u001b[0m Trial 11 finished with value: 2.971487649308846 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9926082365364308 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9863301787592008 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.503707627118644\n",
      "Precision: 0.9967982924226254 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.4962923728813559\n",
      "\u001b[32m[I 2021-12-05 18:52:29,952]\u001b[0m Trial 12 finished with value: 2.976762324919516 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9936305732484076 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9925531914893617 \n",
      "Recall: 0.9883474576271186 \n",
      "Aging Rate: 0.4978813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4978813559322034\n",
      "\u001b[32m[I 2021-12-05 18:52:32,097]\u001b[0m Trial 13 finished with value: 2.9773560715513483 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9925690021231423 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9873284054910243 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9914984059511158 \n",
      "Recall: 0.9883474576271186 \n",
      "Aging Rate: 0.4984110169491525\n",
      "\u001b[32m[I 2021-12-05 18:52:32,239]\u001b[0m Trial 14 finished with value: 2.9706904293825045 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9915611814345991 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5021186440677966\n",
      "Precision: 0.9936440677966102 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9957492029755579 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4984110169491525\n",
      "\u001b[32m[I 2021-12-05 18:52:33,911]\u001b[0m Trial 15 finished with value: 2.9813001432790878 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 12}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9864016736401674 \n",
      "Recall: 0.9989406779661016 \n",
      "Aging Rate: 0.5063559322033898\n",
      "Precision: 0.9915343915343915 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9811912225705329 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.506885593220339\n",
      "\u001b[32m[I 2021-12-05 18:52:35,580]\u001b[0m Trial 16 finished with value: 2.968161129683168 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.989406779661017 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.992600422832981 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5010593220338984\n",
      "Precision: 0.9957492029755579 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4984110169491525\n",
      "\u001b[32m[I 2021-12-05 18:52:37,252]\u001b[0m Trial 17 finished with value: 2.977402575397783 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9904661016949152 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9884088514225501 \n",
      "Recall: 0.9936440677966102 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9904458598726115 \n",
      "Recall: 0.9883474576271186 \n",
      "Aging Rate: 0.4989406779661017\n",
      "\u001b[32m[I 2021-12-05 18:52:37,393]\u001b[0m Trial 18 finished with value: 2.970366417699599 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9915164369034994 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9831932773109243 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.5042372881355932\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4978813559322034\n",
      "\u001b[32m[I 2021-12-05 18:52:39,065]\u001b[0m Trial 19 finished with value: 2.97041291922505 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9915164369034994 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4994703389830508\n",
      "Precision: 0.9894625922023182 \n",
      "Recall: 0.9947033898305084 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9957356076759062 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.4968220338983051\n",
      "\u001b[32m[I 2021-12-05 18:52:40,260]\u001b[0m Trial 20 finished with value: 2.976001848249963 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 21}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9915343915343915 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5005296610169492\n",
      "Precision: 0.9873949579831933 \n",
      "Recall: 0.9957627118644068 \n",
      "Aging Rate: 0.5042372881355932\n",
      "Precision: 0.9883966244725738 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.5021186440677966\n",
      "\u001b[32m[I 2021-12-05 18:52:42,408]\u001b[0m Trial 21 finished with value: 2.971861383790049 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9915700737618546 \n",
      "Recall: 0.996822033898305 \n",
      "Aging Rate: 0.5026483050847458\n",
      "Precision: 0.9925690021231423 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:52:44,559]\u001b[0m Trial 22 finished with value: 2.976714186183218 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 12}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.994692144373673 \n",
      "Recall: 0.9925847457627118 \n",
      "Aging Rate: 0.4989406779661017\n",
      "Precision: 0.9883843717001056 \n",
      "Recall: 0.9915254237288136 \n",
      "Aging Rate: 0.5015889830508474\n",
      "Precision: 0.9946638207043756 \n",
      "Recall: 0.9872881355932204 \n",
      "Aging Rate: 0.4962923728813559\n",
      "\u001b[32m[I 2021-12-05 18:52:46,232]\u001b[0m Trial 23 finished with value: 2.975626326213684 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 15}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Precision: 0.9946808510638298 \n",
      "Recall: 0.9904661016949152 \n",
      "Aging Rate: 0.4978813559322034\n",
      "Precision: 0.9935965848452508 \n",
      "Recall: 0.986228813559322 \n",
      "Aging Rate: 0.4962923728813559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:52:47,513]\u001b[0m A new study created in memory with name: no-name-caed6f17-e1ab-49f4-b198-110ea36dfd12\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9936170212765958 \n",
      "Recall: 0.989406779661017 \n",
      "Aging Rate: 0.4978813559322034\n",
      "\u001b[32m[I 2021-12-05 18:52:47,424]\u001b[0m Trial 24 finished with value: 2.9766302030955356 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 9}. Best is trial 5 with value: 2.9816256517777475.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset3 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32533811eb74a84904d8b65fe3f291e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9961139896373057 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9935567010309279 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.503242542153048\n",
      "\u001b[32m[I 2021-12-05 18:52:49,107]\u001b[0m Trial 0 finished with value: 2.9870778057502547 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 21}. Best is trial 0 with value: 2.9870778057502547.\u001b[0m\n",
      "Precision: 0.9960474308300395 \n",
      "Recall: 0.980544747081712 \n",
      "Aging Rate: 0.49221789883268485\n",
      "Precision: 0.9947916666666666 \n",
      "Recall: 0.9909208819714657 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.9934810951760105 \n",
      "Recall: 0.9883268482490273 \n",
      "Aging Rate: 0.4974059662775616\n",
      "\u001b[32m[I 2021-12-05 18:52:49,235]\u001b[0m Trial 1 finished with value: 2.9761442875492126 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9870778057502547.\u001b[0m\n",
      "Precision: 0.9922580645161291 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9935483870967742 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5025940337224384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9922480620155039 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 18:52:50,900]\u001b[0m Trial 2 finished with value: 2.9827756420298335 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 21}. Best is trial 0 with value: 2.9870778057502547.\u001b[0m\n",
      "Precision: 0.990990990990991 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9961089494163424 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.990990990990991 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5038910505836576\n",
      "\u001b[32m[I 2021-12-05 18:52:51,040]\u001b[0m Trial 3 finished with value: 2.9832322594968512 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.9870778057502547.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9884467265725289 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5051880674448768\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9922779922779923 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5038910505836576\n",
      "\u001b[32m[I 2021-12-05 18:52:51,175]\u001b[0m Trial 4 finished with value: 2.981569468465269 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.9870778057502547.\u001b[0m\n",
      "Precision: 0.9961139896373057 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9961240310077519 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9987046632124352 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5006485084306096\n",
      "\u001b[32m[I 2021-12-05 18:52:52,364]\u001b[0m Trial 5 finished with value: 2.993097111330849 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9948253557567918 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9948051948051948 \n",
      "Recall: 0.993514915693904 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.990990990990991 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5038910505836576\n",
      "\u001b[32m[I 2021-12-05 18:52:52,502]\u001b[0m Trial 6 finished with value: 2.983622316072067 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.990979381443299 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9884467265725289 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5051880674448768\n",
      "Precision: 0.9935567010309279 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.503242542153048\n",
      "\u001b[32m[I 2021-12-05 18:52:52,643]\u001b[0m Trial 7 finished with value: 2.980691522503285 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9986945169712794 \n",
      "Recall: 0.9922178988326849 \n",
      "Aging Rate: 0.496757457846952\n",
      "Precision: 0.9910025706940874 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5045395590142672\n",
      "Precision: 0.9922077922077922 \n",
      "Recall: 0.9909208819714657 \n",
      "Aging Rate: 0.4993514915693904\n",
      "\u001b[32m[I 2021-12-05 18:52:52,781]\u001b[0m Trial 8 finished with value: 2.98231618018349 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9897039897039897 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5038910505836576\n",
      "\u001b[32m[I 2021-12-05 18:52:54,923]\u001b[0m Trial 9 finished with value: 2.9836591448923713 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 18}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.990990990990991 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9871959026888605 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.506485084306096\n",
      "\u001b[32m[I 2021-12-05 18:52:56,118]\u001b[0m Trial 10 finished with value: 2.979854280045073 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9922480620155039 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 18:52:57,311]\u001b[0m Trial 11 finished with value: 2.984922854146308 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.990990990990991 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5038910505836576\n",
      "Precision: 0.9973992197659298 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9897172236503856 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5045395590142672\n",
      "\u001b[32m[I 2021-12-05 18:52:58,983]\u001b[0m Trial 12 finished with value: 2.982810922549099 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 12}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9948119325551232 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9961190168175937 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 18:53:00,656]\u001b[0m Trial 13 finished with value: 2.9883514112647256 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9935316946959897 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9974025974025974 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.4993514915693904\n",
      "Precision: 0.9935316946959897 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5012970168612192\n",
      "\u001b[32m[I 2021-12-05 18:53:02,805]\u001b[0m Trial 14 finished with value: 2.985752940612727 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9922680412371134 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.503242542153048\n",
      "Precision: 0.9922580645161291 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9961190168175937 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5012970168612192\n",
      "\u001b[32m[I 2021-12-05 18:53:03,997]\u001b[0m Trial 15 finished with value: 2.9853673925655984 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.9961089494163424 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9922480620155039 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5019455252918288\n",
      "\u001b[32m[I 2021-12-05 18:53:05,669]\u001b[0m Trial 16 finished with value: 2.988346957037573 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9974059662775616 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9948320413436692 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9961139896373057 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5006485084306096\n",
      "\u001b[32m[I 2021-12-05 18:53:06,866]\u001b[0m Trial 17 finished with value: 2.990072970070326 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 15}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9948320413436692 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9922580645161291 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5025940337224384\n",
      "Precision: 0.9884467265725289 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5051880674448768\n",
      "\u001b[32m[I 2021-12-05 18:53:08,061]\u001b[0m Trial 18 finished with value: 2.9819618658065927 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 18}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9986979166666666 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4980544747081712\n",
      "Precision: 0.9961139896373057 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9935483870967742 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 18:53:09,248]\u001b[0m Trial 19 finished with value: 2.9892138229243197 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9948320413436692 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9948253557567918 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5012970168612192\n",
      "Precision: 0.9961190168175937 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5012970168612192\n",
      "\u001b[32m[I 2021-12-05 18:53:10,445]\u001b[0m Trial 20 finished with value: 2.988788253463744 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9948186528497409 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5006485084306096\n",
      "Precision: 0.9986996098829649 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9897304236200257 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5051880674448768\n",
      "\u001b[32m[I 2021-12-05 18:53:11,640]\u001b[0m Trial 21 finished with value: 2.9862384238460495 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9961240310077519 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5019455252918288\n",
      "Precision: 0.9960835509138382 \n",
      "Recall: 0.9896238651102465 \n",
      "Aging Rate: 0.496757457846952\n",
      "Precision: 0.9922680412371134 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.503242542153048\n",
      "\u001b[32m[I 2021-12-05 18:53:12,833]\u001b[0m Trial 22 finished with value: 2.9857593648554777 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9897172236503856 \n",
      "Recall: 0.9987029831387808 \n",
      "Aging Rate: 0.5045395590142672\n",
      "Precision: 0.9935400516795866 \n",
      "Recall: 0.9974059662775616 \n",
      "Aging Rate: 0.5019455252918288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9948119325551232 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:53:14,024]\u001b[0m Trial 23 finished with value: 2.9823530992472187 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Precision: 0.9973992197659298 \n",
      "Recall: 0.9948119325551232 \n",
      "Aging Rate: 0.4987029831387808\n",
      "Precision: 0.9961089494163424 \n",
      "Recall: 0.9961089494163424 \n",
      "Aging Rate: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:53:15,301]\u001b[0m A new study created in memory with name: no-name-68c0dbee-d0d9-4471-b8d0-095cb9c9c406\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9948387096774194 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5025940337224384\n",
      "\u001b[32m[I 2021-12-05 18:53:15,219]\u001b[0m Trial 24 finished with value: 2.9892048798969495 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 5 with value: 2.993097111330849.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset4 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e88add2d8e54c028be5c5710e9ff787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9957325746799431 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9942938659058488 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:53:16,427]\u001b[0m Trial 0 finished with value: 2.9838568771365055 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.4914651493598862\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:53:18,560]\u001b[0m Trial 1 finished with value: 2.979542909286375 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9957203994293866 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:53:18,686]\u001b[0m Trial 2 finished with value: 2.9809861833673517 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9942528735632183 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 18:53:18,821]\u001b[0m Trial 3 finished with value: 2.977157307347543 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9886524822695035 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9928366762177651 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 18:53:18,955]\u001b[0m Trial 4 finished with value: 2.9719794651896145 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.9942446043165467 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9957081545064378 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9786628733997155 \n",
      "Aging Rate: 0.4907539118065434\n",
      "\u001b[32m[I 2021-12-05 18:53:21,095]\u001b[0m Trial 5 finished with value: 2.975248087523127 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9786628733997155 \n",
      "Aging Rate: 0.4900426742532006\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.4914651493598862\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:53:22,273]\u001b[0m Trial 6 finished with value: 2.977646023501032 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 9}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:53:23,923]\u001b[0m Trial 7 finished with value: 2.983365041291814 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 18}. Best is trial 0 with value: 2.9838568771365055.\u001b[0m\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 1.0 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49431009957325744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 18:53:26,060]\u001b[0m Trial 8 finished with value: 2.9905087041929144 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9899425287356322 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.992867332382311 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:53:26,192]\u001b[0m Trial 9 finished with value: 2.9738301445657638 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 1.0 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9957203994293866 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:53:28,334]\u001b[0m Trial 10 finished with value: 2.9857535458997577 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9772403982930299 \n",
      "Aging Rate: 0.48862019914651494\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49359886201991465\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:53:30,483]\u001b[0m Trial 11 finished with value: 2.98100090571759 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.99002849002849 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 18:53:32,151]\u001b[0m Trial 12 finished with value: 2.9795812566636477 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9928774928774928 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 1.0 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.49288762446657186\n",
      "\u001b[32m[I 2021-12-05 18:53:34,297]\u001b[0m Trial 13 finished with value: 2.9800512256652607 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9913916786226685 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "\u001b[32m[I 2021-12-05 18:53:35,968]\u001b[0m Trial 14 finished with value: 2.9800323153792916 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 21}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9942363112391931 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49359886201991465\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:53:38,105]\u001b[0m Trial 15 finished with value: 2.9795401502753394 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.4900426742532006\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "\u001b[32m[I 2021-12-05 18:53:40,255]\u001b[0m Trial 16 finished with value: 2.98195471155809 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9942363112391931 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49359886201991465\n",
      "\u001b[32m[I 2021-12-05 18:53:41,929]\u001b[0m Trial 17 finished with value: 2.9795374958389957 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9956709956709957 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.492176386913229\n",
      "\u001b[32m[I 2021-12-05 18:53:42,057]\u001b[0m Trial 18 finished with value: 2.976203206855241 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9786628733997155 \n",
      "Aging Rate: 0.4900426742532006\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.492176386913229\n",
      "\u001b[32m[I 2021-12-05 18:53:44,206]\u001b[0m Trial 19 finished with value: 2.9790890040099334 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 9}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9772403982930299 \n",
      "Aging Rate: 0.4900426742532006\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:53:45,878]\u001b[0m Trial 20 finished with value: 2.977637992581203 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 18:53:47,071]\u001b[0m Trial 21 finished with value: 2.981933040296053 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 15}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9928977272727273 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:53:48,264]\u001b[0m Trial 22 finished with value: 2.9829098536829366 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9786628733997155 \n",
      "Aging Rate: 0.4907539118065434\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.492176386913229\n",
      "Precision: 0.9957081545064378 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:53:49,456]\u001b[0m Trial 23 finished with value: 2.9781216268419612 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9928876244665719 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:53:51,677]\u001b[0m A new study created in memory with name: no-name-e228677e-3d7d-40cf-a7f7-2f56d0189675\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49431009957325744\n",
      "\u001b[32m[I 2021-12-05 18:53:51,592]\u001b[0m Trial 24 finished with value: 2.9795689790857462 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 12}. Best is trial 8 with value: 2.9905087041929144.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset5 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea14aabd93741109b3535fff8273e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9970845481049563 \n",
      "Recall: 0.9827586206896551 \n",
      "Aging Rate: 0.49035025017869904\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49392423159399573\n",
      "\u001b[32m[I 2021-12-05 18:53:52,804]\u001b[0m Trial 0 finished with value: 2.986555181600609 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 0 with value: 2.986555181600609.\u001b[0m\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 1.0 \n",
      "Recall: 0.985632183908046 \n",
      "Aging Rate: 0.49035025017869904\n",
      "\u001b[32m[I 2021-12-05 18:53:54,941]\u001b[0m Trial 1 finished with value: 2.9865762204214925 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9942363112391931 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 18:53:55,072]\u001b[0m Trial 2 finished with value: 2.9841705588128122 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49320943531093636\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49392423159399573\n",
      "\u001b[32m[I 2021-12-05 18:53:56,728]\u001b[0m Trial 3 finished with value: 2.9865609282728705 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 18}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 0.9928057553956835 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.496783416726233\n",
      "\u001b[32m[I 2021-12-05 18:53:56,854]\u001b[0m Trial 4 finished with value: 2.983698518504424 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9927953890489913 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 18:53:56,985]\u001b[0m Trial 5 finished with value: 2.9841677705197505 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.4960686204431737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9841954022988506 \n",
      "Aging Rate: 0.49035025017869904\n",
      "\u001b[32m[I 2021-12-05 18:53:58,640]\u001b[0m Trial 6 finished with value: 2.984645154518553 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 0.9914163090128756 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4996426018584703\n",
      "Precision: 0.9914040114613181 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.498927805575411\n",
      "\u001b[32m[I 2021-12-05 18:53:58,776]\u001b[0m Trial 7 finished with value: 2.979912328802691 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9841954022988506 \n",
      "Aging Rate: 0.49035025017869904\n",
      "Precision: 0.998546511627907 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49177984274481773\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "\u001b[32m[I 2021-12-05 18:53:59,961]\u001b[0m Trial 8 finished with value: 2.9851253995552742 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 17, 'max_depth': 12}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9928057553956835 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9927431059506531 \n",
      "Recall: 0.9827586206896551 \n",
      "Aging Rate: 0.49249463902787705\n",
      "\u001b[32m[I 2021-12-05 18:54:00,090]\u001b[0m Trial 9 finished with value: 2.9774265799697246 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9942112879884226 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49392423159399573\n",
      "\u001b[32m[I 2021-12-05 18:54:02,239]\u001b[0m Trial 10 finished with value: 2.9822298228473745 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 1 with value: 2.9865762204214925.\u001b[0m\n",
      "Precision: 0.9985507246376811 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49320943531093636\n",
      "Precision: 1.0 \n",
      "Recall: 0.985632183908046 \n",
      "Aging Rate: 0.49035025017869904\n",
      "Precision: 1.0 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.4910650464617584\n",
      "\u001b[32m[I 2021-12-05 18:54:04,386]\u001b[0m Trial 11 finished with value: 2.9865817091454274 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 18:54:06,524]\u001b[0m Trial 12 finished with value: 2.985118541528957 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.4960686204431737\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9971181556195965 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 18:54:08,669]\u001b[0m Trial 13 finished with value: 2.986086235713081 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 6}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9813218390804598 \n",
      "Aging Rate: 0.4882058613295211\n",
      "Precision: 0.9971014492753624 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49320943531093636\n",
      "Precision: 0.9985486211901307 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49249463902787705\n",
      "\u001b[32m[I 2021-12-05 18:54:10,820]\u001b[0m Trial 14 finished with value: 2.983211158088107 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 32, 'max_depth': 18}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9885057471264368 \n",
      "Aging Rate: 0.49177984274481773\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9841954022988506 \n",
      "Aging Rate: 0.49035025017869904\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 18:54:12,492]\u001b[0m Trial 15 finished with value: 2.9856099279040866 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9942528735632183 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49749821300929237\n",
      "Precision: 0.998546511627907 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49177984274481773\n",
      "\u001b[32m[I 2021-12-05 18:54:14,640]\u001b[0m Trial 16 finished with value: 2.985615509335443 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.985632183908046 \n",
      "Aging Rate: 0.49035025017869904\n",
      "Precision: 0.9985401459854014 \n",
      "Recall: 0.9827586206896551 \n",
      "Aging Rate: 0.4896354538956397\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 18:54:16,792]\u001b[0m Trial 17 finished with value: 2.985612655748573 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 27, 'max_depth': 15}. Best is trial 11 with value: 2.9865817091454274.\u001b[0m\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.496783416726233\n",
      "\u001b[32m[I 2021-12-05 18:54:18,463]\u001b[0m Trial 18 finished with value: 2.9884960996719863 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n",
      "Precision: 0.9956709956709957 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9971056439942113 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49392423159399573\n",
      "Precision: 1.0 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49392423159399573\n",
      "\u001b[32m[I 2021-12-05 18:54:20,129]\u001b[0m Trial 19 finished with value: 2.9865637367882987 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 21}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.9942528735632183 \n",
      "Aging Rate: 0.496783416726233\n",
      "Precision: 0.9956395348837209 \n",
      "Recall: 0.9841954022988506 \n",
      "Aging Rate: 0.49177984274481773\n",
      "\u001b[32m[I 2021-12-05 18:54:21,324]\u001b[0m Trial 20 finished with value: 2.9827127794287827 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n",
      "Precision: 0.998546511627907 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49177984274481773\n",
      "Precision: 0.9985422740524781 \n",
      "Recall: 0.9841954022988506 \n",
      "Aging Rate: 0.49035025017869904\n",
      "Precision: 0.994261119081779 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4982130092923517\n",
      "\u001b[32m[I 2021-12-05 18:54:23,000]\u001b[0m Trial 21 finished with value: 2.983217944170945 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 3}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9827586206896551 \n",
      "Aging Rate: 0.4889206576125804\n",
      "Precision: 0.9956709956709957 \n",
      "Recall: 0.9913793103448276 \n",
      "Aging Rate: 0.49535382416011436\n",
      "Precision: 0.9971139971139971 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49535382416011436\n",
      "\u001b[32m[I 2021-12-05 18:54:25,150]\u001b[0m Trial 22 finished with value: 2.984174669519497 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.49749821300929237\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9870689655172413 \n",
      "Aging Rate: 0.49249463902787705\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9956896551724138 \n",
      "Aging Rate: 0.4960686204431737\n",
      "\u001b[32m[I 2021-12-05 18:54:27,302]\u001b[0m Trial 23 finished with value: 2.987046742195672 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n",
      "Precision: 0.9956647398843931 \n",
      "Recall: 0.9899425287356322 \n",
      "Aging Rate: 0.49463902787705505\n",
      "Precision: 0.9985549132947977 \n",
      "Recall: 0.992816091954023 \n",
      "Aging Rate: 0.49463902787705505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:54:29,061]\u001b[0m A new study created in memory with name: no-name-bea80b2d-af11-45e5-93c0-9d0a55749b15\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.985632183908046 \n",
      "Aging Rate: 0.49035025017869904\n",
      "\u001b[32m[I 2021-12-05 18:54:28,976]\u001b[0m Trial 24 finished with value: 2.9856100369853613 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 18 with value: 2.9884960996719863.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset6 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4245a2ff4bf4625a1a76001ff8ba385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.994199535962877 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5525641025641026\n",
      "Precision: 0.9976498237367802 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9941860465116279 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5512820512820513\n",
      "\u001b[32m[I 2021-12-05 18:54:30,186]\u001b[0m Trial 0 finished with value: 2.986800733662444 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 0 with value: 2.986800733662444.\u001b[0m\n",
      "Precision: 0.991869918699187 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.551923076923077\n",
      "Precision: 0.9941245593419507 \n",
      "Recall: 0.9871645274212368 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9964994165694282 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 18:54:31,858]\u001b[0m Trial 1 finished with value: 2.981717049927075 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 18}. Best is trial 0 with value: 2.986800733662444.\u001b[0m\n",
      "Precision: 0.9918604651162791 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5512820512820513\n",
      "Precision: 0.9793340987370838 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5583333333333333\n",
      "Precision: 0.9883585564610011 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5506410256410257\n",
      "\u001b[32m[I 2021-12-05 18:54:31,983]\u001b[0m Trial 2 finished with value: 2.9668121541107815 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 0 with value: 2.986800733662444.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9907300115874855 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5532051282051282\n",
      "Precision: 0.9976580796252927 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9930232558139535 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5512820512820513\n",
      "\u001b[32m[I 2021-12-05 18:54:32,117]\u001b[0m Trial 3 finished with value: 2.983718027539408 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.986800733662444.\u001b[0m\n",
      "Precision: 0.9953271028037384 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9906976744186047 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5512820512820513\n",
      "Precision: 0.9872389791183295 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5525641025641026\n",
      "\u001b[32m[I 2021-12-05 18:54:32,245]\u001b[0m Trial 4 finished with value: 2.975952578128321 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.986800733662444.\u001b[0m\n",
      "Precision: 0.9988249118683902 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9941451990632318 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9953216374269006 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 18:54:33,914]\u001b[0m Trial 5 finished with value: 2.9840264709010142 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 21}. Best is trial 0 with value: 2.986800733662444.\u001b[0m\n",
      "Precision: 0.9976662777129521 \n",
      "Recall: 0.9976662777129521 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9941588785046729 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9976553341148886 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5467948717948717\n",
      "\u001b[32m[I 2021-12-05 18:54:36,047]\u001b[0m Trial 6 finished with value: 2.9875416415518976 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9953488372093023 \n",
      "Recall: 0.9988331388564761 \n",
      "Aging Rate: 0.5512820512820513\n",
      "Precision: 0.9884259259259259 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5538461538461539\n",
      "Precision: 0.9918414918414918 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 18:54:37,236]\u001b[0m Trial 7 finished with value: 2.9798546328394 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 18}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9930394431554525 \n",
      "Recall: 0.9988331388564761 \n",
      "Aging Rate: 0.5525641025641026\n",
      "Precision: 0.9976608187134502 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9941383352872216 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.5467948717948717\n",
      "\u001b[32m[I 2021-12-05 18:54:39,370]\u001b[0m Trial 8 finished with value: 2.9844470461009713 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 21}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9953325554259043 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9964994165694282 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9941656942823804 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 18:54:41,508]\u001b[0m Trial 9 finished with value: 2.985997666277713 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 21}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9883720930232558 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5512820512820513\n",
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 18:54:41,642]\u001b[0m Trial 10 finished with value: 2.9763118395448593 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9953325554259043 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9976498237367802 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5455128205128205\n",
      "Precision: 0.9976608187134502 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 18:54:42,832]\u001b[0m Trial 11 finished with value: 2.9875388724852954 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9941724941724942 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9941927990708479 \n",
      "Recall: 0.9988331388564761 \n",
      "Aging Rate: 0.551923076923077\n",
      "Precision: 0.9906868451688009 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5506410256410257\n",
      "\u001b[32m[I 2021-12-05 18:54:44,020]\u001b[0m Trial 12 finished with value: 2.9817562680818406 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9953216374269006 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9929906542056075 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9964830011723329 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5467948717948717\n",
      "\u001b[32m[I 2021-12-05 18:54:46,172]\u001b[0m Trial 13 finished with value: 2.982084454246401 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9941724941724942 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.55\n",
      "Precision: 0.9988276670574443 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5467948717948717\n",
      "Precision: 0.9929988331388565 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5493589743589744\n",
      "\u001b[32m[I 2021-12-05 18:54:47,813]\u001b[0m Trial 14 finished with value: 2.9848316905282437 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9918509895227008 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5506410256410257\n",
      "Precision: 0.9941656942823804 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9883449883449883 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 18:54:49,950]\u001b[0m Trial 15 finished with value: 2.975517660857728 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 6 with value: 2.9875416415518976.\u001b[0m\n",
      "Precision: 0.9976608187134502 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9988235294117647 \n",
      "Recall: 0.9906651108518086 \n",
      "Aging Rate: 0.5448717948717948\n",
      "Precision: 0.9964912280701754 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 18:54:51,146]\u001b[0m Trial 16 finished with value: 2.988704837650291 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9929988331388565 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5493589743589744\n",
      "Precision: 0.9964953271028038 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9976580796252927 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5474358974358975\n",
      "\u001b[32m[I 2021-12-05 18:54:52,804]\u001b[0m Trial 17 finished with value: 2.9856005208603484 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 7, 'max_depth': 12}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9907407407407407 \n",
      "Recall: 0.9988331388564761 \n",
      "Aging Rate: 0.5538461538461539\n",
      "Precision: 0.9907084785133565 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.551923076923077\n",
      "Precision: 0.9884259259259259 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5538461538461539\n",
      "\u001b[32m[I 2021-12-05 18:54:52,941]\u001b[0m Trial 18 finished with value: 2.9768051337372845 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9941520467836257 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9953325554259043 \n",
      "Recall: 0.9953325554259043 \n",
      "Aging Rate: 0.5493589743589744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.993006993006993 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.55\n",
      "\u001b[32m[I 2021-12-05 18:54:54,597]\u001b[0m Trial 19 finished with value: 2.982104470712221 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9918509895227008 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5506410256410257\n",
      "Precision: 0.9964664310954063 \n",
      "Recall: 0.9871645274212368 \n",
      "Aging Rate: 0.5442307692307692\n",
      "Precision: 0.9906976744186047 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5512820512820513\n",
      "\u001b[32m[I 2021-12-05 18:54:56,747]\u001b[0m Trial 20 finished with value: 2.9778420353531403 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9941588785046729 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9941792782305006 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5506410256410257\n",
      "Precision: 0.9929906542056075 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5487179487179488\n",
      "\u001b[32m[I 2021-12-05 18:54:57,942]\u001b[0m Trial 21 finished with value: 2.9813292811950594 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9941520467836257 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5480769230769231\n",
      "Precision: 0.9941927990708479 \n",
      "Recall: 0.9988331388564761 \n",
      "Aging Rate: 0.551923076923077\n",
      "Precision: 0.9964912280701754 \n",
      "Recall: 0.9941656942823804 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 18:54:59,129]\u001b[0m Trial 22 finished with value: 2.9848343176611625 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9953161592505855 \n",
      "Recall: 0.9918319719953326 \n",
      "Aging Rate: 0.5474358974358975\n",
      "Precision: 0.9941588785046729 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5487179487179488\n",
      "Precision: 0.9929742388758782 \n",
      "Recall: 0.9894982497082847 \n",
      "Aging Rate: 0.5474358974358975\n",
      "\u001b[32m[I 2021-12-05 18:55:00,325]\u001b[0m Trial 23 finished with value: 2.979742536034916 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Precision: 0.9941792782305006 \n",
      "Recall: 0.9964994165694282 \n",
      "Aging Rate: 0.5506410256410257\n",
      "Precision: 0.9976553341148886 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5467948717948717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:55:01,606]\u001b[0m A new study created in memory with name: no-name-ad3a535c-7032-4da5-8e43-b63285e850fb\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9953216374269006 \n",
      "Recall: 0.9929988331388565 \n",
      "Aging Rate: 0.5480769230769231\n",
      "\u001b[32m[I 2021-12-05 18:55:01,522]\u001b[0m Trial 24 finished with value: 2.9856031941305736 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 16 with value: 2.988704837650291.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset7 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd384bd0743d4a289bc9c3ffbd8da1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9985754985754985 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.4992887624466572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9957446808510638 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 18:55:03,672]\u001b[0m Trial 0 finished with value: 2.9900601551964 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 18}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9957325746799431 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:03,814]\u001b[0m Trial 1 finished with value: 2.9886188405925247 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9985632183908046 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "\u001b[32m[I 2021-12-05 18:55:03,945]\u001b[0m Trial 2 finished with value: 2.9824110867060902 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9985569985569985 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9971469329529244 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:55:04,075]\u001b[0m Trial 3 finished with value: 2.984321485736006 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 18:55:05,266]\u001b[0m Trial 4 finished with value: 2.988671180926262 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 15}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.492176386913229\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:05,397]\u001b[0m Trial 5 finished with value: 2.982891986506504 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'l2'}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9957325746799431 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 18:55:07,063]\u001b[0m Trial 6 finished with value: 2.987679941692633 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 21}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9985775248933144 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 18:55:08,242]\u001b[0m Trial 7 finished with value: 2.989599383505715 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 27, 'max_depth': 9}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:09,898]\u001b[0m Trial 8 finished with value: 2.9867772037061897 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 27, 'max_depth': 21}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9928977272727273 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 18:55:10,039]\u001b[0m Trial 9 finished with value: 2.9848376353406123 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 0 with value: 2.9900601551964.\u001b[0m\n",
      "Precision: 0.9957325746799431 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:12,192]\u001b[0m Trial 10 finished with value: 2.990519526703737 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 3}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:14,334]\u001b[0m Trial 11 finished with value: 2.986291035964445 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9900990099009901 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "\u001b[32m[I 2021-12-05 18:55:16,484]\u001b[0m Trial 12 finished with value: 2.983456816350652 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 15}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9901408450704225 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5049786628733998\n",
      "\u001b[32m[I 2021-12-05 18:55:18,634]\u001b[0m Trial 13 finished with value: 2.9863309732116132 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 3}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9971509971509972 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9915373765867419 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5042674253200569\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "\u001b[32m[I 2021-12-05 18:55:20,779]\u001b[0m Trial 14 finished with value: 2.9849125648354544 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9915254237288136 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5035561877667141\n",
      "Precision: 0.9943422913719944 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9929178470254958 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5021337126600285\n",
      "\u001b[32m[I 2021-12-05 18:55:22,454]\u001b[0m Trial 15 finished with value: 2.9844345663108505 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 18}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9915134370579916 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9957446808510638 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.992867332382311 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:55:24,608]\u001b[0m Trial 16 finished with value: 2.9820087165052924 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 9}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9929178470254958 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:26,761]\u001b[0m Trial 17 finished with value: 2.9891171999696673 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9943100995732574 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:55:28,433]\u001b[0m Trial 18 finished with value: 2.9876759235311865 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9914893617021276 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "\u001b[32m[I 2021-12-05 18:55:30,581]\u001b[0m Trial 19 finished with value: 2.9829893854783065 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 7, 'max_depth': 18}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9957507082152974 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9929178470254958 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9985734664764622 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:55:32,254]\u001b[0m Trial 20 finished with value: 2.989123889300361 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 12}. Best is trial 10 with value: 2.990519526703737.\u001b[0m\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9971550497866287 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:55:33,451]\u001b[0m Trial 21 finished with value: 2.9905356454018297 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 21 with value: 2.9905356454018297.\u001b[0m\n",
      "Precision: 0.9943342776203966 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9957507082152974 \n",
      "Recall: 1.0 \n",
      "Aging Rate: 0.5021337126600285\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:34,643]\u001b[0m Trial 22 finished with value: 2.9905477344253995 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 22 with value: 2.9905477344253995.\u001b[0m\n",
      "Precision: 0.9929278642149929 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5028449502133713\n",
      "Precision: 0.9971590909090909 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.9957386363636364 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5007112375533428\n",
      "\u001b[32m[I 2021-12-05 18:55:35,840]\u001b[0m Trial 23 finished with value: 2.988653760849566 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 22 with value: 2.9905477344253995.\u001b[0m\n",
      "Precision: 0.9943262411347518 \n",
      "Recall: 0.9971550497866287 \n",
      "Aging Rate: 0.5014224751066856\n",
      "Precision: 0.9957446808510638 \n",
      "Recall: 0.9985775248933144 \n",
      "Aging Rate: 0.5014224751066856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:55:37,123]\u001b[0m A new study created in memory with name: no-name-b7a717b5-9b5a-4532-8efc-2e5974115163\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9957325746799431 \n",
      "Recall: 0.9957325746799431 \n",
      "Aging Rate: 0.5\n",
      "\u001b[32m[I 2021-12-05 18:55:37,035]\u001b[0m Trial 24 finished with value: 2.987690714230468 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 6}. Best is trial 22 with value: 2.9905477344253995.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset8 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e93909413f14cc1a5bf877e8ee96d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9971098265895953 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.492176386913229\n",
      "Precision: 0.9885877318116976 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9985693848354793 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:55:37,193]\u001b[0m Trial 0 finished with value: 2.9762348611621157 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 0 with value: 2.9762348611621157.\u001b[0m\n",
      "Precision: 0.9928977272727273 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.5007112375533428\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 18:55:37,330]\u001b[0m Trial 1 finished with value: 2.9781451304090107 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 1 with value: 2.9781451304090107.\u001b[0m\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9928774928774928 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9942938659058488 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:55:37,465]\u001b[0m Trial 2 finished with value: 2.9781425251881424 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 1 with value: 2.9781451304090107.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9957142857142857 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9913916786226685 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:37,606]\u001b[0m Trial 3 finished with value: 2.9800241753711507 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'none'}. Best is trial 3 with value: 2.9800241753711507.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9928774928774928 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.997134670487106 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.994269340974212 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 18:55:37,746]\u001b[0m Trial 4 finished with value: 2.9790895187768456 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'none'}. Best is trial 3 with value: 2.9800241753711507.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9971428571428571 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49786628733997157\n",
      "Precision: 0.9985652797704447 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:39,400]\u001b[0m Trial 5 finished with value: 2.988129748933192 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9914407988587732 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49857752489331436\n",
      "\u001b[32m[I 2021-12-05 18:55:41,060]\u001b[0m Trial 6 finished with value: 2.9786138921039513 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 21}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9957264957264957 \n",
      "Recall: 0.9943100995732574 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 0.9928571428571429 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 18:55:41,198]\u001b[0m Trial 7 finished with value: 2.9800391430455804 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.994261119081779 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.992867332382311 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49857752489331436\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:55:41,331]\u001b[0m Trial 8 finished with value: 2.9786058382721095 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'none'}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9970972423802612 \n",
      "Recall: 0.9772403982930299 \n",
      "Aging Rate: 0.4900426742532006\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9942857142857143 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 18:55:41,470]\u001b[0m Trial 9 finished with value: 2.9757414212361772 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'none'}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9985528219971056 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.4914651493598862\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:43,593]\u001b[0m Trial 10 finished with value: 2.981458701354629 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:55:45,747]\u001b[0m Trial 11 finished with value: 2.983837770834318 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.994277539341917 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:47,423]\u001b[0m Trial 12 finished with value: 2.981458874085582 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 3}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9957081545064378 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.4971550497866287\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:49,571]\u001b[0m Trial 13 finished with value: 2.9809765680522893 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 9}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.99568345323741 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9900426742532006 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.5\n",
      "Precision: 0.9956772334293948 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.49359886201991465\n",
      "\u001b[32m[I 2021-12-05 18:55:50,764]\u001b[0m Trial 14 finished with value: 2.9733774895464804 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 7, 'max_depth': 9}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971387696709585 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4971550497866287\n",
      "\u001b[32m[I 2021-12-05 18:55:51,960]\u001b[0m Trial 15 finished with value: 2.982884039187468 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 32, 'max_depth': 3}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.9928876244665719 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 1.0 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.492176386913229\n",
      "Precision: 0.995702005730659 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.49644381223328593\n",
      "\u001b[32m[I 2021-12-05 18:55:54,088]\u001b[0m Trial 16 finished with value: 2.9857548696336207 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 9}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.984352773826458 \n",
      "Aging Rate: 0.492176386913229\n",
      "Precision: 0.99002849002849 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4992887624466572\n",
      "Precision: 1.0 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:55:55,759]\u001b[0m Trial 17 finished with value: 2.9814983674632796 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9956958393113343 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 18:55:57,896]\u001b[0m Trial 18 finished with value: 2.9814533421545497 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 17, 'max_depth': 9}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 1.0 \n",
      "Recall: 0.9800853485064012 \n",
      "Aging Rate: 0.4900426742532006\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 18:55:59,550]\u001b[0m Trial 19 finished with value: 2.981951990769184 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 32, 'max_depth': 15}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985590778097982 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49359886201991465\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9956896551724138 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 18:56:00,745]\u001b[0m Trial 20 finished with value: 2.9814559966762713 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 22, 'max_depth': 6}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.998567335243553 \n",
      "Recall: 0.9914651493598862 \n",
      "Aging Rate: 0.49644381223328593\n",
      "Precision: 0.9985632183908046 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "\u001b[32m[I 2021-12-05 18:56:02,881]\u001b[0m Trial 21 finished with value: 2.9852656844593874 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9956709956709957 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.49288762446657186\n",
      "Precision: 0.9971264367816092 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.4950213371266003\n",
      "Precision: 0.9971305595408895 \n",
      "Recall: 0.9886201991465149 \n",
      "Aging Rate: 0.4957325746799431\n",
      "\u001b[32m[I 2021-12-05 18:56:05,016]\u001b[0m Trial 22 finished with value: 2.979060576928807 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 6}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9899569583931134 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.4957325746799431\n",
      "Precision: 1.0 \n",
      "Recall: 0.9815078236130867 \n",
      "Aging Rate: 0.4907539118065434\n",
      "Precision: 1.0 \n",
      "Recall: 0.9829302987197724 \n",
      "Aging Rate: 0.4914651493598862\n",
      "\u001b[32m[I 2021-12-05 18:56:07,154]\u001b[0m Trial 23 finished with value: 2.9752866209107243 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 12}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Precision: 0.9985611510791367 \n",
      "Recall: 0.9871977240398293 \n",
      "Aging Rate: 0.49431009957325744\n",
      "Precision: 0.9971223021582734 \n",
      "Recall: 0.9857752489331437 \n",
      "Aging Rate: 0.49431009957325744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "\u001b[32m[I 2021-12-05 18:56:08,885]\u001b[0m A new study created in memory with name: no-name-8a2ffbb2-a082-4508-85c4-ee25231179fa\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9942857142857143 \n",
      "Recall: 0.9900426742532006 \n",
      "Aging Rate: 0.49786628733997157\n",
      "\u001b[32m[I 2021-12-05 18:56:08,804]\u001b[0m Trial 24 finished with value: 2.9809846607574744 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 17, 'max_depth': 6}. Best is trial 5 with value: 2.988129748933192.\u001b[0m\n",
      "Sampler is TPESampler\n",
      "Dataset9 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71b9d4ce34b4540aedfc9b907d0347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9305555555555556 \n",
      "Recall: 0.9436619718309859 \n",
      "Aging Rate: 0.09302325581395349\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08139534883720931\n",
      "\u001b[32m[I 2021-12-05 18:56:10,955]\u001b[0m Trial 0 finished with value: 2.8245584618824053 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 21}. Best is trial 0 with value: 2.8245584618824053.\u001b[0m\n",
      "Precision: 0.9846153846153847 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.09043927648578812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9846153846153847 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08397932816537468\n",
      "\u001b[32m[I 2021-12-05 18:56:12,612]\u001b[0m Trial 1 finished with value: 2.809534127843987 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 22, 'max_depth': 15}. Best is trial 0 with value: 2.8245584618824053.\u001b[0m\n",
      "Precision: 0.9838709677419355 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 1.0 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9253731343283582 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08656330749354005\n",
      "\u001b[32m[I 2021-12-05 18:56:12,747]\u001b[0m Trial 2 finished with value: 2.8033458333050785 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 0 with value: 2.8245584618824053.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07622739018087855\n",
      "Precision: 1.0 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07493540051679587\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "\u001b[32m[I 2021-12-05 18:56:12,876]\u001b[0m Trial 3 finished with value: 2.8297935762724493 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 3 with value: 2.8297935762724493.\u001b[0m\n",
      "Precision: 0.9393939393939394 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08527131782945736\n",
      "Precision: 0.9411764705882353 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 1.0 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07751937984496124\n",
      "\u001b[32m[I 2021-12-05 18:56:14,539]\u001b[0m Trial 4 finished with value: 2.793619709941168 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 6}. Best is trial 3 with value: 2.8297935762724493.\u001b[0m\n",
      "Precision: 0.9838709677419355 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 1.0 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.082687338501292\n",
      "Precision: 1.0 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08010335917312661\n",
      "\u001b[32m[I 2021-12-05 18:56:14,669]\u001b[0m Trial 5 finished with value: 2.8671815841284265 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9516129032258065 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9848484848484849 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 18:56:14,804]\u001b[0m Trial 6 finished with value: 2.8202983514205684 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 1.0 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9661016949152542 \n",
      "Recall: 0.8028169014084507 \n",
      "Aging Rate: 0.07622739018087855\n",
      "\u001b[32m[I 2021-12-05 18:56:14,932]\u001b[0m Trial 7 finished with value: 2.806847755049329 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9027777777777778 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.09302325581395349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9375 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.082687338501292\n",
      "\u001b[32m[I 2021-12-05 18:56:16,958]\u001b[0m Trial 8 finished with value: 2.7455939339742153 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 500, 'min_samples_split': 22, 'max_depth': 3}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9692307692307692 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9833333333333333 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07751937984496124\n",
      "Precision: 1.0 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.07881136950904392\n",
      "\u001b[32m[I 2021-12-05 18:56:17,092]\u001b[0m Trial 9 finished with value: 2.827530997953533 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.01, 'penalty': 'none'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.984375 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.082687338501292\n",
      "Precision: 0.9666666666666667 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07751937984496124\n",
      "Precision: 1.0 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08397932816537468\n",
      "\u001b[32m[I 2021-12-05 18:56:17,219]\u001b[0m Trial 10 finished with value: 2.84060054773083 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9692307692307692 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 1.0 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08010335917312661\n",
      "\u001b[32m[I 2021-12-05 18:56:17,348]\u001b[0m Trial 11 finished with value: 2.8315625949428767 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9701492537313433 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08656330749354005\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9848484848484849 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 18:56:17,472]\u001b[0m Trial 12 finished with value: 2.8510883099806414 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07622739018087855\n",
      "Precision: 1.0 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07622739018087855\n",
      "Precision: 1.0 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 18:56:17,598]\u001b[0m Trial 13 finished with value: 2.863849765258216 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9672131147540983 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 1.0 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9253731343283582 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08656330749354005\n",
      "\u001b[32m[I 2021-12-05 18:56:17,728]\u001b[0m Trial 14 finished with value: 2.8016302693413557 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9848484848484849 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 18:56:17,852]\u001b[0m Trial 15 finished with value: 2.8310454436060533 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 1.0 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07751937984496124\n",
      "Precision: 0.9836065573770492 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.07881136950904392\n",
      "\u001b[32m[I 2021-12-05 18:56:17,975]\u001b[0m Trial 16 finished with value: 2.832949121566069 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9682539682539683 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9848484848484849 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 18:56:18,101]\u001b[0m Trial 17 finished with value: 2.8407820661341785 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9428571428571428 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.09043927648578812\n",
      "Precision: 0.9705882352941176 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.08785529715762273\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "\u001b[32m[I 2021-12-05 18:56:19,297]\u001b[0m Trial 18 finished with value: 2.842513030251224 and parameters: {'meta_learner': 'ExtraTrees', 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 21}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.984375 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.082687338501292\n",
      "Precision: 0.9672131147540983 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9666666666666667 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07751937984496124\n",
      "\u001b[32m[I 2021-12-05 18:56:19,428]\u001b[0m Trial 19 finished with value: 2.7905736101490546 and parameters: {'meta_learner': 'LogisticRegression', 'C': 0.1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.8309859154929577 \n",
      "Aging Rate: 0.07622739018087855\n",
      "Precision: 0.9692307692307692 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.08397932816537468\n",
      "Precision: 0.9428571428571428 \n",
      "Recall: 0.9295774647887324 \n",
      "Aging Rate: 0.09043927648578812\n",
      "\u001b[32m[I 2021-12-05 18:56:19,564]\u001b[0m Trial 20 finished with value: 2.824021049373162 and parameters: {'meta_learner': 'LogisticRegression', 'C': 100, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.984375 \n",
      "Recall: 0.8873239436619719 \n",
      "Aging Rate: 0.082687338501292\n",
      "Precision: 1.0 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9846153846153847 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08397932816537468\n",
      "\u001b[32m[I 2021-12-05 18:56:19,695]\u001b[0m Trial 21 finished with value: 2.861956031058144 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 1.0 \n",
      "Recall: 0.8169014084507042 \n",
      "Aging Rate: 0.07493540051679587\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.9846153846153847 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08397932816537468\n",
      "\u001b[32m[I 2021-12-05 18:56:19,826]\u001b[0m Trial 22 finished with value: 2.843011344419795 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9841269841269841 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08139534883720931\n",
      "Precision: 0.967741935483871 \n",
      "Recall: 0.8450704225352113 \n",
      "Aging Rate: 0.08010335917312661\n",
      "Precision: 0.9701492537313433 \n",
      "Recall: 0.9154929577464789 \n",
      "Aging Rate: 0.08656330749354005\n",
      "\u001b[32m[I 2021-12-05 18:56:19,951]\u001b[0m Trial 23 finished with value: 2.825946387861935 and parameters: {'meta_learner': 'LogisticRegression', 'C': 1, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Precision: 0.9393939393939394 \n",
      "Recall: 0.8732394366197183 \n",
      "Aging Rate: 0.08527131782945736\n",
      "Precision: 1.0 \n",
      "Recall: 0.8591549295774648 \n",
      "Aging Rate: 0.07881136950904392\n",
      "Precision: 0.9696969696969697 \n",
      "Recall: 0.9014084507042254 \n",
      "Aging Rate: 0.08527131782945736\n",
      "\u001b[32m[I 2021-12-05 18:56:20,079]\u001b[0m Trial 24 finished with value: 2.817328211694409 and parameters: {'meta_learner': 'LogisticRegression', 'C': 10, 'penalty': 'l2'}. Best is trial 5 with value: 2.8671815841284265.\u001b[0m\n",
      "Sampler is TPESampler\n"
     ]
    }
   ],
   "source": [
    "best_paramC, _ = all_optuna(num_set = 10, \n",
    "                            all_data = train_firstC, \n",
    "                            mode = 'C', \n",
    "                            TPE_multi = True, \n",
    "                            n_iter = 25,\n",
    "                            filename = f'runhist_array_m2m4_m5_3criteria_StackingCV1',\n",
    "                            creator = stackingCV_creator\n",
    ")\n",
    "\n",
    "# best_paramR, _ = all_optuna(num_set = 10, \n",
    "#                             all_data = train_firstR, \n",
    "#                             mode = 'R', \n",
    "#                             TPE_multi = True, \n",
    "#                             n_iter = 10,\n",
    "#                             filename = f'runhist_array_4criteria_m2m5_StackingCV1',\n",
    "#                             creator = stackingCV_creator\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection by feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T03:08:14.561590Z",
     "start_time": "2021-12-02T03:08:06.871926Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_importance(train_firstC['set7'], mode = 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:56:30.856032Z",
     "start_time": "2021-12-05T10:56:21.976680Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfe0a14bd56481aad1c4405a5249a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset 0:\n",
      "Precision: 0.0 \n",
      "Recall: 0.0 \n",
      "Aging Rate: 6.166622129951284e-05\n",
      "\n",
      " Dataset 1:\n",
      "Precision: 0.0016052974816895757 \n",
      "Recall: 0.3137254901960784 \n",
      "Aging Rate: 0.20487574256408148\n",
      "\n",
      " Dataset 2:\n",
      "Precision: 0.0011611873140285943 \n",
      "Recall: 0.3137254901960784 \n",
      "Aging Rate: 0.2832329544286625\n",
      "\n",
      " Dataset 3:\n",
      "Precision: 0.0011845534233593934 \n",
      "Recall: 0.49019607843137253 \n",
      "Aging Rate: 0.4338218668420728\n",
      "\n",
      " Dataset 4:\n",
      "Precision: 0.0016590626296142678 \n",
      "Recall: 0.39215686274509803 \n",
      "Aging Rate: 0.24779543258854242\n",
      "\n",
      " Dataset 5:\n",
      "Precision: 0.0014359563469270534 \n",
      "Recall: 0.19607843137254902 \n",
      "Aging Rate: 0.14314785504326913\n",
      "\n",
      " Dataset 6:\n",
      "Precision: 0.0012381026077536177 \n",
      "Recall: 0.3137254901960784 \n",
      "Aging Rate: 0.26563752595120144\n",
      "\n",
      " Dataset 7:\n",
      "Precision: 0.0013352860096485183 \n",
      "Recall: 0.6078431372549019 \n",
      "Aging Rate: 0.47721433122983\n",
      "\n",
      " Dataset 8:\n",
      "Precision: 0.0014099953000156666 \n",
      "Recall: 0.35294117647058826 \n",
      "Aging Rate: 0.26241032703652695\n",
      "\n",
      " Dataset 9:\n",
      "Precision: 0.0012319408668383918 \n",
      "Recall: 0.43137254901960786 \n",
      "Aging Rate: 0.3670784599889001\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAIECAYAAAAtj7JSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAADaj0lEQVR4nOzdd3hURRfA4d8kAULoTURQQHovoQvSlCaCiICIIEoRkSYCgp9IETvSpEkTQaVIEQtNmrSE3jtSQ5MaSnp2vj9mwZACIdndu5uc93nyQHbv3nt2k+zec+fMGaW1RgghhBBCCCGE8FReVgcghBBCCCGEEEIkhyS2QgghhBBCCCE8miS2QgghhBBCCCE8miS2QgghhBBCCCE8miS2QgghhBBCCCE8miS2QgghhBBCCCE8miS2QgiRSEqpl5RS65VS/yqlQpVSp5VSvyqlGsXYpo5SaqhSymnvr/b9a6WUzwO2KWDfpqOz4ohxrJxKqc+VUvuVUneUUiFKqX1KqS+UUnmUUhXssQx4wD4+UUrZlFIFH3KsNEqp7kqpTUqpG0qpcKXUSaXUDKVUxRjbrVNKrXPg00w0pdQppdTMWLe9aH9NwuyvRVZXx6iU+lYp9buD91nH/nyec+R+raCUymd/jQLsv8NaKVUgnu1aKKUuKqUyWhCmEEKIBEhiK4QQiaCU6gUsBo4BnYAXgBH2u+vF2LQOMATr318vANWBP515EKVUSWA30AGYDTQDXgR+AFoCE7XWu4B9QPsE9qGA14ENWuuTDzhWBmA18A2wFWgHNMD8HAra73MHLYBP7n5jvwDxE3AOE2914BbQ3f7ldEqpQsDbwDBXHM9DFQZaA9eBDQ/Y7lfgItDfBTEJIYRIpASv9gshhLhPP+BXrXWnGLetAaY6c3Q2qbTW4UCgM49hT9gWAmFADa31vzHuXq2UGgM0tn//AzBSKVXBnujG9CxQgBjJYALGAlWBOlrrgBi3/w1MV0q1SNITcbB4nl9eIBMwX2u9PsbtBx11TKVUOvvPPCF9gD1a6+2OOmYKtF5rnRtAKdUZcxEiDq21VkpNAT5RSn2utQ5zZZBCCCHi53YnY0II4aayY0Zp4tBa28CUCGNGawEi7aWM+u52SqlhSqmdSqlgpdQVpdQapVS12PtTSuVSSk1USp21l9qeVUrNVkqlSyg4pVQjpdRtpdR4pZRXfKXISqmZSqkge2nwBnu55TGlVLd49vecUmqXvXT2uFKqs/3xp2Js9jJQHBgYK6m9+7pEaa3vlr7+BEQT/6htByAUWPCA55cH6AhMjZXUxjze4gc83lcpNdpeLn3bXkr6u1KqeKztHldK/aCUOm9/7S8opf5QSj1mv9/HXjb9j/21uaKU2qiUqhljH/dKke2/E6fsd023/0zW2e+LU4qsTFn3JKXUOfvxDyulusbapqN9P88qpX5RSt0AtjzguafDjIj/HOv2jPbS2zP2Y11SSq2K+ZrYn+8HSqmD9ud7WSm1PPbrBvjZf/eu2Lf5USmVNdbxfJRSg+zPKdz+Gn+jlPKNsc3d39tuypS3X1RK3bLvz08pVVgptcL+MzyulHojnudbTin1m1LqujJTBjYppWol9PrcdffvOJHmA1kxfwNCCCHcgIzYCiFE4mwF3lBKnQCWaK2PxrPNNCAfplS5JiaRiykvMBoIAjJgko31SqlKWuu9AEqpbMBmTCI9AtgLPAY0B9ICcUbllFId7Mf+RGv9if22hJ5HZkyCMwYYDrwJTFJKHdFar7U/tiSmhHkr8Kr9uIOBLEDMk//n7M9xaUIHu0trfVEptQJ4TSnVX2sdbT+WL/AKsFhrffMBu6gLeAO/PexYCUiHGTUdgSnTzo4pAw5UShXXWt+9aDEbyI8pMz0L5AbqA372+z8A3gP+hynBzgxUsu8vPtOA/cAv9mP/CcT7PJVSmYFNQHpgKHASaIj5+aTTWn8b6yE/AXMwr9+DPs+rYZKw2OW1ozGl4x9iSuxzAM/Yt71rLvAS5vdlFeCLGWHPAxyOsd1Y4A/gNaAY8BXmdyNm4vkjpkz9S8zveAnMKH0BTNl6TIOAdfbHl7TvzwZUAKYCI4F3gO+VUtu11gcAlJlnvQHYBXQBQoBuwCqlVA2t9Y4EX6VHoLW+opQ6BDQi1gUDIYQQFtFay5d8yZd8yddDvoCimCRT27+uYJKKBrG2G2q/3+ch+/PGJCNHgLExbh+OSQgqPOCx944BDAAigc6xtilg36ZjjNtm2m+rG+O2dPbnMiXGbT8DlwG/GLflwZQcn4px2zLgwiO8hq3tx28c47ZX7bc1eMhjP7BvVyyRx1oHrHvI6++Hmev6XozbbwO9HvC4P4BFDzn2KWBmjO8Lx/5ZxBcj5uJBGFAk1nZT7T8jH/v3He37G53I1+IDTFKYNtbt+4FRD3hcPftxHvR61LFv80Os28fbn4uyf1/Lvl2HWNu1s99ePtbv7ZpY2y2y3/56jNuyAVHAkBi3rQYOxXyu9p/1IcxUgsT+rna2H6/AA7aZDRxN7D7lS77kS77ky7lfUooshBCJoM0IbQWgNvApZrSuBbBCKfVRYvZhL+9dq5S6ijkhj8QkzMVibNYA2KbjztOMz2hMM6BXtNbTEvlUQrR9ZBbuzcU9BjwVY5tqwFKtdUiM7S5gRtmSYwlwg/vLkTsA5zGjgU6llGqtlNpiL92NAu4AGbn/9d8G9FdK9VZKlVFxh763AU2UUp8qpWoqpdI6MMRGmJLik/ayXR9l5jGvwIymloy1fYKl17E8AdzUWkfEun0b0FEp9aFSqpJSyjvW/Q0wyd3URBwjdpOyfZiLJrnt3zcCIoCFsZ7bSvv9z8Z6/LJY398dHV5x9wat9XXgX+BJAKVUeszf5y+ALcYxFOb3K/Yxkusy5rUVQgjhBiSxFUKIRNJaR2ut12utP9JaPwc8jTmBH2IvIU6QvURyKWZEsBMmeawM7MGUd96VA1OqnBhtgQM8WlJ4PZ7bwmPFkAeTMMR2Kdb3Z4FcSim/eLaNw55EzwNeUkplUkrlBp4HftQPn9941v5v/sQcKzal1Iv2Yx/ClMtWxbz+l7n/ubfBlDsPwIzQn1NKfaz+axD2GWYedTNMyetVpdT3SqmcSYkrlscwyVdkrK9f7PfniLX9hUTu15d4StiBnsB3wFuYJPdfZeYh3/155gCuaa1DE3GMa7G+v3u8u6/tY5iS9tvc/9zu/p7Ffm6xf08jHnD73WNkx4zODibua9gDyKYc2+gtlPt/d4QQQlhI5tgKIUQSaa3PK6WmYeYXFsHMSU1IS8wo4cta68i7N9oT4hsxtruCmYubGPUxI17LlFJNtNa3HyH8B7mASURiyx3r+1WYeYyNMd2RE+MHzLIzr2DmcvoAsxLxuHWYEu0X+W+U71G8ChzXWne8e4NSKg2x5sZq0wTrXeBdpVQxzBzPYZgEeJL9Z/cl8KVS6nGgKTAKU9bcJglxxXQVk+j1TuD+I7G+1/FuFf9+41x4sf++DAIGKaXyY34mX2CSxQ8wv4vZlVLpE5ncPiyGMExJcnzOJ3P/YP6ObMAEEvidSsQFlEeRHfO8hBBCuAEZsRVCiERQSj2ZwF13u8PebT50d6Qqfazt/DCJWcwuyfW4vwQYTNJWRSlVLhFhHcDMcSwCLFdKZUrEYxIjEFNue28kVpmuxM/E2m4RJtn6UimVK/ZO7KWgL8S8TZuOxkcx5cgdgB3a3vjnQbTW5zFzhLsqparHt41S6qUH7MIPc2EhpvaYEb6EjnlEa/0hZpSwdDz3X7SXgK+K7/4kWI75fTqjtd4ez9etJO73MJBGKZUvoQ201qe11t9gKhDuPpeVmDLezkk8bkzLMaObWRJ4bslObLXWdzCj6OWAnfEdJ7nHiKUgcS82CCGEsIiM2AohROLsV0qtxcxrPInphtsE03F1vtb6jH27u2uTvq+UWgZE20+ol2PWEp2plPoeM7d2MHAu1nFGY0plVymlRmASjZyYrsjdYic3WutDSqk6wFpMctsoGQnQXSMwo3crlFIjMXMlB2NKke+NeGmto5RSLwN/AbuVUmOBu8lDOaArJqmKPf9yFqYbrgJ6PUJcfTCv22ql1GRMQnkbUxLeDtOd+NcEHrscUwI9GtMAyt9+7Bt3N1BKZbHv8yd73JGY1z0b9lFipdQSTPn4TkzCWwEzf/S7R3geCRmNGfXdYI/zCKZ7dnGglta6eRL3e3ft3CrEKHNXSgVgyq73YV7H2pif2w8AWuu1SqmFwCj7hZ01QBpMufSfWut1iQ1Aa71OKTUHWKCUGoWpbrBhmkU1AT7Q8Xcaf1R9Mc93hVJqOqb6ICdQEfDWWg980IOVUq/Y/+tv/7exUuoycFlr/XeM7RSmlH2SA2IWQgjhAJLYCiFE4nyAOQEfjinJjcaMPA7ELIVy1x/ARMxSMh9jkjeltV6hlOqFOfFuielI2wG4r/GU1vqGUuoZTHI5EDP38BImqYjd/OfuY44opWpjktuVSqmGyXmiWuuD9pHWrzHrdZ7DlN82wiQisbctB/TDdOsdan/OxzAjumPjOcRszOsYheksndi4biul6mMS5naYkURfe3yrgfcf8PCpmCZDb2FKobdhyppjNmAKwySsXTBzeW2Y5LKd1nqJfZv1QCtMubIfcAazFM2niX0eD3h+wUqpGpjfmw8wJek37DEkttQ7vv2eUkptxTzfRTHuWo/pVD0Qcz5wAtMhelyMbV61x/IG5sJCMOa1S2yzsphex8zrfQuzXFI4poP0CuLO304SrfVOpVRlzDzocZglqi5jfq6TE7GLX2J9P9H+79+Y6oi7amBKkecmJ14hhBCOc7cNvxBCCJEgpVRG4DhmpK6T1fGIR6OU6oi5yJAnZrdrkTRKqUlAaa11QnOGhRBCuJgktkIIIeJQSn2LWd7nPGZJk96YstvKWuu9VsYmHp19KZ99wAyt9Uir4/Fk9qZhJ4BGWuv1D9teCCGEa0gpshBCiPj4YsqPc2NKoLcCz0lS65m01tFKqbcwc01F8hQA3pekVggh3IuM2AohhBBCCCGE8Giy3I8QQgghhBBCCI8mia0QQgghhBBCCI/mcXNsvby8dPr06a0OQwghhBBCCCEsERISorXWMkgZg8cltunTp+fOnTtWhyGEEEIIIYQQllBKhVodg7uRLF8IIYQQQgghhEeTxFYIIYQQQgghhEeTxFYIIYQQQgghhEeTxFYIIYQQQgghhEeTxFYIIYQQQgghhEeTxFYIIYQQQgghhEeTxFYIIYQQQgghhEeTxNaBFi9ejFKKw4cPJ2r7zp07c/DgQYcc29vbm/Lly1O6dGlefPFFbty48cDtd+/ezdKlSx1ybCGEEEIIIdxVzPPkVq1aERISkux9fvzxx6xatSrB+ydPnsysWbOSfRyReEprbXUMjyRDhgz6zp07VocRr9atW3PhwgXq16/P0KFDXXrsjBkzcvv2bQDeeOMNihYtyv/+978Et585cybbt29n/PjxrgpRCCGEEEIIl4t5ntyuXTv8/f3p27fvvfujo6Px9va2KrwkUUqFaK0zWB2HO5ERWwe5ffs2mzZtYvr06cydO/fe7Tabje7du1OqVCmaNm1KkyZNWLBgAQB16tRh+/btgPmD+9///ke5cuWoVq0aly5dAuCff/6hWrVqVK5cmY8//piMGTM+NJbq1atz7tw5ALZu3UqNGjWoUKECNWrU4MiRI0RERPDxxx8zb948ypcvz7x587hz5w5vvfUWlStXpkKFCixZssTRL5EQQgghhBCWqlWrFsePH2fdunXUrVuX1157jTJlyhAdHU3//v2pXLkyZcuW5bvvvrv3mK+++ooyZcpQrlw5Bg4cCEDHjh3vndMPHDiQkiVLUrZsWfr16wfA0KFDGTlyJGAqJatVq0bZsmVp0aIF169fB0wu8MEHH1ClShWKFi3Khg0bXPlSpDiS2DrIr7/+SqNGjShatCjZs2dn586dACxatIhTp06xb98+pk2bRkBAQLyPv3PnDtWqVWPPnj08++yzTJ06FYDevXvTu3dvtm3bxhNPPPHQOKKjo1m9ejXNmjUDoHjx4qxfv55du3YxfPhwPvzwQ9KmTcvw4cNp06YNu3fvpk2bNnz66afUq1ePbdu2sXbtWvr374+7jowLIYQQQgjxqKKioli2bBllypQBzADQp59+ysGDB5k+fTpZsmRh27ZtbNu2jalTp3Ly5EmWLVvGr7/+ypYtW9izZw8DBgy4b5/Xrl1j8eLFHDhwgL179/LRRx/FOW6HDh348ssv2bt3L2XKlGHYsGH3xbR161bGjBlz3+3i0aXIxHboUFDqv68dO8xXzNvuVgo/8cR/t/n7m9u6dr1/2/PnH37MOXPm8OqrrwLw6quvMmfOHAA2btxIq1at8PLy4vHHH6du3brxPj5t2rQ0bdoUAH9/f06dOgVAQEAArVq1AuC1115L8PihoaGUL1+eHDlycO3aNZ5//nkAgoODadWqFaVLl+a9997jwIED8T5+5cqVfPHFF5QvX546deoQFhbGmTNnHv7EhRBCCCGESKSh64aihql7XzvO72DH+R333TZ03VAAnvjmiXu3+U8xJ+pdf+9637bnbz38RP3ueXKlSpV46qmn6NSpEwBVqlShYMGCgDkXnjVrFuXLl6dq1apcvXqVY8eOsWrVKt588038/PwAyJ49+337zpw5M76+vnTu3JlFixbd2+6u4OBgbty4Qe3atQEzZXD9+vX37n/55ZeB+8//RdL4WB2AMwwd+l/iGlN804njS1qnTDFfiXX16lXWrFnD/v37UUoRHR2NUoqvvvqKxM5hTpMmDUopwExwj4qKSnwAQPr06dm9ezfBwcE0bdqUCRMm0KtXLwYPHkzdunVZvHgxp06dok6dOvE+XmvNwoULKVas2CMdVwghhBBCiMQaWmcoQ+sMjXO7HhL3nPn8+3FP1Ke8OIUpLz7CiTr/nSfHliHDf1NUtdZ8++23NGzY8L5tli9ffu8cPT4+Pj5s3bqV1atXM3fuXMaPH8+aNWsSHVu6dOmApJ3/i/ulyBFbV1uwYAEdOnTg9OnTnDp1irNnz1KwYEE2btxIzZo1WbhwITabjUuXLrFu3bpH2ne1atVYuHAhwH1zdxOSJUsWxo0bx8iRI4mMjCQ4OJi8efMCpmHUXZkyZeLWrVv3vm/YsCHffvvtvUR8165djxSnEEIIIYQQnqphw4ZMmjSJyMhIAI4ePcqdO3do0KABM2bMuNdJ+dq1a/c97vbt2wQHB9OkSRPGjBkTJ4HOkiUL2bJluzd/dvbs2fdGb4VjSWLrAHPmzKFFixb33dayZUt+/vlnWrZsSb58+ShdujRvv/02VatWJUuWLIne95gxYxg1ahRVqlThwoULiXpshQoVKFeuHHPnzmXAgAEMGjSIZ555hujo6Hvb1K1bl4MHD95rHjV48GAiIyMpW7YspUuXZvDgwYl/AYQQIhkCgwJpu6At/lP8abugLYFBgVaHJIRwtsBAaNvWzANr29Z8L4SFOnfuTMmSJalYseK98/aoqCgaNWpEs2bNqFSpEuXLl7/XEOquW7du0bRpU8qWLUvt2rUZPXp0nH3/8MMP9O/fn7Jly7J7924+/vhjVz2tVEWW+3GB27dvkzFjRq5evUqVKlXYtGkTjz/+eKIeGxISQvr06VFKMXfuXObMmSMdi4UQKcaQtUMYGTCS0MhQNBov5YWvjy/9qvdjWF1poiFEijRkCIwcCaGhZp6Ylxf4+kK/fiDNc4RIFFnuJy5JbF2gTp063Lhxg4iICAYMGEDHjh0T/dgNGzbQo0cPtNZkzZqVGTNmULhwYecFK4QQLhIYFEj9WfUJiQyJc59fGj9Wd1hNtXzVLIhMCOE0gYFQvz6ExP27x88PVq+GavJ3L8TDSGIblyS2QgghLNF2QVvmHZiHJu7nkJfyonXJ1sx5ZY4FkQkhnKZtW5g3L/6Onl5e0Lo1zJG/eyEeRhLbuGSOrRBCCEscvXY03qQWwKZtHLt2zMURCSGc7ujR+JNaAJsNjsnfvRAiaZya2CqlGimljiiljiulBiawTR2l1G6l1AGl1N/OjEcIIYT7KJq9KF4q/o8hL+VFwWwFE71kmhDCQxQtakZm4+PlZe4XQogkcFpiq5TyBiYAjYGSQFulVMlY22QFJgLNtNalgFbOikcIIYR76V2tN2m80sR7n6+PL1prGv/UmJPXT7o4MiGE0/TubRpFxcfXF3r1cm08QogUw5kjtlWA41rrE1rrCGAu0DzWNq8Bi7TWZwC01v86MR4hhBBuJE/GPHh7eePr7Xtv5NZLeeGXxo9+1fsxp+Uc6hSoQ+WplZmwdYLF0QohHKJaNWjfHpT6b+RWKfDxMV2RpXGUECKJfJy477zA2RjfBwFVY21TFEijlFoHZALGaq1nOTEmp/H29qZMmTJERUVRsGBBZs+eTdasWR22/wIFCrB9+3Zy5sxJxowZuX37tsP2LYQQVvD28mZ6s+kUyFqAsYFjOXbtGEWyF6F3td73uiEPrDmQViVbceTqEbTWHL5ymBK5SlgcuRAiWSIjoUsXuHnTzKl98klYs8bcJoQQSeS0rshKqVZAQ611Z/v37YEqWuueMbYZD1QC6gPpgQDgBa310Vj76gp0BUibNq1/eHi4U2JOjpjJ5htvvEHRokX53//+57D9S2IrhEhJvt/1Pc2KNSOHX45EP+Zs8FmqTKtCq5KtGFFvBJnTZXZihEIIp7h0CYoXNwltzpz/3f7BB3DrFkycaF1sQngQ6YoclzNLkYOAJ2N8nw84H882y7XWd7TWV4D1QLnYO9JaT9FaV9JaV/LxceYgs2NUr16dc+fOAfDPP//QqFEj/P39qVWrFocPHwbg0qVLtGjRgnLlylGuXDk2b94MwEsvvYS/vz+lSpViypQplj0HIYRwlt+P/M7Qv4c+8uOezPIkB7of4E7EHUpNLCVzb4XwRJMmQatW9ye1AP37m2WATp+2Ji4hhMdzZpa4DSiilCoInANexcypjWkJMF4p5QOkxZQqj3ZiTE4XHR3N6tWr6dSpEwBdu3Zl8uTJFClShC1bttC9e3fWrFlDr169qF27NosXLyY6OvreCOyMGTPInj07oaGhVK5cmZYtW5IjR+JHNIQQwp2dCT5D5987s7jN4kcarb0re/rsTG8+nW3ntpE/a342nN5AwWwFyZc5nxOiFUI4VFgYTJ5syo5jy5kT3nkHRoyAqVNdH5sQwuM5LbHVWkcppXoAKwBvYIbW+oBSqpv9/sla60NKqeXAXsAGTNNa70/2wZVK9i7ieEjJdmhoKOXLl+fUqVP4+/vz/PPPc/v2bTZv3kyrVv81e75bRr1mzRpmzTLTib29vcmSJQsA48aNY/HixQCcPXuWY8eOpbrENjAokLGBYzl67ShFsxe9b76dEMKz7Ti/g4HPDKTGkzWStZ/KeSsDsPPCTl6e/zIfP/sx3St3x9vL2xFhCiGc4eefoXx5KFky/vvffx+KFIGBA6FQIZeGJoTwfE6bY+ssGTJk0Hfu3LE6jDjuznsNDg6madOmtGrVio4dO1KsWDEuXLgQZ/tcuXIRFBREunTp7t22bt06PvroI1auXImfnx916tRh6NCh1KlTJ9XMsR2ydggjA0YSGhmKRuOlvPD18aVf9X4MqzvM6vCEEMmw99JeyuYu6/D9Hr5ymLf/eJsSOUswuelkh+9fCOEAWkO5cjByJDRokPB2w4bByZMwc6bLQhPCE8kc27icOcc2VcqSJQvjxo1j5MiRpE+fnoIFC/LLL78AoLVmz549ANSvX59JkyYBpnz55s2bBAcHky1bNvz8/Dh8+DCBgYGWPQ8rBAYFMjJgJCGRIWjMBRebthESGcLIgJEEBqWu10OIlOTPo3/S9Oem3Ilw/IXJ4jmLs/aNtXxa71NuhN1gyNohTjmOECIZVq8Gmw2ef/7B2/XpA3/+CUeOuCQsIUTKIYmtE1SoUIFy5coxd+5cfvrpJ6ZPn065cuUoVaoUS5YsAWDs2LGsXbuWMmXK4O/vz4EDB2jUqBFRUVGULVuWwYMHUy2VreU2NnAsoZGh8d4XFhXG2MCxLo5ICOEIZ4PP0um3Tvzc8mcypHXOxWUv5UUOvxxE26I5fv04pSeVZtmxZU45lhAiCUaPNknrw6aLZckC771nRm6FEOIRSCmycBv+U/zZeWFnwvfn8Wd71+0ujEgI4Qi9l/Xm8YyPM6jWIJcdc8XxFfRe3puV7VfyVJanXHZcIUQ8Dh+G2rXh1ClIn/7h29+6BYULmyZTpUo5PTwhPJGUIsclia1wG20XtGX+wfnYtC3OfV7Kizal2vBzy58tiEwIkVRhUWF4K2+8vbzxUq4tErJpG17Kiw9Xf0iBrAXoXLGzy2MQQgDdusFjj8Hw4Yl/zNdfw9atYJ/OJYS4nyS2ccknvHAbvav1xtfHN977fH186VW1l4sjEkIkx7Jjy6j7Q118vHwsSSjvHvPV0q8yY9cMnv3+WQ5ePujyOIRI1a5eNevTdu/+aI97913YtAnsvUmEEOJhJLEVbqNavmr0q96PtN5p792mUKTxSkO/6v1kyR8hPEjQzSDeXPImXz73JcoZS7A9grK5y7LprU20Ld2WxYfMcmrRtmhLYxIi1fjuO2jeHB5//NEe5+cHH3wAQ4Y4Jy4hRIojpcjC7TSb04zzt84DUCR7EXpV7UW+zPnIlzmf5SfIQojEaTC7AXUK1OHDWh9aHUoch68cpsW8FkxsMpG6BetaHY4QKVdEBBQsCEuXmqV+HlVYmJlru2QJ+Ps7Pj4hPJiUIsclia1wOwf+PUAOvxw8ntFc3dVaU/P7mrxa6lV6Vu1pcXRCiMQ4cuUIRXIUcds5rb8f+Z0ey3pQr2A9xjQcQxbfLFaHJETK8+OP8P33ZqmfpJo40Sz/8+efjotLiBRAEtu43POMQ6RaIZEheCmve0ktgFKK2S1mM2LDCP4+9beF0QkhHmbF8RUMXjOYYjmLuW1SC/BisRfZ/85+CmQpQBrvNFy+cxlPu9ArhFvT2izx8957ydtPp06wfz8Eylr2QogHc9+zDpEqbTyzkW5/dotz+9PZnubHFj/SZ0WfeLsmCyGsd/7WeTou6Uj9p+tbHUqiZEqXiSF1huCXxo/3VrzH87Of59jVY1aHJUTKsGED3L4NTZokbz/p0sFHH8HHHzsmLiFEiiWJrXArgUGBVM9XPd77ni/0PJvf2oxN2wiPCndxZEKIB4myRfHawtfoXqk7dQrUsTqcRzbzpZk0LtyY6tOr893276wORwjPN2oU9O4NXg441ezYEY4fN8myEEIkQBJb4VYCggIe2P04fZr0jNw8krf/eFvKBoVwM6+Vec0tm0Ulho+XD+/XeJ8dXXdQIU8FIqMj2XZum9VhCeGZjh+HjRvhjTccs780acyIrYzaCiEeQBJb4VbalWlHzadqPnCbnlV6svPCTiZsm+CiqIQQD7L25Fq2nttKV/+ueHt5Wx1OsuTPmp8qeatw/Npxms9tTrc/unEj7IbVYQnhWcaNg86dIYMD+9q8/jqcOwdr1jhun0KIFEUSW+E2wqPCaVu6LTn9cj5wuwxpM7C4zWI+Wf8JR68edVF0Qoj4XLh1gdcWvUZYVJjVoThUiVwlOPjuQQBKTSzFlZArFkckhIe4ccN0Q+7Rw7H79fExa9p+/LFpTCWEELHIcj/CbczcPZO1p9byw0s/JGr7oJtB5MucD621rG8rhAWibdE8N/s56uSvw5A6Q6wOx2mOXDlCsZzFWHRoEf55/MmfNb/VIQnhvkaOhF274KefHL/v6GgoU8Z0W27Y0PH7F8KDyHI/ccmIrXAbAWcDqJSnUqK3z5c5Hwf+PUDdH+qmuNEiITzBhdsXKJStEB89+5HVoThVsZzFADh14xT+U/z5ZvM3RNmiLI5KCDcUFQXffpv8JX4S4u0NQ4fKqK0QIl6S2Aq38bDGUfEpmaskj2V4jO5/dpdmUkK40IF/D5AjfQ6mNZvm8fNqE6tv9b4Edg5k2fFlDF4z2OpwhHA/ixbBU09BpcRfpH5kr7wCoaHw55/OO4YQwiNJYivcgtaaZ558hnKPl3ukxymlmNF8BtvOb2P6rulOik4IEdPF2xdp8GMDtp/fbnUoLlc4e2H+av8Xg2sP5uT1k/Rb2Y9b4besDksI9zBqlPNGa+/y8oJhw2TUVggRhyS2wi0opZjUdBJpvdM+8mMzps3Ib6/+RtOiTZ0QmRAipmhbNK8vep1OFTpRK38tq8OxhFIKvzR+ZE6XmauhVyk1sRS/HfnN6rCEsFZAAPz7LzRv7vxjvfSS+ffXX51/LCGEx5DEVriFUQGjmLA16cv3FMxWkFx+uWi7sC1BN4McGJkQIqZ1p9Zh0zaG1E65zaISK4dfDr5v/j0zX5rJpxs+5XrodatDEsI6o0dDr15mHqyzKQXDh5suyTab848nhPAIktgKt7Dm5Boez/h4svbh7eVN2cfK0nJ+S2kmJYQThESGUP/p+qx4fUWqmVebGPUK1iOwUyDZ0mejw+IOTNg6gWhbtNVhCeE6p0/D6tXw1luuO+YLL0D69LBggeuOKYRwa5LYCstprQkMCqT6k9WTva+BNQfyVJanePfPdx0QmRDirku3L1FyQknOBJ8hjXcaq8NxO3eXHBtYcyBz9s/hmRnPsPfSXoujEsJFvv0WOnaEzJldd8y7o7ZDh5plgIQQqZ4ktsJyV0Ov4v+EP09keiLZ+1JK8X3z72lUuJEDIhNCgH1e7eLXeb3s6zyV5Smrw3FrJXOVZP2b6+lUoRO7L+5Gay0VJCJlu3ULvv8eevZ0/bEbNIDs2WHuXNcfWwjhdpSnLZGSIUMGfefOHavDEB7gp70/kT9rfmo+VdPqUITwaOO2jGPhoYWs7rAaHy8fq8PxKJvPbqb94vZMbDKRhoUbWh2OEI43bhxs2AC//GLN8desgW7d4OBB8JH3J5F6KKVCtNYZrI7DnciIrbDc6IDRrD+93uH7zZ4+O20WtOHczXMO37cQqUnH8h2Z/8p8SWqToMaTNRjfeDzd/uzGawtf43bEbatDEsJxoqNhzBjnL/HzIPXqQd688OOP1sUghJtSSjVSSh1RSh1XSg2M536llBpnv3+vUqriwx6rlGqllDqglLIppSrFuL2AUipUKbXb/jU5xn3+Sql99n2NU3fn7ziYJLbCcrP2zkrSMj8P07hIY96t/C4t57ckPCrc4fsXIqX7986/NP25Kem805E7Y26rw/FYjYs0Zv87+6merzp+afw4ef0kNi2dXEUK8NtvkCsXVE9+j4xkGT7cfEVGWhuHEG5EKeUNTAAaAyWBtkqpkrE2awwUsX91BSYl4rH7gZeB+Eal/tFal7d/dYtx+yT7/u8eyylzBiWxFZa6HXGbo1ePUuHxCk7Z/6Cag2hatCl3IqV8XYhHYdM22i9uT5nHypDOJ53V4Xi8DGkz0LNqT7yUF31W9KHuD3U5fOWw1WEJkTyjR5vRWucMviRerVpQuDDMnGltHEK4lyrAca31Ca11BDAXiL3QdHNgljYCgaxKqTwPeqzW+pDW+khig7DvL7PWOkCbObCzgJeS++TiI4mtsNTBywepmKei006clVJ89OxHhEeF8+vhX51yDCFSoi83fsmdiDt8Uu8Tq0NJcRa1XsQrJV6h1ve1+GH3D1aHI0TS7NgBp05By5ZWR2IMHw4jRkC4VGgJYZcXOBvj+yD7bYnZJjGPjU9BpdQupdTfSqlaMY4RlIR9PTJJbIWlquStwro31jn9OLcjbtP1965sOrPJ6ccSIiUo/3h55rScI/NqncDby5ueVXuy6+1d1HiyBjfCbjilz4AQTjV6tOmEnMZNlv+qVg1Kl4bp062ORAhX8VFKbY/x1TXW/fGVUsTuGpzQNol5bGwXgKe01hWAvsDPSqnMSdxXkkhiKyw1bec0zt867/TjFMlRhO+bf0/rBa1dcjwhPNWVkCuM2zKOxkUa82SWJ60OJ0XLlzkfRXIU4cT1E7Rb1I5OSzpxLfSa1WEJ8XDnzsHSpdCli9WR3G/4cPjsMwgNtToSIVwhSmtdKcbXlFj3BwExP8jzAbFPghPaJjGPvY/WOlxrfdX+/x3AP0BR+77yPcq+kkoSW2EZrTUfrv7QZcd7oegL9Knah72X9rrsmEJ4Epu20WFxB+kk7mIV81TkQPcDZEibgfKTy3MnQnoCCDc3YQK0awdZs1odyf38/aFSJfjuO6sjEcIdbAOKKKUKKqXSAq8Cv8Xa5jegg707cjUgWGt9IZGPvY9SKpe96RRKqacxTaJO2Pd3SylVzd4NuQOwxIHP878YZB1bYZUT109Q6/taBL0XhJO6fsdLa8360+upXaC2y44phCf4cuOX/Hb0N9a9sY403m5SXpjKXLh1gTyZ8jBp2yQaFGpAoeyFrA5JiPvduQMFCkBAgGnY5G727IFGjeD4ccggS3yKlCsx69gqpZoAYwBvYIbW+lOlVDcArfVke6I5HtOlOAR4U2u9PaHH2m9vAXwL5AJuALu11g2VUi2B4UAUEA0M0Vr/bn9MJWAmkB5YBvTUTkhCZfKUsEzA2QCq56vu0qQWIDw6nHf+fIc+1frQ1T/2dAQhUietNaeDTzO35VxJai2UJ1MeAO5E3qHKtCr0q96PfjX6yc9EuI9Zs6BGDfdMagHKlYOaNWHiROjf3+pohLCU1nopsDTWbZNj/F8D7yb2sfbbFwOL47l9IbAwgX1tB0o/SuxJISO2wjJhUWFcD71+70TOlY5ePUrNGTVZ8uoSqj9p8fp7QljsashVroZepWiOolaHImI4ef0k3Zd257mCz/F+jfetDkcIsNmgRAlT6lunjtXRJOzAAahXz4zaZspkdTRCOEViRmxTG5ljKyyz+sRqsvpmteTYRXMUZUbzGUzZGXuevRCpi03beOPXN5i5e6bVoYhYCmYryNLXltK7Wm8CgwJ59893CQ4LtjoskZotW2bKe2u7+VSeUqWgfn349lurIxFCuJAktsISIZEhtF7Q2tIYmhZtyoxmMzh/6zzhUbLunUidRgWM4mroVYbVGWZ1KCIeSil8vHwolqMYkbZISk4syYKDC/C0aiuRQoweDe+9By6eQpQkQ4aYeIPlYpAQqYUktsISO87voFSuUqRPk97SOJRSfLz2Y3ov721pHEJY4XbEbb7f/b3Mq/UA2dJnY8qLU5j3yjxm7p5JeHS4JLfCtfbuhYMHoU0bqyNJnGLF4IUXYMwYqyMRQriIzLEVlvhq01ecu3mOsY3HWh0KN8NvUnVaVfpW60sXfzdbk08IJ7kTcYf0adJj0zZ8vKSPoKfRWvPc7OdoWqQpPav2lJ+hcL633oJCheB//7M6ksT75x+oWhWOHYNs2ayORgiHkjm2cUliKyxx+MphFIpiOYtZHQoAR64cofFPjdndbTeZ02W2OhwhnEprTfO5zXmhyAu8Xeltq8MRSXT06lG6/dGN4PBgpjebTvnHy1sdkkipLl2C4sVNgpgzp9XRPJouXSB3bhgxwupIhHAoSWzjksRWuJzWmuPXjlM4e2GXL/XzIGFRYaTzTsedyDtkTJvR6nCEcJpRAaOYd2AeG97cQFrvtFaHI5JBa82sPbPImzkvtfPXJjw6nIxpMxIYFMjYwLEcvXaUotmL0rtab6rlq2Z1uMJTDRkCFy+absie5vRpqFgRjhzxvKRciAeQxDYuSWyFy526cYrq06tzvu95t0psAX49/Ctfb/6atW+slRN+kSIduXKEWt/XYmuXrRTIWsDqcIQD/XH0D3os7UGNJ2uw5MgSQiND0Wi8lBe+Pr70q96PYXWlSZh4RGFhkD8/rFtnlvrxRN27m2V/vvzS6kiEcBhJbOOS5lHC5QKDAqmer7rbJbUAzYo1I5dfLnovk2ZSImUqmqMoWzpvkaQ2BWpatCl9q/dl7v65hESGoDEXrm3aRkhkCCMDRhIYFGhxlMLj/PQT+Pt7blIL8OGHMG2aKakWQqRYktgKlws4G+C2JXFeyotZLWax9tRalh5banU4QjiM1pr2i9uz48IOCmYraHU4wkkCzgYkeF9YVBhjA61v2Cc8iNb/LfHjyfLlg9dflxFbIVI4aaMoXK5W/lqUzFXS6jASlDldZtZ1XMdjGR7Dpm14Kbn+Izzf2C1jOXzlMGVzl7U6FOFER68dvTdSG5tN2zh27ZiLIxIebdUqs2btc89ZHUnyDRwIpUtDv37wxBNWRyOEcAI5YxcuZdM2Xi7xslsntgCPZ3ycm+E38Z/iz8XbF60OR4hk2XZuG59t+Ix5r8yTueMpXNHsRRO8GKdQFM1R1MURCY82ejT06WOSW0+XJw+8+SZ8/rnVkQghnEQSW+FSgUGB1PuhntVhJEpW36y8VOwlWv3SiojoCKvDESLJHs/4OHNfmcvT2Z62OhThZL2r9cbXxzfe+3x9fHmlxCu88esbnAk+4+LIhMc5dAh27IB27ayOxHEGDICff4YzKfP3PzAokLYL2uI/xZ+2C9rKnHqR6khiK1wq4GwApR8rbXUYiTa49mCyp8/OoFWDrA5FiEemteaLjV+QMW1G6hX0jAtKInmq5atGv+r98Evjd2/k1kt54ZfGj/41+tOgcAPyZ8lPhe8qMGjVIG6F37I4YuG2xoyBbt3AN/4LJR7pscega1f47DOrI3G4IWuHUH9WfeYdmMfOCzuZf3A+9WfVZ8jaIVaHJoTLyHI/wqVemf8KLYq3oF1Zz7kCHBwWzMXbFymWs5jVoQjxSL7d8i0z98xk81ubSeeTzupwhAvdXcf22LVjFMleJM46tkE3g/jk708YUW8ESimy+mbFx0vabgi7K1egSBE4fBhy57Y6Gse6ehWKFYNt26BgymikFxgUSP1Z9QmJDIlzn18aP1Z3WO22TTtF0slyP3FJYitcqt/KfvSs0pP8WfNbHcoj67m0J+3LtadK3ipWhyLEQ20/v50mPzUhoFMAhbIXsjoc4cYGrxnMwkML+fr5r2lSpIlbLsUmXOzTT+Gff2DGDKsjcY6PP4Zz52D6dKsjcYi2C9oy78C8eBvHeSkvWpdszZxX5lgQmXAmSWzjksRWiERacngJPZf1ZFuXbeTOmMKuYIsU5+d9P5POOx0tS7a0OhTh5rTWLD22lH5/9aNc7nLMfWWu1SEJK0VEQIECsGIFlCljdTTOceOGGZEOCIDCha2OJsl+2P0DG85s4Kd9PxEWFZbgdv55/NnedbsLIxOuIIltXDLHVrjM70d+5+O1H1sdRpI1L96cN8u/SatfWhEZHWl1OELES2vNhtMbeK3Ma5LUikRRSvFC0RfY220vHz370b252edunrM6NGGFefOgZMmUm9QCZM0KvXrBsGFWR5Io4VHhzN0/l74r+lJzRk36rugLwJngM1TMU5Fn8z+bYDd0L+VFoWxStSNSB0lshcusPbWW9D7prQ4jWYbUGcI7ld7B28vb6lCEiNfEbRPpvbw3UbYoq0MRHiaNdxpKP1aaKFsUN8NvUnZyWYasHcLtiNtWhyZcRWuzxM9771kdifP17m1GpQ8dsjqS+9wKv8UfR/9gyNohNP6pMXP3z8VLebHo0CJyZ8jNJ3U/YWidoYBpcNm9cneG1RmWcDd0b182nt1I72W9OX/rvAufiRCuJ4mtcJmAoACqP1nd6jCSxUt50bZMW/4+9Tc/7/vZ6nCEuM/OCzsZ9vcw5reaL42ARJKl8U7DZ/U/Y2fXnfxz/R+WHF6CTduItkVbHZpwtvXrISQEGje2OhLny5wZ+va1dNT2TsQdNpzewDebv6HNgjYcunyI08GnGbtlLFG2KLr5d+O5p58jjXca5reazwc1P6BuwbpkTpf5vv08qBt6vxr92NF1B95e3pSeWJqf9v5kxVMVwiVkjq1wCZu2UXlqZdZ3XE+GtJ4/HeDg5YPUnlmbpa8tpXLeylaHIwQA7Re3p2mRprQp3cbqUEQKs/TYUj5Y9QEjnx9Jw8INrQ5HOEvz5tCoEbzzjtWRuMbt22aO7V9/Ob30OsoWxf5/97P13FZ2nN/BhBcm8PO+n5mwbQJV81alSt4qNCnShOzpsyf5GA/rhn7x9kWibFEoFKMCRtH/mf48nvFxRzw9YQGZYxuXJLZCJNGvh3+l17Je0kxKWE5rzc3wm2RKlynBeVZCJIfWmiVHljDgrwE8ne1p5r0yjyy+WawOSzjS8eNQvTqcOgUZUtG58jffwObNsHChw3aptebE9RNsO7+NS7cv0btab3os7cGak2uokrcKVfJW4c3yb5I+jTXTs66EXGHE+hHM2jOLjuU78mGtD8npl9OSWETSSWIblyS2wiUWHVqEt/KmefHmVofiUJO2TaJh4YY8ne1pq0MRqdjk7ZP5/ejv/Pnan1aHIlK4iOgIFh5cyKulX2X96fUUy1lMRnxSip49IVMm+OwzqyNxrZAQM2r7559QoUKSdnH5zmW2nd+GQtG4SGPq/lCXY1ePUSVvFZ7N/yx9qvXBpm1ud+Hx/K3zfLXpKwY8MwCbtuHj5SN/zx5EEtu4nJrYKqUaAWMBb2Ca1vqLWPfXAZYAJ+03LdJaD3/QPiWx9UxtFrThhSIv0KFcB6tDcbhb4bdYeGghHct3tDoUkQrturCLBj82YNNbmyiao6jV4YhU5PMNn/NNwDf0rtqb92u8j18aP6tDEkl14wY8/TTs2wd581odjeuNGwerVsFvvz1009sRt9l5YSc5/XJSLEcxSkwoweWQy1R6ohKtS7ami38XbkfcJmPajC4I3HFm75lN7+W9ebP8mwx4ZoBUonkASWzjclpiq5TyBo4CzwNBwDagrdb6YIxt6gD9tNZNE7tfSWw9U/4x+fmr/V8p8sT7RtgNqkytwkfPfpQiE3fhvmzaRrnJ5fiw5oe0LdPW6nBEKnTi+gkGrR5EWu+0zG4x2+pwRFJ9/TXs2QM//mh1JNYICzPr2i5aBJX/65txd17sU1me4mb4TZrNacbxa8cpk7sM/Wv055WSr3D6xmmezPKk243GJsW5m+f4ctOX/HnsTw6/exgfLx+UUlaHJRIgiW1czkxsqwNDtdYN7d8PAtBafx5jmzpIYpviXb5zmVITS3Gp36UU+wZ5t5nU8nbL8X/C3+pwRCpyNvgsT2Z50uowRCoXER3BnYg7tJzfko+e/Yh6BetZHZJIrKgoM1q7eDH4p97PLz1pEidXzKXA4rVsPruZgasGsvvibp7K8hTTmk3DP48/+/7dR5nHypDOJ53V4TpVWFQYvj6+vLrgVZ7K8hT9avTjsQyPWR2WiEUS27iceXkpL3A2xvdB9ttiq66U2qOUWqaUKuXEeIRFcmXIxZn3zqTYpBagZK6SfNf0O3Zc2GF1KCKVmLJjCl9s/EKSWuEW0nqnJatvVt6p9A6df+vMi3Ne5MT1E1aHJRJj4UIoUCDVJbU3w28CphFk458ak+vWYJ4tuomLa3+nQNYCfFL3E86/f56D7x6kxpM1SOeTjkpPVErxSS1wb03ckQ1GcifiDsXHF2fk5pEWRyXEwzlzxLYV0FBr3dn+fXugita6Z4xtMgM2rfVtpVQTYKzWukg8++oKdAVImzatf3h4uFNiFs7x59E/eTrb05TIVcLqUFxi/en1VM9XnTTeaawORaRQey7u4bnZz7HxzY0Uy1nM6nCEuE94VDjjt46nefHmZEmXBY2W0R53pTVUqwYDB0KLFlZH4zR3GzfN3jObP479wdZzW7FpG6d6n2LLuS1cun2Jynkr88T8ZTBnjplvK+45G3yWE9dP8Gz+Zxm3ZRztyraTLspuQEZs43LmiG0QEHMoIR9wPuYGWuubWuvb9v8vBdIopeL8pWitp2itK2mtK/n4+DgxZOEMn2/8nPO3zj98wxRAa81Xm76i/1/9rQ5FpFC3wm/RekFrxjQcI0mtcEvpfNLxfo33KZy9MGtOrqHkhJJ8sfELQiNDrQ5NxBYQAFeuQLNmVkficNN3TqfLb10oO6ksTX5qAoBSiqZFmrK83XJO9j6JUopq+arRvHhznsj0BHToYJY7+vtva4N3M09meZLaBWoTFhXGkatHKDa+GINWDeJKyBWrQxPiPs4csfXBNI+qD5zDNI96TWt9IMY2jwOXtNZaKVUFWADk1w8ISubYepaI6AiyfZmNC+9fIHO6zFaH4xLXQ69TZVoVPn72Y9qXa291OMIZAgNh7Fg4ehSKFoXevc2ohwtERkey5MgSXin5ikuOJ0RyHbt6jA9WfcChK4fY984+fLzkArXbaNUKatWCXr2sjiTJQiNDWXJkCVvPbWXrua3UL1ifYXWHMWjVIJ7M8iRV8lZ5tHmxs2bB9Omwbh2k4ClUyXEm+Ayfb/ic+k/X56XiLxEcFkwOvxxWh5XqyIhtXM5e7qcJMAaz3M8MrfWnSqluAFrryUqpHsA7QBQQCvTVWm9+0D4lsfUs285to9Nvndj7zl6rQ3Gp/f/up++Kvix/fXmK6JQoYhgyBEaOhNBQU8bn5QW+vtCvHwwb5tRDz9k3h6ezPU3VfFWdehwhnCHoZhD5Mudj6Lqh1C9Yn1r5a1kdUup26pSZV3vqlFm/1sUCgwIZGziWo9eOUjR7UXpX6021fA++QHgz/CYbz2y8l8R2r9ydOgXq0GFxB6rkrUKVvFXwz+NPFt8sSQ8sKgpKlYIJE+C555K+n1Qi4GwATec05W3/t3m/+vuS4LqQJLZxOTWxdQZJbD1LaGQoZ2+eTZHL/DyM1pqb4TeJtEXKXJSUIjAQ6teHkJC49/n5werVThu53XdpH/Vm1WN9x/WpZr66SJnm7JvDwNUD8c/jz5fPfUmRHHFaawhXeP99MyI50vVNgYasHcLIgJGERoai0XgpL3x9fOlXvR/D6poLhHci7rDzwk6TxJ7fylfPfcWF2xcYvHYwVZ6oQuW8lan1VC3nJFJz5sC338KmTTJqmwinb5zmsw2fseDQArZ12cbT2Z62OqRUQRLbuCSxFU4VGBRIyVwlU00ZcmzfbP6GP479wV/t/5Lyu5SgbVuYN8+M1Mbm5QWtW5sTIge7HXGbSlMq8WGtD2WtZJEihEaGMnbLWDKmzUiPKj24FX6LTOlcP2qYat26ZToh79wJ+fO79NCBQYHUn1WfkMi4FwjTeqelebHmzHtlHhO2TWD23tlUecKMxL5Y7EWy+mZ1TZDR0VC2rEn6Gzd2zTFTgHM3z/FEpieYsmMKZ2+epW/1vmRPn93qsFIsSWzjkhpJ4VRtF7ZNNY2j4tOnWh98fXzpv1KaSaUIR4/Gn9QC2Gxw7JhTDnvh1gVal2otSa1IMdKnSc/AmgPpUaUHR68e5elxT/PN5m8Ij5JVD1xixgxTfeLipBZgbODYBBuJRURHcOL6CaJsUfSo0oMtnbfwbZNvaV+uveuSWgBvbzO15OOPE37PF3HkzZwXpRQNCjXg0u1LFP22KB+v/RibtlkdmkglJLEVTnPx9kWCw4JTZRnyXd5e3vz88s8s/2c5R64csTockVxFi5qR2fh4eZn7HWzTmU3ky5yP4XWHO3zfQriDojmKsr7jev4+/TclJ5bk+LXjVoeUskVHm+Z3773nskNqrQkOCwZg1clVaB6cLLrFcnkvvwyRkfD771ZH4nEKZivI1GZT2dplK7n8cuGlvFh1YhXXQ69bHZpI4SSxFU4TGBRI1XxVU33zpGzps7H77d0Uy1mMW+G3rA5HJEfv3uZKfnyUgo4dHXq4A/8e4KV5L3E6+LRD9yuEuymRqwS/tf2Nmc1nkj9Lfv4+9TcBZwOsDitlWrIEHnsMqld36mG01vxy4Be6/NaFAmML0PWPrgBUzFMxwfMCL+XlPhfDvbz+G7W1yYhjUjyd7Wl6Vu0JwB9H/6DIt0UYsnaIJLjCaVJ3xiGcqmzusgx+drDVYbiFdD7pOHj5IGUnl+XynctWhyOSKm1aSJMG0qf/b+TWy8s0jvL3h7ffhq1bHXKoOxF3aPVLK75+/muK5yzukH0K4e5q5a9FGu803Ai7QesFrWmzoA0nr5+0OqyUZfRo6NvX4buNjI5kw+kNDF4zmO+2f4dSinWn1lEmdxlWvL6CuS3nAjCszjB8fXzj3Yevjy+9qrrR0kPNmoGPDyxebHUkHm9MozFs7bKVszfPMnitOTeMiI6wOCqR0kjzKOE0F29fJHeG3CjpKHjPoFWD2HJuCyvbr5RmUp7m1i2oWBFGjDDz0saONXNqixT5bx3bRYugWzf43//MupDJ+N1fcHABfx77k++bf+/AJyGE5wiJDGFUwCjWn17PyvYr0VrL50lybd8OLVvCP/+YhC0ZtNYcv3Ycby9v8mbKS95ReSmQtQANCjXg5RIvU+mJSgk+9m5X5LCoMGzaFm9XZLexdCkMGAB79iRcsSMeidaa4PBgSk0sRdeKXelTrU/ylmhKpaR5VFyS2AqniLJFkfWLrAT1DXJtwwc3F22L5oWfX6BBoQb0re74K+bCSbSG9u3NyOyUKQ/e9sQJaNXKdBydPh2yZn3kw10LvUb29NnvnfAJkZpprbFpG9WmV6N92fa8U+kd95iD6YnatYMKFcy620l0JeQKH635iJX/rCQ8Opwvn/uS18u+TnBY8CMlJ9OWB/LJyrFc0cfIqYowuEFvOjdyznJpyaI11KhhLmC++qrV0aQox68dZ8T6Efx57E9+fvlnni/0vNUheRRJbOOSxFY4xc4LO2m/uD0Huh+wOhS3cyPsBum805HOJ50kLZ5i5kyz7MPWrSa5fZjwcLNG5LJlMH++KVNOpIOXD1J/Vn0Odj9ItvTZkh6zECnMvkv7GLBqAP9c+4dpzabxbP5nrQ7Js5w7B2XKmItvibzgFmWLYuu5raz8ZyUr/1lJr6q9eKn4S0zcNpEGhRpQKlepJI2iDxli3lJDQ03e6OUFvr4m3x7mZgO2APz1F/TsCfv3J3ukW8R17Ooxsvpm5dKdSyw+tJheVXvJCG4iSGIbl5xVC6cIOBtA9XzObUzhqbL6ZiWtd1qemfEMuy7ssjoc8TCHDkH//mb92sQktQDp0sH48fD559CoEUyYkKglI+7Oq/203qeS1AoRS5ncZVjWbhnjm4wnU9pM3Ay/yfbz260Oy3OMHw+vv/7QpPbk9ZNM3j6ZoJtB7Lqwi+5/dickMoThdYfzUvGX8PXxpW/1vpR+rHSSktrAQJPUhoT897Zos5nvR44097ud554zDbecsE65gCI5ipArQy4ypc3EsWvHKPxtYUasH8GdCBnIEo9GRmyFUyw/vpy03mmpV7Ce1aG4rfkH5vPBqg/Y1mUbOf1yWh2OiE9oKFStaq7Ud+mStH0cO2ZKk4sVg6lTIXPmBDcdtGoQQbeCmPXSLJlLKMRDbDu3jeZzm1OvYD0+q/8ZT2V5yuqQ3NedO6Y3wJYtUKjQfXfdDL9JxrQZWX96PV1+78Kt8Fs0KNSAj579yCkditu2NdcJ4zv99PKC1q3dNH9ctw46dzYXO9NIKbwzHb16lG82f8PIBiO5EnKFHH45yJwu4c/O1EpGbOOSxFY4hTT5SJyBqway59IelrVbZnUoIj7du8O1a+YsKzm/z6Gh0KcPrFkDv/wC5cvH2URrza2IW3gpLzKmzZj0YwmRityOuM3Xm75m8o7JHOx+kBx+OawOyT1NnAgrV8KvvwImcZh/YD4r/1nJrou72P32bjKly8TF2xcp81gZp35++/vDzp0Pvn+7uw7E169v5im/9ZbVkaQaX236ipGbR9KnWh96VulJpnSZrA7JbUhiG5cktsLhLt+5TI0ZNTja46gktw8RbYtm37/7KP94eatDEbEtXGg6Ye7cCVkcNNfn559NA5JPPzUjwPa/j0OXD9FreS9WvL5C5l0LkQQ3w2+SOV1mhq0bxmMZHqOLfxfpPH+XzcaZioVYObAV69Ke5/vm37Ps+DL+PvU3DQo1oFb+WvilSeQ0Cwdo29a0HohvaVgvL2jTxrxVuqVNm0w595EjZvk34RKHrxzmk/WfcOzqMbZ03iLnlnaS2MYlZ1DC4QKDAimYtaC88SSCt5c35R8vz8jNI/l5n7t+kqdCp07BO+/A3LmOS2oBXnsNNmyAb781J0e3bxMSGWLW6yzVRpJaIZLobpli8+LNWXhoIWUmlWHpsaUWR2Wd2xG3CTgbAMBHU9tSqUkQa9Oc4/mnnydaR9OsWDO+afgNDQs3dGlSC+banm/8y9ji62tWSnNbzzxjppXMmGF1JKlK8ZzF+enln1jXcR0ADX9syBcbv+BW+C1rAxNuR86ihMMFBEnjqEfVoFADei/vze6Lu60ORURGmiGFgQOhcmXH7794cTPPzdcXKlWiz88dKJu7LJ0qdHL8sYRIZco/Xp6/2v/FyOdHcvjKYQCCbgZZHJXrzNw9k7o/1CXPN3kY9vcwtNYMWHiRiyWm81PLn3ij/Bv4+iSQVbpIuXJmmm/69GaEFv6b6fHuu2ZJcLc2fLipugkLszqSVMcvjR9KKcY0HMPeS3sp/G1hZu2ZZXVYwo1IKbJwuM82fEbNp2rKUgyPaN7+eQxcPZDtXbbLPDErffCBWdLh99//O+tyllmz+GtMT6p1/4xMnbonbx6vECKOW+G3KD6hOA0KNWBE3RHkzZzX6pAcJuhmEH/98xcrT6zkeuh1lr++nF8P/0oarzTULlDbzNXfsweaNIGTJ92mdHbIEDhwwCztM3as6a9XpIhZJa1SJfjwQ6sjTIQXX4QGDUxjQWGZg5cPcjP8JhXzVOS77d/xZoU3U1WPCilFjksSWyHcyKJDi3ihyAuk80lndSip04oV0KkT7NoFuXI59VBHrhxh2fFl9MncAF55xYwOT5wIGeQzSghHuhl+ky82fsF3O77jl1a/eGy3/pDIENafXk9IZAgvl3iZZnOakT5Neho83YDnCz0ff1foN980WaObZItHj0KNGrB7N+TLd/99ISFmpTRvb0tCezS7dsELL8A//5ihZ2GpKyFX6LG0B2tPreX96u/zbuV3yZA25X+WSmIbl5QiC4c68O8B3vnjHavD8Fgvl3iZf67/w1ebvrI6lNTnwgXo2BF+/NHpSW1oZCitF7QmvU96KFkStm0za19UqQIHDzr12EKkNpnTZeaz+p+x6+1dVH6iMlvPbWXqjqlE26KtDu2BbNrGzfCbaK1pNqcZuUfm5vONn3M15CoAv7X9jXmvzKNTxU7xJ7UXL5ouyG+/7drAH+DsWfjyy7hJLZhlws+dM9f5ot37RwMVKpia6UmTrI5EADn9cjL3lbms7rCaHRd2sP38dkIiQ2Qd3FRIElvhUBvObCAsWuadJEfuDLmZvH0yc/a540J+KVR0tGnm9PbbUKeO0w/XZ3kfSuQsQVf/ruaGDBnghx+gb1+oXRtmz3Z6DEKkNk9leYpM6TKR3ic9P+77kfLflWfF8RVWh3WfiOgIZu+ZTfvF7XnimycY/vdwlFIMrDmQc33P8XfHv+nin8g1tSdONC2Gc7jH1JYTJ6BuXVMUk5B8+cw1xunTXRdXkg0bBl99ZdYIFm6h9GOlmffKPGoXqM2K4ysoNK4Q32z+hpDIEKtDEy4iia1wqMCgQKrldffOD+4th18OFrdZTK/lvdhzcY/V4aQOX3xhktvBg51+qChbFBnSZmDKi1Pu7xyulDnjW70aRoyAzp3N+rdCCIcqk7sM695Yx4i6I/gm4BsioiOIiI6wJJawqDD++ucv+q/sz8KDC/FW3qw8sZJaT9UioFMAIxuMBKDGkzXudX5OlNBQ+O47s362G7hxA2rWhH37Hrydl5fJxwcPhitXXBJa0pUpYy5Ejh9vdSQiHi1KtOCv9n8ReC6QIt8W4UbYDatDsoRSqpFS6ohS6rhSamA89yul1Dj7/XuVUhUf9lilVCul1AGllE0pVSnG7c8rpXYopfbZ/60X47519n3ttn895pTnK3NshSM1+rERXz3/FWVzl7U6FI+3+NBi8mXOR+W8TujMK/6zcaOpfduxA/I6t7HM8WvH0VpTJEeRB29465YZPd6/H375xSwvIYRwmgazG5A/S34+qfcJj2d83GnH0Vpz4PIBcvrlJDI6kpITS1I2d1kaPN2A1qVaUyJXCcccaNo0WLwY/vzTMftLpnffhagok2snxrffQsOGULSoc+NKtkOHTHJ7/DhkfoQLD8KlzgSf4aksT/HFxi9I652WbpW6uXyZK2d42BxbpZQ3cBR4HggCtgFttdYHY2zTBOgJNAGqAmO11lUf9FilVAnABnwH9NNab7fvqwJwSWt9XilVGlihtc5rv29dzG2dRUZshUMtf325JLUO0qJEC8o/Xp6J2yYSZYuyOpyU6do1aNfO1L05OakNiwrjlfmvsPrk6odvnCkT/PSTORusWRN+ljWOhXCm+a3mk9U3K6UnlmbazmkO3/+J6yfo+GtH8o7KS7M5zdh7aS/5Mufj7Htn2fTWJobUGeK4pFZrGDMG3nvPMftLpoMHYeFC+PzzxD+mZ0/Ik8f0ZnJrJUqY7sjjxlkdiXiAu3PQmxRpwqazmyg0rhDjt6aKkfYqwHGt9QmtdQQwF2gea5vmwCxtBAJZlVJ5HvRYrfUhrfWR2AfTWu/SWp+3f3sA8FVKubQbqiS2wmG2ntsq64k5mFKKRYcW8eFq9+homaJobTqGtmxpuls6Wd8VfSmWsxhv+yeykYtSZtR25UqzPka3brJuohBOktU3K183+JptXbZRNndZIqIjmLt/LjZtu7dNYFAgbRe0xX+KP20XtCUwKDDefYVHhbP25FoGrhpIxe8qsvHMRvzS+FElbxXWv7meE71P0KBQA5RSZPXN6vgn89dfpqa3fn3H7zsJSpQwS3dnz/5oj1u2zBTTRLn7dd0hQ8y6RTduWB2JeIiyucuysPVClrdbTlpvs/zVX//8RWhkip32kxc4G+P7IPttidkmMY99kJbALq11eIzbvreXIQ9WyjnrG0piKxzm9yO/c/TqUavDSFF8vHyY98o8FhxcwNz9c60OJ2UZP9604PziC6cf6tLtS+y6uIupL07lkd/LK1QwZdLXrkH16qbkTQjhFAWzFaRK3ipcvnOZcVvG4T/FnzUn1zBk7RDqz6rPvAPz2HlhJ/MPzqf+rPoMWTsErTWHLh9ibOBY7kTcYemxpQxaPYi03mkZ13gcVfNW5fGMj9O9cncKZy/s/CcxerSZW+sG62J//z0sXQr58z/6Y1u1gqxZPaDxcJEi0LSped2FRyj3eDm6+nfFpm1M2TmFQuMKMTZwbJwEN7EXsyzko5TaHuOra6z743sTiD0HNaFtEvPYeCmlSgFfAjGv5LfTWpcBatm/2idmX49K5tgKh3l+9vP0qdqHF4o6f/QrtdlzcQ/zD8zn0/qfWh1KyrBzp5nAFRgIhQo59VBXQq6QPX12FOrRk9qYtDZdVYYONf+2auWwGIUQcWmtWXBwAX1W9OFa6DXCouJWTKT1TkuWdFnurSfr7Dm6D3XwINSrB6dOga+vdXFguhuXLQt//21WNUuKu0/nn3/cfInvkyfNWuRHjrhNF2qReLsu7GLY38Pwz+PP4NqDiYyOZMT6EYwMGEloZCgajZfywtfHl37V+zGs7jCrQwYSNce2OjBUa93Q/v0gAK315zG2+Q5Yp7WeY//+CFAHKJCIx64j1rxZpVQ+YA3wptZ6UwJxdQQqaa17PPKTfghJbIVD2LSN7F9m53iv4+T0y2l1OCnW9vPbeTrb02RP/4g1XeI/t26Bvz8MHw6vvurUQ4VFhVFjeg0GPzuYFiVaOGan27ebJTwaN4ZvvoF0Lp2+IkSq0+aXNvxy8Bd0PIMVXsqLRoUb8UfbP5J34cpRunaFJ54wF8As1rYtFCwIn32WvP1cugS5czsmJqd6+22T1Cb3CQvL2LSNk9dPUn16dYLDg+Ptlu6Xxo/VHVZTLZ/1K4AkIrH1wTSAqg+cwzSAek1rfSDGNi8APfivedQ4rXWVRD52Hfc3j8oK/A0M11ovjBVHVq31FaVUGmAOsEprPTn5r8L9pBRZOISX8uJUn1OS1DrZ/APzabuwLdE2d1+93k1pDd27my6WTk5qAfqt7EfBbAV5qfhLjttppUqmNPn8eXjmGbM4pBDCaY5fPx5vUgvmRPjS7UvukdReuWK6qL/zjtWREB4O6dPDRx8lf1+5c5vGU+vXJ39fTvW//5m2z5cvWx2JSCIv5UWh7IWokKdCgkuAhUWFMTZwrIsjSxqtdRQmaV0BHALma60PKKW6KaW62TdbCpwAjgNTge4PeiyAUqqFUioIqA78qZS6uyB4D6AwMDjWsj7pgBVKqb3AbkyiPNUZz1kSW+EQ285t4+T1k1aHkeJ9Vv8zomxR/G/N/6wOxTPNmmXKkMc6/0Mp4GwAy44vY3qz6Y4/6c2a1bQZbd8eqlUzy3oIIZyiaPaieKn4T5e8lBdFc7jJmjSTJ8PLL1s+vBkWBsHBMGMG+DloRZUiRcw1ychIx+zPKZ56ygxTf/WV1ZGIZLoSkvAiyjZt49i1Yy6MJnm01ku11kW11oW01p/ab5t8d7TU3g35Xfv9ZWKWFcf3WPvti7XW+bTW6bTWue+WK2utR2itM2ity8f4+ldrfUdr7a+1Lqu1LqW17q21dsoIjSS2wiEmbZ/kjpPqU5y7zaQCgwK5GX7T6nA8y+HD0K8fzJvnuLOtBGitqZavGgGdApzT9RRMY5jeveGPP8yyHn36QET8V5iFEEnXu1pvfH3in6/q6+NLr6q9XBxRPMLDzdz7Pn2sjoSvvoL333fsPlu2NCuyuf2qOoMGmYz+4kWrIxHJ4DEXs0QcktgKhwgMCqT6k9WtDiNVyOmXk3Ud1+Hr48vpG6etDsczhIaaeamffgqlSzv1UOFR4dSfVZ9j147xWIbHnHosAKpUMaPQJ05ArVpwWn4nhHCkavmq0a96P/zS+N072fVSXvil8aNf9X5uMdeOefOgVCkoU8bSMI4fN8nnpw7uc6iUaWQfHOzY/Tpc3rzQoYNLuu0L5/GIi1kiXpLYimS7HnqdszfPUvox5yYM4n6rTqyi/qz6XAu9ZnUo7q9fPyheHLp0cfqh+v/Vn6y+WSmSvYjTj3VP9uywZInplFylCvz+u+uOLUQqMKzuMFZ3WE3rkq3xz+NP65KtWd1htXt0R9XaLDXz3ntWR8L778PAgaYq19GKFDE9/066+6ynDz6A2bMhKMjqSEQSecTFLBEv6Yoski08Kpy9l/ZSOW9lq0NJdfqu6MvBywf587U/8fbytjoc97RwIfTvD7t2QZYsTj3U4kOLeX/l++x8e6fzSpAfZvNm0xirTRvTnTNNGmviEEK4xrp10K2bWRvHy9rxipMnIV8+573tREeba5STJsFzzznnGA4xYADcuQMTJlgdiUiGwKBAxgaO5di1YxTJXoTe1Xq7VVL7sK7IqZEktiLZjlw5wmMZHiNb+mxWh5LqRNmiaPhjQ96v/j5NijSxOhz3c+qUGcH84w/zr5OdDT7LtdBrlHu8nNOP9UBXrphyuOBgmDsXnnzS2niEEM7TrBk0aWKSW4vcvGmaAo8ZA95Ovsb6+++mCGfvXjde7ezyZZOB79wJ+fNbHY1IoSSxjUtKkUWy9V7em/Wn3b0Pf8rk4+XDsnbLaFKkCcFh7j75yMUiI02Hyg8+cHpSGxEdQb+V/ciePrv1SS1AzpwmmX/xRahcGZYtszoiIYQzHDsGAQHmQpaFBg+GkBDnJ7Vg3taKFXPzwdBcucyFhhEjrI5EiFRFRmxFsti0jRxf5eBIjyOuaZQj4nUj7AalJpZixesrZK7zXYMGwZ49JsFzcnnee8vf48SNE/za5lf3WM8ypg0b4LXX4PXX4ZNPwMfH6oiEEI7So4eZYuHobk2PYOdOaNwYDhww19Rc4dIl09w+UybXHC9Jrl2DokVhyxYoVMjqaEQKJCO2ccmIrUiWw1cOkz19dklqLZbVNytfPvclL819ieuh160Ox3orV5rmHT/84JSkdtryQPL3bYvfe/7kGlCLWbvm8n3z790vqQXTKXnHDnP2Wa8enDtndURCCEe4fh1++gnefdfSMA4ehJEjXZfUglmq98YN+PBD1x3zkWXPbi48fPKJ1ZEIkWrIiK1IlvO3zrPt3DaaF29udSgCM3J4J/IOU16cYnUo1rl4ESpWNCd8des6fPe1hwxhfdRI8AkFLw02BbZ0POs1gL+HuUGH1ITYbKaZ1IQJJuFv0MDqiIQQyfHVV7Bvn7mIZ5GLF+Hxx605dliYWd1ozBh44QVrYnio4GAoXBg2bTKjt8KzBAbC2LFw9Kj5+fXuDdWkeZQ7k8RWJMut8FtkSufOtUCpS2R0JLcjbpM5XebU2SXZZjMJ2zPPgBOSzGnLA+myoT6kDYl7Z6QfU2uupnMj9/nQi9fataYsuVMnGDLENZPihBCOFRkJTz9tlvmqWNGSEC5dMonlzp2mE7IVVq40U1kPHID06a2J4aE+/dQMa//0k9WRiEcxZIgpRQgNNUtqeXmBr6/pXOYmF7ElsY1LSpFFstSYUYNdF3ZZHYawS+Odhmzps/HqwldZcHCB1eG43hdfQESE6WSSBDZt40bYjXuNuDad2cTCgwuZtnMaP+z+gU9WjjUjtfHxDjP3u7u6dU1p8saN8PzzZshFCOFZFi40ia1FSS2YNWs7drQuqQVzHbNZMzhxwroYHqpXL1i1yiS3wjMEBpqkNiTEJLVgLpyHhJjbAwOtjU8kSLqIiCQLDgvm5PWT0qzIDQ18ZiCNfmpE8ZzFU8/PZ9MmGDeO6K1bCNcR+OHD4SuHORt8luth17kdcZu3KrzF+tPr+WnvT1wPu871sOt8Xv9zcqTPgf8Uf26G38QvjR99q/dlaJ2hTNk5hZvhN8num51C2QtxWR815cfx8bJxRR9z7XNOqscfh7/+guHDzYnxjz+a+bdCCPenNYwaZekE0w0bzNeBA5aFcM+YMWZ922vXzLRWt5Mpk7kKMHQozJ9vdTQiMcaONSO18QkLM/e7UUmy+I8ktiLJtp7bSsU8FUnj7aSV2EWS+T/hz6gGo2gxrwXTm01n0rZJHL12lKLZi7rdAuPxibZF4+3lzflb5zkTfIYbYTe4Hnqdl4q/xOng00zbOY3rode5EX6DLhW78HxWfwovrs21d9NxZ2YhWpVqxZyWc/hu+3fs+3cf2dJnI7tvdt4s/yaZ02WmQp4KZPPNRrb02Xg629NkTpeZoz2PktU3Kz5e/70t/vDSD/fF9d3ifZyx7QYvW9ygbV7k9PKgOVTe3qacqmZNaNcO3nnHLEQppclCuLfNm+HqVbPujUWqVjVlwBkzWhbCfWbONIPYf/4J7tjDj3ffNXNt9+6FsmWtjkY8zNGj/43UxmazmWW2hFuSObYiyTad2cSJ6ydoX6691aGIBHT7vRuz980mNDIUjcZLeeHr40u/6v0YVtf5c0Ruht8k6GYQ10PN6Gi1fNXQWjNp+6R7tz3/9PO0K9uOBrMbcOjKIa6HXqdw9sLs7rabT/7+hN+P/k629NnI5puN8U3GcyPsBosPLSZb+mxk9c1K5TyVyP/We5x6OhtZPx1F5nSZ8VLOmWUxan4g7+/x8Dm28Tl/3qz5mzatmQf2mHQ5F8JtvfIK1K4NPXtacvipU6F8ebNEtruIiDAxffoptGhhdTQJGD0a1q+HxYutjkQ8TKtW5kpJfDmSlxe0aQM//+z6uGKRObZxSWIrRAoVGBRI/Vn1CYmMm4T5pfFjdYfVDx25jYiO4N87/95LQgtnL0wuv1xM2Dbh3m0lcpbgncrv0GNpD9aeWsv10OtEREdwZcAVpuyYwqiAUfcS06+e/4rcGXIzbss4svpmJVv6bFTMU5Hyj5fnyJUjpE+Tnqy+WcmUNlPil84ZP95crt+0CdKlS8IrlTh33yr9+w1hV/qR4B1mRm5tXhDly7M+/dy7K/LDREWZZhk//GCS29q1rY5ICBHbyZMmozx1ypLh0hMnoEoVM00/f36XH/6B1q2DN96AI0dMjx+3ExpqRm1/+w38/a2ORsRHa/j1V1PBdOWKqXGPzc8PVq92i1JkSWzjksRWJInWmjKTyrDprU1k8c1idTgiHm0XtGXegXlo4v6NKxTlcpfjhaIvkNU3K/1q9GNUwCjmH5hv5p6GXmd/9/1sO7eNLr93uZeYfvTsRzQo1ID3lr9377bSj5Wm/tP1OXj5IDZtu1fi65fGz/lPctcu0z0kIMCcMDhJeDi0bGl6U5Uubbojf7JyLFf0MXKqInz0fG9a16hG5sxuWgb3KJYvNx1hevWCgQOdsg6wECKJ+vY10wW+/trlh9baLKtTuzZ88IHLD58oO3a4ec44frx5j/3jD6sjEbEdP24+906eNMvi/f23aRQVFmbKj6UrskeQxFYkyZErR2j4Y0NO9TlldSgiAf5T/Nl5YWeC9z+W4TF6VO7B09mepl3Zdhy8fJAbYTfuJaaPZXjMaSW9DnHrljmDGTbMlNE6idbQoQPcuQMLFiSc5738sum/1KOH00JxnaAgePVV0/Rk9mzImdPqiIQQN29CwYLmgt5TT7n88Neuwdtvm4KOtGldfvhEmzIFnn0Wihe3OpJ4hIdDkSKmiZQbjPgJzEj6F1+YZHbAAOjT579f8Lvr2B47Zn5uso6t25PmUSJJAoMCqf5kdavDEA9QNHtRdl/cjU3HbXTkpbyoX7A+g2v/tyxOyVwlXRle8r37LtSq5dSkFszAyNGjZvnXBw1efvklVK9uktuSHvZSxpEvn3nCH30EFSrA3LlmbWAhhHVmzIDnnrMkqb1zx6wT+8svLj/0IwsNNR8Pq1a5YQVNunTmfXXIEFixwupoxB9/mFFaf39zwejJJ++/v1o1t0pkxcO58XCMcGfh0eE0LNTQ6jDEA/Su1htfn/gnGvn6+NKrai8XR+RAs2bB9u0wbpzTD9WmjZkS5feQyuoiRUzjkrfeSriZokdJk8Zk65MmmeHor7825VhCCNeLjjYjR++9Z8nhP/7YfHmCd981TaPnzbM6kgR07Giulm7caHUkqdfJk2YB5L59YfJkc8UmdlIrPJIktiJJuvp3pWP5jlaHIR6gWr5q9KveD780fvdKir2UF35p/OhXvZ/bL/mToCNHzJqA8+ZBBudV4KxZYxqR5M8PuXMn7jFdu5rKXbcbJUiOpk1h2zZYtAiaNzf1iEII1/r1V7P+tAWjR3v2mPe1AQNcfugk8fGBiRNNA2K3lDatZ10pSEnCwuCTT6BSJfO3tG+f6dMhUgyZYyse2a3wW3T5vQtzWs5JfOdaYZnAoEDGBo7l2LVjFMlexCPWsU1QWJhZQLF7dzPZy0kOHYI6dUwFbt26j/74jz+Ghg1TWPVuRAQMGmSWQJg7V8qzhHClmjXN/L5WrVx6WK3NoTt2hC5dXHpoh7h9233W2r1PVBSUKGEmBCflQ0Y8uuXLzRJZpUvDmDHu19Y7CWSObVyS2IpHtubkGgavHcymtzZZHYpIbXr0gH//NaO1TrqocusWlCtnpkC98UbS9rFkiakY3L0bMmd2aHjW+/VXMzQ9aJBpsiEXt4Rwrm3bTEJ7/LgZjnSxnTvNGrGe1iA9KgqKFTNvWWXKWB1NPH78Eb77zgwty/uo85w5Yz6Q9+wx05eaNLE6IoeRxDYuD3ubEu4gMCiQ6vmkcZRwsUWLYOlSc4XbiScBGTOaddeTmtSCqditV8/kfSnOSy/Bli3mRXr5Zbh+3eqIhEjZRo82I00uTmovXzZ9AypW9LykFszL1b+/KfBxyzGctm3NWql//WV1JClTRITpdlyxorlavX9/ikpqRfw88K1KWO3YtWOeW8oqPNPp09CtG8yZA1mzOuUQNhu0b2+6+zuiynbMGNNQKkX2WypY0DQ+efJJ001y2zarIxIiZQoKMiWUnTu7/ND9+3v+lPouXcwMltmzrY4kHt7eMHQoDB7sppm3B1u1CsqWNZ9TW7ea+UG+8TfTFCmLlCKLJNFay/xa4RqRkVC7NrRoYc60nKR/f/P5t3KlWZHBUTZvhqefNn1fUqQFC+Cdd8yJQ48eUlInhCMNHGjWrxk71qWHXb8e2rWDgwfNctae7NAh8xzy5bM6knjYbGY08Ysv4IUXrI7G8wUFmeaSW7eav5lmzayOyKmkFDkuGbEVj+Rs8FkmbpsoSa1wnSFDzCjt++877RAzZ5olfRYvdmxSC2awJcUsARSfV16BgAD4/nto3RqCg62OSIiU4fZtmDbNrLPpYps2mbzA05NaMD2afHzMMsBux8sLhg0zFwZT7IeEC0RGwsiRZjJ40aJw4ECKT2pF/CSxFY9k3al1rDu1zuowRGrx119mzdqZM506yatBA1i2DLJnd/y+Bw82c9UmT3b8vt1G4cJmaDpXLlOavHOn1REJ4fl++AFq1YJChVx62OBg0xvu5ZddelinSpsWPvzQTd+aXnrJjNwuWWJ1JJ5p3TqT0K5aZS6yfvLJwxeeFymWJLbikUjjKOEyFy+aDk6zZsFjjznlELt3m0HGPHlMubAzpEljml/OmgXR0c45hlvw9TWLR44YYdY6mjxZRiCESCqbzQyZ9u3r0sOeOmVGOG/edOlhnS57dvjsM9NIyu36Hnh5wfDhZtTW7YJzYxcumHr5Dh1MMrtsmWlsIVI1SWzFIwkICpDGUcL57nZy6tzZtBd2gqAgePFFs4qGsyvrixUzA5pRUaZiKkV79VVTxzhpErz2mlk/SQjxaP7806wVVrOmyw6ptWm+/O67KXCZMsxavFmzmnnDbqdpU3NxcOFCqyNxf1FRpjtjmTLw1FNmEvXLL0t/BwE4ObFVSjVSSh1RSh1XSg18wHaVlVLRSqlXnBmPSL7lry+n0hOVrA5DpHRffgnh4eYKthNERJjziJ49TWLrCkrBgAHmwnKKV7SoaS+dMSNUqgR791odkRCeZfRos/amC0/Wf/8djh2Dfv1cdkiX8vIyg3qlS7vhBUalzKjtkCEpvLQnmTZuNMv3/PGH+f/nn0MG6Z0k/uO0rshKKW/gKPA8EARsA9pqrQ/Gs91fQBgwQ2u94EH7la7I1jl5/SSX7lySEVvhXJs3mw7I27eb5WScZP16M33NlRd5L1yAChVMk6rqqaWi/8cfzQn6559Dp05yVV2Ih9m921x5O3HCTA51kWvXzHtUqVIuO6Qlpk41TXOnTrU6kli0NiP0775rql3Efy5dgg8+MPNoR41yTamVB5CuyHE5c8S2CnBca31Cax0BzAWax7NdT2Ah8K8TYxEOMP/AfObun2t1GCIlu3bNfKBPneqUpFZr6NPH9KR69lnXfy7myWMqdDt0MKPGqcLrr5urCGPGmCd++7bVEQnh3saMMcmNC5PaadPgxo2Un9SC6avw55+mqMSt3B21HTrUlNsKM3o9frwZZs+Vy5Qdt24tSa1IkDMT27zA2RjfB9lvu0cplRdoAaTkfqEpRkBQgDSOEs6jtZlT+9JLTmvTP2oUrFkDVas6ZfeJ0qIFLFrk0nNW65UoAVu2mDU3Klc2SzEIIeK6cMF0x337bZcdcv9+0zE4tVR0ZskCX39tGkm5XdVvvXrwxBPw009WR2K9wEDzebFggel8/PXXKWP9KeFUzkxs47ucErvueQzwgdb6gW8tSqmuSqntSqntUXIVyxJaa5PYPimJrXCSiRPh9Gkzv9YJ/vjDTFu725PFSmXKwJQpZu3cVCNDBrPW7QcfQJ06ZgknIcT9Jk40DdicsfZYPGw2eOcds5Rq7twuOaRbeO01GJhg5xcL3R21HT7cDScCu8iVK+Yi98svm/Xr165NHaUEwiGcmdgGATFrCfMB52NtUwmYq5Q6BbwCTFRKvRR7R1rrKVrrSlrrSj4+Pk4KVzyIRjPrpVk8mdl5cx5FKrZ7tym/mjsX0qVzyiGqVjWNQ5w4bfeRlCxpBmUuXbI6Ehfr2NGcqHz5Jbz5JoSEmCvzbduaNXDbtnXDGkEhXCA0FL77zsyXcJFTpyBbNuja1WWHdAtK/VeS7Hbvwc8+a9afS20X/6Kjze9/yZJmZPbQIbOcj5Qdi0fgzOZRPpjmUfWBc5jmUa9preOtQVNKzQT+kOZR7un8rfP4+viSPb1rriKLVOT2bZPQDBnilIYZJ06YLp+//ALe3g7ffbJ8+CHs22dGblPdZ/ft22aoaMUKsyRQeLgpR/fyMste9OtnhpGESC2mTjVlyH/84ZLDhYSYPzWvVLzwY//+JrGdNcvqSGIJCDAj90ePOu1ir1vZts3UhqdLBxMmQLlyVkfkEaR5VFxOezvTWkcBPYAVwCFgvtb6gFKqm1Kqm7OOK5zj8w2fM33ndKvDECnRu+/CM884Jam9fh1eeMFMW3K3pBbMIHW5cianS3UyZjQnMsHBEBZmklowtZEhITBypIzcitRDa9M06r33XHbIXr1g7FiXHc4tDRliCkjWr7c6kliqVzflt9NT+HnXtWvQrZvpq9Gjh/lBSFIrksGp1+m01ku11kW11oW01p/ab5ustY7TLEpr3fFho7XCOoHnAmV+rXC8WbPMugvffuvwXdts0LIlNGpkPi/dUdq0MGIEnDkD//xjdTQWGDcu4XlkYWFy1i1Sj5UrTXO1evVccrhNm8zUjE6dXHI4t5Uxo2kq6HZL/4CpWPnsM/NemNLYbCZpL1nS/N4fOgRvvJG6yweEQ8iEVfFQoZGhHLx8EP88/laHIlKSI0dMY4jVq53SjtPLCz76CGrXdviuHW7lSrPc68aN5jM+1Th69L+R2thsNjh2zLXxCGGV0aPN3FoXzEmIijKDZKNHW99Izx288orpU6S1m00JqVzZTNP57jvo3dvqaBxn1y5TraM1LF0KFStaHZFIQeTSiHio8OhwRjUYRfo06a0ORaQUYWFm/tAnn0DZsg7f/YgRZkkddy1Bjq17d7MExaefWh2JixUtmvAVei8vc78QKd2BA6aBXtu2Ljmct7dJalu1csnh3J5Spm9RxYpw7pzV0cQybBh88YWZnuHpbtyAnj1NGVXnzrB5syS1wuEksRUP5ZfGj7cruW5NPZEK9O8PhQs7Za3GH380FU7PPOPwXTuNl5dZCeeXX0xPpVSjd2/TvSY+vr5mEqAQKd2YMaaRWkJ/Cw505oyZAfLcc242OmmxtGlNP4b337c6kljKlzcfZhMnWh1J0mltfulKlDBTTw4eNDXwUnYsnMBpXZGdRboiu17L+S1pV6YdL5d42epQREqweDH07WvKkbJmdeiuN20yJWVr15qpO54mOtqcbEZGpo5GmIDp3jJypBnFt9nMyY7W0Ly5+V0RIiW7fNlUJhw5Ao895vTDtWhhcqUhQ5x+KI8TEmL6NU2bBvXrWx1NDPv3m4D++cdMCvYk+/aZkqSwMJOcV65sdUQpinRFjksul4gH0loTcDaACo9XsDoUkRKcPm1GaefMcXhSCyaZ/e03z0xqwZQIfvNNyppO9VDDhpl51q1bm/lkrVub4etNm+DCBaujE8K5Jk82Xe5ckNT+8Yepev7gA6cfyiP5+ZmlY3PmtDqSWEqXNvNqnNBk0Wlu3jQdvuvXN2vRBgZKUitcQkZsxQOdCT5DlalVuPD+BZTULYnkiIyEOnXMSNyAAQ7d9eXLppLvxx9dUs3nVMHBZrWD8eOhaVOro7HQ4MGwdy/8+qvUTIqUKTwcChSAv/4yyYuTdekCbdqYMmSRsJs3zbKqbjVqe/gw1KoFx4+bhgzuSmtz4bp/f2jcGD7/HHLlsjqqFEtGbOOSEVvxQHci7tCjSg9JakXyDR0KmTJBv34O3W1oqFkCr3hxz09qwZyzzJoFXbuadXhTrY8+ghMnzEmSECnR3LlQpoxLktqwMJgyRZLaxLh61VwAOHPG6khiKF7cJIruvATawYNmZPnrr2HBAlPTLUmtcDEZsRVCON+qVWaNul27HF5y9+qrZlrmTz+lrIG9bdugUqWU9Zwe2fbtpqPLnj3w+ONWRyOE42gNFSqYEa3GjZ16qIMHzdzaAwdS2XJiyTB8uGlUvWiR1ZHEcPw4VKtmlkHLls3qaP5z+7Z5wb7/3kzefucdz1iOIAWQEdu4ZMRWPFDL+S05dPmQ1WEIT3bpEnToYIYhnTCPrHNnmDEj5SWAlSubOXGzZ1sdiYUqVTI/4HfeSXi9WyE80bp1phS5YUOnHkZr8+fTs6cktY9iwABzQeDwYasjiaFwYTOVZ9QoqyMxtIb5802340uXTJOrHj0kqRWWStSIrVKkB57SmiPOD+nBZMTWdcKiwsjxVQ7+7fcvGdLKBSGRBDabWbOualWzZq0DTZliOge/8YZDd+tW9u+HunVN341ChayOxiLh4Watw8GDzfC8EClBs2amGsEJS57F9MMPpufQli2SbzyqsDA3nN5y6pRpsnfkiLVdro4cMUnspUswYYKZ/ytcTkZs43roiK1SvAjsBpbbvy+vFL85OS7hBnZd2EXxnMUlqRVJ99VXZhKsg9eWWL7c7NKT1qpNitKl4X//g/btISrK6mgski6dKXHr0wf+/dfqaIRIvqNHzdWq9u2dfqiaNU1yK0nto/P1NV2SR4ywOpIYChSAVq3MEmlWuHMHPvzQfPi+8ALs3ClJrXAriSlFHgpUAW4AaM1uoICzAhLu49KdSzQp3MTqMISnCgiA0aPh558dWgO3f7+pbF640FRmpXS9ekHt2qZbcqpVpQp07Ajvvmt1JEIk39ixpjucn59TDzNzpllVrVQppx4mRatfH8aMMUvIuo3//Q+mTjWjpa6itVlXvGRJs2zf3r3mYqPUtws389BSZKXYojVVlWKX1lSw37ZXa8q6JMJYpBRZCA9w/bppjDJunCm5c6Bbt0xPobp1Hbpbt/fvv3Dliueu0ZtsYWHmd2r4cDNiIYQnunbNzCs4eBDy5HHaYQICzPK4Bw86ZcnwVOXLL2H9etPzwG16OfTqBWnSmIXPne34cXO806dN2XGdOs4/ppsIDDTXoY4ehaJFzRrz1apZHdV/pBQ5rsSM2O5XitcAb6UoohTfApudHJdwA11+68Kt8FtWhyE8jdam4U/z5g5Nam/fhtdeg+jo1JfUAvz9N7z8MoSEWB2JRXx9TUlyr15m4WIhPNHUqfDii05NaqOiTMOob76RpNYR3nvPlHRHR1sdSQyDBpkh+fPnnXeM0FD4+GOTydWrZ9pEp6KkdsgQM2I/b56puJ4/33zv4JlVTqeUaqSUOqKUOq6UGhjP/UopNc5+/16lVMWHPVYp1UopdUApZVNKVYq1v0H27Y8opRrGuN1fKbXPft845aR1RBOT2PYESgHhwM9AMNDbGcEI9xF0M4hfj/xKxrQZrQ5FeJpJk+DkSTO/1kGiokzfID8/916b3platTI9QwYMsDoSC1WrBq+/blq8CuFpIiNh/HiTKTnRzp3w1FPSa81R0qY1eeTevWaKqVvIk8dMz/j8c+fs//ffTQ37kSMmoe3Xz4wQpxKBgWYac0jIfw35bTbz/ciR5n5PoJTyBiYAjYGSQFulVOy6r8ZAEftXV2BSIh67H3gZWB/reCWBVzF5YyNgon0/2PfbNcaxGjnsicaQmMT2Ba35n9ZUtn99BDi2tlC4ncCgQKrnq46TLqiIlGr3bnM5c9480/THQfr2hYgIkzOn5l/JCRPMksBBQVZHYqHhw816yAsXWh2JEI9mwQJThlyhgtMOERlppqQvWZK63yud4Ztv4NNPrY4ihg8+MAu4nz3ruH2ePGkqrfr1g+++M5/l+fI5bv8eYuxYM2Adn7Awc7+HqAIc11qf0FpHAHOB5rG2aQ7M0kYgkFUpledBj9VaH9Jax7dSTnNgrtY6XGt9EjgOVLHvL7PWOkCbObCzgJcc/3QTl9gOSuRtIgXZe2kv1fK50UQC4f5u3zZDBGPGQJEiDt118+bwyy+p6oJxvLJmhX37zHlGRITV0VgkfXpTktyjh5l0LIQn0NqsP+rk0drXXjPvlZLUOt7IkaaS3G3Wtn3sMdOEzBHZdliYWZKvcmWoXt0MTz//fPL366GOHk146XSbDY4dc208yZAXiHnlI8h+W2K2ScxjE3u8vPb/P8q+kiTBdmZK0RhoAuRVinEx7soMpNaFJ1KNYXWGEWWTH7N4BD16mA/Edu0ctsvFi83opFSe/idNGpg715y8LliQSk9ga9SAtm1NJ4+ffrI6GiEebtMmuHEDmjZ12iGWLTNlyLNmOe0QqVqePKYh8YgR8OOPVkdj168fFCsGAweapYCSYtky8yFbtux/deypXNGipjAovuTWy8vc7yZ8lFLbY3w/RWs9Jcb38Z0hxH5WCW2TmMfG5sh9JcmDRmzPA9uBMGBHjK/fgIYPeJzwcBHREXwT8A0+XtLGXSTS7Nlm0sn48Q7b5dat5mJ0jRoO22WK0aKFuWL8ww9WR2KhESPML8mvv1odiRAPN3q0uRDjpAVlQ0PNtcUJE0xRg3COHj1g8mSro4ghZ07o3t2Mtj6qM2dMR8KePeHbb2HRIklq7apVS3jE1tfX9DB0E1Fa60oxvqbEuj8IeDLG9/kw+V1itknMY2N70L7yxXO7wyWY2GrNHq35ASisNT/E+FqkNdedEYxwD7sv7mb23tkyv1YkztGjZhLs/PmQwTFd50+fhpdegunTTcMkcb906cxAZf/+5twkVfLzgxkzzNq2165ZHY0QCTt50rQ179jRaYfw9oavv4ZGTmnHIu7y8TGvdbNmZuk5t9C3r5lUffx44raPiDBNpypWNPO99++Hxo2dG6OHCAsz5x9duph838/PjNCC+dfPzwySu9OSPw+xDSiilCqolEqLaez0W6xtfgM62LsjVwOCtdYXEvnY2H4DXlVKpVNKFcQ0idpq398tpVQ1ezfkDsAShz3LGBIzx7aAUixQioNKceLulzOCEe4h4GwA1fNVtzoM4QnCw6FNG9PQp6zjlrbOlcvMZXLwErgpSpky8Ntv8MQTVkdioVq14JVXoE8fqyMRImHjxsFbb0FG56wycOQIbNhgBt+E86VPD9mzw7BhVkdily2bGUIcPvzh265aZT6rAwJg2zYYPNgMQQrOnYPatc2fq5+f+Xf1amjd2lxgb93afO82P/dE0FpHAT2AFcAhYL7W+oBSqptSqpt9s6XACUyjp6lA9wc9FkAp1UIpFQRUB/5USq2wP+YAMB84CCwH3tVa310o6x1gmv04/wDLnPGclU5orP3uBoqNwBBgNPAi8KZ5HJas5JQhQwZ9x236radMry54lcaFG/NG+TesDkW4u169zDp6DupWEhFhBuA++QQef9wB8aUCW7eaaVHduj182xTpzh1zojZmjFkfVAh3cvOmmfu4Zw88+eRDN39UWpu1NZs1k+s7rvTvv1C6NKxZY/613M2bULiwycaWLDGVVEWLmvL3atVMs4q+fWH7dtPSV94r7xMQYK6R9uhhpit7SsGiUipEa+2YUrkUIjGJ7Q6t8VeKfVpTxn7bBq2p5ZIIY5HE1vnO3zpPhjQZyOKbShcMFYnz66/mTGrXLnPFOJm0NoMa166ZqT5OmoqW4pw5A5UqwYoVTl1FxL39/bdpWrZvn0N+F4VwmNGjYcsW0/HNCX76yXTr3bbNlMkK1/n5Z9Oh/tlnrY7Ern59815os5kPVC8vMxpbo4b5nO7e3SzIK5Ow76O1qXi4dQteeMHqaB6NJLZxJSax3QTUAhYAa4BzwBdaU8z54cUlia1zXQ+9zoHLB6j5VE2rQxHu7MwZsyzAr7+aTsgO8Omnpgvy3387bKpuqvHTT+b127EjFZ+z9OhhlpyaOdPqSIQwoqLM0mdz50LVqk45RIMGpsLFSbsXDxEVZRr5lShhcSCBgSaxDQmJe5+Xl/kdbNXK9XG5sYgIM6BduDC8/77V0SSNJLZxJWaObR/AD+gF+APtMZN+RQq09tRaPtvwmdVhCHcWFWUWS+zb12FJLcAzz8Dvv0tSmxSvvWaabV26ZHUkFvriC1i/HpYutToSIYxffzWT4J2UdUZHw/LlktRaad8+k0/euGFxIGPHmtbYCVm0yHWxeICLF6FePTOTqksXq6MRjvTQxFZrtmnNba0J0po3gdZAYeeHJqwQGBQojaPEgw0darLP/v0dsrsNG0xOUqeOWSdQPDql4LPPTNOtffusjsYiGTPCtGnw9ttucJYpBKYM+b33nLLrrVvNe6anzAVMqSpUMNNVBw+2OJCjRxNen8ZmM8PK4p5Ro+C550yVWObMVkcjHCnBxFYpMivFIKUYrxQNlEIpRQ9MN6vWrgtRuFJAUADVn5TEViRg1Sr4/nuYNeu/HvjJcPSoadhQsaIDYhNs325WbUi1q9/UqwdNm3puXZlIObZuNW1WX3rJ4buOijLN4t5+WxJbd/DZZ6Z/4j//WBhE0aIJfyZ7eZn7BbNmmT5uX35prtE74DRGuJkE59gqxRLgOhAA1AeyAWmB3lqz21UBxiZzbJ1r6bGlPJv/WTKmdc6yBMKDXbpkMtAffjCXOpPpyhXTrHHgQOjc2QHxCcAMEJ07B/PmpdKT3lu3zFpI330HDRtaHY1Irdq2NX0I+vZ1+K7HjTNVzqtXp9K/cTd0/rzFS689aI6tn5/5ZfGgxVcdLTLSrD+7dKlpGl2ypNUROYbMsY3rQYltzC7I3sAV4CmtsXRJaklsnSc4LBiNJqtvVqtDEe7GZjNDgZUqmS5FDhAebj5gWkv9h0OFhpo5dz/8kIq7JK9aZVps79sHWaS7u3Cxs2ehXDk4edIpv3+bN0OOHFDMkhaeIiHz55vPtfbtLQpgyBDTIjsszHxm3+2K3K+fZy2+6gSvvQbXr5tO1impcb4ktnE9KLHdqTUVE/reKpLYOs/4rePZe2kvU16cYnUowt18+aXp7LRuXbLXlLDZTKVor15QsKBjwhP3Cw+HdOlMc5lUu2zS22+bOWdT5P1MuNgHH5g/wjFjHL7ruXNNdbOvr8N3LZJp925TJHLwoLnwYInAQNNI6tgx05H77jq2qdThw6br8ZkzkD9/yvs8lMQ2rgclttHA3QxSAemBEPv/tdZYMt1aElvnabeoHfUL1uetCm9ZHYpwJwEB5kxq2zZ46qlk727QINMwatUqOTlzpg0bzByilStT3od5oty8aUqSp093SOm8EIly+zYUKGDm2D79tEN3vXKlmVu7f7+pLhXup1cvM2Aq19OsN2eO+XmsWJFy+3hIYhtXgtOmtcZbazLbvzJpjU+M/0sPsRRIOiKLOK5fNzU8333nkKR22jRYsMDMD5Ok1rlq1DDzikaNsjoSi2TObM4uO3c2826FcIWZM6F2bYcntWFh8O67MH68JLXu7JNPzL/R0dbGkZppDQMGwIcfmgvoKTWpFfGTfmACgGhbNO3LtqdYTpm0I+y0Ngu8vfiiwzp7ligBf/4JOXM6ZHfiAby9TQfIr76CvXutjsYiDRua0doBA6yORKQGNpspA3XCEj8rV0L58tCkicN3LRwoSxZzPe3iRUlurWCzmYZqTz1liszKlbM6IuFqCZYiuyspRXYOm7bhpeQ6h4hh0iTzCR0QkOzh1f37zUjt0KGOCU0k3ubNULasWeY1VQoOhtKlTTetevWsjkakZL/9Zobstm51aLviu32AUvWceQ/zwgvmq3t3qyNJPfbvN83Ily2DfPmsjsY1pBQ5LslkBAAfrfmIsYFjrQ5DuIs9e+Djj82aMclMai9cMB/wsoyeNWrUMKMHTuhj4xmyZDGl9J07m/mPQjjL6NFmtNaBSa3W0KyZmTMvSa3n+OorcyH333+tjiR1WLQI6tY1xTmpJakV8ZPEVgCw+exmiucsbnUYwh3cuQNt2pjJmcnMRu/cMZXMXbqYqbrCGtmymVUgVq+2OhKLNGli5j0OHGh1JCKl2r3bdKJt1cqhu503z6welIob23qkUqWgQwf46COrI0n5QkPN6cqyZRYutSTcxkNLkZXiFhB7o2BgO/C+1pxwUmzxklJkx4uyRZHty2ycfe+srGEr4M03zTDBzJnJ3lVkpFk3rkMHhw5iiCRYudIMWu7Zk7LW8Uu069dNl+Qff4Q6dayORqQ0b7xhmgg48OJJcDCULAm//GIqL4RnuXXLfD3xhNWRpEzBwfD116a4LE2a1HmOIaXIcSVmxHYU0B/IC+QD+gFTgbnADOeFJlzlWug12pRqI0mtMCf9AQGm9WYyDRtmBjDeeCN1fuC4mwYNTGJ78qTVkVgkWzYzb7xTJ1NKIISjXLhg5td27erwXX/1lSS1nipTJjMT4r33zEVe4TiHD0PVquZ6Jcg5hvhPYkZst2hN1Vi3BWpNNaXYozUu7TkmI7ZCOMmxY+YMatWqZLcSHDvW9J3atAmyZnVMeMIxoqLMSUHp0lZHYpEOHUySO1Z6CggHGTwYrl2DCRMctsv9+83JeqlSDtulsIDW5qJikyZOaZadKp0/bzqEf/aZuVibmqXkEVs1TD0DDAXyAz6AArQeoh+4llpiRmxtStFaKbzsX61j3OdZLZVFvAatGsTak2utDkNYKTzczKsdNizZSe2SJWaUYelSSWrd0ZEjpjnw2bNWR2KRMWNMi+71662ORKQEoaGmOVnv3g7bZXS0mRGybZvDdiksopQpgPr0U5OQiaSz2WDXLlPavW2bJLWpwHRM1XBNoDJQyf7vA/kkYsftgLHAREwiGwi8rhTpgR5JjVa4jwWHFtCubDurwxBWGjAAChSAd95J9q7y5jXJbf78yQ9LOF6pUtCnjykRX7XKLCOSqmTPDhMnmpLkPXvAz8/qiIQnmz3b1EQ6sO37d99B+vTmb1R4vmLFoF8/s564zLdNmlu3oGNHUxixerWcX6QSwXqIXvaoD5J1bFO5KyFXKDSuENc/uC7r2KZWS5aY0YZdu5LVVejMGZg82VyZlvku7i062jQJHjTILMWUKrVrB7lzm3aaQiSF1uZK0fjxDlsjOTralFnOnStlyCmN1iYxy5HD6kg8y6lT0LQpVK9u/tTSpbM6IveRwkuRvwC8gUVA+N3b9RC980GPe+iIrVLkAroABWJurzVvJTFW4Ub+ufYPDQs1lKQ2tTp71jQ8Wbw4WUltcLCZQ9SpkyS1nsDbG1asgAwZTHlXqhu1BRg3znRJbtkSnnnG6miEJ1qxwrRjrVvXYbv08oKdO81uRcoSEPBfoUjatFZH4xmioiBjRnj/fTNiK+cXqcrd/k6VYtymgQdeRUxM86jNwAZgBxB9b8+ahUkKM5lkxFYIB4mKMsueNG2arCUqIiPNqF/RovDtt/LB40mOHjXljuvWpdKr4IsWmWHr3btN7acQj6JhQ2jb1pxxO8Dq1TBjBvz0k0N2J9yM1mZd95o1ZUnth9HarL0eEGDepkX8UvKIbVIlJrHdrTXlXRPOw0li61ifrv+U18u+Tv6sMmEh1Rk8GLZsgeXLkzVkFx0N06fDW2+BT2Jm7Qu3obUZsCxUyKwHmCq9+io8+WQqfgFEkhw4AM89Z+okHXBVKDwcypY1J/Mvvpj88IR7+ucfqFLFjNrmy2d1NO4pJMSMbB8/bpLaJ5+0OiL3lZITWzVMZQGGAM/ab/obGK6H6OAHPS4xZ7N/KEWTZMYn3FC0LZovN31JpnSZrA5FuNrdoYHZs5OV1I4bZwa7unaVpNYTKWWWZfr5ZzNqmyp9++1/6zcLkVhjxkD37g4rdfjqKyhRQpLalK5QIVPBnieP1ZG4rzVrTKn2+vWS1KZyM4BbQGv7103g+4c9KDGnor2BD5UiHIjk7jpCmsxJj1W4gwOXD/BEpifInj671aEIV/r3X7OW5w8/mOY5STRnjhldeOUVB8YmXC5nTnNVPNV2mcyVyyS3b75pGqhJSbJ4mH//NUtGHT3qsF0WLy5dkFOLSpVMoVTatA7rOZYirFljCiDeesvMkBKpXiE9RLeM8f0wNUztftiDHjpUozWZtMZLa9JrTWb795LUpgDbzm2jWr5qVocRV2Cgmbfk72/+DQy0OiLPFvP1fPVVaNbMnEE991ySd7lxo2mk/McfsnxBSlC1qhm4HznS6kgs8sorppHU0KFWRyLc2d330vLlzbJR//yT7F1qDX/+aaYEPPVU8kMUnkFrU+kUFmZ1JNbTGsaOhddeS8UXWEV8QtUwVfPuN2qYegYIfdiDEpxjqxTFteawUlSM736teWC7ZWeRObaOo7UmJDKEDGndqDx/yBBzdh0aat7tvLzA19csAjdsmNXReZ7Yr6dS5mvQIBgxIsm73bfPDFrUr+/AWIWlQkKgYkXzZ9amjdXRWODff80kx99+M5PghIjJSZ9Nv/xiHr5rl3RCTm1efhnKlTO/WqnZmDEwc6ZZnKFgQauj8SwpfI5teeAHIAumWvga0FEP0Xse+LgHJLZTtKarUqyN526t9YPbLTuLJLaOM33ndF4r8xrp07hJ6V1goMmUQkLi3ufnZ+aFVnPDEWZ35YTX8+pV02Pns89S6RIxKdy2babD9a5dkDev1dFYYN48GD4cduwwSYsQ4LTPpps3oWRJs2ZtzZoP316kLGfOQLdupvIpNX6eBgWZa0SZMpmLOhlSZHrmXCk5sb1LDVOZAfQQfTNR2z+sK7K7kcTWMa6FXqPAmAJc/+A63l7eVodjtG1rTizj+51UynTWaNHC9XF5qsWL4eDB+O/z8oLWrc1E2UQKC4PnnzdLfn7xhYNiFG7nm2+gVClo1MjqSCxwt0108eLm6o0Q8ODPpiS8l941aZK5mDRjhgNiFB4rJMRM7U9NS+Vt3Gj+bL78Etq3tzoaz5USE1s1TL2uh+gf1TDVN7779RA96kGPf2jzKKV4OZ6bg4F9WvNv4sIU7mZL0BYqPVHJfZJaMI04ErrQojXcuCGjKI/ixo2E77PZ4NixRO/KZjMNHZ54Qs73U7r33zd/bocPm/wuVVEKJk409YEvv2y6vAjxoM+mR3wvjalbN9OzTKRuzZpBz57QvLnVkbjG9Onw4Yemf2WqvIAqHuZuop6kJVsS0xW5E1Ad7pUk1wECgaJKMVxrZiflwMJaAUEBVM9X3eow7le0qFk7xmaLe5+XF9SuDR995PKwPNaBAzB/fsKvZ9Gij7S7mjXNSVhqLJlKbS5ehFq1zBJApUpZHY2LPf44jB5tftm3b3fYci7Cgz35JOxMoK1IEt5LbTbT9XXUqFR48UjE8eGH5sLx88+byvaUKjLSLAuYLx9s2gSFC1sdkXBHeoj+zv5vkpoXPLQUWSl+BzprzSX797mBSUBnYL3WlE7KgZNKSpEd49zNcwDkzexGE+lkjq1jOej1/PFHKFBA5oClNtOmwYQJsGWLWZYiVdHaTHsoUwY++cTqaISVjh0zF1WvXDFn5rEl4bPpu+9g1izYsEEuFAqjbVt4+mn49FOrI3GOCxdM8/n+/eGll6yOJuVIiaXId6lh6itgBKYT8nKgHNBHD9E/PuhxiXlLLXA3qbX7FyiqNdcw69oKD2PTNk7eOOleSS2YE4O2bU054N1Pey8vc+LQr58ktY+qWjXzuvn5Jfn1XLXKlKbmyuXkWIXb6dTJLD/y/UOXQ0+BlDITIKdMSXikTqR827fDs8+atsWDBiXrvfSuf/+FwYPNr5ckteKub76BVq2sjsI5tmwxjeYbNzZl10IkUgN7w6imQBBQFOj/sAclphR5g1L8Afxi//4V+20ZgBtJi1VY6eDlg7y55E2O9UzavCCnioqCd96Ba9fMlfIiRcyCqZLUJs2wYebTZOzYR349Dxww68r98gsUK+aCWIVbUcqM1mfI8N9KUalKnjxmeZc33zQdflLdsHUq99df5g1w6tT/hpiS+F4aU0SEaZhTtqzjQxae64knIGtWUyXTvXvKer+dOtW0LnjxRasjER7m7gJoTYA5eoi+poY9/A8jMaXICngZqIlZR2ij1ixITERKqUbAWMAbmKa1/iLW/c2BTwAbEAX00VpvfNA+pRQ5+abtnMbfp/9mdgs3mx599SoUKgTHj0POnFZHk+odOWKS25fjax8nUo0rV8wSQCtXQpYsVkfjYlqbIQZ/fxg61OpohKvMmQN9+sCCBWayuYPs3Qu5c5svIWKLjDRriX/8seeP3kZGmpYo3brJ2rTOlMJLkb8AXsKUIlcBsgJ/6CG66oMe99BCGK3RWrNQa97Tmj7ARaWY8NCAlPIGJgCNgZJAW6VUyVibrQbKaa3LA28B0x62X5F8AWfdsHEUmJrH5s0lqbXYnTvmA6lgQUlqhflzLF/eDE6lOkrB5MlmuGH3bqujEa4wZgwMGGDmzTowqY2IgDZtYPNmh+1SpDBp0pi3mr594dYtq6NJun//heeeMxfGs2e3OhrhqfQQPRDTvLiSHqIjgTvAQ3uHJ2qGh1KUV4ovleIUZoT1cCIeVgU4rrU+obWOAObGDkhrfVv/N2ScAfCsRXU9VJvSbWhatKnVYdzPZjOTjrp3tzqSVC06Gtq1g3PnzIesEGDmf23aZAawUp28eeGrr0xJcnzNg0TKoDUMHGg6O23cCKUd2xdz5EhTkCSNc8SD1KoFDRuajvSeyGYz3Z1r1YLffkuFVT4i2dQwVc/+78tAXaC5/f+NgBoPe3yCc2yVoijwKtAWuArMA5TW1E1kbHmBszG+DwLiDB8rpVoAnwOPAS8kct8iiSKiI6jxZA0yps1odSj3W7nSTDCpUsXqSFK1fv3MleL581PWHB+RPBkzmt+J9OmtjsQib7xhXoAvvjCdf0TKEhkJXbqYxZs3bHB41dCdO+a67fr18r4qHm7qVPN7EhVllsfxFH//bXqt/fUXPPaY1dEID1YbWAPENytbA4se9OAE59gqhQ3YAHTSmuP2205ozdOJiUop1QpoqLXubP++PVBFa90zge2fBT7WWj8Xz31dga4AadOm9Q8PD09MCCIeK/9ZyecbP2ftG2sfvrErNWtmypA7dbI6klRLa1MG1a6ducYgRGx3T9Dffz8VnqAHBUGFCrBmjVkGSKQMd+5A69b/b+++w6Oqtj6Of1dC76iINAsKKHZQCKLiVVGwYVdEUa4CIvarr/WK2K8dC11BFAt2VASxgpogSBEQlWIhFAGlh5Zkv3/sQSOkZ86cmcnv8zx5JjPnnL1XRkxmnb332v77MWN8tbSAugmoaUlCkyb5Zf2ffBL/v2uzs+GWW+Cdd/xUe60hj51kXmNbWoVNRT4HWA58ZsYwM07AF48qrkygSZ7njYGlBZ3snJsE7GtmO90qdc4Ndc4d4Zw7okIi3b6KQ+mL02nbqNB117H3yy9+nmPXrmFHUm598IG/y9q3r5JaKVilSr5K9rNFVllIQo0b+xHbyy7TlORk8ccffjFgvXr+U3kAmefbb/uaBUpqpSTat4e1a2H06LAjKdzGjb5Y+OzZvni8klqJFutvD1h/q5PneV3rb/cVdV2Bia1zvO0cFwD7A58DNwD1zRhkxknFiGkq0MzM9jGzSvhpzWP/EbTZfmb+XpSZtQIq4ac9S0AylmTEX+GooUOhe3e/J6DE3PTp/rO61sJIUSpW9FsA9e8P8+aFHU0I/v1vP031kUfCjkTK6rff4Oij/dzJESMCKSqwYYMvutaxY9SbliSXmupnUP3f/8GaNWFHk7/Nm/3HtksugXHjVChKoq6z6+fWbH/i+rnV+K1/ClWcqsgbnWO0c5yGH3WdCdxa9HUuG7gamADMA8Y45+aa2ZVmdmXktHOAOWY2E19B+QJX1P5DUibtm7SnXZM4Smy3bIHnnvM14SXmFi/2s8CHDIG2cTaQL/GpWTN48EHIyAg7khCY+QVwTzwBc+aEHY2U1pw5fkisVy+/qWxAcz3vvhv+9S/o0CGQ5iXJtW0LTz8dn4UcX3/d78W8ZYsfl9BkSglAqvW3ytufWH+rClQu5Hx/XqLlkdrHNsm8/LK/Wz5xYtiRlEu//OKrL152WciBSEJatAiaFqvqQpIZOtQnuOnp+kSXaL78Es45Bx5/3BcUCNATT/guVEhHymLaNKhSJeqFukslJ8fXz3v5ZXjrLb/vroQnmdfYWn/7P+AMYAS+aNS/gbGun3u4sOuKtd2PJIcXZr7A/038v7DD+KeBA7XFTwi2bYN77vEfuJTUSmls3Ohncn71VdiRhKBnT78Y/bHHwo5ESmLsWDjrLHjxxUCT2txcX0TnhhuU1ErZzZzpf+Xk5oYdCSxb5ouHT52qpFaCFUlg7wMOAA4E7i0qqQUltuXKV4u/okmtJkWfGCuzZsGvv8Lp+VX0lqA45+8lTJniiwGJlEb16r5C8iWXwLp1YUcTY9unJD/6KHz/fdjRSHEMHw69e/vFgCcVp0xI6T3/vK8cnmAT4iRO/fvf/t/SyJHhxTBvHlx/vd/W+623fL01kRiYB4x3/dx/gMnW32oWdUGxE1szauT5fr/SxSdhysjMiK/1tQMH+jVOmsoXqIwMX3C6dWv/2Levn9r02mt666VsunSB448vp7WU9t4b7r3Xf+rMyQk7GimIc3DfffDAA36jzSOPDLS7Vavgjjv8TZ9436ZFEkNKiv+4NGRIODdL3n3XrxM/7DD9m5bYsf7WE3gDGBJ5qRHwTlHXleRj7Vdm/Ay8DDwI7FvCGCVEObk5NKrViEPqHxJ2KN7atX7PQI12BKpfPz+otGmT/4M4c6ZPZvv2hRo1irxcpEgDBvgKnuVSr17+99gTT8BNN4UdjewoJ8eXJf7ySz9nvkGDwLt88km46CKfBIhES6tW4Sz7SE+Hq6+G99+HNm1i37+Ua32BNsAUANfPzbf+VuTijgJHbM2oZvZ34usch+IrHL9CMaoiS3xJTUnlw24fUik1Tuaejhrlp4PF4INGeZWR4ZParKy/7/Lm5sLWrf7Ob7msaitRt31/zg4dYPnycGOJuZQUX9X9oYf8wjOJH1u2+Ckqc+b4kdoY/a3p1w/uvz8mXUk5U6ECnHaaX98atPXrfVKbluZXjSmplRBscf3c1u1PrL9VwBeRKlRhU5E/BXb7q0HjLKAPcDJwWanDlFAM+3YYY+aOCTsMzzkVjYqBAQP8SG1+Nm/2x0WioUoVOOaYv9eClSv77OM39tWU5Pixbh2ccoq/kzd+fEw26d62zU/NX71aW7JLcC64APr0CfZXzfz5PqF94w0/9Vj700pIvrD+djtQ1fpbR+B14L2iLiossa3qHMsBzOgF3A6c4BwfA/WjELDE0Ds/vkOFlDhZUPn5537u4rHHhh1JUvvpp4KTjNxc/8dLJFr69YMVK2Dw4LAjCUGfPr4Sm+4WhW/5cj99oEULX0igSpWYdPvEE342jIrqSJAuucTfOBkypOhzS+OLL/wWz9deq6LvErpbgJXAbKA3MA64s6iLCst0/jCjH9AEOBto4RwrzWgAxMl8VikO5xwZmRkMO31Y2KF420drVYUgMFu3+uQ1JSX/LQJSUqB589jHJcmrYkUYPdont+XO9inJbdv6Ku/NmoUdUfm0YAGcfLLfw+zOO2P2N+bXX+Hhh32lef1ZkyCZ+cJkixdHt13n/AyvvfeGt9/2ya1IWKy/pQDfuX7uIKBEyUthI7bnATnAT0BPYLwZzwNfAw+VMlYJwdL1S2lUsxENazYMOxRYuhQ++QQuvjjsSJLWZ5/5wiVVqkDlyvmfU6WKvyMrEk0tWkC7dvDMM35qZrmy775w113Qo4emJIfh22/9LKBbboH//jemGebatT6x3VclNSUGDjwQTjwRJkyITnsbNsD55/tZN3vtpaQ22ZhZJzP70cwWmNlONZLMeypy/Dsza1XUtWa2i5lNNLP5kce6kde7mdnMPF+5ZnZY5Njnkba2HyuwEJTr53KBWdbf9izxz+uKuSDKjIZAe+A75/ixpB1FS/Xq1d3GjRvD6j5hOeeweLiV3L8//P67H7WVqPvjD7/W8YEH/Hqvu+/2BaQ2b/57BLdKFV/AtX//sKOVZOScX97Ypk05/DeWmwvHHQfnnOOr8UpsTJwI3brB0KFw5pkx7XruXGjaFKpWjWm3Us6tXw8HHOBn25clEV20yP8v07q1HwmO0cx9iRIzy3LOVS/keCp+gLIjkAlMBbo6577Pc84pwDXAKUBbYIBzrm1h15rZw8CfzrmHIglvXefcLTv0fTDwrnOuaeT558BNzrlpxfrZ+tunwJHAN8BfiZ/r584o9LriJrbxQoltyb0w8wUOb3B4+Fv9bNvm57mMHw8HHxxuLElk2zZ4+mm/ZnbQoL8T2O0yMvzSv/nz/QzJ667zhSFEgrJsGRx+OLzzTjn8tzZ/vh+2zsiA/bTle+BefdX/Unv99ZjXbdi4EVq2hBde8PczRGLptdf8Texvvy39nvRDh/oC4ldfrWn0iagYiW074G7n3MmR57cBOOcezHPOEOBz59wrkec/AscBexd07fZznHPLzKxB5PoWO/T9gD/d3RF5/jklS2w75Pe66+e+KOy6OKkmJEF6IuMJhpwWUKWBkhg71s/VUlIbNZMn++XKDRr46Z/wz6QWfGJR7pILCVWDBn5SxkcflcN/e82awR13wOWX+3UBO/4PKdEzYICfkvLxx6H8XbnnHjj6aCW1Eo7zz4dhw2DcODij0DGsf3IOHn/cjzP06hVYeBIbFcwsb6I41Dk3NM/zRkDeFdmZ+FFZijinURHX1nfOLQOIJLf5TSu+AOiyw2sjzCwHeBO4z+Uzumr9rQpwJbAfvnDUc66fy86n/XwpsU1yG7ZuYP6f8zlsj8PCDkVb/ETRmjVQpw78+KNfF3POObrbKvHl7LP915Il0KhR2NHE2LXX+r0yBg70QyESXc7B7bf7KjdffukXBsbY8uUwcqTf41MkDGbw3nslmwaflQU9e8K8ef5/H0l42c65Iwo5nt8nwx2TyYLOKc61+Xdq1hbIcs7NyfNyN+fcEjOriU9sLwFG5XP5C8A2YDLQGWgJFHttj24lJ7lpS6dxaP1DqVyhgCpCsTJvnl+MdPbZ4caR4LKz4amn/KDQggVwxRVw7rlKaiU+5eT4Iidjx4YdSYylpsLzz/tF7osWhR1NcsnO9qPhn34a86Q2IwO6dvXrEW+4AUaNgj32iFn3IjupWhUmTYLevYt3/nXX+ftCId0PktjLxO9us11jYGkxzyns2t8jU5CJPO64H8KFwCt5X3DOLYk8rgdeBtoUEHNL189d7Pq5IcC5wDEF/XD5KTCxNWO9Gevy+VpvxrqSdCLhOXavYxnbNQ4+VQ4e7LOwStopqrQWL4YjjvDrFidN0vI9iX+pqX6qXO/evmZcudKiBdx6q0/C8ttzS0ouKwvOOssv4v70U9htt5h13a8fnHCCX9c4fbp/PPts/7pImFq39hWSP/us4HO+/NIXl3zsMb8tW7VqsYtPQjUVaGZm+5hZJXzCuWNSMBboHqmOnAasjUwzLuzascClke8vBd7d3piZpeB31nk1z2sVzGy3yPcVgdOAvKO5ef21p0JJpiD/1ZeKRyW39396n3aN27FrtV3DC2LjRthzT5gxwz9KiaxYAZmZfgnZhx/6bTI1QiuJ5Pbb/dTN558PO5IYy8nxizC7d4c+fcKOJrH98Yf/5bfffn7P4IoVY9Z1RoZParOydj5WrZrfwa7crSWXuPLOO3Dbbb4Y1MCB8NNPfq/6a6/1xaXuvRfefVf/TpNNUcWjIuecAjwJpALPO+fuN7MrAZxzg81vmfIM0AnIAnpsL/CU37WR13cFxgB7Ar8B5znn/owcOw54yDn31782M6sOTAIqRtr6GLjRObfT3njW33L4uwqyAVUjcRngXD9Xq9Cft6DE1oxdCrvQOf4s7HhQlNgWn3OO3R/dnRm9Z9C4VuPwAhk61Fc3eOed8GJIQDk5fqD77rv91ow33RR2RCKls3Wr3ytxl0L/qiSpefN8td6pU321Fim5xYvh5JPhtNPgoYdiXpCra1c/Qpvfx6WUFF/E55VXdj4mEivOQceO8PXXfns/5/y/zZQUqFsX0tO1z3IyKk5iW94UVjzqWwpfPNw0kIgkahatXkTl1MrhJrXOwbPPwiOPhBdDgvr3v+HXX/30ooMOCjsakdKrVMl/uOrSxRexbdYs7Ihi6IAD/F2pK67w+61qukXJzJ0LnTvD9dfDjTeGEsJPP+Wf1IKfZT5/fmzjEdnRlCk+ed206e/XcnP914YNsHKlElspHwq87ekc+zhH08jjjl9KahNAemY67Zq0CzmIdD9/68QTw40jQaxcCTff7N+yRx9VUivJw8xP57zkEl//p1z5z39g3Tq/4FiK76uv4Pjj4cEHQ0tqwU/pLOh+REqKPy4SpgED/pnU5rVliz8uUh4Uaz6PGXXNaGPGsdu/gg5Myu6EfU6g/3H9ww1i4EC/tkx7ORYqJweGDIEDD4Rt2/xd1nr1NLgjyeXqq6FWLXjggbAjibEKFWDECL+/7W+/hR1NYnjvPV8oatQo6NYttDByc31iUNDv4ipV/DpGkTBpVoGIV2S2YcYV+AW/E4D+kce7gw1LomFl1kpa7NoivABWrIAPPoAePcKLIQE4B7Nn+0qFEyfCk09CjRphRyUSfSkpPr9r2TLsSEJw4IF+j5iePQv+BCre889Dr17w/vt+bW2IrrnG1626+WZfKGr7PdqUFP/8pptUkEfC17x5weMHmlUg5UmRVZHNmA0cCWQ4x2Fm7A/0d44LYhHgjlQ8qniytmVR75F6/PF/f1ClQpVwgnjoIX+b8Lnnwuk/zv3xhx/AadgQ7rrLf9bVCK2UB875gbhzz4Xq5ansRXa2z4L69PHbAMk/OeenHQ8b5vcvCfHT+JYtvvDynDm+EHO1ar468oAB/s9as2Z+T1AltRIPVLm7fFLxqJ0VZ37oZufYDGBGZef4AQhxGFCKY9rSaRy0+0HhJbXbS/pedVU4/ce5ESP8AE7Fin9PY1NSK+WFmf+gVe4qfW+fknzrrb7Sr/wtN9dniq+95tfWhpjUbtgAp54KL7wAhxzy956faWm++vG0af5RiYLEi7Q0//tUswqkvCusKvJ2mWbUAd4BJpqxGlgaZFBSdhmZGbRrHGLhqA8/hPr1/c7h8pdly6BBA19HZtw4aNUq7IhEwvH003Doof7/g1NOCTuaGDr4YJ/A9erlf3jd0fLDo927+82Ov/gC6tQJLZTVq31Se8ABPiSRRNG/vy8grlkFUp4VORX5HycbHYDawHjn2BpYVIXQVOTimfTrJKpXrE7rhiEllqecAhdcAJdeGk7/cWb1arjzTr9B+g8/aA2tCMCkSfDii+WwWPC2bdC2rZ+ucdllYUcTrnXrfJGoOnV8oYEqIc0yiujfH9asgcceU81DEYlvmoq8s+KssU0D5jrH+sjzmkBL55gSg/h2osS2aM45NmdvpmrFquEEsHChv0X4229QNaQY4siUKX7/zrPOgvvvh112CTsikfgybpxPcH/6yc9ALRejDLNmQceOMGMGNGoUdjTh+P13P8TUti088wykpoYWyuLFPqFt2dIntBpIF5F4p8R2Z8W5HzkI2JDn+cbIaxKnflnzC/s/u394AQwZ4kchynlSO2sWfP897L+/L+45aJCSWpEd9esHp5/ul1ZOnw5jxvgiKP36hR1ZwA49FPr2hd69y2eV5IULoX17OPNMvy1ciEnt/PlwzDHw5Zc+DCW1IiKJqTiJrTnHX391nSOX4q3NlZBkZGbQukFIU5A3bfLFUa68Mpz+48DatX7E6aSTYMECqF0bjjgi7KhE4k9GBjz6qK8btD23y831lT0ffdQfT2q33eaHCl96KexIYmv6dJ9J3nyzLwkfYib53Xdw3HF+qUifPqGFISIiUVCcxHaRGdeaUTHydR2wKOjApPRCLRz1+us+i9t333D6D5lzcOKJPr+fOxfOOCPsiETi14AB/v+V/Gze7I8ntUqVYORI+M9/fGW58uCTT6BTJz/1uHfvsKNh2zZ44gm44oqwIxERkbIqTmJ7JXAUsATIBNoCvYIMSsqmad2mnNj0xHA6HziwXG7xM3s2XH21T2w/+QSGDoXddgs7KpH49tNPBc/Czc31U0ST3uGH+xku5WFK8muvQdeu/gbo2WeHGspnn8Ett/jC/eefH2ooIiISJUUmts6xwjkudI7dnaO+c1zkHCtiEZyUznVp13F4g8Nj3/G33/pRh3K0d8e6dXDjjXD88b7oiHNQq1bYUYkkhubNC648axbqVqaxdeed8Msv8PLLYUcSnKef9iPTH38MHTqEGsp77/mi/Z07hxqGiIhEWZGJrRnNzfjEjDmR54eYcWfwoUlppC9Op/vbIW2+N3CgH3kIsQhIrDjnR5Q+/dRv5TN3rh+oLgc/ukjUXHdd4bu7VKuW/IOYgJ+SPGKEv0u2fHnY0USXc3D77X7q8ZdfwiGHhBrOlCnQs6cv6HfccaGGIiIiUVacqcjDgNuAbQDO8R1wYZBBSel9tfgr6lSpE/uOV6+GN9+Eyy+Pfd8x9v33vmrrK6/4gp4jRsDuu4cdlUjiSUuDm27yCez2kduUFP/8hht8DmRWTpLb1q39Qs8+fZLnB87O9j/TJ5/4pHbvvUMN548/fAmIjAxo0ybUUEREJADFSWyrOcc3O7yWHUQwUnbpmemkNQ5hA8iRI+HUU5M6w8vO9muyOnTwe9JecEHYEYkkvv79fd5z/vl/r3f85BN47DG49lo/Q7d1a78eN+nddZf/QV97LexIyi4ry6+jXbLE/wetVy/UcB59FE4+2d84CTm/FhGRgBRn255VZuwLfssfM84Fykn5xsRjWOwrIufm+k1aR46Mbb8x4hwsWgRNm0L9+jBnjn8UkehIS/Nf+dl7b7/C4eij/a+YpF7CX7my/yFPP90v3E/UG4V//ul/hqZN4fnnoWLF0EJxzt8veP11v7xXe9SKiCQvc0VMeTKjKTAUXxl5NfAz0M05fg0+vJ1Vr17dbdy4MYyupSATJ/r9CGfMSLpPDT/+6Ksdb94MkyYl3Y8nkjC++spPW/78c5//JbVbb4WFC302lmgWL/bb+XTuDA8/XHB1sBhZsgS6d/dLRxL1PoGISH7MLMs5Vz3sOOJJcaoiL3KOE4F6wP7AccDRAcclpTBx4UQGTR0U+463b/GTZFnfyy9D+/Z+hvVnnyXdjyeSUNq3h6+/9v8f3n03bNgQdkQBuvtuPzUk0RLb77/3Q+s9evi5vyEmtdnZfrC4QQM/E1pJrYhI8ivwr44Ztcy4zYxnzOgIZAGXAgsA7foWh8bNH8faLWtj2+nixX4o86KLYttvQJyDt9+GpUv9WtrvvoPrr4cKxZm0LyKB2l5I6tdfoV07P6iZlKpU8VXprrkGVq4MO5ri+fpr+Ne/4L77/NB6iLZsgQsvhFdf9d+LiEj5UNjt1BeBFsBsoCfwEXAecKZzdIlBbFJCGUsyYl84auhQ6NYNatSIbb8BmD/fr9+74w7/WbJRI2jYMOyoRCSvypX9SFzv3nDUUfD772FHFJC0NLjkEp/cxrv33oMuXeCFF3zMIdq82Verz831YVWtGmo4IiISQwWusTVjtnMcHPk+FVgF7Okc62MY3060xjZ/2bnZNHysIT9f9zPVK8Vouv3WrbDXXn4z1wMOiE2fAcnKgpYt/Xra664LtdaJiBTT9qJu330HBx+chMsFNm2Cww6DBx6Ac84JO5r8jRgBt90G774LbduGHQ25ub7+VvfummkjIslNa2x3VlhiO905WhX0PCxKbAuWk5tDakpq7Dp87TUYMsQntglq7Fi/fvaJJ/yd/ipVwo5IREoiJ8cv62zaFIYN83vgJpWvv/ZJ7ezZsNtuYUfzN+fgf//zfwPGj4cWLUINZ9UqP/34+edhzz1DDUVEJCaU2O6ssKnIh5qxLvK1Hjhk+/dmrItVgFI8ExdOJCMzI7adbi8alYAWLfK7UdxyC5x2mn9NSa1I4klN9cWBwCe4mZnhxhN1Rx0FXbv6qSTxIjfXFx94+WVfrjrkpHbJEjj2WD9g3KRJqKGIiEiICkxsnSPVOWpFvmo6R4U839eKZZBStGHTh7Fo9aLYdThnDixY4NdVJZCtW/3j+PH+Q/CsWXDCCeHGJCJlU60avPQS9Orl11Tm5IQdUZTddx988w28807YkfhqTN26wcyZvnBgyIUInIOzz4ZLL4X770/C6egiIlJsRe5jG280FTl/ez6xJ59e+in77bJfbDrs2xfq1fPbUiSIcePg2mt9pcwjjgg7GhEJyhlnwEkn+V9TSZPoTJ7s59rOng277BJODOvX+yyyZk0/WhvyNJdffvEjtGvWwK67hhqKiEjMaSryzsLdOV2iYsm6JWzK3sS+dfeNTYfr1/vd7nv2jE1/ZbR6NZx1lp/J98wzSmpFkt2TT/qC7Zdf7tfOJ4VjjoFzz/VTgMPw++9w3HF+MfPrr4ee1H77rS8cPWWKkloREfGU2CaBPWrswdSeU7FYDU289BIcf7zfDyeObdkCP/zgBxc6dvSzpzt1CjsqEQla06a+5tKGDfD552FHE0UPPODXtL73Xmz7XbgQ2rf3hQkGD/YLm0M0aRJ07uzrVh11VKihiIhIHFFimwSmLp1K5dTKsenMOXj22bgvGjVhgt/+49ln/ZYPV13l978UkfKhRg1fuL1TJ18p96uvwo4oCqpX9z9Mnz5+KkoszJjhR4tvuskvPYmDud3jx/uZ0AlW4kFERAKmxDYJ3PTRTcxbNS82nU2e7Cuz/OtfsemvFO680yeyTzwBTz8ddjQiEpbtOViDBn45wtCh4cYTFR06+B/mhhuC7+vTT+Hkk/0v0iuvDL6/Irz5pq+h9cADcOKJYUcjIiLxRoltgtuas5WZy2dyZMMjY9Ph9i1+4uCufV5bt/p1dRs3+qW/c+fCqaeGHZWIxIPOnf2I7ZNPwqBBYUcTBQ8+6OfjjhsXXB9jxvhiVa+/7vfRDdmIEXDNNZp5IyIiBVNim+BmLZ/FvrvsS83KNYPvbPlyP8e3e/fg+yqBjz+GQw6BiRN9Xau99gq9romIxJlmzSAjAy66CBYv9r/OElaNGjB8OPTu7UsCR9szz8CNN/pfqh06RL/9Eho61M+C/uwzOPTQsKMREZF4pcQ2we1dZ2+ePeXZ2HQ2fDicfz7Urh2b/oph0SL/2e7hh+H992GPPcKOSETiVa1a/tfXhAnQpg1MnRp2RGVw/PG+mNN//hO9Np3zazmeesovOwk5i3TOr3w56ig/QN2iRajhiIhInNM+tgnulzW/0LhWYyqkVAi2o+xs2GcfX43zsMOC7asI27bBgAF+2nG/fj60CgH/+CKSXN591y9beOIJ6NYt7GhKaf16XyVvyBC/FrYssrP9OtpZs/wU53r1ohNjKTkHN98M1arBPfeEGoqISFzSPrY704htgjtu5HEs/HNh8B29/z7suWfoSe3nn/sQPv7YTykEJbUiUnJduvjfJ7vt5pOo7OywIyqFmjX9TJqePWHt2tK3k5Xl19EuXuzn+4ac1Obk+Jk4X34Z3ra9IiKSeJTYJrBl65exbss6mu3aLPjOtheNioGMDOjaFVq39o8ZGX50FnwBmPvugw8/9GvmRERKq2VLP9D55ptw0kmwcmXYEZXCiSf66lg331y66//80//wNWv6GTk1akQ3vlJ4+WW/de7EibDLLmFHIyIiiUJTkRPY2/PeZtj0YYzrFmBlTICffvL7GP72W+AlKfv1g0cfhU2b/ChKSgqkpkKlSvDzz6EPJIhIEsrJgbvu8gnVW2/B4YeHHVEJrVvnpyQPHw4dOxb/usxMv9HvySfDI4/4X7gh2rQJ5s+Hgw7yS05UAVlEpGCairwzjdgmsCa1m9D3yL7BdzR4MPz734F/ysjI8EltVpZPagFyc/0HnJwcfwdfRCTaUlPh/vt9EbqBA8OOphRq1fKlg3v29Otui2PePGjfHi67DB57LPSkdv16OOUU//6npCipFRGRkgt0xNbMOgEDgFRguHPuoR2OdwNuiTzdAPRxzs0qrE2N2P4tJzeH1JTUYDvJyvJra6dNg733DrSrrl3htdf+TmrzSknxBZlfeSXQEEREWLgQRo70M0gSag3/FVdAxYpFb9abng5nneUz+TjYvu2PP/xs6sMP94ltasB/1kREkoFGbHcW2C1aM0sFngU6Ay2BrmbWcofTfgY6OOcOAe4FhgYVT7LZlrON+o/WJ2tbVrAdvfoqtGsXeFILfsZzQfdZcnP9FDURkaDVqeNzv1NO8UtQE8Zjj8EHH8CnnxZ8zgcf+MpZI0bERVILsGoVnHqqnxykpFZEREoryLlHbYAFzrlFzrmtwKtAl7wnOOe+ds6tjjzNABoHGE9S+e7372hQswHVKlYLrhPn4NlnY1Y0qnnzgmfDpaT44yIiQdt1Vxg/Hg45xNdVSphSFLVr+61/rrgCNmzY+fjIkXD55b5IVOfOMQ9vR7/84rfhbd7cj46bhR2RiIgksiAnWTUCFud5ngm0LeT8y4EPA4wnqaRnppPWKC3YTqZOhdWry74/YjFdd52vTpqbu/OxKlXg2mtjEoaICBUq+DX/y5f751995Zekxr3OnaFDB792tmJFPxWmeXNfXviDD/weR/vvH3aU/PCDv2lw001KaEVEJDqCTGzz+1OV731vM/sXPrE9uoDjvYBeAJUqVYpWfAmtZqWanN7i9GA7efZZ6NMnZkVF0tJ8japRo2DLFp/gpqT4pPamm/xxEZFY2mMPWLrU75t98cVwzz0JMF22Xj144QX/vXMwY4b//vrr4yKpXbIE/vUvePBBn3+LiIhEQ2DFo8ysHXC3c+7kyPPbAJxzD+5w3iHA20Bn59xPRbWr4lExsmoV7LcfLFgAu+0WeHc5OX63ieuvh5kzYcAAv6a2WTM/kqukVkTCtGIFnHee3+b1jTegatWwIypARgaccIIv/LejatXgk09C/YWaleXfuxkzoFWr0MIQEUl4Kh61syCH4qYCzcxsHzOrBFwIjM17gpntCbwFXFKcpFa8FRtXcPJLAU8PHjECzjwzJkktwDPPwIQJfouHtDRf/XjaNP+opFZEwrb77vDxx3DuuX4WyZYtYUdUgAED/Iaw+dm82R8PycSJfrvdTZuU1IqISPQFltg657KBq4EJwDxgjHNurpldaWZXRk67C9gVGGhmM81sWlDxJJOMzIxgO8jN9dtFxKho1K+/wr33+ponWmslIvGqYkXo0cMnZgcdBGPHFn1NzMVpefm334Zu3fwM6WoB1jwUEZHyK9Ad+pxz44BxO7w2OM/3VwBXBBlDMsrIzKBd43bBdTBhgi80cuSRwfWRx6RJcPPNqnosIomhWjV46SU45xy/dOLOO2NWiqBozZv7oPKrwhdSeflNm/zNy/HjNVIrIiLBiZc/xVICazav4eg9862zFR0DB/rR2hgMn65bB5dcArfcEnhXIiJR07atLxw/derflZPjwnXX+bnS+QmhvPwHH/gK09OmKakVEZFgKbFNQANPHciJTU8MpvFffoH0dLjwwmDaz2P1amjZ0k9FFhFJNA0a+C1h99gDbrghtFm+/5SW5svIV6v29zBySop/HuPy8g8+6PPolSvjaERbRESSlv7UJJgfVv3Ag5MfLPrE0hoyBLp3j8kiqJtv9vWp9tor8K5ERAKTkuJ30WnfHj6Mh93Y+/f31Y/PPx9at/aPn3ziX48B5+C22/x07cmToWHDmHQrIiI7MLNOZvajmS0ws1vzOW5m9lTk+Hdm1qqoa81sFzObaGbzI491I6/vbWabInWTZprZ4DzXtDaz2ZG2njILZlpooGtsJfo++/kz5v8Z0LDAli3w/PP+k0jAJk/2S3nnzg28KxGRwPXuDQce6HPId9+NWYmCgqWlhVpSfo894IsvYlZYX0REdmBmqcCzQEcgE5hqZmOdc9/nOa0z0Czy1RYYBLQt4tpbgU+ccw9FEt5bge2LChc65w7LJ5xBQC8gA19/qRMQ9VvBGrFNMOmZ6cEVjnrjDTj00JgUF2nd2q+9qlUr8K5ERGLi6KNh9mw44gi/oqO8bbmenQ29esH06X6pr5JaEZFQtQEWOOcWOee2Aq8CXXY4pwswynkZQB0za1DEtV2AFyLfvwCcWVgQkfZqOefSnXMOGFXUNaWlxDbBTF82nbTGAd2F3140KmBDhsDPP8MhhwTelYhITO26q6+7N3o0HHWU/11XHmze7Pf4zcyEAw4IOxoREQEaAYvzPM+MvFaccwq7tr5zbhlA5HH3POftY2YzzOwLMzsmTx+ZRcQRFUpsE8y3vb7lwN0PjH7DM2fCb7/BaadFv+085szxW2PUrRtoNyIioXr6abjiCmjXDr7+Ouxogte7N1SqBO+8o31qRURipIKZTcvz1WuH4/mtY91xo/OCzinOtTtaBuzpnDscuBF42cxqlbKtUtEa2wTy3e/f8fuG3+m4b8foNz5okP9kUiG4fxK5uX6a2r33qpiIiCQ3M7jmGjj4YGjSxO/lWqVKTHZRi6k1a3wie//9vkp0amrYEYmIlBvZzrkjCjmeCTTJ87wxsLSY51Qq5NrfzayBc25ZZJrxCgDn3BZgS+T7b81sIdA80kfjIuKICo3YJpDX577OpF8nRb/htWthzBg/vBCgxYuhaVOf3IqIlAfHHecT29tu8wXnN20KO6LoWbHC/3yvvAKNGyupFRGJM1OBZma2j5lVAi4Exu5wzlige6Q6chqwNjK9uLBrxwKXRr6/FHgXwMzqRYpOYWZN8QWpFkXaW29maZFqyN23XxNtSmwTSHpmejDra0eNgpNP9mUsA7J6tR+lfekl7WcoIuXPAw9ATg4cc4y/yZfoFi/2P8uZZ/qEXURE4otzLhu4GpgAzAPGOOfmmtmVZnZl5LRxwCJgATAMuKqwayPXPAR0NLP5+KrJD0VePxb4zsxmAW8AVzrn/owc6wMMj/SzkAAqIgOYL06VOKpXr+42lrdSk0BObg67PLwLi65dxK7Vdo1ew85By5a+otOxx0av3R2cdRYcf7yfmiciUh45B48/DrVrBz5BJnCPPOJHaG+8MexIRETKJzPLcs5VDzuOeKLENkHkulzmrJjDIfWjXEr400/h2mv9HhUBLf56+20/DW/WLKhcOZAuREQSyuuvw8qV0KdPYq27nT0b/vjDT0EWEZHwKLHdmSaFJoj5f8ynTpU60W944EDo2zewT1abNvlR2mHDlNSKiGzXqpWv2dezJ2zZEnY0xTNlCpx4ol9bKyIiEm+U2CaIh756iHHzx0W30SVL/IjtxRdHt908qlaF8eP9WiwREfH23RfS0339gRtuCDuaok2aBKefDs8/D+efH3Y0IiIiO9N2PwkiIzOD69peF91Ghw2Drl2hZs3othvx1Vd+39revQNpXkQkodWoAW+8AevWwapVsGABpAVQH7CsnINGjfz06Q4dwo5GREQkfxqxTQB/bvqTzHWZHLT7QdFrdNs2n9j26RO9NvPYssVPsdtll0CaFxFJCma+mNQPP8AZZ8Dw4WFH9E+vvOJ/l++7r5JaERGJbxqxTQAVUirw4lkvUiEliv+53n0X9tsPDopispzH//7nmz/33ECaFxFJKkcfDZMnQ5cufqbLk0+GHREMHQr9+8OECWFHIiIiUjQltglga85WTm9+enQbHTgQrroqum1GOAdLl8KzzyZWtU8RkTC1aOELNE2Z4p9v2OCnK4chPR0efBC++MLfpBQREYl3moqcAC5686LoFo6aN89/nXVW9NqMyM31NakGD4YmTaLevIhIUqtdG046yW+P1rIlTJ0a2/6dg/nz/VrfGTOU1IqISOJQYhvncl0u3yz5hraN20av0UGD4IoroFKl6LUZMXw4dO8e9WZFRMqVQw+FAQPglFNg1KjY9JmbC9dfDz16+Od16sSmXxERkWhQYhvn5q2cx27VdmP36rtHp8ENG+Cll6BXr+i0l8eyZXDHHf7DmIiIlM1ZZ8Hnn/td2XJygu0rOxsuvxy+/Rbef1/LSEREJPEosY1zFVIqcOvRt0avwZdf9qUtA5gnfNttPl8++OCoNy0iUi4deCCMHOn3u+3WzW8LFIRVq/w05AkTNFIrIiKJyZxzYcdQItWrV3cbN24MO4yYcc5h0bp17hwcdhg8+ih07BidNvNYtsx/IKpaNepNi4iUazk5cOed8Oqr8Pbb/ld5NGRlwcMP+xuTlStHp00REQmemWU556qHHUc80YhtnGv3XDt+XPVjdBpLT4dNm+CEE6LTXsS6dX5N1m67KakVEQlCaqqvUvy///n9btevL3uba9fCySfDokW+fRERkUSmxDaOrd28lrkr59K0btPoNDhwIPTpAynR/c9+551+PVbFilFtVkREdnD++fD991Czph+5Le3a202b4PjjfZGqkSOhgjb/ExGRBKfENo5NWTKFVg1aUTE1ChnjihXwwQdw2WVlbyuPjAx4/XV45JGoNisiIgWoUQM2b4ZnnoFTT/Xrb0siO9vPrnnoIXj66ajf6xQREQmF/pzFsa05WznngHOi09hzz8E550DdutFpL2LJEv/hatddo9qsiIgUokoVX+ipZUs48kjIzCzedYsWwSGHwG+/+VILqn4sIiLJQsWjyoOcHGja1M9ba9Uqas3+8AO0aKEPRiIiYRo/Hk48ETZuhNq1Cz5v7ly/pvaOO/yqFBERSVwqHrUzjdjGqVyXS+fRndm0bVPZGxs3Dho0iGpSO38+HH00/P571JoUEZFS6NTJF386+WS46y7IzfXLRLp2hdat/WN6OvTs6acfK6kVEZFkpHIRceqnP37ih1U/ULViFMoMDxwIV11V9nYinIPevf1d/z32iFqzIiJSSmbw7rtw3nmw//5+mcimTf739YwZMHYsXH89XHxx2JGKiIgEQyO2cSp9cTrtGrcre0MLF8K0ab6UZpSMHeu3+Lnmmqg1KSIiZVS/Ptx/P/z8s9+fdvtKI+f88yef9CO5IiIiyUiJbZxavG4xR+95dNkbGjzYbzJbpUrZ24o4/XQ/u1nbQ4iIxJeBAwveAmjzZhgwILbxiIiIxIqKRyWzTZtgzz39Lfp9941Kk9df73cMOuywqDQnIiJR1Lo1TJ9e+PFp02IXj4iIBEPFo3amEds4tH7Lem7/5PayNzRmjN8HIkpJ7YcfwnvvQfPmUWlORESirHnzgvelTUnR728REUleSmzj0NSlU5n066SyNxTFolEbNvhKmoMGQbVqUWlSRESi7LrrCl55UqUKXHttbOMRERGJFSW2cSh9cTppjdPK1si0aX4vns6doxLTn39Cr15w0klRaU5ERAKQlgY33eRvQG4fuU1J8c9vuskfFxERSUYq/xOHpiyZwqWHXlq2RgYNgiuv9JsbltGCBbDbbnB7FGZHi4hIsPr39/c0Bwzwe443a+ZHcpXUiohIMlPxqDi0adsmzIwqFUpZyXj1amjaFH78EXbfvUyxZGf7Zbo33wwXXVSmpkREREREJApUPGpnmoocZ5atX8YnP39S+qQWYORIOPXUMie14Pc9rFcPunYtc1MiIiIiIiKBUGIbZyYumsiL371Y+gZyc/005CgUjVq3Dh57zG+Fa1bm5kRERERERAKhNbZxJiMzg7RGZVgI9fHHUL06tGtX5lhq1YLvv4e6dcvclIiIiIiISGA0Yhtn0jPTadekDEnp9i1+yjjEOno0PPqokloREREREYl/Kh4VZ75Z8g2H1j+UyhUql/zi336Dww/3j9VLv5Z81So46CB47z1fOEpEREREROKHikftTCO2cWT5huXUr16/dEktwNChcPHFZUpqAf7zH18sSkmtiIiIiIgkAq2xjSMjZoxgZdZKHj/58ZJfvHUrDB8On39ephicg+bN/Z6HIiIiIiIiiUAjtnEkY0kGaY1LWTjqrbfgwANh//1L3X9WFkydCnfcATVqlLoZERERERGRmFJiGyecc6QvTqdd41IWjtpeNKoM7rnH71srIiIiIiKSSDQVOU5k52Zz3/H30bhW45JfPHs2LFwIZ5xR6v5nzoTnn/dNiYiIiIiIJBJVRY4TG7dupGrFqqRYKQbRr7oK6teHfv1K3f9550HnzvDvf5e6CRERERERiQFVRd6ZEts4cc24a2hatyk3tLuhZBeuWwd77w1z5kDDhqXuPysLqlYt8/a3IiIiIiISMCW2O9Ma2ziRnplOm0ZtSn7hSy/BCSeUOqn99Vd/eeXKSmpFRERERCQxKbGNA1nbspi3ah6tGrQq2YXOlalolHPQty8cfzykppaqCRERERERkdCpeFQcWLdlHTe1u4mqFauW7MLJkyEnB447rlT9jhnjR2zfeqtUl4uIiIiIiMQFrbFNZBdeCO3bwzXXlOryTz6BmjWhTSlmQIuIiIiISDi0xnZnmoocBy575zI+/fnTkl20fDl89BF0716qPseNg2OPVVIrIiIiIiKJT4ltyJxzTFg4gaZ1m5bswmHD4PzzoXbtEvf52WfQuzds2lTiS0VEREREROJOoImtmXUysx/NbIGZ3ZrP8f3NLN3MtpjZTUHGEq9+XfsrAHvV3qv4F2Vnw5Ah0KdPifvbtMkntc88A7VqlfhyERERERGRuBNY8SgzSwWeBToCmcBUMxvrnPs+z2l/AtcCZwYVR7z7fcPvXHDgBVhJ9tp57z2/d+2hh5a4v7Fj4ZBDoEuXEl8qIiIiIiISlwIrHmVm7YC7nXMnR57fBuCcezCfc+8GNjjnHi2qXRWPAjp2hB494KKLSnSZc36v2q1boVKlgGITEREREZFAqXjUzoKcitwIWJzneWbkNcmj7wd9+XXNr8W/4Mcf4bvv4JxzStRPTo7Ph7/7TkmtiIiIiIgklyAT2/zm1pZqeNjMepnZNDOblp2dXcaw4sfm7M2MnDWSetXrFf+iwYPh8suhcuUS9TV4MGzZAgcdVMIgRURERERE4lxga2zxI7RN8jxvDCwtTUPOuaHAUPBTkcseWnyYvmw6B+x2ANUqViveBRs3wqhRMH16ifrJzIR+/WDyZEhRHWwREREREUkyQaY5U4FmZraPmVUCLgTGBthfwvlh1Q+0b9K++Be8+iq0bw97laCCMn7q8ZAhcMABJQxQREREREQSUjF2qDEzeypy/Dsza1XUtWa2i5lNNLP5kce6kdc7mtm3ZjY78nh8nms+j7Q1M/K1eyA/b1DFowDM7BTgSSAVeN45d7+ZXQngnBtsZnsA04BaQC6wAWjpnFtXUJvJVjzKOVe8isjOQevW8MAD0KlTsdufPBmaNPFFlEVEREREJPEVVTwqskPNT+TZoQbomneHmkiudg1wCtAWGOCca1vYtWb2MPCnc+6hSMJb1zl3i5kdDvzunFtqZgcBE5xzjSL9fA7c5JybFu33Ia9AJ6Y658Y555o75/Z1zt0feW2wc25w5PvlzrnGzrlazrk6ke8LTGqTzT1f3MOWnC3FO/mbb2DtWjjppGK3v2YNdO3qpyKLiIiIiEi50QZY4Jxb5JzbCrwK7LjhZxdglPMygDpm1qCIa7sAL0S+f4HItq3OuRnOue3LTucCVcysZEWBykgrLkOyeO1invnmGSqnFvO/98CB0KdPiRbJ3nornHYaHH10KYMUEREREZF4VGF7cd3IV68djhdnh5qCzins2vrOuWUAkcf8phWfA8xwzuUdwRsRmYb8XyvWdNWSC7J4lBQiIzODtMZpxZuGvGoVjB0Ljz9e7Pb/+AO+/NJ/iYiIiIhIUsl2zh1RyPHi7FBT0Dml3t3GzA4E/gfknWbazTm3xMxqAm8ClwCjitNeSWjENiQZmRm0a9yueCePGAFdusCuuxbr9Jwc2GUXmDUL6tQpfYwiIiIiIpKQirNDTUHnFHbt75HpykQeV2w/ycwaA28D3Z1zC7e/7pxbEnlcD7yMn+ocdRqxDcndx91Njssp+sTcXBg0CF57rdht33cfVKgAd9xRhgBFRERERCRR/bVDDbAEv0PNRTucMxa42sxexRePWuucW2ZmKwu5dixwKfBQ5PFdADOrA3wA3Oac+2p7B2ZWAajjnFtlZhWB04CPA/h5ldiGYUv2Fj5e9DFnHXBW0SdPmOBHao88slhtz5sHTz8NM2eWLUYREREREUlMzrlsM7samMDfO9TMzbtDDTAOXxF5AZAF9Cjs2kjTDwFjzOxy4DfgvMjrVwP7Af81s/9GXjsJ2AhMiCS1qfikdlgQP3Og2/0EIRm2+8nIzKDPB32Y0XtG0Sefdhqccw706FHkqbm50KEDXHABXH11FAIVEREREZG4U9R2P+WRRmxDUOz1tT//DBkZMGZMsdu+5hqfB4uIiIiIiJQXSmxDkJ6ZzqnNTi36xCFD4NJLoVq1Ik9dtgy+/hrOPz8KAYqIiIiIiCQQTUUOwTdLvmGfOvtQr3q9gk/avBn23BO++gqaNSuyzfPOg+bN4f77oxioiIiIiIjEHU1F3plGbGNsw9YN7FFjj8KTWoA33oDDDy9WUjt2rN/a58UXoxSkiIiIiIhIAtE+tjH20cKPuOqDq4o+ceBAuKoY5wGjR8PQoVClShmDExERERERSUAasY2x9MXpRReOmjEDMjPh1KLX4W7ZAq++CmZRClBERERERCTBaMQ2xtIz00lrnFb4SYMGQe/eUKHw+w4ZGdCuGMWVRUREREREkplGbGOs60FdadOoTcEnrFkDr78O8+YV2s7WrdCzJ9xxh0ZrRURERESkfFNiG0PbcrZx1ZFXYYVloqNGQadOsMcehbb16KO+aPIFF0Q5SBERERERkQSj7X5i6OkpT7Nw9UKe7PRk/ic4BwccAMOGwTHHFNrW7NlQu7ZPbkVEREREpPzQdj870xrbGErPTOfQ+ocWfMJnn0HFinD00QWe4hw88QTsu6+SWhEREREREVBiG1Ppmem0a1JItaeBA6Fv30IXzY4YAS+/DJUrBxCgiIiIiIhIAtIa2xjZmrOV4/Y+jua7Ns//hCVL4NNPfeZagN9/h1tvhY8+gtTUgAIVERERERFJMEpsY6RSaiVGdCk4aWXYMLjoIqhZs8BTvvjCV0I+7LDoxyciIiIiIpKoVDwqRh6c/CB71t6Tbod02/ngtm2w114wcSIceGC+12/YADVqBBykiIiIiIjEPRWP2pnW2MbI+IXjqVe9Xv4H33kHmjcvNKk95BD48cfg4hMREREREUlUSmxjYFvONr5d+i1tG7XN/4SBA+Gqqwq8/q67fKHkFi0CClBERERERCSBaY1tDCxdv5QOe3egdpXaOx/8/nv44Qc488x8r50+3VdBnjMn2BhFREREREQSldbYhu2aa6BuXbjnnnwPZ2XBvHnQunWM4xIRERERkbikNbY701TkGHjkq0eYvmz6zgc2bIDRo6FXr3yve/FFWLBASa2IiIiIiEhhlNjGwJBvh1A5tfLOB0aPhuOOg8aNdzq0aBHccIMqIYuIiIiIiBRFiW3AVm5cyaqsVRxQ74B/HnCuwKJRzsGVV8Itt0DTpjEKVEREREREJEEpsQ3YrN9n0bZxW1Jsh7f6669h82Y4/vidrlm+3I/U3nBDjIIUERERERFJYCoeFQPbcrZRMbXiP1/s1g2OPBKuv/4fL69bB1WrQsUdThcREREREQEVj8qPRmwDNnz6cNZsXvPPF1esgHHj4NJLdzr/6qvh8cdjE5uIiIiIiEgyUGIboOzcbG6ccCMVUnbYLvi55+Dcc/02P3lMnAiTJkHfvjEMUkREREREJMFVKPoUKa25K+bSqFYj6lbNk8Dm5MDgwfD22/84d+tWXzBq0CBVQhYRERERESkJJbYBSs9MJ61x2j9fHDcOGjaEVq3+8XKlSvDmm3DYYbGLT0REREREJBmoeFSA1m5ey4atG2hUq9HfL3bq5AtHXXLJXy/NnAlffaUpyCIiIiIiUjQVj9qZ1tgGaMqSKexeffe/X1iwAKZPh/PO++ulnBzo2dNXQhYREREREZGSU2IbkD+y/uC818/75/61gwdDjx5QpcpfLz39tF9T26NHCEGKiIiIiIgkAa2xDciUJVM4suGRpKak+hc2bYIXXoApU/5x3o8/wpAhYBZCkCIiIiIiIklAiW1A0hfvUDjqtdegTRto2hQA52D5cl8FWUREREREREpPU5EDctYBZ9H90O5/vzBwIFx11V9PX3sNzjzTJ7giIiIiIiJSehqxDUBObg571NiDhjUb+hemToWVK31FZODPP+GGG/xWtpqCLCIiIiIiUjYasQ3AvFXz6DCyw98vDBoEV14JqX697V13wbnnQlpaAQ2IiIiIiIhIsWnENgDpi9Np17idf/Lnn35o9qef/jp+551QXbtOiYiIiIiIRIVGbAOQnpknsR05Ek47DerVY9Mm6NULateGmjVDDVFERERERCRpaMQ2AEfveTTH7nUs5Ob6acijRgFw//1+ALdq1ZADFBERERERSSJKbKPMOUePw3pgZvDRR35oNi2N2bNh6FCYNSvsCEVERERERJKLpiJH2UcLP+Lc18/1T7Zv8WPGokXw6KPQoEG48YmIiIiIiCQbjdhGWXpmOs13aQ6//QaTJ8Po0SxcCGecoa19REREREREgqAR2yhLz0ynXZN2ft7xJZew+M/qtG3r81wRERERERGJPo3YRlnjmo1Jq3c4DO+F+/wLrr4arrkG9tor7MhERERERESSkxLbKHuuy3Pwyitw0EF8vLgF8+fDmDFhRyUiIiIiIpK8lNhG0Zi5Y8hcl8mNA9+GG27ghBPg44+hcuWwIxMREREREUleWmMbRRMXTqTS8pXw88/cNe0Mvv0WGjYMOyoREREREZHkpsQ2ijKWZNDu0/n8enIvnnuhAs2ahR2RiIiIiIhI8tNU5CjZmrOVnOxtHPzyx5xY9ymeegrq1Ak7KhERERERkeSnxDZKKqVW4vuUa9h01Bd0OKwhZ58ddkQiIiIiIiLlQ6BTkc2sk5n9aGYLzOzWfI6bmT0VOf6dmbUKMp4gjB44nBO770XjGyrTYep1vLvfXtx7L5iFHZmIiIiIiJRXZcnFCrrWzHYxs4lmNj/yWDfPsdsi5/9oZifneb21mc2OHHvKLJhMKbDE1sxSgWeBzkBLoKuZtdzhtM5As8hXL2BQUPEEofulHei1pCef7vMbS+psZfJeOVxe7VG6X9oh7NBERERERKScKksuVsS1twKfOOeaAZ9EnhM5fiFwINAJGBhph0i7vfL01SnaPy8EO2LbBljgnFvknNsKvAp02eGcLsAo52UAdcysQYAxRc3ogcN5s/EksiqBi7yLLgWyKsEbjScxeuDwcAMUEREREZHyqiy5WGHXdgFeiHz/AnBmntdfdc5tcc79DCwA2kTaq+WcS3fOOWBUnmuiKsjEthGwOM/zzMhrJT0nLo3IuJdNBaxQ3lLBHxcREREREQlBWXKxwq6t75xbBhB53L0YbWUWEUdUBFk8Kr+5064U52BmvfDD11SqVKnskUXBsmor/xqp3VFuCiyvtiq2AYmIiIiISHlRwcym5Xk+1Dk3NM/zsuRixcrRAmyrVIJMbDOBJnmeNwaWluIcIv+RhgJUr149kDeipBpk1eOH3N/IzSe5TcmFBlm7xT4oEREREREpD7Kdc0cUcrwsuVilQq793cwaOOeWRaYZryiirczI94XFERVBTkWeCjQzs33MrBJ+MfHYHc4ZC3SPVORKA9ZuH9qOdz3S/kuV7PyPVc6Gy9L+G9uAREREREREvLLkYoVdOxa4NPL9pcC7eV6/0Mwqm9k++CJR30TaW29maZFqyN3zXBNVgSW2zrls4GpgAjAPGOOcm2tmV5rZlZHTxgGL8IuLhwFXBRVPtHW76grOyTyWqlv9CC34x6pb4dzMY+l21RXhBigiIiIiIuVSWXKxgq6NXPMQ0NHM5gMdI8+JHB8DfA+MB/o653Ii1/QBhkf6WQh8GMTPbL44VeKoXr2627hxY9hh/GX0wOGMyLiX5dVWsUfWbvRI+6+SWhERERERCYyZZTnnqocdRzxRYisiIiIiIpJAlNjuLMg1tiIiIiIiIiKBU2IrIiIiIiIiCU2JrYiIiIiIiCQ0JbYiIiIiIiKS0JTYioiIiIiISEJTYisiIiIiIiIJTYmtiIiIiIiIJDQltiIiIiIiIpLQlNiKiIiIiIhIQlNiKyIiIiIiIglNia2IiIiIiIgkNCW2IiIiIiIiktCU2IqIiIiIiEhCU2IrIiIiIiIiCc2cc2HHUCJmlgtsCjsOiYkKQHbYQSQRvZ/Rp/c0uvR+Rp/e0+jTexpdej+jT+9pdMXr+1nVOadByjwSLrGV8sPMpjnnjgg7jmSh9zP69J5Gl97P6NN7Gn16T6NL72f06T2NLr2fiUNZvoiIiIiIiCQ0JbYiIiIiIiKS0JTYSjwbGnYASUbvZ/TpPY0uvZ/Rp/c0+vSeRpfez+jTexpdej8ThNbYioiIiIiISELTiK2IiIiIiIgkNCW2EnfM7HkzW2Fmc8KOJRmYWRMz+8zM5pnZXDO7LuyYEpmZVTGzb8xsVuT97B92TMnCzFLNbIaZvR92LMnAzH4xs9lmNtPMpoUdT6Izszpm9oaZ/RD5fdou7JgSmZm1iPzb3P61zsyuDzuuRGZmN0T+Ls0xs1fMrErYMSU6M7su8n7O1b/P+KepyBJ3zOxYYAMwyjl3UNjxJDozawA0cM5NN7OawLfAmc6570MOLSGZmQHVnXMbzKwi8CVwnXMuI+TQEp6Z3QgcAdRyzp0WdjyJzsx+AY5wzq0KO5ZkYGYvAJOdc8PNrBJQzTm3JuSwkoKZpQJLgLbOuV/DjicRmVkj/N+jls65TWY2BhjnnBsZbmSJy8wOAl4F2gBbgfFAH+fc/FADkwJpxFbijnNuEvBn2HEkC+fcMufc9Mj364F5QKNwo0pcztsQeVox8qU7hGVkZo2BU4HhYccisiMzqwUcCzwH4JzbqqQ2qk4AFiqpLbMKQFUzqwBUA5aGHE+iOwDIcM5lOeeygS+As0KOSQqhxFakHDGzvYHDgSkhh5LQIlNmZwIrgInOOb2fZfck8H9AbshxJBMHfGRm35pZr7CDSXBNgZXAiMh0+eFmVj3soJLIhcArYQeRyJxzS4BHgd+AZcBa59xH4UaV8OYAx5rZrmZWDTgFaBJyTFIIJbYi5YSZ1QDeBK53zq0LO55E5pzLcc4dBjQG2kSmK0kpmdlpwArn3Ldhx5Jk2jvnWgGdgb6RZR5SOhWAVsAg59zhwEbg1nBDSg6Rad1nAK+HHUsiM7O6QBdgH6AhUN3MLg43qsTmnJsH/A+YiJ+GPAvIDjUoKZQSW5FyILIW9E1gtHPurbDjSRaRqYifA53CjSThtQfOiKwJfRU43sxeCjekxOecWxp5XAG8jV8nJqWTCWTmmZ3xBj7RlbLrDEx3zv0ediAJ7kTgZ+fcSufcNuAt4KiQY0p4zrnnnHOtnHPH4pfJaX1tHFNiK5LkIsWOngPmOeceDzueRGdm9cysTuT7qvgPEz+EGlSCc87d5pxr7JzbGz8l8VPnnEYaysDMqkeKxRGZMnsSflqdlIJzbjmw2MxaRF46AVABvujoiqYhR8NvQJqZVYv83T8BX1NDysDMdo887gmcjf6txrUKYQcgsiMzewU4DtjNzDKBfs6558KNKqG1By4BZkfWhQLc7pwbF15ICa0B8EKkimcKMMY5p+1pJN7UB972n2+pALzsnBsfbkgJ7xpgdGTq7CKgR8jxJLzIusWOQO+wY0l0zrkpZvYGMB0/XXYGMDTcqJLCm2a2K7AN6OucWx12QFIwbfcjIiIiIiIiCU1TkUVERERERCShKbEVERERERGRhKbEVkRERERERBKaElsRERERERFJaEpsRUREREREJKEpsRUREREREZGEpsRWREREREREEpoSWxEREREREUloSmxFREREREQkoSmxFRERERERkYSmxFZEREREREQSmhJbERERERERSWhKbEVERERERCShKbEVERERERGRhKbEVkRERERERBKaElsRERERERFJaEpsRUREREREJKEpsRUREREREZGEpsRWREREREREEpoSWxEREREREUloSmxFREREREQkoSmxFRGRpGRmv5jZJjNbb2ZrzOxrM7vSzIr822dme5uZM7MKAccYk35ERESSnRJbERFJZqc752oCewEPAbcAz4UbkoiIiESbElsREUl6zrm1zrmxwAXApWZ2kJmdamYzzGydmS02s7vzXDIp8rjGzDaYWTsz29fMPjWzP8xslZmNNrM62y8ws1vMbElkhPhHMzsh8nqKmd1qZgsj144xs10K6ifYd0JERCQ5KbEVEZFywzn3DZAJHANsBLoDdYBTgT5mdmbk1GMjj3WcczWcc+mAAQ8CDYEDgCbA3QBm1gK4GjgyMkJ8MvBLpI1rgTOBDpFrVwPPFtKPiIiIlJASWxERKW+WArs45z53zs12zuU6574DXsEnn/lyzi1wzk10zm1xzq0EHs9zfg5QGWhpZhWdc7845xZGjvUG7nDOZTrntuCT4XO1rlZERCR6lNiKiEh50wj408zamtlnZrbSzNYCVwK7FXSRme1uZq9GphuvA17afr5zbgFwPT5pXRE5r2Hk0r2AtyMFrNYA8/CJcP1gfjwREZHyR4mtiIiUG2Z2JD6x/RJ4GRgLNHHO1QYG46cbA7h8Ln8w8vohzrlawMV5zsc597Jz7mh8IuuA/0UOLQY6O+fq5Pmq4pxbUkA/IiIiUkJKbEVEJOmZWS0zOw14FXjJOTcbqAn86ZzbbGZtgIvyXLISyAWa5nmtJrABX+ipEXBznvZbmNnxZlYZ2Axswo/Kgk+Y7zezvSLn1jOzLoX0IyIiIiWkxFZERJLZe2a2Hj9qegd+XWyPyLGrgHsix+8Cxmy/yDmXBdwPfBWZQpwG9AdaAWuBD4C38vRTGb+d0CpgObA7cHvk2AD8yPBHkb4ygLaF9CMiIiIlZM5pFpSIiIiIiIgkLo3YioiIiIiISEJTYisiIiIiIiIJTYmtiIiIiIiIJDQltiIiIiIiIpLQlNiKiIiIiIhIQlNiKyIiIiIiIglNia2IiIiIiIgkNCW2IiIiIiIiktCU2IqIiIiIiEhC+39JkEi55fPI8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_setC, coefC = runall_LR(10, train_firstC_x, test_firstC_x, train_firstC_y, test_firstC_y, best_paramC)\n",
    "line_chart(table_setC, title = 'StackingCV Classifier (scheme 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:56:32.689133Z",
     "start_time": "2021-12-05T10:56:32.674249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balance Ratio</th>\n",
       "      <th>Train_OK</th>\n",
       "      <th>Train_NG</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Aging Rate</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset 0</th>\n",
       "      <td>558.245552</td>\n",
       "      <td>156867.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>48595.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9951.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>38647.0</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.204876</td>\n",
       "      <td>1.531296</td>\n",
       "      <td>0.291367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13763.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34835.0</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.283233</td>\n",
       "      <td>1.107659</td>\n",
       "      <td>0.233907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21080.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27518.0</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.433822</td>\n",
       "      <td>1.129948</td>\n",
       "      <td>0.370636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12035.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36563.0</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.247795</td>\n",
       "      <td>1.582583</td>\n",
       "      <td>0.372075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 5</th>\n",
       "      <td>1.009339</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6954.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41644.0</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.143148</td>\n",
       "      <td>1.369762</td>\n",
       "      <td>0.169174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 6</th>\n",
       "      <td>0.820198</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12907.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35691.0</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.265638</td>\n",
       "      <td>1.181029</td>\n",
       "      <td>0.244642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23185.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25413.0</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.477214</td>\n",
       "      <td>1.273732</td>\n",
       "      <td>0.499296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12748.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35850.0</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.262410</td>\n",
       "      <td>1.344997</td>\n",
       "      <td>0.300806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset 9</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17836.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30762.0</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.367078</td>\n",
       "      <td>1.175151</td>\n",
       "      <td>0.335219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Balance Ratio  Train_OK  Train_NG    TP       FP    FN       TN  \\\n",
       "dataset 0     558.245552  156867.0     281.0   0.0      3.0  51.0  48595.0   \n",
       "dataset 1       1.000000    2758.0    2758.0  16.0   9951.0  35.0  38647.0   \n",
       "dataset 2       1.000000    3775.0    3775.0  16.0  13763.0  35.0  34835.0   \n",
       "dataset 3       1.000000    3084.0    3084.0  25.0  21080.0  26.0  27518.0   \n",
       "dataset 4       1.000000    2809.0    2809.0  20.0  12035.0  31.0  36563.0   \n",
       "dataset 5       1.009339    2810.0    2784.0  10.0   6954.0  41.0  41644.0   \n",
       "dataset 6       0.820198    2810.0    3426.0  16.0  12907.0  35.0  35691.0   \n",
       "dataset 7       1.000000    2810.0    2810.0  31.0  23185.0  20.0  25413.0   \n",
       "dataset 8       1.000000    2810.0    2810.0  18.0  12748.0  33.0  35850.0   \n",
       "dataset 9      10.000000    2810.0     281.0  22.0  17836.0  29.0  30762.0   \n",
       "\n",
       "           Precision    Recall  Aging Rate  Efficiency     Score  \n",
       "dataset 0   0.000000  0.000000    0.000062    0.000000  0.000000  \n",
       "dataset 1   0.001605  0.313725    0.204876    1.531296  0.291367  \n",
       "dataset 2   0.001161  0.313725    0.283233    1.107659  0.233907  \n",
       "dataset 3   0.001185  0.490196    0.433822    1.129948  0.370636  \n",
       "dataset 4   0.001659  0.392157    0.247795    1.582583  0.372075  \n",
       "dataset 5   0.001436  0.196078    0.143148    1.369762  0.169174  \n",
       "dataset 6   0.001238  0.313725    0.265638    1.181029  0.244642  \n",
       "dataset 7   0.001335  0.607843    0.477214    1.273732  0.499296  \n",
       "dataset 8   0.001410  0.352941    0.262410    1.344997  0.300806  \n",
       "dataset 9   0.001232  0.431373    0.367078    1.175151  0.335219  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T07:25:34.121202Z",
     "start_time": "2021-10-16T07:25:33.174919Z"
    }
   },
   "outputs": [],
   "source": [
    "pr_dict, table_setR, coefR = runall_RidgeR(10, train_firstR_x, test_firstR_x, train_firstR_y, test_firstR_y, \n",
    "                                           best_paramR, thres_target = 'Recall', threshold = 0.7)\n",
    "line_chart(table_setR, title = 'StackingCV Regressor (scheme 1)')\n",
    "multiple_curve(4, 3, pr_dict, table_setR, target = 'Aging Rate')\n",
    "multiple_curve(4, 3, pr_dict, table_setR, target = 'Precision')\n",
    "print(coefR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:56:34.905250Z",
     "start_time": "2021-12-05T10:56:34.509291Z"
    }
   },
   "outputs": [],
   "source": [
    "savedate = '20211207'\n",
    "TPE_multi = True\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'StackingCV_1_tree'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'StackingCV_1_tree')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
