{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:25:21.919737Z",
     "start_time": "2021-10-13T03:25:21.902784Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\Darui_R08621110'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import openpyxl\n",
    "\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor,\\\n",
    "                                AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import optuna\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, cf_matrix, AUC, PR_curve, \\\n",
    "     multiple_curve, PR_matrix, best_threshold, all_optuna, optuna_history , runall_AdaBoostC, runall_AdaBoostR\n",
    "from library.XGBoost import runall_XGBoostC, runall_XGBoostR\n",
    "from library.CatBoost import runall_CatBoostC, runall_CatBoostR\n",
    "from library.LightGBM import runall_LightGBMC, runall_LightGBMR\n",
    "from library.Random_Forest import runall_ForestC, runall_ForestR\n",
    "from library.Extra_Trees import runall_ExtraTreesC, runall_ExtraTreesR\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:25:30.002642Z",
     "start_time": "2021-10-13T03:25:29.985688Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_hyper(num_set, date, model_list, iter_list, filename, mode, sampler_list) :\n",
    "    \n",
    "    allset_dict = {}\n",
    "    for j in range(num_set) :\n",
    "\n",
    "        sampler_dict = {}\n",
    "        for sampler in sampler_list :\n",
    "        \n",
    "            model_dict = {}\n",
    "            for i, model in enumerate(model_list) :\n",
    "            \n",
    "                    with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[i]}.data', 'rb') as f:\n",
    "                        temp_dict = pickle.load(f)\n",
    "                        model_dict[model] = temp_dict[f'set{j}']\n",
    "            \n",
    "            sampler_dict[sampler] = model_dict\n",
    "        \n",
    "        allset_dict[f'set{j}'] = sampler_dict\n",
    "        \n",
    "    return allset_dict\n",
    "\n",
    "\n",
    "def tableau_hyper(num_set, date, model_list, iter_list, filename, mode, sampler_list) :\n",
    "    \n",
    "    model_dict = {}\n",
    "    for j, model in enumerate(model_list) :\n",
    "\n",
    "        sampler_dict = {}\n",
    "        for i, sampler in enumerate(sampler_list) :\n",
    "\n",
    "            with open(f'hyperparameter/{date}/{filename}_{model}{mode}_{sampler}_{iter_list[j]}.data', 'rb') as f :\n",
    "                temp_dict = pickle.load(f)\n",
    "                sampler_dict[sampler] = temp_dict\n",
    "                \n",
    "        model_dict[model] = sampler_dict\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackingCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T13:21:50.259942Z",
     "start_time": "2021-10-03T13:21:50.244915Z"
    }
   },
   "outputs": [],
   "source": [
    "def stackingCVC(train_x, train_y, test_x, test_y, config, TPE_multi, meta_config) :\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    model_list = config[sampler].keys()\n",
    "    clf_list = []\n",
    "    \n",
    "    if 'CatBoost' in model_list :\n",
    "        clf1 = CatBoostClassifier(**config[sampler]['CatBoost'])\n",
    "        clf_list.append(clf1)\n",
    "    \n",
    "    if 'LightGBM' in model_list :\n",
    "        clf2 = LGBMClassifier(**config[sampler]['LightGBM'])\n",
    "        clf_list.append(clf2)\n",
    "        \n",
    "    if 'XGBoost' in model_list :\n",
    "        clf3 = XGBClassifier(**config[sampler]['XGBoost'])\n",
    "        clf_list.append(clf3)\n",
    "        \n",
    "    if 'AdaBoost' in model_list :\n",
    "        tree_param = {'base_estimator': DecisionTreeClassifier(max_depth = config[sampler]['AdaBoost']['max_depth'])}\n",
    "        boost_param = dict((key, config[sampler]['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators'])\n",
    "        boost_param.update(tree_param)\n",
    "        clf4 = AdaBoostClassifier(**boost_param)\n",
    "        clf_list.append(clf4)\n",
    "        \n",
    "    if 'RandomForest' in model_list :\n",
    "        clf5 = RandomForestClassifier(**config[sampler]['RandomForest'])\n",
    "        clf_list.append(clf5)\n",
    "        \n",
    "    if 'ExtraTrees' in model_list :\n",
    "        clf6 = ExtraTreesClassifier(**config[sampler]['ExtraTrees'])\n",
    "        clf_list.append(clf6)\n",
    "    \n",
    "    second_config = meta_config.copy()\n",
    "    del second_config['meta_learner']\n",
    "    \n",
    "    if meta_config['meta_learner'] == 'Logistic Regression' :\n",
    "        meta_clf = LogisticRegression(**second_config)\n",
    "    elif meta_config['meta_learner'] == 'Extra Trees' :\n",
    "        meta_clf = ExtraTreesClassifier(**second_config)\n",
    "\n",
    "    sclf = StackingCVClassifier(classifiers = clf_list, \n",
    "                                meta_classifier = meta_clf, \n",
    "                                use_probas = True,\n",
    "                                drop_proba_col = 'last',\n",
    "                                cv = 5,\n",
    "                                shuffle = True,\n",
    "                                stratify = True,\n",
    "                                n_jobs = -1)\n",
    "    sclf.fit(train_x, train_y)\n",
    "    predict_y = sclf.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def stackingCVR(train_x, train_y, test_x, test_y, config, TPE_multi, meta_config) :\n",
    "    \n",
    "    sampler = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    model_list = config[sampler].keys()\n",
    "    reg_list = []\n",
    "    \n",
    "    if 'CatBoost' in model_list :\n",
    "        reg1 = CatBoostRegressor(**config[sampler]['CatBoost'])\n",
    "        reg_list.append(reg1)\n",
    "    \n",
    "    if 'LightGBM' in model_list :\n",
    "        reg2 = LGBMRegressor(**config[sampler]['LightGBM'])\n",
    "        reg_list.append(reg2)\n",
    "        \n",
    "    if 'XGBoost' in model_list :\n",
    "        reg3 = XGBRegressor(**config[sampler]['XGBoost'])\n",
    "        reg_list.append(reg3)\n",
    "        \n",
    "    if 'AdaBoost' in model_list :\n",
    "        tree_param = {'base_estimator': DecisionTreeRegressor(max_depth = config[sampler]['AdaBoost']['max_depth'])}\n",
    "        boost_param = dict((key, config[sampler]['AdaBoost'][key]) for key in ['learning_rate', 'n_estimators'])\n",
    "        boost_param.update(tree_param)\n",
    "        reg4 = AdaBoostRegressor(**boost_param)\n",
    "        reg_list.append(reg4)\n",
    "        \n",
    "    if 'RandomForest' in model_list :\n",
    "        reg5 = RandomForestRegressor(**config[sampler]['RandomForest'])\n",
    "        reg_list.append(reg5)\n",
    "        \n",
    "    if 'ExtraTrees' in model_list :\n",
    "        reg6 = ExtraTreesRegressor(**config[sampler]['ExtraTrees'])\n",
    "        reg_list.append(reg6)\n",
    "\n",
    "    second_config = meta_config.copy()\n",
    "    del second_config['meta_learner']\n",
    "        \n",
    "    if meta_config['meta_learner'] == 'Ridge Regression' :\n",
    "        meta_reg = Ridge(**second_config)\n",
    "    elif meta_config['meta_learner'] == 'Extra Trees' :\n",
    "        meta_reg = ExtraTreesRegressor(**second_config)\n",
    "\n",
    "    sreg = StackingCVRegressor(regressors = reg_list, \n",
    "                               meta_regressor = meta_reg, \n",
    "                               cv = 5,\n",
    "                               shuffle = True,\n",
    "                               n_jobs = -1)\n",
    "    sreg.fit(train_x, train_y)\n",
    "    predict_y = sreg.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T15:12:48.225550Z",
     "start_time": "2021-10-03T15:12:48.218526Z"
    }
   },
   "outputs": [],
   "source": [
    "def runall_stackingCVC(num_set, train_x, train_y, test_x, test_y, config, TPE_multi, meta_config) :    \n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    for i in tqdm(range(num_set)) :\n",
    "        \n",
    "        result = stackingCVC(train_x[f'set{i}'], train_y[f'set{i}'], test_x, test_y, config[f'set{i}'], TPE_multi, \n",
    "                             meta_config[f'set{i}'])\n",
    "        table = cf_matrix(result, train_y[f'set{i}'])\n",
    "        table_set = pd.concat([table_set, table]).rename(index = {0: f'dataset {i}'})\n",
    "        \n",
    "    return table_set\n",
    "\n",
    "\n",
    "def runall_stackingCVR(num_set, train_x, train_y, test_x, test_y, config, TPE_multi, meta_config, thres_target, threshold):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    pr_dict = {}\n",
    "    for i in tqdm(range(num_set)) :\n",
    "        \n",
    "        result = stackingCVR(train_x[f'set{i}'], train_y[f'set{i}'], test_x, test_y, config[f'set{i}'], TPE_multi, \n",
    "                             meta_config[f'set{i}'])\n",
    "        pr_matrix = PR_matrix(result, train_y[f'set{i}'])\n",
    "        pr_dict[f'set{i}'] = pr_matrix\n",
    "        \n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = thres_target, threshold = threshold)\n",
    "        table_set = pd.concat([table_set, best_data]).rename(index = {best_data.index.values[0]: f'dataset {i}'})\n",
    "        \n",
    "    return pr_dict, table_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runhist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T15:12:50.856491Z",
     "start_time": "2021-10-03T15:12:49.826806Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bad types: 62\n",
      "\n",
      "training data: (77138, 83) \n",
      "Balance Ratio: 18.17902\n",
      "\n",
      "testing data: (55903, 83) \n",
      "Balance Ratio: 3104.72222\n",
      "Dimension of dataset 0 : (57744, 103)  balance ratio: 543.75472\n",
      "Dimension of dataset 1 : (1764, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 2 : (2778, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 3 : (1936, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 4 : (1728, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 5 : (2116, 103)  balance ratio: 1.00379\n",
      "Dimension of dataset 6 : (2456, 103)  balance ratio: 0.75931\n",
      "Dimension of dataset 7 : (2120, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 8 : (2120, 103)  balance ratio: 1.0\n",
      "Dimension of dataset 9 : (1166, 103)  balance ratio: 10.0\n",
      "\n",
      " 10 datasets are loaded.\n",
      "\n",
      "Labels of  10 datasets are divided.\n",
      "\n",
      " Dimension of run test: (48649, 103)\n"
     ]
    }
   ],
   "source": [
    "###bad types###\n",
    "bad = pd.read_csv('event/Bad_Types.csv').iloc[:, 1:]\n",
    "Bad_Types = {bad.cb[i]:i for i in range (len(bad))}\n",
    "print('Total bad types:', len(bad))\n",
    "\n",
    "###single dataset###\n",
    "test = pd.read_csv('event/TestingSet_0.csv').iloc[:, 2:]\n",
    "train = pd.read_csv('event/TrainingSet_new.csv').iloc[:, 2:]\n",
    "print('\\ntraining data:', train.shape, '\\nBalance Ratio:', Balance_Ratio(train))\n",
    "print('\\ntesting data:', test.shape, '\\nBalance Ratio:', Balance_Ratio(test))\n",
    "\n",
    "train_x, train_y, test_x, test_y = label_divide(train, test, 'GB')\n",
    "\n",
    "###multiple dataset###\n",
    "data_dict = multiple_set(num_set = 10)\n",
    "trainset_x, trainset_y = train_set(data_dict, num_set = 10, label = 'GB')\n",
    "test_x, test_y = label_divide(test, None, 'GB', train_only = True)\n",
    "\n",
    "\n",
    "#####for runhist dataset#####\n",
    "# bad = pd.read_csv('run_bad_types.csv').iloc[:, 1:]\n",
    "# Bad_Types = {bad.cb[i]:i for i in range (len(bad))}\n",
    "# print('Total bad types:', len(bad))\n",
    "\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of run test:', run_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T08:02:00.737480Z",
     "start_time": "2021-09-29T08:02:00.695968Z"
    }
   },
   "outputs": [],
   "source": [
    "##### loading hyperparameters #####\n",
    "hyper_info = {\n",
    "    'num_set': 10,\n",
    "    'date': '20211019',\n",
    "    'model_list': ['LightGBM', 'XGBoost', 'CatBoost', 'AdaBoost', 'RandomForest', 'ExtraTrees'],\n",
    "    'iter_list': [200, 200, 200, 25, 50, 50],\n",
    "    'filename': 'runhist_array_m2m5_4selection',\n",
    "    'sampler_list': ['univariate-TPE', 'multivariate-TPE']\n",
    "}\n",
    "\n",
    "hyper_infoC = hyper_info.copy()\n",
    "hyper_infoC.update({'mode': 'C'})\n",
    "hyper_infoR = hyper_info.copy()\n",
    "hyper_infoR.update({'mode': 'R'})\n",
    "\n",
    "all_hyperC = load_hyper(**hyper_infoC)\n",
    "all_hyperR = load_hyper(**hyper_infoR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T08:02:54.643561Z",
     "start_time": "2021-09-29T08:02:54.624577Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective_creator(train_data, mode, TPE_multi, config, num_valid = 3) :\n",
    "    \n",
    "    def objective(trial) :\n",
    "        # hyperparameters randomize setting\n",
    "        if mode == 'C' :\n",
    "            meta_learner = 'Logistic Regression'\n",
    "            \n",
    "            if meta_learner == 'Logistic Regression' :\n",
    "                param = {\n",
    "                    'meta_learner': 'Logistic Regression',\n",
    "                    'solver': 'lbfgs',\n",
    "                    'C': trial.suggest_categorical('C', [100, 10 ,1 ,0.1, 0.01]),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['none', 'l2']),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "\n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                param = {\n",
    "                    'meta_learner': 'Extra Trees',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }     \n",
    "\n",
    "        elif mode == 'R' :\n",
    "            meta_learner = 'Ridge Regression'\n",
    "            \n",
    "            if meta_learner == 'Ridge Regression' :\n",
    "                param = {\n",
    "                    'meta_learner': 'Ridge Regression',\n",
    "                    'alpha': trial.suggest_float('alpha', 0, 1, step = 0.1)\n",
    "                }\n",
    "            \n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                \n",
    "                param = {\n",
    "                    'meta_learner': 'Extra Trees',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "        \n",
    "        # objective function\n",
    "        result_list = []\n",
    "        for i in range(num_valid):\n",
    "\n",
    "            train_x, train_y = label_divide(train_data, None, 'GB', train_only = True)\n",
    "            train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size = 0.25)\n",
    "\n",
    "            if mode == 'C':\n",
    "                result = stackingCVC(train_x, train_y, valid_x, valid_y, config, TPE_multi, param)\n",
    "                table = cf_matrix(result, valid_y)\n",
    "                recall = table['Recall']\n",
    "                aging = table['Aging Rate']\n",
    "\n",
    "                result_list.append(recall - 0.1*aging)\n",
    "\n",
    "            elif mode == 'R':\n",
    "                result = stackingCVR(train_x, train_y, valid_x, valid_y, config, TPE_multi, param)\n",
    "                pr_matrix = PR_matrix(result, valid_y)\n",
    "                auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "                \n",
    "                result_list.append((-1)*auc)\n",
    "\n",
    "        return np.mean(result_list)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "\n",
    "def all_optuna(num_set, all_data, mode, TPE_multi, config, n_iter, filename, creator, num_valid = 3) :\n",
    "\n",
    "    best_param = {}\n",
    "    all_score = {}\n",
    "    for i in tqdm(range(num_set)) :\n",
    "        \n",
    "        ##### define objective function and change optimized target dataset in each loop #####\n",
    "        objective = creator(all_data[f'set{i}'], mode, TPE_multi, config[f'set{i}'], num_valid = num_valid)\n",
    "        \n",
    "        ##### optimize one dataset in each loop #####\n",
    "        print(f'Dataset{i} :')\n",
    "        \n",
    "        study = optuna.create_study(sampler = optuna.samplers.TPESampler(multivariate = TPE_multi), direction = 'maximize')\n",
    "        study.optimize(objective, n_trials = n_iter, show_progress_bar = True, gc_after_trial = True)\n",
    "        best_param[f'set{i}'] = study.best_trial.params\n",
    "        \n",
    "        ##### return score and entire params for score plot or feature importance\n",
    "        collect_score = []\n",
    "        [collect_score.append(x.values) for x in study.trials]\n",
    "        all_score[f'set{i}'] = collect_score \n",
    "        \n",
    "        print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
    "    \n",
    "    ##### store the best hyperparameters #####\n",
    "    multi_mode = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "    with open(f'{filename}{mode}_{multi_mode}_{n_iter}.data', 'wb') as f:\n",
    "        pickle.dump(best_param, f)\n",
    "    \n",
    "    return best_param, all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T13:08:44.132581Z",
     "start_time": "2021-09-29T08:02:55.456454Z"
    }
   },
   "outputs": [],
   "source": [
    "best_paramC, all_scoreC = all_optuna(num_set = 10, \n",
    "                                     all_data = data_dict, \n",
    "                                     mode = 'C', \n",
    "                                     TPE_multi = False, \n",
    "                                     config = all_hyperC, \n",
    "                                     n_iter = 5, \n",
    "                                     filename = 'runhist_array_m2m5_4selection_stackingCV(CLXARE)', \n",
    "                                     creator = objective_creator, \n",
    "                                     num_valid = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T18:14:14.077498Z",
     "start_time": "2021-09-29T13:08:44.632425Z"
    }
   },
   "outputs": [],
   "source": [
    "best_paramR, all_scoreR = all_optuna(num_set = 10, \n",
    "                                     all_data = data_dict, \n",
    "                                     mode = 'R', \n",
    "                                     TPE_multi = False, \n",
    "                                     config = all_hyperR, \n",
    "                                     n_iter = 5, \n",
    "                                     filename = 'runhist_array_m2m5_4selection_stackingCV(CLXARE)', \n",
    "                                     creator = objective_creator, \n",
    "                                     num_valid = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T18:28:10.895422Z",
     "start_time": "2021-09-29T18:14:14.604138Z"
    }
   },
   "outputs": [],
   "source": [
    "table_setC = runall_stackingCVC(10, \n",
    "                                trainset_x, \n",
    "                                trainset_y, \n",
    "                                run_test_x, \n",
    "                                run_test_y, \n",
    "                                all_hyperC, \n",
    "                                TPE_multi = False,  \n",
    "                                meta_config = best_paramC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T18:28:11.633189Z",
     "start_time": "2021-09-29T18:28:11.429166Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "line_chart(table_setC, title = 'StackingCV Classifier')\n",
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T06:35:50.209666Z",
     "start_time": "2021-09-14T06:35:50.202651Z"
    }
   },
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T04:01:09.997545Z",
     "start_time": "2021-09-30T03:33:50.284706Z"
    }
   },
   "outputs": [],
   "source": [
    "pr_dict, table_setR = runall_stackingCVR(10, \n",
    "                                         trainset_x, \n",
    "                                         trainset_y, \n",
    "                                         run_test_x,\n",
    "                                         run_test_y, \n",
    "                                         all_hyperR, \n",
    "                                         TPE_multi = False,  \n",
    "                                         meta_config = best_paramR,\n",
    "                                         thres_target = 'Recall',\n",
    "                                         threshold = 0.7\n",
    "                                        )\n",
    "\n",
    "line_chart(table_setR, title = 'StackingCV Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T04:01:13.386948Z",
     "start_time": "2021-09-30T04:01:10.645231Z"
    }
   },
   "outputs": [],
   "source": [
    "multiple_curve(4, 3, pr_dict, table_setR, target = 'Aging Rate')\n",
    "multiple_curve(4, 3, pr_dict, table_setR, target = 'Precision')\n",
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedate = '20211019'\n",
    "TPE_multi = False\n",
    "\n",
    "table_setC['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setR['sampler'] = 'multivariate-TPE' if TPE_multi else 'univariate-TPE'\n",
    "table_setC['model'] = 'StackingCV (scheme 1)'\n",
    "table_setR['model'] = 'StackingCV (scheme 1)'\n",
    "with pd.ExcelWriter(f'{savedate}_Classifier.xlsx', mode = 'a') as writer:\n",
    "    table_setC.to_excel(writer, sheet_name = 'StackingCV (scheme 1)')\n",
    "with pd.ExcelWriter(f'{savedate}_Regressor.xlsx', mode = 'a') as writer:\n",
    "    table_setR.to_excel(writer, sheet_name = 'StackingCV (scheme 1)')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
