{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T02:45:16.804808Z",
     "start_time": "2021-10-11T02:45:15.535216Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import optuna\n",
    "\n",
    "from library.Data_Preprocessing import Balance_Ratio\n",
    "from library.Imbalance_Sampling import label_divide\n",
    "from library.Aging_Score_Contour import score1\n",
    "from library.AdaBoost import train_set, multiple_set, multiple_month, line_chart, cf_matrix, AUC, PR_curve, \\\n",
    "     multiple_curve, PR_matrix, best_threshold, all_optuna, optuna_history, AdaBoost_creator \n",
    "from library.XGBoost import XGBoost_creator\n",
    "from library.LightGBM import LightGBM_creator\n",
    "from library.CatBoost import CatBoost_creator\n",
    "from library.Random_Forest import RandomForest_creator\n",
    "from library.Extra_Trees import ExtraTrees_creator\n",
    "\n",
    "os.chdir('C:/Users/user/Desktop/Darui_R08621110')  \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T02:45:18.228594Z",
     "start_time": "2021-10-11T02:45:18.204268Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_base(num_set, train_data, mode, TPE_multi, base_list, iter_list, filename):\n",
    "    \n",
    "    best_param = {}\n",
    "    month_list = list(train_data.keys())\n",
    "    \n",
    "    for i in tqdm(month_list):\n",
    "        \n",
    "        best_param[f'{i}'] = {}\n",
    "        if 'XGBoost' in base_list:\n",
    "            model_index = base_list.index('XGBoost')\n",
    "            best_param[f'{i}'][f'XGBoost'], _ = all_optuna(num_set = num_set, \n",
    "                                                           all_data = train_data[f'{i}'], \n",
    "                                                           mode = mode, \n",
    "                                                           TPE_multi = TPE_multi, \n",
    "                                                           n_iter = iter_list[model_index],\n",
    "                                                           filename = f'{filename}_{i}_XGBoost',\n",
    "                                                           creator = XGBoost_creator)\n",
    "\n",
    "        if 'LightGBM' in base_list:\n",
    "            model_index = base_list.index('LightGBM')\n",
    "            best_param[f'{i}'][f'LightGBM'], _ = all_optuna(num_set = num_set, \n",
    "                                                            all_data = train_data[f'{i}'], \n",
    "                                                            mode = mode, \n",
    "                                                            TPE_multi = TPE_multi, \n",
    "                                                            n_iter = iter_list[model_index],\n",
    "                                                            filename = f'{filename}_{i}_LightGBM',\n",
    "                                                            creator = LightGBM_creator)\n",
    "        \n",
    "        if 'AdaBoost' in base_list:\n",
    "            model_index = base_list.index('AdaBoost')\n",
    "            best_param[f'{i}'][f'AdaBoost'], _ = all_optuna(num_set = num_set, \n",
    "                                                            all_data = train_data[f'{i}'], \n",
    "                                                            mode = mode, \n",
    "                                                            TPE_multi = TPE_multi, \n",
    "                                                            n_iter = iter_list[model_index],\n",
    "                                                            filename = f'{filename}_{i}_AdaBoost',\n",
    "                                                            creator = AdaBoost_creator)\n",
    "            \n",
    "        if 'CatBoost' in base_list:\n",
    "            model_index = base_list.index('CatBoost')\n",
    "            best_param[f'{i}'][f'CatBoost'], _ = all_optuna(num_set = num_set, \n",
    "                                                            all_data = train_data[f'{i}'], \n",
    "                                                            mode = mode, \n",
    "                                                            TPE_multi = TPE_multi, \n",
    "                                                            n_iter = iter_list[model_index],\n",
    "                                                            filename = f'{filename}_{i}_CatBoost',\n",
    "                                                            creator = CatBoost_creator)\n",
    "            \n",
    "        if 'RandomForest' in base_list:\n",
    "            model_index = base_list.index('RandomForest')\n",
    "            best_param[f'{i}'][f'RandomForest'], _ = all_optuna(num_set = num_set, \n",
    "                                                                all_data = train_data[f'{i}'], \n",
    "                                                                mode = mode, \n",
    "                                                                TPE_multi = TPE_multi, \n",
    "                                                                n_iter = iter_list[model_index],\n",
    "                                                                filename = f'{filename}_{i}_RandomForest',\n",
    "                                                                creator = RandomForest_creator)\n",
    "\n",
    "        if 'ExtraTrees' in base_list:\n",
    "            model_index = base_list.index('ExtraTrees')\n",
    "            best_param[f'{i}'][f'ExtraTrees'], _ = all_optuna(num_set = num_set, \n",
    "                                                              all_data = train_data[f'{i}'], \n",
    "                                                              mode = mode, \n",
    "                                                              TPE_multi = TPE_multi, \n",
    "                                                              n_iter = iter_list[model_index],\n",
    "                                                              filename = f'{filename}_{i}_ExtraTrees',\n",
    "                                                              creator = ExtraTrees_creator)\n",
    "            \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data by base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T11:52:45.963793Z",
     "start_time": "2021-10-11T11:52:45.931512Z"
    }
   },
   "outputs": [],
   "source": [
    "def stratified_data(train_data, cv):\n",
    "    \n",
    "    good = train_data[train_data.GB == 0]\n",
    "    bad = train_data[train_data.GB == 1]\n",
    "    good_index = random.sample(good.index.to_list(), k = len(good))\n",
    "    bad_index = random.sample(bad.index.to_list(), k = len(bad))\n",
    "    \n",
    "    train_x_dict = {}\n",
    "    train_y_dict = {}\n",
    "    valid_x_dict = {}\n",
    "    valid_y_dict = {}\n",
    "    for i in range(cv):\n",
    "        \n",
    "        if (i+1) == cv:\n",
    "            good_valid_index = good_index[int(np.floor((i/cv)*len(good))) : ]\n",
    "            bad_valid_index = bad_index[int(np.floor((i/cv)*len(bad))) : ]\n",
    "        else:\n",
    "            good_valid_index = good_index[int(np.floor((i/cv)*len(good))) : int(np.floor(((i+1)/cv)*len(good)))]\n",
    "            bad_valid_index = bad_index[int(np.floor((i/cv)*len(bad))) : int(np.floor(((i+1)/cv)*len(bad)))]\n",
    "        good_train_index = [x for x in good_index if x not in good_valid_index]\n",
    "        bad_train_index = [x for x in bad_index if x not in bad_valid_index]\n",
    "        \n",
    "        good_train = good.loc[good_train_index]\n",
    "        good_valid = good.loc[good_valid_index]\n",
    "        bad_train = bad.loc[bad_train_index]\n",
    "        bad_valid = bad.loc[bad_valid_index]\n",
    "        train = pd.concat([good_train, bad_train], axis = 0)\n",
    "        valid = pd.concat([good_valid, bad_valid], axis = 0)\n",
    "        train_x_dict[i], train_y_dict[i], valid_x_dict[i], valid_y_dict[i] = label_divide(train, valid, \n",
    "                                                                                          train_only = False)\n",
    "\n",
    "    return train_x_dict, train_y_dict, valid_x_dict, valid_y_dict\n",
    "\n",
    "\n",
    "def transform_train(train_data, num_set, mode, base_param, cv):\n",
    "    \n",
    "    month_list = list(base_param.keys())\n",
    "    model_list = list(base_param[month_list[0]].keys())\n",
    "    set_dict = {}\n",
    "    for x in range(num_set):\n",
    "        set_dict[f'set{x}'] = pd.DataFrame()\n",
    "        \n",
    "    for month in tqdm(month_list):\n",
    "        \n",
    "        for i in tqdm(range(num_set)):\n",
    "            \n",
    "            train_x_dict, train_y_dict, valid_x_dict, valid_y_dict = stratified_data(train_data[month][f'set{i}'], cv = cv)\n",
    "            all_cv = pd.DataFrame()\n",
    "            for j in range(cv):\n",
    "                \n",
    "                model_predict = pd.DataFrame()\n",
    "                if mode == 'C':\n",
    "\n",
    "                    if 'XGBoost' in model_list:\n",
    "                        \n",
    "                        clf = XGBClassifier(**base_param[month]['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'LightGBM' in model_list:\n",
    "                        \n",
    "                        clf = LGBMClassifier(**base_param[month]['LightGBM'][f'set{i}'])\n",
    "                        clf.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = clf.predict_proba(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                elif mode == 'R':\n",
    "                    \n",
    "                    if 'XGBoost' in model_list:\n",
    "\n",
    "                        reg = XGBRegressor(**base_param[month]['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'X': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "\n",
    "                    if 'LightGBM' in model_list:\n",
    "                        reg = LGBMRegressor(**base_param[month]['LightGBM'][f'set{i}'])\n",
    "                        reg.fit(train_x_dict[j], train_y_dict[j])\n",
    "                        predict_y = reg.predict(valid_x_dict[j])\n",
    "                        predict = pd.DataFrame({'L': predict_y})\n",
    "                        model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                        \n",
    "                test_label = valid_y_dict[j].reset_index(drop = True)\n",
    "                done_cv = pd.concat([model_predict, test_label], axis = 1)\n",
    "                all_cv = pd.concat([all_cv, done_cv], axis = 0)\n",
    "                \n",
    "            set_dict[f'set{i}'] = pd.concat([set_dict[f'set{i}'], all_cv], axis = 0)\n",
    "            \n",
    "    \n",
    "    return set_dict\n",
    "\n",
    "\n",
    "def transform_test(train_data, test_data, num_set, mode, base_param):\n",
    "    \n",
    "    model_list = base_param['all'].keys()\n",
    "    test_dict = {}\n",
    "    for i in tqdm(range(num_set)):\n",
    "        \n",
    "        train_x, train_y, test_x, test_y = label_divide(train_data[f'set{i}'], test_data, train_only = False)\n",
    "        model_predict = pd.DataFrame()\n",
    "        if mode == 'C':\n",
    "            \n",
    "            if 'XGBoost' in model_list:\n",
    "                \n",
    "                clf = XGBClassifier(**base_param['all']['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'X': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "            \n",
    "            if 'LightGBM' in model_list:\n",
    "                clf = LGBMClassifier(**base_param['all']['LightGBM'][f'set{i}'])\n",
    "                clf.fit(train_x, train_y)\n",
    "                predict_y = clf.predict_proba(test_x)\n",
    "                predict = pd.DataFrame({'L': predict_y[:, 0]})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "                \n",
    "        elif mode == 'R':\n",
    "            \n",
    "            if 'XGBoost' in model_list:\n",
    "                \n",
    "                reg = XGBRegressor(**base_param['all']['XGBoost'][f'set{i}'], n_jobs = -1)\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'X': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "            \n",
    "            if 'LightGBM' in model_list:\n",
    "                reg = LGBMRegressor(**base_param['all']['LightGBM'][f'set{i}'])\n",
    "                reg.fit(train_x, train_y)\n",
    "                predict_y = reg.predict(test_x)\n",
    "                predict = pd.DataFrame({'L': predict_y})\n",
    "                model_predict = pd.concat([model_predict, predict], axis = 1)\n",
    "        \n",
    "        done_test = pd.concat([model_predict, test_y], axis = 1)\n",
    "        test_dict[f'set{i}'] = done_test\n",
    "        \n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T02:45:23.772355Z",
     "start_time": "2021-10-11T02:45:23.762350Z"
    }
   },
   "outputs": [],
   "source": [
    "def LR(train_x, test_x, train_y, test_y, config):\n",
    "    \n",
    "    clf = LogisticRegression(**config)\n",
    "    clf.fit(train_x, train_y)\n",
    "    predict_y = clf.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def RidgeR(train_x, test_x, train_y, test_y, config):\n",
    "    \n",
    "    reg = Ridge(**config)\n",
    "    reg.fit(train_x, train_y)\n",
    "    predict_y = reg.predict(test_x)\n",
    "    result = pd.DataFrame({'truth': test_y, 'predict': predict_y})\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def runall_LR(num_set, trainset_x, testset_x, trainset_y, testset_y, config):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    bad_set = pd.DataFrame()\n",
    "    judge = list(config.keys())[0]\n",
    "\n",
    "    for i in tqdm(range(num_set)):\n",
    "        print('\\n', f'Dataset {i}:')\n",
    "        \n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[f'set{i}']\n",
    "        else :\n",
    "            best_config = config\n",
    "\n",
    "        result = LR(trainset_x[f'set{i}'], testset_x[f'set{i}'], trainset_y[f'set{i}'], testset_y[f'set{i}'], best_config)\n",
    "        table = cf_matrix(result, trainset_y[f'set{i}'])\n",
    "        table_set = pd.concat([table_set, table]).rename(index = {0: f'dataset {i}'})\n",
    "    \n",
    "    return table_set\n",
    "\n",
    "\n",
    "def runall_RidgeR(num_set, trainset_x, testset_x, trainset_y, testset_y, config, thres_target = 'Recall', \n",
    "                    threshold = False):\n",
    "    \n",
    "    table_set = pd.DataFrame()\n",
    "    bad_set = pd.DataFrame()\n",
    "    pr_dict = {}\n",
    "    judge = list(config.keys())[0]\n",
    "\n",
    "    for i in range(num_set):\n",
    "        print('\\n', f'Dataset {i}:')\n",
    "        \n",
    "        if isinstance(config[judge], dict) :\n",
    "            best_config = config[f'set{i}']\n",
    "        else :\n",
    "            best_config = config\n",
    "\n",
    "        predict = RidgeR(trainset_x[f'set{i}'], testset_x[f'set{i}'], trainset_y[f'set{i}'], testset_y[f'set{i}'], \n",
    "                           best_config)\n",
    "        pr_matrix = PR_matrix(predict, trainset_y[f'set{i}'])\n",
    "        pr_dict[f'set{i}'] = pr_matrix\n",
    "        \n",
    "        best_data, best_thres = best_threshold(pr_matrix, target = thres_target, threshold = threshold)\n",
    "        table_set = pd.concat([table_set, best_data]).rename(index = {best_data.index.values[0]: f'dataset {i}'})\n",
    "        \n",
    "    return pr_dict, table_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T02:45:25.727714Z",
     "start_time": "2021-10-11T02:45:25.692960Z"
    }
   },
   "outputs": [],
   "source": [
    "def stackingCV_creator(train_data, mode, num_valid = 3) :\n",
    "    \n",
    "    def objective(trial) :\n",
    "        # hyperparameters randomize setting\n",
    "        if mode == 'C' :\n",
    "            meta_learner = 'Logistic Regression'\n",
    "            \n",
    "            if meta_learner == 'Logistic Regression' :      \n",
    "                param = {\n",
    "                    'solver': 'lbfgs',\n",
    "                    'C': trial.suggest_categorical('C', [100, 10 ,1 ,0.1, 0.01]),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['none', 'l2']),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "\n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                param = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }     \n",
    "\n",
    "        elif mode == 'R' :\n",
    "            meta_learner = 'RidgeCV'\n",
    "            \n",
    "            if meta_learner == 'RidgeCV' :\n",
    "                param = {\n",
    "                    'alpha': trial.suggest_float('alpha', 0, 1, step = 0.1)\n",
    "                }\n",
    "            \n",
    "            elif meta_learner == 'Extra Trees' :\n",
    "                param = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500, step = 100),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 32, step = 5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 21, step = 3),\n",
    "                    'n_jobs': -1\n",
    "                }\n",
    "        \n",
    "        # objective function\n",
    "        result_list = []\n",
    "        for i in range(num_valid):\n",
    "\n",
    "            train_x, train_y = label_divide(train_data, None, 'GB', train_only = True)\n",
    "            train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size = 0.25)\n",
    "\n",
    "            if mode == 'C':\n",
    "                result = LR(train_x, valid_x, train_y, valid_y, param)\n",
    "                table = cf_matrix(result, valid_y)\n",
    "                recall = table['Recall']\n",
    "                aging = table['Aging Rate']\n",
    "                result_list.append(recall - 0.1*aging)\n",
    "\n",
    "            elif mode == 'R':\n",
    "                result = RidgeR(train_x, valid_x, train_y, valid_y, param)\n",
    "                pr_matrix = PR_matrix(result, valid_y)\n",
    "                auc = AUC(pr_matrix['Recall'], pr_matrix['Aging Rate'])\n",
    "                result_list.append((-1)*auc)\n",
    "\n",
    "        return np.mean(result_list)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T02:45:30.367447Z",
     "start_time": "2021-10-11T02:45:28.210996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### training data ### \n",
    "training_month = [2, 3, 4]\n",
    "\n",
    "data_dict, trainset_x, trainset_y = multiple_month(training_month, num_set = 10, filename = 'dataset')\n",
    "\n",
    "print('\\nCombined training data:\\n')\n",
    "run_train = multiple_set(num_set = 10)\n",
    "run_train_x, run_train_y = train_set(run_train, num_set = 10)\n",
    "\n",
    "### testing data ###\n",
    "run_test = pd.read_csv('test_runhist.csv').iloc[:, 2:]\n",
    "run_test_x, run_test_y = label_divide(run_test, None, 'GB', train_only = True)\n",
    "print('\\n', 'Dimension of testing data:', run_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize the base learners by one-month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T05:28:50.376022Z",
     "start_time": "2021-10-11T02:45:48.164322Z"
    }
   },
   "outputs": [],
   "source": [
    "##### for training data transformation ##### \n",
    "base_param_trainC = optimize_base(num_set = 10, \n",
    "                                 train_data = data_dict, \n",
    "                                 mode = 'C', \n",
    "                                 TPE_multi = False, \n",
    "                                 base_list = ['XGBoost', 'LightGBM'],\n",
    "                                 iter_list = [200, 200],\n",
    "                                 filename = 'runhist_array_4criteria_m2m5')\n",
    "\n",
    "base_param_trainR = optimize_base(num_set = 10, \n",
    "                                 train_data = data_dict, \n",
    "                                 mode = 'R', \n",
    "                                 TPE_multi = False, \n",
    "                                 base_list = ['XGBoost', 'LightGBM'],\n",
    "                                 iter_list = [200, 200],\n",
    "                                 filename = 'runhist_array_4criteria_m2m5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T08:21:56.702928Z",
     "start_time": "2021-10-11T05:28:54.526257Z"
    }
   },
   "outputs": [],
   "source": [
    "##### for testing data transformation ##### \n",
    "base_param_testC = optimize_base(num_set = 10, \n",
    "                                train_data = {'all': run_train}, \n",
    "                                mode = 'C', \n",
    "                                TPE_multi = False, \n",
    "                                base_list = ['XGBoost', 'LightGBM'], \n",
    "                                iter_list = [200, 200],\n",
    "                                filename = 'runhist_array_4criteria_m2m5')\n",
    "\n",
    "base_param_testR = optimize_base(num_set = 10, \n",
    "                                train_data = {'all': run_train}, \n",
    "                                mode = 'R', \n",
    "                                TPE_multi = False, \n",
    "                                base_list = ['XGBoost', 'LightGBM'], \n",
    "                                iter_list = [200, 200],\n",
    "                                filename = 'runhist_array_4criteria_m2m5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transform for scheme 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transform for scheme 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T11:56:36.031421Z",
     "start_time": "2021-10-11T11:52:52.982601Z"
    }
   },
   "outputs": [],
   "source": [
    "train_firstC = transform_train(data_dict, num_set = 10, mode = 'C', base_param = base_param_trainC, cv = 5)\n",
    "test_firstC = transform_test(run_train, run_test, num_set = 10, mode = 'R', base_param = base_param_testC)\n",
    "train_firstC_x, train_firstC_y = train_set(train_firstC, num_set = 10)\n",
    "test_firstC_x, test_firstC_y = train_set(test_firstC, num_set = 10) \n",
    "\n",
    "train_firstR = transform_train(data_dict, num_set = 10, mode = 'R', base_param = base_param_trainR, cv = 5)\n",
    "test_firstR = transform_test(run_train, run_test, num_set = 10, mode = 'R', base_param = base_param_testR)\n",
    "train_firstR_x, train_firstR_y = train_set(train_firstR, num_set = 10)\n",
    "test_firstR_x, test_firstR_y = train_set(test_firstR, num_set = 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### searching for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T11:57:11.421501Z",
     "start_time": "2021-10-11T11:56:40.906749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_paramC, _ = all_optuna(num_set = 10, \n",
    "                            all_data = train_firstC, \n",
    "                            mode = 'C', \n",
    "                            TPE_multi = False, \n",
    "                            n_iter = 10,\n",
    "                            filename = f'runhist_array_4criteria_m2m5_StackingCV3',\n",
    "                            creator = stackingCV_creator\n",
    ")\n",
    "\n",
    "best_paramR, _ = all_optuna(num_set = 10, \n",
    "                            all_data = train_firstR, \n",
    "                            mode = 'R', \n",
    "                            TPE_multi = False, \n",
    "                            n_iter = 10,\n",
    "                            filename = f'runhist_array_4criteria_m2m5_StackingCV3',\n",
    "                            creator = stackingCV_creator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-10T08:02:23.243612Z",
     "start_time": "2021-10-10T08:02:23.114466Z"
    }
   },
   "outputs": [],
   "source": [
    "##### optimization history plot #####\n",
    "optuna_history(best_paramC, all_scoreC, num_row = 4, num_col = 3, model = 'StackingCV Classifier (scheme 3)')\n",
    "            \n",
    "##### best hyperparameter table #####\n",
    "param_table = pd.DataFrame(best_paramC).T\n",
    "param_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T11:57:52.875713Z",
     "start_time": "2021-10-11T11:57:52.299209Z"
    }
   },
   "outputs": [],
   "source": [
    "table_setC = runall_LR(10, train_firstC_x, test_firstC_x, train_firstC_y, test_firstC_y, best_paramC)\n",
    "line_chart(table_setC, title = 'StackingCV Classifier (scheme 3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T11:57:57.992172Z",
     "start_time": "2021-10-11T11:57:57.970027Z"
    }
   },
   "outputs": [],
   "source": [
    "table_setC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T12:00:46.288327Z",
     "start_time": "2021-10-11T12:00:45.699885Z"
    }
   },
   "outputs": [],
   "source": [
    "pr_dict, table_setR = runall_RidgeR(10, train_firstR_x, test_firstR_x, train_firstR_y, test_firstR_y, best_paramR,\n",
    "                                                                  thres_target = 'Recall', threshold = 0.8)\n",
    "line_chart(table_setR, title = 'StackingCV Regressor (scheme 3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T12:00:53.452144Z",
     "start_time": "2021-10-11T12:00:53.428208Z"
    }
   },
   "outputs": [],
   "source": [
    "table_setR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T12:00:37.874899Z",
     "start_time": "2021-10-11T12:00:37.664843Z"
    }
   },
   "outputs": [],
   "source": [
    "##### saving the output table for tableau #####\n",
    "table_setC['sampler'] = 'univariate-TPE'\n",
    "table_setR['sampler'] = 'univariate-TPE'\n",
    "table_setC['model'] = 'StackingCV 3'\n",
    "table_setR['model'] = 'StackingCV 3'\n",
    "\n",
    "\n",
    "# with pd.ExcelWriter('20211012_Classifier.xlsx') as writer:\n",
    "#     table_setC.to_excel(writer, sheet_name = 'StackingCV_3')\n",
    "\n",
    "# with pd.ExcelWriter('20211012_Regressor.xlsx') as writer:\n",
    "#     table_setR.to_excel(writer, sheet_name = 'StackingCV_3')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:aging]",
   "language": "python",
   "name": "conda-env-aging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
